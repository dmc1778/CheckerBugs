[{
    "Id": 17,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/4b45f08f8765549915417997c30ae8981f2ad125",
    "date": "2019-06-13T13:15:23-07:00",
    "message": "Added dim check for index_copy_ (#21617)\n\nSummary:\nFixing reported [bug](https://github.com/pytorch/pytorch/issues/20322)\nThe issue was related to not checking the dimensions of source vs destination tensors.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/21617\n\nDifferential Revision: D15749963\n\nPulled By: izdeby\n\nfbshipit-source-id: acff114c729fd9c0a9a51325e0ebd8b42e1f2fc1",
    "changes": [
        {
            "name": "Indexing.cpp",
            "path": "aten/src/ATen/native/Indexing.cpp",
            "patches": [
                {
                    "old_start": 272,
                    "old_length": 7,
                    "new_start": 272,
                    "new_length": 11,
                    "hunk_buggy": "['   int64_t numIndices = index.numel();\\n', '   if (source.dim() == 0 && numIndices != 1) {\\n', '     AT_INDEX_ERROR(\"index_copy_(): When source is scalar, index should have one element (got \", numIndices, \")\");\\n', '   }\\n', '   if (index.scalar_type() != ScalarType::Long) {\\n', '     AT_INDEX_ERROR(\"index_copy_(): Expected LongTensor for index\");\\n', '   }']",
                    "hunk_fix": "@@ -272,7 +272,11 @@ Tensor & index_copy_(Tensor & self, int64_t dim, const Tensor & index, const Ten\n   int64_t numIndices = index.numel();\n   if (source.dim() == 0 && numIndices != 1) {\n     AT_INDEX_ERROR(\"index_copy_(): When source is scalar, index should have one element (got \", numIndices, \")\");\n+  } else if ((source.dim() != self.dim()) && (source.dim() != 0 && self.dim() != 0)) {\n+    AT_INDEX_ERROR(\"index_copy_(): When source and destination are not scalars, their dimensionality must match. Source dimensionality (\",\n+                   source.dim(), \"), destination dimensionality (\", self.dim(), \")\");\n   }\n+\n   if (index.scalar_type() != ScalarType::Long) {\n     AT_INDEX_ERROR(\"index_copy_(): Expected LongTensor for index\");\n   }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 18,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/f3a2094065c8b4b7bae426e71c923a8a8abb74b5",
    "date": "2024-02-09T18:24:09+00:00",
    "message": "[Dynamo][Export] Mitigate legacy issue that aten op as export entrance function (#119528)\n\nThis is going to fix a legacy issue like:\n```\ntorch._dynamo.export(torch.ops.aten.scaled_dot_product_attention, ...)(*inputs,)\n```\nThis is not supported any more, now the top level ```torch.export``` only support ```nn.Module```, but there are still some tests using the internal APIs and caused the ```trace_rules.check``` assertion error. This PR is going to mitigate such cases.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/119528\nApproved by: https://github.com/ydwu4",
    "changes": [
        {
            "name": "eval_frame.py",
            "path": "torch/_dynamo/eval_frame.py",
            "patches": [
                {
                    "old_start": 1315,
                    "old_length": 6,
                    "new_start": 1315,
                    "new_length": 9,
                    "hunk_buggy": "['             not disable_constraint_solver\\n', '             and (shape_env := getattr(fake_mode, \"shape_env\", None)) is not None\\n', '             and (dim_constraints := shape_env.dim_constraints) is not None\\n', '             and not trace_rules.check(call_to_inspect)\\n', '         ):\\n', '             dim_constraints.solve()']",
                    "hunk_fix": "@@ -1315,6 +1315,9 @@ def export(\n             not disable_constraint_solver\n             and (shape_env := getattr(fake_mode, \"shape_env\", None)) is not None\n             and (dim_constraints := shape_env.dim_constraints) is not None\n+            and not isinstance(\n+                call_to_inspect, (torch._ops.OpOverloadPacket, torch._ops.OpOverload)\n+            )\n             and not trace_rules.check(call_to_inspect)\n         ):\n             dim_constraints.solve()"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 19,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/bdbd3ed312e0fc81e75302239ea78b3445fe95e7",
    "date": "2023-01-19T00:59:33+00:00",
    "message": "When nopython=True, Dynamo can't allow graph breaks. (#90970)\n\nI count the number of sub-graphs (for tiny-GPT2 in huggingface) by\n```\n    class GraphCaptureCompiler:\n        def __init__(self):\n            self.captured_graphs = []\n        def compile(self, gm, example_inputs):\n            self.captured_graphs.append(gm)\n            return gm\n    compiler = GraphCaptureCompiler()\n    torch._dynamo.optimize(compiler, nopython=True)(Wrapper(fn))(*args)\n```\n\nAlthough `len(compiler.captured_graphs)` is 2, no error was thrown during the compilation. This observation conflicts with `nopython=True`. After some digging, I found a check is missed before making graph break. This PR adds it.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/90970\nApproved by: https://github.com/ezyang, https://github.com/jansel, https://github.com/thiagocrepaldi",
    "changes": [
        {
            "name": "symbolic_convert.py",
            "path": "torch/_dynamo/symbolic_convert.py",
            "patches": [
                {
                    "old_start": 306,
                    "old_length": 7,
                    "new_start": 306,
                    "new_length": 7,
                    "hunk_buggy": "['             try:\\n', '                 return inner_fn(self, inst)\\n', '             except Unsupported as excp:\\n', '-                if self.has_backedge():\\n', '                     msg = \"Skipping frame because there is a graph break in a for/while loop\"\\n', '                     log.debug(msg)\\n', '                     raise exc.SkipFrame(msg) from excp']",
                    "hunk_fix": "@@ -306,7 +306,7 @@ def break_graph_if_unsupported(*, push):\n             try:\n                 return inner_fn(self, inst)\n             except Unsupported as excp:\n-                if self.has_backedge():\n+                if self.has_backedge() and self.should_compile_partial_graph():\n                     msg = \"Skipping frame because there is a graph break in a for/while loop\"\n                     log.debug(msg)\n                     raise exc.SkipFrame(msg) from excp"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 20,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/4ee179c9528c8c6aae17a01f2b0d7e8235219219",
    "date": "2023-09-16T00:07:19+00:00",
    "message": "Fix `ConstantVariable` init method if NumPy is missing (#109388)\n\nBy adding `np is not None` check before `isinstance(value, np.number)`\n\nPartially addresses https://github.com/pytorch/pytorch/issues/109387\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109388\nApproved by: https://github.com/ezyang",
    "changes": [
        {
            "name": "constant.py",
            "path": "torch/_dynamo/variables/constant.py",
            "patches": [
                {
                    "old_start": 33,
                    "old_length": 7,
                    "new_start": 33,
                    "new_length": 7,
                    "hunk_buggy": "['             for disallowed_type, reason in _type_to_assert_reason.items():\\n', '                 assert not isinstance(value, disallowed_type), reason\\n', ' \\n', '-        if isinstance(value, np.number):\\n', '             self.value = value.item()\\n', '         else:\\n', '             self.value = value']",
                    "hunk_fix": "@@ -33,7 +33,7 @@ class ConstantVariable(VariableTracker):\n             for disallowed_type, reason in _type_to_assert_reason.items():\n                 assert not isinstance(value, disallowed_type), reason\n \n-        if isinstance(value, np.number):\n+        if np is not None and isinstance(value, np.number):\n             self.value = value.item()\n         else:\n             self.value = value"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 21,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/2dafa70d61a1a5af849ab79c7aed4c84686337a0",
    "date": "2023-06-07T14:40:12+00:00",
    "message": "Add a little more error checking to minifier (#103057)\n\nPrompted by https://github.com/pytorch/pytorch/issues/101408\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/103057\nApproved by: https://github.com/bdhirsh",
    "changes": [
        {
            "name": "fx_minifier.py",
            "path": "torch/_functorch/fx_minifier.py",
            "patches": [
                {
                    "old_start": 135,
                    "old_length": 6,
                    "new_start": 135,
                    "new_length": 10,
                    "hunk_buggy": "['     graph: fx.Graph\\n', '     inps: List[torch.Tensor]\\n', ' \\n', ' def minifier(\\n', '     fail_f: fx.GraphModule, inps, module_fails, dump_state: Callable = dump_state, *,\\n', '     save_dir=None, offload_to_disk=False, skip_offload=False, skip_sanity=False,\\n']",
                    "hunk_fix": "@@ -135,6 +135,10 @@ class ReproState:\n     graph: fx.Graph\n     inps: List[torch.Tensor]\n \n+    def __post_init__(self):\n+        ph_nodes = get_placeholders(self.graph)\n+        assert len(ph_nodes) == len(self.inps)\n+\n def minifier(\n     fail_f: fx.GraphModule, inps, module_fails, dump_state: Callable = dump_state, *,\n     save_dir=None, offload_to_disk=False, skip_offload=False, skip_sanity=False,\n"
                },
                {
                    "old_start": 154,
                    "old_length": 6,
                    "new_start": 158,
                    "new_length": 8,
                    "hunk_buggy": "[' \\n', '     note: module_fails returns True if it fails.\\n', '     \"\"\"\\n', '     failing_graph = fail_f.graph\\n', '     cur_size = len(failing_graph.nodes)\\n', ' ']",
                    "hunk_fix": "@@ -154,6 +158,8 @@ def minifier(\n \n     note: module_fails returns True if it fails.\n     \"\"\"\n+    assert isinstance(inps, (tuple, list))\n+\n     failing_graph = fail_f.graph\n     cur_size = len(failing_graph.nodes)\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 22,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/43f810fa96a0d2c40387c8c84f710926d9ede3c1",
    "date": "2021-03-02T10:58:10-08:00",
    "message": "Add streams boundary check to `torch::cuda::scatter`` (#53057)\n\nSummary:\nAccessing elements of `std::vector` outside of its boundaries can lead to crashes/memory corruptions\n\nFixes https://github.com/pytorch/pytorch/issues/52526\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53057\n\nReviewed By: janeyx99\n\nDifferential Revision: D26736829\n\nPulled By: malfet\n\nfbshipit-source-id: 7aa13c53c8d062adfef082153809a7a724a74ee5",
    "changes": [
        {
            "name": "comm.cpp",
            "path": "torch/csrc/cuda/comm.cpp",
            "patches": [
                {
                    "old_start": 278,
                    "old_length": 7,
                    "new_start": 278,
                    "new_length": 7,
                    "hunk_buggy": "['       tensor.split_with_sizes(/*split_sizes=*/chunk_sizes, /*dim=*/dim);\\n', '   at::cuda::OptionalCUDAStreamGuard cuda_guard;\\n', '   for (size_t i = 0; i < chunks.size(); i++) {\\n', '-    if (streams && (*streams)[i]) {\\n', '       const auto device_index =\\n', '           static_cast<int16_t>(out_tensors[i].get_device());\\n', '       TORCH_CHECK(\\n']",
                    "hunk_fix": "@@ -278,7 +278,7 @@ std::vector<at::Tensor>& scatter_out(\n       tensor.split_with_sizes(/*split_sizes=*/chunk_sizes, /*dim=*/dim);\n   at::cuda::OptionalCUDAStreamGuard cuda_guard;\n   for (size_t i = 0; i < chunks.size(); i++) {\n-    if (streams && (*streams)[i]) {\n+    if (i < (streams ? streams->size() : 0U) && (*streams)[i]) {\n       const auto device_index =\n           static_cast<int16_t>(out_tensors[i].get_device());\n       TORCH_CHECK(\n"
                },
                {
                    "old_start": 328,
                    "old_length": 7,
                    "new_start": 328,
                    "new_length": 7,
                    "hunk_buggy": "['   for (size_t i = 0; i < chunks.size(); ++i) {\\n', '     const auto device_index = static_cast<int16_t>(devices[i]);\\n', '     if (device_index != tensor.get_device()) {\\n', '-      if (streams && (*streams)[i]) {\\n', '         TORCH_CHECK(\\n', '             (*streams)[i]->device_index() == device_index,\\n', '             \"Expected the device associated with the stream at index \",']",
                    "hunk_fix": "@@ -328,7 +328,7 @@ std::vector<at::Tensor> scatter(\n   for (size_t i = 0; i < chunks.size(); ++i) {\n     const auto device_index = static_cast<int16_t>(devices[i]);\n     if (device_index != tensor.get_device()) {\n-      if (streams && (*streams)[i]) {\n+      if (i < (streams ? streams->size() : 0U) && (*streams)[i]) {\n         TORCH_CHECK(\n             (*streams)[i]->device_index() == device_index,\n             \"Expected the device associated with the stream at index \","
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 23,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/40a7c317bc60713528320b9786765e4ec5707982",
    "date": "2021-06-24T18:44:41-07:00",
    "message": "Run BLAS F2C checks on host architecture (#60703)\n\nSummary:\nFixes https://github.com/pytorch/pytorch/issues/60351\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/60703\n\nReviewed By: driazati\n\nDifferential Revision: D29379727\n\nPulled By: malfet\n\nfbshipit-source-id: dadbb1d39373887f07d59d0a05e093a5d070b016",
    "changes": [
        {
            "name": "FindBLAS.cmake",
            "path": "cmake/Modules/FindBLAS.cmake",
            "patches": [
                {
                    "old_start": 292,
                    "old_length": 6,
                    "new_start": 292,
                    "new_length": 12,
                    "hunk_buggy": "[' \\n', ' # Determine if blas was compiled with the f2c conventions\\n', ' IF (BLAS_LIBRARIES)\\n', '   SET(CMAKE_REQUIRED_LIBRARIES ${BLAS_LIBRARIES})\\n', '   CHECK_C_SOURCE_RUNS(\"\\n', ' #include <stdlib.h>\\n']",
                    "hunk_fix": "@@ -292,6 +292,12 @@ endif()\n \n # Determine if blas was compiled with the f2c conventions\n IF (BLAS_LIBRARIES)\n+   # Push host architecture when cross-compiling otherwise check would fail\n+   # when cross-compiling for arm64 on x86_64\n+   cmake_push_check_state(RESET)\n+  if(CMAKE_SYSTEM_NAME STREQUAL \"Darwin\" AND CMAKE_OSX_ARCHITECTURES MATCHES \"^(x86_64|arm64)$\")\n+    list(APPEND CMAKE_REQUIRED_FLAGS \"-arch ${CMAKE_HOST_SYSTEM_PROCESSOR}\")\n+  endif()\n   SET(CMAKE_REQUIRED_LIBRARIES ${BLAS_LIBRARIES})\n   CHECK_C_SOURCE_RUNS(\"\n #include <stdlib.h>\n"
                },
                {
                    "old_start": 342,
                    "old_length": 6,
                    "new_start": 348,
                    "new_length": 7,
                    "hunk_buggy": "['     SET(BLAS_USE_CBLAS_DOT FALSE)\\n', '   ENDIF(BLAS_USE_CBLAS_DOT)\\n', '   SET(CMAKE_REQUIRED_LIBRARIES)\\n', ' ENDIF(BLAS_LIBRARIES)\\n', ' \\n', ' # epilogue']",
                    "hunk_fix": "@@ -342,6 +348,7 @@ int main() {\n     SET(BLAS_USE_CBLAS_DOT FALSE)\n   ENDIF(BLAS_USE_CBLAS_DOT)\n   SET(CMAKE_REQUIRED_LIBRARIES)\n+  cmake_pop_check_state()\n ENDIF(BLAS_LIBRARIES)\n \n # epilogue"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 24,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/324dc1623e2f91892038fb1b151450a7c6529dd9",
    "date": "2020-05-10T23:15:45-07:00",
    "message": "add dtype checking for gather and scatter (#38025)\n\nSummary:\nFixed https://github.com/pytorch/pytorch/issues/37996\n\nin the `cpu_scatter_gather_base_kernel`, it interpret a pointer as `int64_t` regardless the actual dtype.\nhttps://github.com/pytorch/pytorch/blob/2b41b9bceb01851df83d40c1280b3d3b09e1395b/aten/src/ATen/native/cpu/ScatterGatherKernel.cpp#L106\nadd a index dtype checking will avoid the nasty index out of bound error. As using `int64_t` is convention in ATen code (a.k.a, a limitation), no further fix is needed at the moment.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/38025\n\nDifferential Revision: D21498146\n\nPulled By: ezyang\n\nfbshipit-source-id: b1f96f394a460c4bc63d21ec8d4a2cfbf3e97b03",
    "changes": [
        {
            "name": "TensorAdvancedIndexing.cpp",
            "path": "aten/src/ATen/native/TensorAdvancedIndexing.cpp",
            "patches": [
                {
                    "old_start": 518,
                    "old_length": 40,
                    "new_start": 518,
                    "new_length": 48,
                    "hunk_buggy": "[' }\\n', ' \\n', ' Tensor & gather_out_cpu(Tensor & result, const Tensor & self, int64_t dim, const Tensor & index, bool sparse_grad) {\\n', '   result.resize_(index.sizes());\\n', '   gather_stub(result.device().type(), result, self, dim, index);\\n', '   return result;\\n', ' }\\n', ' \\n', ' Tensor gather_cpu(const Tensor & self, int64_t dim, const Tensor & index, bool sparse_grad) {\\n', '   Tensor result = at::empty({0}, self.options());\\n', '   return gather_out_cpu(result, self, dim, index, sparse_grad);\\n', ' }\\n', ' \\n', ' Tensor & scatter_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & src) {\\n', '   scatter_stub(self.device().type(), self, dim, index, src);\\n', '   return self;\\n', ' }\\n', ' \\n', ' Tensor & scatter_fill_(Tensor & self, int64_t dim, const Tensor & index, Scalar src) {\\n', '   scatter_fill_stub(self.device().type(), self, dim, index, src);\\n', '   return self;\\n', ' }\\n', ' \\n', ' Tensor scatter(const Tensor & self, int64_t dim, const Tensor & index, const Tensor & source) {\\n', '   return self.clone(at::MemoryFormat::Preserve).scatter_(dim, index, source);\\n', ' }\\n', ' \\n', ' Tensor scatter(const Tensor & self, int64_t dim, const Tensor & index, Scalar source) {\\n', '   return self.clone(at::MemoryFormat::Preserve).scatter_(dim, index, source);\\n', ' }\\n', ' \\n', ' Tensor & scatter_add_cpu_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & src) {\\n', '   scatter_add_stub(self.device().type(), self, dim, index, src);\\n', '   return self;\\n', ' }\\n', ' \\n', ' Tensor scatter_add(const Tensor & self, int64_t dim, const Tensor & index, const Tensor & source) {\\n', '   return self.clone(at::MemoryFormat::Preserve).scatter_add_(dim, index, source);\\n', ' }\\n', ' ']",
                    "hunk_fix": "@@ -518,40 +518,48 @@ Tensor index_fill(const Tensor & self, int64_t dim, const Tensor & index, const\n }\n \n Tensor & gather_out_cpu(Tensor & result, const Tensor & self, int64_t dim, const Tensor & index, bool sparse_grad) {\n+  TORCH_CHECK_INDEX(index.scalar_type() == ScalarType::Long, \"gather_out(): Expected dtype int64 for index\");\n   result.resize_(index.sizes());\n   gather_stub(result.device().type(), result, self, dim, index);\n   return result;\n }\n \n Tensor gather_cpu(const Tensor & self, int64_t dim, const Tensor & index, bool sparse_grad) {\n+  TORCH_CHECK_INDEX(index.scalar_type() == ScalarType::Long, \"gather(): Expected dtype int64 for index\");\n   Tensor result = at::empty({0}, self.options());\n   return gather_out_cpu(result, self, dim, index, sparse_grad);\n }\n \n Tensor & scatter_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & src) {\n+  TORCH_CHECK_INDEX(index.scalar_type() == ScalarType::Long, \"scatter_(): Expected dtype int64 for index\");\n   scatter_stub(self.device().type(), self, dim, index, src);\n   return self;\n }\n \n Tensor & scatter_fill_(Tensor & self, int64_t dim, const Tensor & index, Scalar src) {\n+  TORCH_CHECK_INDEX(index.scalar_type() == ScalarType::Long, \"scatter_fill_(): Expected dtype int64 for index\");\n   scatter_fill_stub(self.device().type(), self, dim, index, src);\n   return self;\n }\n \n Tensor scatter(const Tensor & self, int64_t dim, const Tensor & index, const Tensor & source) {\n+  TORCH_CHECK_INDEX(index.scalar_type() == ScalarType::Long, \"scatter(): Expected dtype int64 for index\");\n   return self.clone(at::MemoryFormat::Preserve).scatter_(dim, index, source);\n }\n \n Tensor scatter(const Tensor & self, int64_t dim, const Tensor & index, Scalar source) {\n+  TORCH_CHECK_INDEX(index.scalar_type() == ScalarType::Long, \"scatter(): Expected dtype int64 for index\");\n   return self.clone(at::MemoryFormat::Preserve).scatter_(dim, index, source);\n }\n \n Tensor & scatter_add_cpu_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & src) {\n+  TORCH_CHECK_INDEX(index.scalar_type() == ScalarType::Long, \"scatter_add_(): Expected dtype int64 for index\");\n   scatter_add_stub(self.device().type(), self, dim, index, src);\n   return self;\n }\n \n Tensor scatter_add(const Tensor & self, int64_t dim, const Tensor & index, const Tensor & source) {\n+  TORCH_CHECK_INDEX(index.scalar_type() == ScalarType::Long, \"scatter_add(): Expected dtype int64 for index\");\n   return self.clone(at::MemoryFormat::Preserve).scatter_add_(dim, index, source);\n }\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 25,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/ba59d720cd5c5c81601b53d2c3397c46c1f87883",
    "date": "2019-10-21T13:03:02-07:00",
    "message": "Change error message for torch.linspace(). (#28274)\n\nSummary:\nFix for https://github.com/pytorch/pytorch/issues/25810\n\nBasically moves the error checking from the device-specific function to the native function.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/28274\n\nDifferential Revision: D18032189\n\nPulled By: ezyang\n\nfbshipit-source-id: 9072b5980aa2057274e79bc7241db853bfc36f11",
    "changes": [
        {
            "name": "TensorFactories.cpp",
            "path": "aten/src/ATen/native/TensorFactories.cpp",
            "patches": [
                {
                    "old_start": 337,
                    "old_length": 6,
                    "new_start": 337,
                    "new_length": 7,
                    "hunk_buggy": "['     Scalar end,\\n', '     int64_t steps,\\n', '     const TensorOptions& options) {\\n', '   Tensor result = at::empty({steps}, options);\\n', '   return at::linspace_out(result, start, end, steps);\\n', ' }']",
                    "hunk_fix": "@@ -337,6 +337,7 @@ Tensor linspace(\n     Scalar end,\n     int64_t steps,\n     const TensorOptions& options) {\n+  TORCH_CHECK(steps >= 0, \"number of steps must be non-negative\");\n   Tensor result = at::empty({steps}, options);\n   return at::linspace_out(result, start, end, steps);\n }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 26,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/2c9dd886afc656a8bfe5d8bbcb601ee5877cee21",
    "date": "2021-12-14T09:55:59-08:00",
    "message": "Modify torch.movedim to handle scalar as no-op (#69537)\n\nSummary:\n`torch.movedim` directly handle the case of a scalar tensor (0-dim) in input as a no-op by returning a view of the input tensor (after all the usual checks for the other parameters)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69537\n\nTest Plan:\nThis code now works fine and res1 is a view of tensor\n```\nimport torch\n\ntensor = torch.rand(torch.Size([]))\nres1 = torch.movedim(tensor, 0, 0)\n```\n\nFixes https://github.com/pytorch/pytorch/issues/69432\n\nReviewed By: jbschlosser\n\nDifferential Revision: D33020014\n\nPulled By: albanD\n\nfbshipit-source-id: b3b2d380d70158bd3b3d6b40c073377104e09007",
    "changes": [
        {
            "name": "TensorShape.cpp",
            "path": "aten/src/ATen/native/TensorShape.cpp",
            "patches": [
                {
                    "old_start": 2447,
                    "old_length": 6,
                    "new_start": 2447,
                    "new_length": 10,
                    "hunk_buggy": "['   TORCH_CHECK(all_unique(normalized_src), \"movedim: repeated dim in `source` (\", src, \")\");\\n', '   TORCH_CHECK(all_unique(normalized_dst), \"movedim: repeated dim in `destination` (\", dst, \")\");\\n', ' \\n', '   // TODO: The algorithm below can probably be optimized.\\n', '   // Reference: https://github.com/pytorch/pytorch/pull/41480#discussion_r456100505\\n', ' ']",
                    "hunk_fix": "@@ -2447,6 +2447,10 @@ Tensor movedim(const Tensor& self, IntArrayRef src, IntArrayRef dst) {\n   TORCH_CHECK(all_unique(normalized_src), \"movedim: repeated dim in `source` (\", src, \")\");\n   TORCH_CHECK(all_unique(normalized_dst), \"movedim: repeated dim in `destination` (\", dst, \")\");\n \n+  // handle the case of scalar tensor as a no-op\n+  if (self_dim == 0)\n+    return self.alias();\n+\n   // TODO: The algorithm below can probably be optimized.\n   // Reference: https://github.com/pytorch/pytorch/pull/41480#discussion_r456100505\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 27,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/48a49b2683ffa21eb1b472e503c129c043c18f87",
    "date": "2023-07-10T19:00:47+00:00",
    "message": "use more informative error message for ConstandPad2d/3d (#104762)\n\nFixes #104508\n\nAs discussed in #104508, the current error message for `torch.nn.ConstantPad2d` and `torch.nn.ConstantPad3d` is misleading, this PR fixes the problem.\nThe fixed error message is shown below:\nFor `torch.nn.ConstantPad2d`:\n<img width=\"619\" alt=\"image\" src=\"https://github.com/pytorch/pytorch/assets/6964699/dd15f42a-b6ad-4c6d-aa41-f26d08144189\">\nFor `torch.nn.ConstantPad3d`:\n<img width=\"630\" alt=\"image\" src=\"https://github.com/pytorch/pytorch/assets/6964699/ac99b80f-73c1-4d7f-b9a1-74bf45ee4c21\">\n\ncc:\n@mikaylagawarecki Please help me check this PR, thanks!\nPull Request resolved: https://github.com/pytorch/pytorch/pull/104762\nApproved by: https://github.com/mikaylagawarecki",
    "changes": [
        {
            "name": "PadNd.cpp",
            "path": "aten/src/ATen/native/PadNd.cpp",
            "patches": [
                {
                    "old_start": 178,
                    "old_length": 7,
                    "new_start": 178,
                    "new_length": 8,
                    "hunk_buggy": "[' Tensor _pad_enum_symint(const Tensor &self, c10::SymIntArrayRef pad, int64_t mode_int, c10::optional<double> value) {\\n', '   const auto input_dim = self.dim();\\n', '   TORCH_CHECK(pad.size() % 2 == 0, \"Padding length must be divisible by 2\");\\n', '-  TORCH_CHECK(static_cast<int64_t>(pad.size()) <= input_dim * 2, \"Padding length too large\");\\n', '   auto mode = static_cast<at::padding_mode>(mode_int);\\n', ' \\n', '   if (mode == at::padding_mode::constant) {']",
                    "hunk_fix": "@@ -178,7 +178,8 @@ Tensor _pad_circular_symint(const Tensor &self, c10::SymIntArrayRef padding) {\n Tensor _pad_enum_symint(const Tensor &self, c10::SymIntArrayRef pad, int64_t mode_int, c10::optional<double> value) {\n   const auto input_dim = self.dim();\n   TORCH_CHECK(pad.size() % 2 == 0, \"Padding length must be divisible by 2\");\n-  TORCH_CHECK(static_cast<int64_t>(pad.size()) <= input_dim * 2, \"Padding length too large\");\n+  TORCH_CHECK(static_cast<int64_t>(pad.size()) <= input_dim * 2,\n+              \"Padding length should be less than or equal to two times the input dimension but got padding length \", pad.size(), \" and input of dimension \", input_dim);\n   auto mode = static_cast<at::padding_mode>(mode_int);\n \n   if (mode == at::padding_mode::constant) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 28,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/999bae0f54108ffc5b7cf2524a02a83901554b16",
    "date": "2023-05-30T05:07:59+00:00",
    "message": "Add padding check for use_nnpack (#92238)\n\nFixes #90142\nnnp_convolution_output doesn't support the case of input padding > = kernel_size.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/92238\nApproved by: https://github.com/jgong5, https://github.com/ganler",
    "changes": [
        {
            "name": "Convolution.cpp",
            "path": "aten/src/ATen/native/Convolution.cpp",
            "patches": [
                {
                    "old_start": 540,
                    "old_length": 7,
                    "new_start": 540,
                    "new_length": 8,
                    "hunk_buggy": "['            !transposed &&   // or transposed tensors\\n', '            input.ndimension() == 4 && // must be in NCHW format\\n', '            weight.ndimension() == 4 &&\\n', '-           (at::symint::size<T>(weight, 2) < 17) && (at::symint::size<T>(weight, 3) < 17) // NNPACK only supports kernels up to 16x16\\n', ' #if !defined(C10_MOBILE)\\n', '            && at::symint::size<T>(input, 0) >= 16 // ensure large enough batch size to ensure perf, tuneable\\n', ' #endif']",
                    "hunk_fix": "@@ -540,7 +540,8 @@ struct ConvParams {\n            !transposed &&   // or transposed tensors\n            input.ndimension() == 4 && // must be in NCHW format\n            weight.ndimension() == 4 &&\n-           (at::symint::size<T>(weight, 2) < 17) && (at::symint::size<T>(weight, 3) < 17) // NNPACK only supports kernels up to 16x16\n+           (at::symint::size<T>(weight, 2) < 17) && (at::symint::size<T>(weight, 3) < 17) && // NNPACK only supports kernels up to 16x16\n+           (padding[0] < at::symint::size<T>(weight, 2)) && (padding[1] < at::symint::size<T>(weight, 3)) // NNPACK only supports padding < kernel_size. See https://github.com/pytorch/pytorch/issues/90142.\n #if !defined(C10_MOBILE)\n            && at::symint::size<T>(input, 0) >= 16 // ensure large enough batch size to ensure perf, tuneable\n #endif"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 29,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/1becd2c314f45bded8d3fbec91d785e7190b4afe",
    "date": "2023-12-12T22:20:20+00:00",
    "message": "Align checks in `_use_cudnn_ctc_loss` with those in `_cudnn_ctc_loss` (#115617)\n\nThis PR is intended to fix the following problem:\n\nWhen using `CTCLoss`, there is a cudnn path gated by a call to [`_use_cudnn_ctc_loss`](\nhttps://github.com/pytorch/pytorch/blob/e91846137795e2d976b9e0ba2e1d886f34fcfb7a/aten/src/ATen/native/cudnn/LossCTC.cpp#L73-L101) which checks some conditions\n\nhttps://github.com/pytorch/pytorch/blob/e91846137795e2d976b9e0ba2e1d886f34fcfb7a/aten/src/ATen/native/LossCTC.cpp#L486-L496\n\nHowever, there are more checks in `_cudnn_ctc_loss`\nhttps://github.com/pytorch/pytorch/blob/e91846137795e2d976b9e0ba2e1d886f34fcfb7a/aten/src/ATen/native/cudnn/LossCTC.cpp#L122-L130\n\nsome of which are not present in `_use_cudnn_ctc_loss` (e.g. the check that `targets` is on CPU which will cause a RuntimeError after dispatching to `_cudnn_ctc_loss`). Instead, these checks should be in `_use_cudnn_ctc_loss` so that the normal `_ctc_loss` path will be used if the checks are not met)\n\ne.g. Before this PR\n\n```python\n>>> import torch\n>>> ctcloss = torch.nn.CTCLoss()\n>>> log_probs = torch.randn((50, 3, 15), device='cuda').log_softmax(2)\n>>> target = torch.randint(1, 15, (30 + 25 + 20,), dtype = torch.int)\n>>> input_lengths = torch.tensor((50, 50, 50), device='cuda')\n>>> target_lengths = torch.tensor((30, 25, 20), device='cuda')\n>>> ctcloss(log_probs, target, input_lengths, target_lengths)\ntensor(4.1172, device='cuda:0')\n>>> target = target.to('cuda')\n>>> ctcloss(log_probs, target, input_lengths, target_lengths)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/data/users/mg1998/pytorch/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/data/users/mg1998/pytorch/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/data/users/mg1998/pytorch/torch/nn/modules/loss.py\", line 1779, in forward\n    return F.ctc_loss(log_probs, targets, input_lengths, target_lengths, self.blank, self.reduction,\n  File \"/data/users/mg1998/pytorch/torch/nn/functional.py\", line 2660, in ctc_loss\n    return torch.ctc_loss(\nRuntimeError: Expected tensor to have CPU Backend, but got tensor with CUDA Backend (while checking arguments for cudnn_ctc_loss)\n```\n\nAfter this PR the above snippet runs without error.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/115617\nApproved by: https://github.com/janeyx99",
    "changes": [
        {
            "name": "LossCTC.cpp",
            "path": "aten/src/ATen/native/cudnn/LossCTC.cpp",
            "patches": [
                {
                    "old_start": 81,
                    "old_length": 7,
                    "new_start": 81,
                    "new_length": 10,
                    "hunk_buggy": "['   bool use_cudnn = ctx.userEnabledCuDNN() && (BLANK == 0) &&\\n', '       (targets.dim() == 1) && (log_probs.scalar_type() == at::kFloat) &&\\n', '       (targets.scalar_type() == at::kInt) &&\\n', '-      (log_probs.device().type() == at::kCUDA);\\n', ' \\n', '   if (use_cudnn) {\\n', \"     // we don't know that input_lengths and target_lengths have the same size\"]",
                    "hunk_fix": "@@ -81,7 +81,10 @@ bool _use_cudnn_ctc_loss(\n   bool use_cudnn = ctx.userEnabledCuDNN() && (BLANK == 0) &&\n       (targets.dim() == 1) && (log_probs.scalar_type() == at::kFloat) &&\n       (targets.scalar_type() == at::kInt) &&\n-      (log_probs.device().type() == at::kCUDA);\n+      (log_probs.device().type() == at::kCUDA) &&\n+      (targets.device().type() == at::kCPU) &&\n+      (targets.is_contiguous()) &&\n+      (log_probs.dim() == 3);\n \n   if (use_cudnn) {\n     // we don't know that input_lengths and target_lengths have the same size"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 30,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/8f26d6aabcad991da88b663467ee2080a38631f7",
    "date": "2017-10-13T16:56:19+02:00",
    "message": "More shape checking for ConvNd (#3052)\n\n* check conv weight & bias dims\r\n\r\n* address comments",
    "changes": [
        {
            "name": "convolution.cpp",
            "path": "torch/csrc/autograd/functions/convolution.cpp",
            "patches": [
                {
                    "old_start": 156,
                    "old_length": 26,
                    "new_start": 156,
                    "new_length": 57,
                    "hunk_buggy": "[' }\\n', ' \\n', ' static void check_input_shape_forward(const at::Tensor& input,\\n', '-\\t\\t\\t\\t      const at::Tensor& weight,\\n', ' \\t\\t\\t\\t      int64_t groups, bool transposed) {\\n', '   if (!transposed) {\\n', '     if (input.size(1) != (weight.size(1) * groups)) {\\n', '       std::stringstream ss;\\n', '       ss << \"Given groups=\" << groups << \", weight\" << weight.sizes()\\n', '-\\t << \", so expected input\" << input.sizes() << \"  to have \"\\n', ' \\t << (weight.size(1) * groups) << \" channels, but got \" << input.size(1)\\n', ' \\t << \" channels instead\";\\n', '       throw std::runtime_error(ss.str());\\n', '     }\\n', '   } else { // transposed\\n', '     if (input.size(1) != weight.size(0)) {\\n', '       std::stringstream ss;\\n', '       ss << \"Given transposed=\" << transposed << \", weight\" << weight.sizes()\\n', '-\\t << \", so expected input\" << input.sizes() << \"  to have \"\\n', ' \\t << weight.size(0) << \" channels, but got \" << input.size(1)\\n', ' \\t << \" channels instead\";\\n', '       throw std::runtime_error(ss.str());\\n', '     }\\n', '   }\\n', ' }\\n', ' \\n']",
                    "hunk_fix": "@@ -156,26 +156,57 @@ static auto view3d(const at::Tensor& tensor) -> at::Tensor {\n }\n \n static void check_input_shape_forward(const at::Tensor& input,\n-\t\t\t\t      const at::Tensor& weight,\n+\t\t\t\t      const at::Tensor& weight, const at::Tensor& bias,\n \t\t\t\t      int64_t groups, bool transposed) {\n+  int k = input.ndimension();\n+\n+  if (weight.ndimension() != k) {\n+      std::stringstream ss;\n+      ss << \"Expected \" << k << \"-dimensional input for \" << k\n+         << \"-dimensional weight \" << weight.sizes() << \", but got input of size \"\n+         << input.sizes() << \" instead\";\n+      throw std::runtime_error(ss.str());\n+  }\n+  if (weight.size(0) < groups) {\n+    std::stringstream ss;\n+    ss << \"Given groups=\" << groups << \", expected weight to be at least \"\n+       << groups << \" at dimension 0, but got weight of size \" << weight.sizes()\n+       << \" instead\";\n+    throw std::runtime_error(ss.str());\n+  }\n+\n   if (!transposed) {\n     if (input.size(1) != (weight.size(1) * groups)) {\n       std::stringstream ss;\n       ss << \"Given groups=\" << groups << \", weight\" << weight.sizes()\n-\t << \", so expected input\" << input.sizes() << \"  to have \"\n+\t << \", so expected input\" << input.sizes() << \" to have \"\n \t << (weight.size(1) * groups) << \" channels, but got \" << input.size(1)\n \t << \" channels instead\";\n       throw std::runtime_error(ss.str());\n     }\n+    if (bias.defined() && (bias.ndimension() != 1 || bias.size(0) != weight.size(0))) {\n+      std::stringstream ss;\n+      ss << \"Given weight of size \" << weight.sizes()\n+         << \", expected bias to be 1-dimensional with \" << weight.size(0) << \" elements\"\n+         << \", but got bias of size \" << bias.sizes() << \" instead\";\n+      throw std::runtime_error(ss.str());\n+    }\n   } else { // transposed\n     if (input.size(1) != weight.size(0)) {\n       std::stringstream ss;\n       ss << \"Given transposed=\" << transposed << \", weight\" << weight.sizes()\n-\t << \", so expected input\" << input.sizes() << \"  to have \"\n+\t << \", so expected input\" << input.sizes() << \" to have \"\n \t << weight.size(0) << \" channels, but got \" << input.size(1)\n \t << \" channels instead\";\n       throw std::runtime_error(ss.str());\n     }\n+    if (bias.defined() && (bias.ndimension() != 1 || bias.size(0) != weight.size(1) * groups)) {\n+      std::stringstream ss;\n+      ss << \"Given transposed=\" << transposed << \", weight of size \" << weight.sizes()\n+         << \", expected bias to be 1-dimensional with \" << weight.size(1) * groups << \" elements\"\n+         << \", but got bias of size \" << bias.sizes() << \" instead\";\n+      throw std::runtime_error(ss.str());\n+    }\n   }\n }\n \n"
                },
                {
                    "old_start": 218,
                    "old_length": 9,
                    "new_start": 249,
                    "new_length": 10,
                    "hunk_buggy": "['   auto weight = inputs[1].data();\\n', '   auto bias = inputs[2].opt_data();\\n', ' \\n', '-  check_input_shape_forward(input, weight, groups, transposed);\\n', ' \\n', '   int k = input.ndimension();\\n', '   if (k == 3) {\\n', '     view1d_as_2d();\\n', '     input = view4d(input);']",
                    "hunk_fix": "@@ -218,9 +249,10 @@ auto ConvForward::apply(const variable_list& inputs) -> variable_list {\n   auto weight = inputs[1].data();\n   auto bias = inputs[2].opt_data();\n \n-  check_input_shape_forward(input, weight, groups, transposed);\n+  check_input_shape_forward(input, weight, bias, groups, transposed);\n \n   int k = input.ndimension();\n+\n   if (k == 3) {\n     view1d_as_2d();\n     input = view4d(input);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 31,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/cf732053e4f6b93b0a93006613552cd97f415b80",
    "date": "2023-03-17T02:01:43+00:00",
    "message": "nn.EmbeddingBag bound check (#96022)\n\nSummary: Today if we're accessing out of bound embedding rows, it'll either go through or throw IMA. This is not ideal - adding bound checks. This will probably slow things down - need to benchmark it.\n\nTest Plan:\nTODO: add some tests\n\nTried a simple example and it's showing this:\n```\naten/src/ATen/native/cuda/EmbeddingBag.cu:143: EmbeddingBag_updateOutputKernel_sum_mean: block: [0,0,0], thread: [0,1,0] Assertion `input[emb] < numRows` failed.\n```\n\nDifferential Revision: D43810777\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/96022\nApproved by: https://github.com/cpuhrsch, https://github.com/ngimel",
    "changes": [
        {
            "name": "EmbeddingBag.cu",
            "path": "aten/src/ATen/native/cuda/EmbeddingBag.cu",
            "patches": [
                {
                    "old_start": 67,
                    "old_length": 7,
                    "new_start": 67,
                    "new_length": 7,
                    "hunk_buggy": "['     index_t *offset2bag, int64_t numIndices, int64_t numBags,\\n', '     int64_t featureSize, int64_t weight_stride0, int64_t weight_stride1,\\n', '     index_t *bag_size, index_t *max_indices,\\n', '-    index_t padding_idx) {\\n', ' \\n', '   // the strategy here is that each bag x feature is handled by a single thread\\n', ' \\n']",
                    "hunk_fix": "@@ -67,7 +67,7 @@ __global__ void EmbeddingBag_updateOutputKernel_max(\n     index_t *offset2bag, int64_t numIndices, int64_t numBags,\n     int64_t featureSize, int64_t weight_stride0, int64_t weight_stride1,\n     index_t *bag_size, index_t *max_indices,\n-    index_t padding_idx) {\n+    index_t padding_idx, int64_t numRows) {\n \n   // the strategy here is that each bag x feature is handled by a single thread\n \n"
                },
                {
                    "old_start": 89,
                    "old_length": 6,
                    "new_start": 89,
                    "new_length": 7,
                    "hunk_buggy": "['       int64_t maxWord = -1;\\n', '       for (int64_t emb = begin; emb < end; emb++) {\\n', '         bool pad = (input[emb] == padding_idx);\\n', '         const int64_t weightRow = input[emb] * weight_stride0;\\n', '         scalar_t weightValue = weightFeat[weightRow];\\n', '         if (bag_size_ == 0 || weightValue > weightFeatMax) {\\n']",
                    "hunk_fix": "@@ -89,6 +89,7 @@ __global__ void EmbeddingBag_updateOutputKernel_max(\n       int64_t maxWord = -1;\n       for (int64_t emb = begin; emb < end; emb++) {\n         bool pad = (input[emb] == padding_idx);\n+        CUDA_KERNEL_ASSERT(input[emb] < numRows);\n         const int64_t weightRow = input[emb] * weight_stride0;\n         scalar_t weightValue = weightFeat[weightRow];\n         if (bag_size_ == 0 || weightValue > weightFeatMax) {\n"
                },
                {
                    "old_start": 117,
                    "old_length": 7,
                    "new_start": 118,
                    "new_length": 7,
                    "hunk_buggy": "['     int64_t featureSize, int64_t weight_stride0, int64_t weight_stride1,\\n', '     int mode, index_t *bag_size,\\n', '     scalar_t* per_sample_weights, int64_t per_sample_weights_stride,\\n', '-    index_t padding_idx) {\\n', ' \\n', '   // the strategy here is that each bag x feature is handled by a single thread\\n', ' \\n']",
                    "hunk_fix": "@@ -117,7 +118,7 @@ __global__ void EmbeddingBag_updateOutputKernel_sum_mean(\n     int64_t featureSize, int64_t weight_stride0, int64_t weight_stride1,\n     int mode, index_t *bag_size,\n     scalar_t* per_sample_weights, int64_t per_sample_weights_stride,\n-    index_t padding_idx) {\n+    index_t padding_idx, int64_t numRows) {\n \n   // the strategy here is that each bag x feature is handled by a single thread\n \n"
                },
                {
                    "old_start": 139,
                    "old_length": 6,
                    "new_start": 140,
                    "new_length": 7,
                    "hunk_buggy": "['       int64_t bag_size_ = 0;\\n', '       for (int64_t emb = begin; emb < end; emb++) {\\n', '         bool pad = (input[emb] == padding_idx);\\n', '         const int64_t weightRow = input[emb] * weight_stride0;\\n', '         scalar_t weightValue = weightFeat[weightRow];\\n', '         weightValue = pad ? static_cast<scalar_t>(0) : weightValue;\\n']",
                    "hunk_fix": "@@ -139,6 +140,7 @@ __global__ void EmbeddingBag_updateOutputKernel_sum_mean(\n       int64_t bag_size_ = 0;\n       for (int64_t emb = begin; emb < end; emb++) {\n         bool pad = (input[emb] == padding_idx);\n+        CUDA_KERNEL_ASSERT(input[emb] < numRows);\n         const int64_t weightRow = input[emb] * weight_stride0;\n         scalar_t weightValue = weightFeat[weightRow];\n         weightValue = pad ? static_cast<scalar_t>(0) : weightValue;\n"
                },
                {
                    "old_start": 406,
                    "old_length": 7,
                    "new_start": 408,
                    "new_length": 7,
                    "hunk_buggy": "['             offset2bag.data_ptr<index_t>(), numIndices, numBags, featureSize,\\n', '             weight.stride(0), weight.stride(1), bag_size.data_ptr<index_t>(),\\n', '             max_indices.data_ptr<index_t>(),\\n', '-            padding_idx);\\n', '         C10_CUDA_KERNEL_LAUNCH_CHECK();\\n', '       } else {\\n', '         EmbeddingBag_updateOutputKernel_sum_mean<scalar_t, index_t><<<grid, block, 0, stream>>>(\\n']",
                    "hunk_fix": "@@ -406,7 +408,7 @@ _embedding_bag_cuda(const Tensor &weight, const Tensor &indices_,\n             offset2bag.data_ptr<index_t>(), numIndices, numBags, featureSize,\n             weight.stride(0), weight.stride(1), bag_size.data_ptr<index_t>(),\n             max_indices.data_ptr<index_t>(),\n-            padding_idx);\n+            padding_idx, weight.size(0));\n         C10_CUDA_KERNEL_LAUNCH_CHECK();\n       } else {\n         EmbeddingBag_updateOutputKernel_sum_mean<scalar_t, index_t><<<grid, block, 0, stream>>>(\n"
                },
                {
                    "old_start": 416,
                    "old_length": 7,
                    "new_start": 418,
                    "new_length": 7,
                    "hunk_buggy": "['             weight.stride(0), weight.stride(1), mode, bag_size.data_ptr<index_t>(),\\n', '             per_sample_weights.defined() ? per_sample_weights.data_ptr<scalar_t>() : NULL,\\n', '             per_sample_weights.defined() ? per_sample_weights.stride(0) : 0,\\n', '-            padding_idx);\\n', '         C10_CUDA_KERNEL_LAUNCH_CHECK();\\n', '       }\\n', '     });']",
                    "hunk_fix": "@@ -416,7 +418,7 @@ _embedding_bag_cuda(const Tensor &weight, const Tensor &indices_,\n             weight.stride(0), weight.stride(1), mode, bag_size.data_ptr<index_t>(),\n             per_sample_weights.defined() ? per_sample_weights.data_ptr<scalar_t>() : NULL,\n             per_sample_weights.defined() ? per_sample_weights.stride(0) : 0,\n-            padding_idx);\n+            padding_idx, weight.size(0));\n         C10_CUDA_KERNEL_LAUNCH_CHECK();\n       }\n     });"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 32,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/9a9eadacc6ac3b734a6d607ae6f63ec1a0d1438d",
    "date": "2018-06-19T11:53:46-04:00",
    "message": "explicitly check device for grid_sampler (fixes: #8599) (#8646)",
    "changes": [
        {
            "name": "vision.py",
            "path": "torch/nn/_functions/vision.py",
            "patches": [
                {
                    "old_start": 41,
                    "old_length": 6,
                    "new_start": 41,
                    "new_length": 9,
                    "hunk_buggy": "[\"     def forward(ctx, input, grid, padding_mode='zeros'):\\n\", '         ctx.save_for_backward(input, grid)\\n', ' \\n', \"         if padding_mode == 'zeros':\\n\", '             ctx.padding_mode = MODE_ZEROS\\n', \"         elif padding_mode == 'border':\"]",
                    "hunk_fix": "@@ -41,6 +41,9 @@ class GridSampler(Function):\n     def forward(ctx, input, grid, padding_mode='zeros'):\n         ctx.save_for_backward(input, grid)\n \n+        if input.device != grid.device:\n+            raise RuntimeError((\"input (device {}) and grid (device {}) must be on the same device\" +\n+                                \"for grid_sampler\").format(input.device, grid.device))\n         if padding_mode == 'zeros':\n             ctx.padding_mode = MODE_ZEROS\n         elif padding_mode == 'border':"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 33,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/bf32ea80942ce720b105efcd517fd11182edeb08",
    "date": "2018-07-27T09:24:07-07:00",
    "message": "Fix dimension check in 1D instance norm, allowing 2D tensors alongside 3D. (#9924)\n\nSummary:\nFixes #9776.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/9924\n\nDifferential Revision: D9028328\n\nPulled By: soumith\n\nfbshipit-source-id: d5f22abb2be83b34aee95ebe144c97519a6854f8",
    "changes": [
        {
            "name": "instancenorm.py",
            "path": "torch/nn/modules/instancenorm.py",
            "patches": [
                {
                    "old_start": 109,
                    "old_length": 8,
                    "new_start": 109,
                    "new_length": 8,
                    "hunk_buggy": "['     \"\"\"\\n', ' \\n', '     def _check_input_dim(self, input):\\n', '-        if input.dim() != 3:\\n', \"-            raise ValueError('expected 3D input (got {}D input)'\\n\", '                              .format(input.dim()))\\n', ' \\n', ' ']",
                    "hunk_fix": "@@ -109,8 +109,8 @@ class InstanceNorm1d(_InstanceNorm):\n     \"\"\"\n \n     def _check_input_dim(self, input):\n-        if input.dim() != 3:\n-            raise ValueError('expected 3D input (got {}D input)'\n+        if input.dim() != 2 and input.dim() != 3:\n+            raise ValueError('expected 2D or 3D input (got {}D input)'\n                              .format(input.dim()))\n \n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 34,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/d8466964b348b6172317f70b8e52de02402bad54",
    "date": "2022-11-15T20:35:51+00:00",
    "message": "Add range check to multi margin loss target (#89008)\n\nFixes https://github.com/pytorch/pytorch/issues/88724\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/89008\nApproved by: https://github.com/ngimel",
    "changes": [
        {
            "name": "MultiMarginLoss.cu",
            "path": "aten/src/ATen/native/cuda/MultiMarginLoss.cu",
            "patches": [
                {
                    "old_start": 31,
                    "old_length": 6,
                    "new_start": 31,
                    "new_length": 7,
                    "hunk_buggy": "['   scalar_t *input_k = input + k*dim;\\n', '   scalar_t *output_k = output + k;\\n', '   int target_k = static_cast<int>(target[k]);\\n', '   scalar_t input_target_k = input_k[target_k];\\n', ' \\n', '   int i_start = threadIdx.x;']",
                    "hunk_fix": "@@ -31,6 +31,7 @@ __global__ void MultiMarginLoss_forward_kernel(\n   scalar_t *input_k = input + k*dim;\n   scalar_t *output_k = output + k;\n   int target_k = static_cast<int>(target[k]);\n+  CUDA_KERNEL_ASSERT(target_k >= 0 && target_k < dim && \"target index is out of bounds\");\n   scalar_t input_target_k = input_k[target_k];\n \n   int i_start = threadIdx.x;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 35,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/7ddf167ba5db277e02f983a6bde2bc3f5fbe1caa",
    "date": "2022-09-30T18:30:06+00:00",
    "message": "Move the asserts in shape functions upsample_nearest_2d op. (#85801)\n\nThe assert check are moved to top and the function now returns out. This is needed by the downstream torch-mlir project to correctly determine the output type.\n\nFixes #ISSUE_NUMBER\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/85801\nApproved by: https://github.com/eellison",
    "changes": [
        {
            "name": "_shape_functions.py",
            "path": "torch/jit/_shape_functions.py",
            "patches": [
                {
                    "old_start": 355,
                    "old_length": 6,
                    "new_start": 355,
                    "new_length": 10,
                    "hunk_buggy": "['     out: List[int] = []\\n', '     out.append(input[0])\\n', '     out.append(input[1])\\n', '     if output_size is not None:\\n', '         assert (\\n', '             scale_factors is None\\n']",
                    "hunk_fix": "@@ -355,6 +355,10 @@ def upsample_nearest2d(\n     out: List[int] = []\n     out.append(input[0])\n     out.append(input[1])\n+\n+    if (scale_factors is None and output_size is None):\n+        assert 0, \"Either output_size or scale_factors must be presented\"\n+\n     if output_size is not None:\n         assert (\n             scale_factors is None\n"
                },
                {
                    "old_start": 362,
                    "old_length": 7,
                    "new_start": 366,
                    "new_length": 6,
                    "hunk_buggy": "['         assert len(output_size) == 2\\n', '         out.append(output_size[0])\\n', '         out.append(output_size[1])\\n', '-        return out\\n', ' \\n', '     if scale_factors is not None:\\n', '         assert (\\n']",
                    "hunk_fix": "@@ -362,7 +366,6 @@ def upsample_nearest2d(\n         assert len(output_size) == 2\n         out.append(output_size[0])\n         out.append(output_size[1])\n-        return out\n \n     if scale_factors is not None:\n         assert (\n"
                },
                {
                    "old_start": 371,
                    "old_length": 8,
                    "new_start": 374,
                    "new_length": 8,
                    "hunk_buggy": "['         assert len(scale_factors) == 2\\n', '         out.append(int(input[2] * scale_factors[0]))\\n', '         out.append(int(input[3] * scale_factors[1]))\\n', '-        return out\\n', '-    assert 0, \"Either output_size or scale_factors must be presented\"\\n', ' \\n', ' \\n', ' def mm(self: List[int], mat2: List[int]):']",
                    "hunk_fix": "@@ -371,8 +374,8 @@ def upsample_nearest2d(\n         assert len(scale_factors) == 2\n         out.append(int(input[2] * scale_factors[0]))\n         out.append(int(input[3] * scale_factors[1]))\n-        return out\n-    assert 0, \"Either output_size or scale_factors must be presented\"\n+\n+    return out\n \n \n def mm(self: List[int], mat2: List[int]):"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 36,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/8198474eb763c8d526ede3418211479c2f4cbd30",
    "date": "2023-11-02T19:31:32+00:00",
    "message": "Fix scope name when parent scope is empty for torch.onnx.export (#112654)\n\nPrevious to this PR, we only checked TorchScript nodes for scope compatibility, skipping their parent's scope reference check.\nThis PR fixes adds a check not only for the node being traversed, but its parents as well\nPull Request resolved: https://github.com/pytorch/pytorch/pull/112654\nApproved by: https://github.com/BowenBao",
    "changes": [
        {
            "name": "naming.cpp",
            "path": "torch/csrc/jit/passes/onnx/naming.cpp",
            "patches": [
                {
                    "old_start": 24,
                    "old_length": 7,
                    "new_start": 24,
                    "new_length": 7,
                    "hunk_buggy": "['     return out;\\n', '   }\\n', '   auto parent = scope->parent();\\n', '-  while (!parent->isRoot()) {\\n', '     out = std::string((*name_func)(parent)).append(layer_separator).append(out);\\n', '     parent = parent->parent();\\n', '   }']",
                    "hunk_fix": "@@ -24,7 +24,7 @@ std::string nameFromRoot(\n     return out;\n   }\n   auto parent = scope->parent();\n-  while (!parent->isRoot()) {\n+  while (isCompatibleScope(parent)) {\n     out = std::string((*name_func)(parent)).append(layer_separator).append(out);\n     parent = parent->parent();\n   }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 37,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/8340762211e3b55caa178bac748bd902249f6fc0",
    "date": "2023-06-14T02:13:05+00:00",
    "message": "Update lr_scheduler.py to check the type of eta_min (#97003)\n\nAdd float assertion to `eta_min` parameter in `CosineAnnealingWarmRestarts`.\n\nFixes #87757\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/97003\nApproved by: https://github.com/janeyx99",
    "changes": [
        {
            "name": "lr_scheduler.py",
            "path": "torch/optim/lr_scheduler.py",
            "patches": [
                {
                    "old_start": 1367,
                    "old_length": 6,
                    "new_start": 1367,
                    "new_length": 8,
                    "hunk_buggy": "['             raise ValueError(\"Expected positive integer T_0, but got {}\".format(T_0))\\n', '         if T_mult < 1 or not isinstance(T_mult, int):\\n', '             raise ValueError(\"Expected integer T_mult >= 1, but got {}\".format(T_mult))\\n', '         self.T_0 = T_0\\n', '         self.T_i = T_0\\n', '         self.T_mult = T_mult']",
                    "hunk_fix": "@@ -1367,6 +1367,8 @@ class CosineAnnealingWarmRestarts(LRScheduler):\n             raise ValueError(\"Expected positive integer T_0, but got {}\".format(T_0))\n         if T_mult < 1 or not isinstance(T_mult, int):\n             raise ValueError(\"Expected integer T_mult >= 1, but got {}\".format(T_mult))\n+        if not isinstance(eta_min, (float, int)):\n+            raise ValueError(\"Expected float or int eta_min, but got {} of type {}\".format(eta_min, type(eta_min)))\n         self.T_0 = T_0\n         self.T_i = T_0\n         self.T_mult = T_mult"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 38,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/bbc7c79b20e67da450dd9b7de70cc6b68e656714",
    "date": "2023-03-27T18:57:27+00:00",
    "message": "add device checks for sparse csr (#97520)\n\nFixes #95373\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/97520\nApproved by: https://github.com/cpuhrsch",
    "changes": [
        {
            "name": "SparseCsrTensorMath.cu",
            "path": "aten/src/ATen/native/sparse/cuda/SparseCsrTensorMath.cu",
            "patches": [
                {
                    "old_start": 263,
                    "old_length": 6,
                    "new_start": 263,
                    "new_length": 18,
                    "hunk_buggy": "['         self.sizes(),\\n', '         \" and tensor `other` with shape \",\\n', '         other.sizes());\\n', ' \\n', '     if (only_sparse_compressed_add_trivial_cases(self, other, alpha, out)) {\\n', '       return out;']",
                    "hunk_fix": "@@ -263,6 +263,18 @@ Tensor& add_out_sparse_csr_cuda(\n         self.sizes(),\n         \" and tensor `other` with shape \",\n         other.sizes());\n+    TORCH_CHECK(\n+      self.is_cuda(),\n+      \"add: expected 'self' to be CUDA tensor, but got tensor on device: \",\n+      self.device());\n+    TORCH_CHECK(\n+      other.is_cuda(),\n+      \"add: expected 'other' to be CUDA tensor, but got tensor on device: \",\n+      other.device());\n+    TORCH_CHECK(\n+      out.is_cuda(),\n+      \"add: expected 'out' to be CUDA tensor, but got tensor on device: \",\n+      out.device());\n \n     if (only_sparse_compressed_add_trivial_cases(self, other, alpha, out)) {\n       return out;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 39,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/7ea6559658a6f650363f8b96f462bbc047e29124",
    "date": "2020-02-04T15:00:54-08:00",
    "message": "Add size checks to `torch.stack` (#32931)\n\nSummary:\nChecks the size of each tensor passed to `torch.stack` before calling `cat` to address https://github.com/pytorch/pytorch/issues/29510. This is done in the `get_stack_input` function as that is a common path. The function now compares the size of each tensor in the TensorList to the size of the first tensor and throws an exception when the sizes are not equal.\n\nTo compare:\n```\nx = torch.zeros([1, 2])\ny = torch.zeros([1, 3])\ntorch.stack([x, y]) # Errors due to size differences\n```\nCurrent error:\n```\nRuntimeError: invalid argument 0: Sizes of tensors must match\nexcept in dimension 0. Got 2 and 3 in dimension 2 at (path)\\aten\\src\\TH/generic/THTensor.cpp:612\n```\nNew error:\n```\nRuntimeError: stack expects each tensor to be equal size, but\ngot [1, 2] at entry 0 and [1, 3] at entry 1\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/32931\n\nDifferential Revision: D19700110\n\nPulled By: ezyang\n\nfbshipit-source-id: 7e18bb00fa2c137e418e340d719b6b76170b83e3",
    "changes": [
        {
            "name": "TensorShape.cpp",
            "path": "aten/src/ATen/native/TensorShape.cpp",
            "patches": [
                {
                    "old_start": 777,
                    "old_length": 9,
                    "new_start": 777,
                    "new_length": 15,
                    "hunk_buggy": "['   return splits;\\n', ' }\\n', ' \\n', ' static inline std::vector<Tensor> get_stack_inputs(TensorList tensors, int64_t dim) {\\n', '   std::vector<Tensor> inputs(tensors.size());\\n', '-  for (size_t i = 0; i < tensors.size(); ++i) {\\n', '     inputs[i] = tensors[i].unsqueeze(dim);\\n', '   }\\n', '   return inputs;']",
                    "hunk_fix": "@@ -777,9 +777,15 @@ std::vector<Tensor> split_with_sizes(const Tensor& self, IntArrayRef split_sizes\n   return splits;\n }\n \n+// Precondition: tensors is non-empty\n static inline std::vector<Tensor> get_stack_inputs(TensorList tensors, int64_t dim) {\n   std::vector<Tensor> inputs(tensors.size());\n-  for (size_t i = 0; i < tensors.size(); ++i) {\n+  at::IntArrayRef entry_shape = tensors[0].sizes();\n+  inputs[0] = tensors[0].unsqueeze(dim);\n+  for (size_t i = 1; i < tensors.size(); ++i) {\n+    TORCH_CHECK(tensors[i].sizes() == entry_shape,\n+      \"stack expects each tensor to be equal size, but got \", entry_shape,\n+      \" at entry 0 and \", tensors[i].sizes(), \" at entry \", i);\n     inputs[i] = tensors[i].unsqueeze(dim);\n   }\n   return inputs;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 40,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/92ebb04f9206882e6d312a8b91318545f43a53c2",
    "date": "2020-09-24T16:26:59-07:00",
    "message": "added check for NumberType (#44375)\n\nSummary:\nFixes https://github.com/pytorch/pytorch/issues/44107\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44375\n\nReviewed By: mrshenli\n\nDifferential Revision: D23906728\n\nPulled By: eellison\n\nfbshipit-source-id: 3b534e5dd3af1f5e43a7314953e64117cbe8ffe4",
    "changes": [
        {
            "name": "ir_emitter.cpp",
            "path": "torch/csrc/jit/frontend/ir_emitter.cpp",
            "patches": [
                {
                    "old_start": 2189,
                    "old_length": 13,
                    "new_start": 2189,
                    "new_length": 13,
                    "hunk_buggy": "['   NamedValue emitValueToTensor(\\n', '       const NamedValue& value,\\n', '       const NamedValue& matchTypeOf) {\\n', '-    // Add implicit conversion of int/float/bool types to tensors\\n', '     // Used in emitSubscriptAssign to convert:\\n', '     //   `tensor(...)[x] = 99` to `tensor(...)[x] = tensor(99)`\\n', '     // Mirrors the `valueToTensor` behavior in python_variable_indexing.cpp\\n', '     const auto kind = value.type()->kind();\\n', '-    if (kind == c10::TypeKind::IntType || kind == c10::TypeKind::BoolType ||\\n', '-        kind == c10::TypeKind::FloatType) {\\n', '       auto dtype = graph->insert(prim::dtype, {matchTypeOf}, {});\\n', '       auto device = graph->insert(prim::device, {matchTypeOf}, {});\\n', '       auto converted = graph->insert(']",
                    "hunk_fix": "@@ -2189,13 +2189,13 @@ struct to_ir {\n   NamedValue emitValueToTensor(\n       const NamedValue& value,\n       const NamedValue& matchTypeOf) {\n-    // Add implicit conversion of int/float/bool types to tensors\n+    // Add implicit conversion of int/float/bool/number types to tensors\n     // Used in emitSubscriptAssign to convert:\n     //   `tensor(...)[x] = 99` to `tensor(...)[x] = tensor(99)`\n     // Mirrors the `valueToTensor` behavior in python_variable_indexing.cpp\n     const auto kind = value.type()->kind();\n-    if (kind == c10::TypeKind::IntType || kind == c10::TypeKind::BoolType ||\n-        kind == c10::TypeKind::FloatType) {\n+    if (kind == c10::TypeKind::NumberType || kind == c10::TypeKind::IntType ||\n+        kind == c10::TypeKind::BoolType || kind == c10::TypeKind::FloatType) {\n       auto dtype = graph->insert(prim::dtype, {matchTypeOf}, {});\n       auto device = graph->insert(prim::device, {matchTypeOf}, {});\n       auto converted = graph->insert("
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 41,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/e9c1ccee2247a7746fde202067a7d47b72809968",
    "date": "2022-03-01T23:28:14+00:00",
    "message": "Bug fix: allow std 0 in the meta definition of `normal_` (#70085)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70085\n\nAll other `normal` variants allow 0.  Looks like a mistake made while\ncopying the check.  Even the `normal_` implementation disagrees:\n\n```\n>>> t = torch.rand(2, 3, device='meta')\n>>> t.normal_(mean=4, std=0)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nRuntimeError: normal_ expects std > 0.0, but found std=0\n>>>\n>>>\n>>> t = torch.rand(2, 3)\n>>> t.normal_(mean=4, std=0)\ntensor([[4., 4., 4.],\n        [4., 4., 4.]])\n```\n\nFixes #69523.\n\nTest Plan: Imported from OSS\n\nReviewed By: davidberard98\n\nDifferential Revision: D34089967\n\nPulled By: bdhirsh\n\nfbshipit-source-id: c57963e55f06c9513c4f0839f8f7a21eca86b584\n(cherry picked from commit d6ffe43ddddd24daa5d9eb8befc852dd2108fc89)",
    "changes": [
        {
            "name": "Distributions.cpp",
            "path": "aten/src/ATen/native/Distributions.cpp",
            "patches": [
                {
                    "old_start": 262,
                    "old_length": 7,
                    "new_start": 262,
                    "new_length": 7,
                    "hunk_buggy": "[' }\\n', ' \\n', ' Tensor& normal_meta_(Tensor& self, double mean, double std, c10::optional<Generator> gen) {\\n', '-  TORCH_CHECK(std > 0.0, \"normal_ expects std > 0.0, but found std=\", std);  // TODO: dedupe\\n', '   return self;\\n', ' }\\n', ' ']",
                    "hunk_fix": "@@ -262,7 +262,7 @@ Tensor& normal_(Tensor& self, double mean, double std, c10::optional<Generator>\n }\n \n Tensor& normal_meta_(Tensor& self, double mean, double std, c10::optional<Generator> gen) {\n-  TORCH_CHECK(std > 0.0, \"normal_ expects std > 0.0, but found std=\", std);  // TODO: dedupe\n+  TORCH_CHECK(std >= 0.0, \"normal_ expects std >= 0.0, but found std=\", std);  // TODO: dedupe\n   return self;\n }\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 42,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/ae55619a2bb73ebcdc80b02a6ccd72275a9ce23e",
    "date": "2023-04-14T16:57:35+00:00",
    "message": "Add check for same dtype in tensordot implementation (#98938)\n\nFixes #77517\n\nI believe[ the first bullet point in this comment](https://github.com/pytorch/pytorch/issues/77517#issuecomment-1129233539) from the linked issue is no longer a concern, but please let me know if I'm incorrect here.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/98938\nApproved by: https://github.com/lezcano",
    "changes": [
        {
            "name": "Linear.cpp",
            "path": "aten/src/ATen/native/Linear.cpp",
            "patches": [
                {
                    "old_start": 726,
                    "old_length": 6,
                    "new_start": 726,
                    "new_length": 7,
                    "hunk_buggy": "[' // in the two dimension lists\\n', ' Tensor tensordot(const Tensor& input1, const Tensor& input2, IntArrayRef dims1, IntArrayRef dims2) {\\n', '   TORCH_CHECK(dims1.size() == dims2.size(), \"both dimension lists should have same length\");\\n', '   int64_t csize = 1;  // total size of the contracted dimensions\\n', '   Tensor t1 = input1;\\n', '   Tensor t2 = input2;']",
                    "hunk_fix": "@@ -726,6 +726,7 @@ Tensor bilinear(const Tensor& input1, const Tensor& input2, const Tensor& weight\n // in the two dimension lists\n Tensor tensordot(const Tensor& input1, const Tensor& input2, IntArrayRef dims1, IntArrayRef dims2) {\n   TORCH_CHECK(dims1.size() == dims2.size(), \"both dimension lists should have same length\");\n+  TORCH_CHECK(input1.scalar_type() == input2.scalar_type(), \"both inputs should have same dtype\");\n   int64_t csize = 1;  // total size of the contracted dimensions\n   Tensor t1 = input1;\n   Tensor t2 = input2;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 43,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/cc6a51c9f3ee97a06ff9c0b84477e88e33e31137",
    "date": "2022-06-02T21:12:14+00:00",
    "message": "added shape checking to WeightedRandomSampler (#78585)\n\nFixes #78236\n\nAn erronously shaped weights vector will result in the following output\n\n```\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n~/datarwe/pytorch/torch/utils/data/sampler.py in <module>\n      [274](file:///home/oliver/datarwe/pytorch/torch/utils/data/sampler.py?line=273) WeightedRandomSampler([1,2,3], 10)\n----> [275](file:///home/oliver/datarwe/pytorch/torch/utils/data/sampler.py?line=274) WeightedRandomSampler([[1,2,3], [4,5,6]], 10)\n\n~/datarwe/pytorch/torch/utils/data/sampler.py in __init__(self, weights, num_samples, replacement, generator)\n    [192](file:///home/oliver/datarwe/pytorch/torch/utils/data/sampler.py?line=191)         weights = torch.as_tensor(weights, dtype=torch.double)\n    [193](file:///home/oliver/datarwe/pytorch/torch/utils/data/sampler.py?line=192)         if len(weights.shape) != 1:\n--> [194](file:///home/oliver/datarwe/pytorch/torch/utils/data/sampler.py?line=193)             raise ValueError(\"weights should be a 1d sequence but given \"\n    [195](file:///home/oliver/datarwe/pytorch/torch/utils/data/sampler.py?line=194)                              \"weights have shape {}\".format(tuple(weights.shape)))\n    [196](file:///home/oliver/datarwe/pytorch/torch/utils/data/sampler.py?line=195)\n\nValueError: weights should be a 1d sequence but given weights have shape (2, 3)\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/78585\nApproved by: https://github.com/NivekT, https://github.com/ejguan",
    "changes": [
        {
            "name": "sampler.py",
            "path": "torch/utils/data/sampler.py",
            "patches": [
                {
                    "old_start": 187,
                    "old_length": 7,
                    "new_start": 187,
                    "new_length": 13,
                    "hunk_buggy": "['         if not isinstance(replacement, bool):\\n', '             raise ValueError(\"replacement should be a boolean value, but got \"\\n', '                              \"replacement={}\".format(replacement))\\n', '-        self.weights = torch.as_tensor(weights, dtype=torch.double)\\n', '         self.num_samples = num_samples\\n', '         self.replacement = replacement\\n', '         self.generator = generator']",
                    "hunk_fix": "@@ -187,7 +187,13 @@ class WeightedRandomSampler(Sampler[int]):\n         if not isinstance(replacement, bool):\n             raise ValueError(\"replacement should be a boolean value, but got \"\n                              \"replacement={}\".format(replacement))\n-        self.weights = torch.as_tensor(weights, dtype=torch.double)\n+\n+        weights_tensor = torch.as_tensor(weights, dtype=torch.double)\n+        if len(weights_tensor.shape) != 1:\n+            raise ValueError(\"weights should be a 1d sequence but given \"\n+                             \"weights have shape {}\".format(tuple(weights_tensor.shape)))\n+\n+        self.weights = weights_tensor\n         self.num_samples = num_samples\n         self.replacement = replacement\n         self.generator = generator"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 333,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/41902a6ebc1806e7f4d6ce1da604cc9921c6515e",
    "date": "2024-01-29T08:31:26+00:00",
    "message": "[dynamo] Optimize is_tracing checks (#118474)\n\nbenchmarks/dynamo/microbenchmarks/overheads.py\n- before: 10.4us\n- after: 9.9us\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/118474\nApproved by: https://github.com/yanboliang",
    "changes": [
        {
            "name": "eval_frame.py",
            "path": "torch/_dynamo/eval_frame.py",
            "patches": [
                {
                    "old_start": 275,
                    "old_length": 6,
                    "new_start": 275,
                    "new_length": 10,
                    "hunk_buggy": "['     pass\\n', ' \\n', ' \\n', ' def innermost_fn(fn):\\n', '     \"\"\"\\n', '     In case of nesting of _TorchDynamoContext calls, find the innermost\\n']",
                    "hunk_fix": "@@ -275,6 +275,10 @@ def nothing():\n     pass\n \n \n+def always_false():\n+    return False\n+\n+\n def innermost_fn(fn):\n     \"\"\"\n     In case of nesting of _TorchDynamoContext calls, find the innermost\n"
                },
                {
                    "old_start": 415,
                    "old_length": 11,
                    "new_start": 419,
                    "new_length": 16,
                    "hunk_buggy": "[' \\n', '         callback = self.callback\\n', ' \\n', '         @functools.wraps(fn)\\n', '         def _fn(*args, **kwargs):\\n', '-            if torch.fx._symbolic_trace.is_fx_tracing() and not isinstance(\\n', '-                self, DisableContext\\n', '-            ):\\n', '                 if config.error_on_nested_fx_trace:\\n', '                     raise RuntimeError(\\n', '                         \"Detected that you are using FX to symbolically trace \"\\n']",
                    "hunk_fix": "@@ -415,11 +419,16 @@ class _TorchDynamoContext:\n \n         callback = self.callback\n \n+        if isinstance(self, DisableContext):\n+            is_jit_tracing = always_false\n+            is_fx_tracing = always_false\n+        else:\n+            is_jit_tracing = torch._C._is_tracing\n+            is_fx_tracing = torch.fx._symbolic_trace.is_fx_tracing\n+\n         @functools.wraps(fn)\n         def _fn(*args, **kwargs):\n-            if torch.fx._symbolic_trace.is_fx_tracing() and not isinstance(\n-                self, DisableContext\n-            ):\n+            if is_fx_tracing():\n                 if config.error_on_nested_fx_trace:\n                     raise RuntimeError(\n                         \"Detected that you are using FX to symbolically trace \"\n"
                },
                {
                    "old_start": 428,
                    "old_length": 7,
                    "new_start": 437,
                    "new_length": 7,
                    "hunk_buggy": "['                 else:\\n', '                     return fn(*args, **kwargs)\\n', ' \\n', '-            if torch.jit.is_tracing():\\n', '                 if config.error_on_nested_jit_trace:\\n', '                     raise RuntimeError(\\n', '                         \"Detected that you are using FX to torch.jit.trace \"']",
                    "hunk_fix": "@@ -428,7 +437,7 @@ class _TorchDynamoContext:\n                 else:\n                     return fn(*args, **kwargs)\n \n-            if torch.jit.is_tracing():\n+            if is_jit_tracing():\n                 if config.error_on_nested_jit_trace:\n                     raise RuntimeError(\n                         \"Detected that you are using FX to torch.jit.trace \""
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 334,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/dc43ad428603539a2051940c09b191825f66203d",
    "date": "2024-01-11T03:37:45+00:00",
    "message": "add is_grad_enabled check in runtime_wrapper before running with torch.no_grad (#117089)\n\nWe observed that `with torch.no_grad()` in runtime_wrapper introduced ~10% (0.06ms->0.066ms) inference performance regression on lennard_jones on cpu.\nFor inference tasks in benchmark, grad has been disabled, but in the current runtime_wrapper, no_grad is set again and its time is counted into the running time.\nTherefore, we add `is_grad_enabled` check in runtime_wrapper before running with torch.no_grad. If grad has been disabled, there is no need to set no_grad.\n\nBefore this pr:\n1.043x\ndev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio,eager_peak_mem,dynamo_peak_mem,calls_captured,unique_graphs,graph_breaks,unique_graph_breaks\ncpu,lennard_jones,1,**1.043427**,**0.068366**,4.756151,0.941846,45.056819,47.838822,9,1,0,0\n\nAfter this pr:\n1.146x\ndev,name,batch_size,speedup,abs_latency,compilation_latency,compression_ratio,eager_peak_mem,dynamo_peak_mem,calls_captured,unique_graphs,graph_breaks,unique_graph_breaks\ncpu,lennard_jones,1,**1.146190**,**0.061844**,4.468380,0.936456,44.427264,47.441920,9,1,0,0\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/117089\nApproved by: https://github.com/jgong5, https://github.com/bdhirsh",
    "changes": [
        {
            "name": "runtime_wrappers.py",
            "path": "torch/_functorch/_aot_autograd/runtime_wrappers.py",
            "patches": [
                {
                    "old_start": 90,
                    "old_length": 7,
                    "new_start": 90,
                    "new_length": 14,
                    "hunk_buggy": "[\"             # It's possible to get an inference graph with inputs that require grad,\\n\", '             # in which case we want to make sure autograd is disabled\\n', '             # (since e.g., inductor will generate aten.addmm.out calls which autograd will complain on)\\n', '-            with torch.no_grad():\\n', '                 all_outs = call_func_at_runtime_with_args(\\n', '                     compiled_fn,\\n', '                     args,']",
                    "hunk_fix": "@@ -90,7 +90,14 @@ def create_runtime_wrapper(\n             # It's possible to get an inference graph with inputs that require grad,\n             # in which case we want to make sure autograd is disabled\n             # (since e.g., inductor will generate aten.addmm.out calls which autograd will complain on)\n-            with torch.no_grad():\n+            if torch.is_grad_enabled():\n+                with torch.no_grad():\n+                    all_outs = call_func_at_runtime_with_args(\n+                        compiled_fn,\n+                        args,\n+                        disable_amp=disable_amp,\n+                    )\n+            else:\n                 all_outs = call_func_at_runtime_with_args(\n                     compiled_fn,\n                     args,"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 335,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/f6639359357452de8bfc691430396ded98ea399c",
    "date": "2024-01-08T04:55:35+00:00",
    "message": "[MPS] Fix boundary checks in generateKernelOffsets (#116915)\n\n`TORCH_CHECK(i<UINT32_MAX)` is always false, it should be `TORCH_CHECK(iterShape[i] < UINT32_MAX)`\nPull Request resolved: https://github.com/pytorch/pytorch/pull/116915\nApproved by: https://github.com/Skylion007, https://github.com/kulinseth\nghstack dependencies: #116903, #116904",
    "changes": [
        {
            "name": "OperationUtils.mm",
            "path": "aten/src/ATen/native/mps/OperationUtils.mm",
            "patches": [
                {
                    "old_start": 557,
                    "old_length": 15,
                    "new_start": 557,
                    "new_length": 15,
                    "hunk_buggy": "['   std::vector<uint32_t> iterShapeData(iterShape.size());\\n', '   std::vector<std::array<uint32_t, nOffsets>> strides(nDim);\\n', '   TORCH_INTERNAL_ASSERT(iter.ntensors() >= nOffsets);\\n', ' \\n', '   for (const auto i : c10::irange(iterShape.size())) {\\n', '-    TORCH_CHECK(i <= UINT32_MAX);\\n', '-    iterShapeData[i] = (uint32_t)(iterShape[i]);\\n', '   }\\n', ' \\n', '   for (const auto i : c10::irange(nDim)) {\\n', '     for (const auto offset : c10::irange(nOffsets)) {\\n', '-      strides[i][offset] = iter.strides(offset)[i];\\n', '     }\\n', '   }\\n', ' ']",
                    "hunk_fix": "@@ -557,15 +557,15 @@ id<MTLBuffer> generateKernelDataOffsets(id<MTLComputeCommandEncoder> commandEnco\n   std::vector<uint32_t> iterShapeData(iterShape.size());\n   std::vector<std::array<uint32_t, nOffsets>> strides(nDim);\n   TORCH_INTERNAL_ASSERT(iter.ntensors() >= nOffsets);\n+  TORCH_CHECK(iter.can_use_32bit_indexing(), \"Can't be indexed using 32-bit iterator\");\n \n   for (const auto i : c10::irange(iterShape.size())) {\n-    TORCH_CHECK(i <= UINT32_MAX);\n-    iterShapeData[i] = (uint32_t)(iterShape[i]);\n+    iterShapeData[i] = static_cast<uint32_t>(iterShape[i]);\n   }\n \n   for (const auto i : c10::irange(nDim)) {\n     for (const auto offset : c10::irange(nOffsets)) {\n-      strides[i][offset] = iter.strides(offset)[i];\n+      strides[i][offset] = static_cast<uint32_t>(iter.strides(offset)[i]);\n     }\n   }\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 336,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/02e2158e754bafda46e663052c838aeb6ab6b560",
    "date": "2023-12-28T22:09:03+00:00",
    "message": "Fix for out of bounds read in mobile interpreter INTERFACE_CALL opcode handler (#110301)\n\nSummary:\nThe INTERFACE_CALL opcode for the mobile TorchScript interpreter contained an out of bounds read issue leading to memory corruption.\n\nThis change adds an explicit check that the number of inputs passed to the format method called when handling the INTERFACE_CALL opcode is a valid and within bounds of the stack.\n\nTest Plan: contbuild + OSS signals\n\nDifferential Revision: D49739450\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110301\nApproved by: https://github.com/dbort",
    "changes": [
        {
            "name": "interpreter.cpp",
            "path": "torch/csrc/jit/mobile/interpreter.cpp",
            "patches": [
                {
                    "old_start": 159,
                    "old_length": 6,
                    "new_start": 159,
                    "new_length": 15,
                    "hunk_buggy": "['               static_cast<size_t>(inst.X) >= code.constants_.size()) {\\n', '             TORCH_CHECK(false, \"Can\\'t load constant with index: \", inst.X);\\n', '           }\\n', '           torch::jit::Function& method =\\n', '               peek(stack, 0, inst.N)\\n', '                   .toObject()']",
                    "hunk_fix": "@@ -159,6 +159,15 @@ bool InterpreterState::run(Stack& stack) {\n               static_cast<size_t>(inst.X) >= code.constants_.size()) {\n             TORCH_CHECK(false, \"Can't load constant with index: \", inst.X);\n           }\n+          if (inst.N == 0 || inst.N > stack.size()) {\n+            TORCH_CHECK(\n+                false,\n+                \"INTERFACE_CALL N=\",\n+                inst.N,\n+                \" not in range [1, \",\n+                stack.size(),\n+                \"]\");\n+          }\n           torch::jit::Function& method =\n               peek(stack, 0, inst.N)\n                   .toObject()"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 337,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/66b04e3cb707d30c4698b269c83cb6221848f17a",
    "date": "2023-12-14T23:40:54+00:00",
    "message": "[nccl flight recorder] nullptr profiling name (#115851)\n\nSometimes profiling name can be a nullptr, which\nthrows on conversion to std::string. This adds a check.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/115851\nApproved by: https://github.com/wconstab",
    "changes": [
        {
            "name": "TraceUtils.h",
            "path": "torch/csrc/distributed/c10d/TraceUtils.h",
            "patches": [
                {
                    "old_start": 386,
                    "old_length": 7,
                    "new_start": 386,
                    "new_length": 7,
                    "hunk_buggy": "['         id_,\\n', '         pg_id,\\n', '         seq_id,\\n', '-        profiling_name,\\n', '         std::move(traceback),\\n', '         std::move(start),\\n', '         std::move(end),']",
                    "hunk_fix": "@@ -386,7 +386,7 @@ struct NCCLTraceBuffer {\n         id_,\n         pg_id,\n         seq_id,\n-        profiling_name,\n+        profiling_name == nullptr ? \"\" : profiling_name,\n         std::move(traceback),\n         std::move(start),\n         std::move(end),"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 338,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/7553c495147f3e21a1e27d392d277906a47768e7",
    "date": "2023-12-12T18:02:05+00:00",
    "message": "[S382174] Fix distributed debug w/ non-equal split (#115483)\n\nSummary:\nIn collectives, it's possible to have non-equal split that has a different implementation and the output tensor size will be different, e.g. https://www.internalfb.com/code/fbsource/[460afb1172b5]/fbcode/caffe2/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp?lines=3104. However, TORCH_DISTRIBUTED_DEBUG=DETAIL will assume the output tensor size is the same and does the check and will fail the job if they don't: https://fburl.com/code/mhte9ty8. c10d code should handle this.\n\nIdeally we should check the input size across ranks and make sure they're the same. Maybe for next diff.\n\nTest Plan: Test torchrec's TWRW w/ non-even split and it's working now.\n\nReviewed By: zhangruiskyline\n\nDifferential Revision: D52010942\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/115483\nApproved by: https://github.com/kwen2501, https://github.com/fegin, https://github.com/XilunWu",
    "changes": [
        {
            "name": "ProcessGroupWrapper.cpp",
            "path": "torch/csrc/distributed/c10d/ProcessGroupWrapper.cpp",
            "patches": [
                {
                    "old_start": 371,
                    "old_length": 6,
                    "new_start": 371,
                    "new_length": 15,
                    "hunk_buggy": "['   return output << collectiveInfo;\\n', ' }\\n', ' \\n', ' } // namespace\\n', ' \\n', ' ProcessGroupWrapper::ProcessGroupWrapper(\\n']",
                    "hunk_fix": "@@ -371,6 +371,15 @@ std::ostream& operator<<(\n   return output << collectiveInfo;\n }\n \n+bool check_same_size(const std::vector<at::Tensor>& input_tensors) {\n+  for (const auto& input_tensor : input_tensors) {\n+    if (!input_tensors[0].is_same_size(input_tensor)) {\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n } // namespace\n \n ProcessGroupWrapper::ProcessGroupWrapper(\n"
                },
                {
                    "old_start": 423,
                    "old_length": 7,
                    "new_start": 432,
                    "new_length": 11,
                    "hunk_buggy": "['     std::vector<std::vector<at::Tensor>>& outputTensors,\\n', '     std::vector<at::Tensor>& inputTensors,\\n', '     const AllgatherOptions& opts) {\\n', '-  runCollectiveChecks(OpType::ALLGATHER, inputTensors);\\n', '   return backend_->allgather(outputTensors, inputTensors, opts);\\n', ' }\\n', ' \\n']",
                    "hunk_fix": "@@ -423,7 +432,11 @@ c10::intrusive_ptr<Work> ProcessGroupWrapper::allgather(\n     std::vector<std::vector<at::Tensor>>& outputTensors,\n     std::vector<at::Tensor>& inputTensors,\n     const AllgatherOptions& opts) {\n-  runCollectiveChecks(OpType::ALLGATHER, inputTensors);\n+  if (check_same_size(outputTensors.back())) {\n+    runCollectiveChecks(OpType::ALLGATHER, inputTensors);\n+  } else {\n+    runCollectiveChecks(OpType::ALLGATHER, {});\n+  }\n   return backend_->allgather(outputTensors, inputTensors, opts);\n }\n \n"
                },
                {
                    "old_start": 468,
                    "old_length": 7,
                    "new_start": 481,
                    "new_length": 11,
                    "hunk_buggy": "['     std::vector<at::Tensor>& outputTensors,\\n', '     std::vector<std::vector<at::Tensor>>& inputTensors,\\n', '     const ReduceScatterOptions& opts) {\\n', '-  runCollectiveChecks(OpType::REDUCE_SCATTER, outputTensors);\\n', '   return backend_->reduce_scatter(outputTensors, inputTensors, opts);\\n', ' }\\n', ' ']",
                    "hunk_fix": "@@ -468,7 +481,11 @@ c10::intrusive_ptr<Work> ProcessGroupWrapper::reduce_scatter(\n     std::vector<at::Tensor>& outputTensors,\n     std::vector<std::vector<at::Tensor>>& inputTensors,\n     const ReduceScatterOptions& opts) {\n-  runCollectiveChecks(OpType::REDUCE_SCATTER, outputTensors);\n+  if (check_same_size(inputTensors.back())) {\n+    runCollectiveChecks(OpType::REDUCE_SCATTER, outputTensors);\n+  } else {\n+    runCollectiveChecks(OpType::REDUCE_SCATTER, {});\n+  }\n   return backend_->reduce_scatter(outputTensors, inputTensors, opts);\n }\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 339,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/99f06c0cc2a907d8fbf613768356838548f1f8c0",
    "date": "2023-12-11T21:21:10+00:00",
    "message": "[BE] update errors to be more descriptive (#115443)\n\nwe call `_check_single_tensor` and `_check_tensor_list` as validation but don't print out the param types that were invalid\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/115443\nApproved by: https://github.com/XilunWu",
    "changes": [
        {
            "name": "distributed_c10d.py",
            "path": "torch/distributed/distributed_c10d.py",
            "patches": [
                {
                    "old_start": 844,
                    "old_length": 19,
                    "new_start": 844,
                    "new_length": 25,
                    "hunk_buggy": "['     \"\"\"Check that the parameter ``param_name`` is a single tensor.\"\"\"\\n', '     if not isinstance(param, torch.Tensor):\\n', '         raise TypeError(\\n', '-            f\"Invalid function argument. Expected parameter `{param_name}` to be of type torch.Tensor.\"\\n', '         )\\n', ' \\n', ' \\n', ' def _check_tensor_list(param, param_name) -> None:\\n', '     \"\"\"Check that the parameter ``param_name`` is a list of tensors.\"\"\"\\n', '-    if not isinstance(param, list) or not all(\\n', '-        isinstance(p, torch.Tensor) for p in param\\n', '-    ):\\n', '         raise TypeError(\\n', '-            f\"Invalid function argument. Expected parameter `{param_name}` to be of type List[torch.Tensor].\"\\n', '         )\\n', ' \\n', ' def _as_iterable(obj) -> collections.abc.Iterable:\\n', '     return obj if isinstance(obj, list) else (obj,)\\n', ' ']",
                    "hunk_fix": "@@ -844,19 +844,25 @@ def _check_single_tensor(param, param_name) -> None:\n     \"\"\"Check that the parameter ``param_name`` is a single tensor.\"\"\"\n     if not isinstance(param, torch.Tensor):\n         raise TypeError(\n-            f\"Invalid function argument. Expected parameter `{param_name}` to be of type torch.Tensor.\"\n+            f\"\"\"Invalid function argument. Expected parameter `{param_name}` of type torch.Tensor\n+             but got {type(param)} instead.\"\"\"\n         )\n \n \n def _check_tensor_list(param, param_name) -> None:\n     \"\"\"Check that the parameter ``param_name`` is a list of tensors.\"\"\"\n-    if not isinstance(param, list) or not all(\n-        isinstance(p, torch.Tensor) for p in param\n-    ):\n+    if not isinstance(param, list):\n+        raise TypeError(\n+            f\"\"\"Invalid function argument. Expected parameter `{param_name}` of type List[torch.Tensor]\n+             but got {type(param)} instead.\"\"\"\n+        )\n+    elif not all(isinstance(p, torch.Tensor) for p in param):\n         raise TypeError(\n-            f\"Invalid function argument. Expected parameter `{param_name}` to be of type List[torch.Tensor].\"\n+            f\"\"\"Invalid function argument. Expected parameter `{param_name}` of type List[torch.Tensor]\n+             but got {type(param)} with elements of type {[type(p) for p in param]}.\"\"\"\n         )\n \n+\n def _as_iterable(obj) -> collections.abc.Iterable:\n     return obj if isinstance(obj, list) else (obj,)\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 340,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/490f2d75700a806bdc6110e881e78493cde163e3",
    "date": "2023-12-06T04:44:49+00:00",
    "message": "Skip privateuse1's checkZeroPoints (#114117)\n\nWe want to use ``quantize_per_channel`` to create a quantized tensor, but we found that ``checkZeroPoints`` for ``privateuse1`` backend failed.\n\n``quantize_tensor_per_channel_affine`` will ``checkZeroPoints`` for all backends expect ``CUDA``:\nhttps://github.com/pytorch/pytorch/blob/140c54e6ccc5e97f1b7f1e0fcd3d8c6af7dd2ab2/aten/src/ATen/native/quantized/AffineQuantizer.cpp#L162-L164\n\nHowever, our ``privateuse1`` backend will get a segmentation error if we try to cast our data to int64_t in ``checkZeroPoints``:\nhttps://github.com/pytorch/pytorch/blob/140c54e6ccc5e97f1b7f1e0fcd3d8c6af7dd2ab2/aten/src/ATen/native/quantized/AffineQuantizer.cpp#L82-L88\n\nSo if we can skip ``privateuse1``'s ``checkZeroPoints`` and check this item in the actual device function? What do you think?\nPull Request resolved: https://github.com/pytorch/pytorch/pull/114117\nApproved by: https://github.com/jerryzh168",
    "changes": [
        {
            "name": "AffineQuantizer.cpp",
            "path": "aten/src/ATen/native/quantized/AffineQuantizer.cpp",
            "patches": [
                {
                    "old_start": 159,
                    "old_length": 9,
                    "new_start": 159,
                    "new_length": 10,
                    "hunk_buggy": "[' \\n', '   AT_DISPATCH_QINT_TYPES(qtensor.scalar_type(), fn_name, [&]() {\\n', '     checkQuantizedTensor<scalar_t>(fn_name, qtensor);\\n', '-    if(qtensor.device().type() != c10::DeviceType::CUDA){\\n', '       checkZeroPoints<underlying_t>(fn_name, zero_points);\\n', '-    }  // for cuda, this check will occur in the actual cuda function\\n', '   });\\n', ' \\n', '   TORCH_CHECK(\\n']",
                    "hunk_fix": "@@ -159,9 +159,10 @@ Tensor& quantize_tensor_per_channel_affine(\n \n   AT_DISPATCH_QINT_TYPES(qtensor.scalar_type(), fn_name, [&]() {\n     checkQuantizedTensor<scalar_t>(fn_name, qtensor);\n-    if(qtensor.device().type() != c10::DeviceType::CUDA){\n+    if (qtensor.device().type() != c10::DeviceType::CUDA &&\n+        qtensor.device().type() != c10::DeviceType::PrivateUse1) {\n       checkZeroPoints<underlying_t>(fn_name, zero_points);\n-    }  // for cuda, this check will occur in the actual cuda function\n+    }  // for cuda and privateuse1, this check will occur in the actual device function\n   });\n \n   TORCH_CHECK(\n"
                },
                {
                    "old_start": 251,
                    "old_length": 9,
                    "new_start": 252,
                    "new_length": 10,
                    "hunk_buggy": "[' \\n', '   AT_DISPATCH_QINT_TYPES(qtensor.scalar_type(), fn_name, [&]() {\\n', '     checkQuantizedTensor<scalar_t>(fn_name, qtensor);\\n', '-    if(qtensor.device().type() != c10::DeviceType::CUDA){\\n', '       checkZeroPoints<underlying_t>(fn_name, zero_points);\\n', '-    }  // for cuda, this check will occur in the actual cuda function\\n', '   });\\n', ' \\n', '   TORCH_CHECK(']",
                    "hunk_fix": "@@ -251,9 +252,10 @@ Tensor& dequantize_tensor_per_channel_affine(\n \n   AT_DISPATCH_QINT_TYPES(qtensor.scalar_type(), fn_name, [&]() {\n     checkQuantizedTensor<scalar_t>(fn_name, qtensor);\n-    if(qtensor.device().type() != c10::DeviceType::CUDA){\n+    if(qtensor.device().type() != c10::DeviceType::CUDA &&\n+       qtensor.device().type() != c10::DeviceType::PrivateUse1){\n       checkZeroPoints<underlying_t>(fn_name, zero_points);\n-    }  // for cuda, this check will occur in the actual cuda function\n+    }  // for cuda and privateuse1, this check will occur in the actual device function\n   });\n \n   TORCH_CHECK("
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 341,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/c1e51fcbfc70c089276530ee64fb626e3f7f4f2b",
    "date": "2023-11-30T04:43:46+00:00",
    "message": "[ONNX][Bench] Relax tolerance for cuda accuracy check (#114767)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/114767\nApproved by: https://github.com/thiagocrepaldi\nghstack dependencies: #112179",
    "changes": [
        {
            "name": "common.py",
            "path": "benchmarks/dynamo/common.py",
            "patches": [
                {
                    "old_start": 2241,
                    "old_length": 11,
                    "new_start": 2241,
                    "new_length": 11,
                    "hunk_buggy": "['             if name in self.skip_accuracy_check_as_eager_non_deterministic:\\n', '                 return record_status(\"pass_due_to_skip\", dynamo_start_stats=start_stats)\\n', ' \\n', '-            # Workaround for ONNX for non-tensor outputs\\n', '             if (\\n', '                 current_onnx_compiler == \"torchscript\"\\n', '                 or current_onnx_compiler == \"dynamo\"\\n', '             ):\\n', '                 (\\n', '                     correct_result,\\n', '                     new_result,\\n']",
                    "hunk_fix": "@@ -2241,11 +2241,11 @@ class BenchmarkRunner:\n             if name in self.skip_accuracy_check_as_eager_non_deterministic:\n                 return record_status(\"pass_due_to_skip\", dynamo_start_stats=start_stats)\n \n-            # Workaround for ONNX for non-tensor outputs\n             if (\n                 current_onnx_compiler == \"torchscript\"\n                 or current_onnx_compiler == \"dynamo\"\n             ):\n+                # Workaround for ONNX for non-tensor outputs\n                 (\n                     correct_result,\n                     new_result,\n"
                },
                {
                    "old_start": 2253,
                    "old_length": 6,
                    "new_start": 2253,
                    "new_length": 10,
                    "hunk_buggy": "['                 ) = _OnnxPatch.patch_non_tensor_outputs(\\n', '                     correct_result, new_result, fp64_outputs\\n', '                 )\\n', '                 # TODO: store correct_result into the dumped file for offline onnx model validation.\\n', '                 # The downside and potential problem, is that the output formats may be different.\\n', '                 # E.g., the output order might not match, None might be part of output, etc.']",
                    "hunk_fix": "@@ -2253,6 +2253,10 @@ class BenchmarkRunner:\n                 ) = _OnnxPatch.patch_non_tensor_outputs(\n                     correct_result, new_result, fp64_outputs\n                 )\n+                # Relax tolerance for ONNX cuda\n+                if current_device == \"cuda\":\n+                    tolerance = 1e-2\n+\n                 # TODO: store correct_result into the dumped file for offline onnx model validation.\n                 # The downside and potential problem, is that the output formats may be different.\n                 # E.g., the output order might not match, None might be part of output, etc."
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 342,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/a076a74f1118da171cf70d00d1de4abbe27cf85d",
    "date": "2023-11-29T05:59:35+00:00",
    "message": "[Nested Tensor] Add xpu device in assertion for nested tensor creation (#114664)\n\nAdd xpu device checking in nested tensor creation\nPull Request resolved: https://github.com/pytorch/pytorch/pull/114664\nApproved by: https://github.com/jgong5, https://github.com/xunnanxu",
    "changes": [
        {
            "name": "NestedTensorImpl.cpp",
            "path": "aten/src/ATen/NestedTensorImpl.cpp",
            "patches": [
                {
                    "old_start": 179,
                    "old_length": 8,
                    "new_start": 179,
                    "new_length": 8,
                    "hunk_buggy": "['       \"in the near future.\");\\n', '   auto storage_device = storage_.device();\\n', '   TORCH_INTERNAL_ASSERT(\\n', '-      storage_device.is_cpu() || storage_device.is_cuda() || storage_device.is_privateuseone(),\\n', '-      \"NestedTensorImpl storage must be either CUDA, CPU or \", get_privateuse1_backend(), \" but got \",\\n', '       storage_device);\\n', '   validate_nested_tensor_metadata(nested_sizes_, nested_strides_, storage_offsets_);\\n', '   refresh_dim();']",
                    "hunk_fix": "@@ -179,8 +179,8 @@ NestedTensorImpl::NestedTensorImpl(\n       \"in the near future.\");\n   auto storage_device = storage_.device();\n   TORCH_INTERNAL_ASSERT(\n-      storage_device.is_cpu() || storage_device.is_cuda() || storage_device.is_privateuseone(),\n-      \"NestedTensorImpl storage must be either CUDA, CPU or \", get_privateuse1_backend(), \" but got \",\n+      storage_device.is_cpu() || storage_device.is_cuda() || storage_device.is_xpu() || storage_device.is_privateuseone(),\n+      \"NestedTensorImpl storage must be either CUDA, CPU, XPU or \", get_privateuse1_backend(), \" but got \",\n       storage_device);\n   validate_nested_tensor_metadata(nested_sizes_, nested_strides_, storage_offsets_);\n   refresh_dim();"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 343,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/4d07428edee863e7f5920f0672957a9711a9f0b5",
    "date": "2023-11-22T01:05:42+00:00",
    "message": "Fix for out of bounds read in mobile interpreter FORMAT opcode handler (#110303)\n\nSummary:\nThe FORMAT opcode for the mobile TorchScript interpreter contained an out of bounds read issue leading to memory corruption.\n\nThis change adds an explicit check that the number of inputs passed to the format method called when handling the FORMAT opcode is a valid and within bounds of the stack.\n\nTest Plan: contbuild + OSS signals\n\nDifferential Revision: D49739095\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110303\nApproved by: https://github.com/malfet",
    "changes": [
        {
            "name": "vararg_functions.cpp",
            "path": "torch/csrc/jit/runtime/vararg_functions.cpp",
            "patches": [
                {
                    "old_start": 106,
                    "old_length": 6,
                    "new_start": 106,
                    "new_length": 10,
                    "hunk_buggy": "[' }\\n', ' \\n', ' void format(Stack& stack, size_t num_inputs) {\\n', '   // static const std::regex unsupported_options(\"\\\\\\\\{(.*?)\\\\\\\\}\");\\n', '   auto format = peek(stack, 0, num_inputs).toStringRef();\\n', '   // // Temporally comment out the warning message because of']",
                    "hunk_fix": "@@ -106,6 +106,10 @@ void tupleUnpack(Stack& stack) {\n }\n \n void format(Stack& stack, size_t num_inputs) {\n+  if (num_inputs == 0 || num_inputs > stack.size()) {\n+    AT_ERROR(\"Invalid number of inputs for format string: \", num_inputs);\n+  }\n+\n   // static const std::regex unsupported_options(\"\\\\{(.*?)\\\\}\");\n   auto format = peek(stack, 0, num_inputs).toStringRef();\n   // // Temporally comment out the warning message because of"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 344,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/fb25fd6f865ed0532caf710ca130b6cc23a772a8",
    "date": "2023-11-21T01:24:21+00:00",
    "message": "[DTensor] Replaced neg dim normalization with assert in helper (#114141)\n\nThis is a replacement for https://github.com/pytorch/pytorch/pull/113922. I think we can still leave the check for negative shard dimension in `compute_local_shape_and_global_offset` and replace the normalization logic with an assert. This should provide us a stack trace to see which user-facing API did not normalize the dim as expected.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/114141\nApproved by: https://github.com/wanchaol\nghstack dependencies: #113919, #113924, #114134, #113925, #113930",
    "changes": [
        {
            "name": "_utils.py",
            "path": "torch/distributed/_tensor/_utils.py",
            "patches": [
                {
                    "old_start": 145,
                    "old_length": 8,
                    "new_start": 145,
                    "new_length": 10,
                    "hunk_buggy": "['         if placement.is_shard():\\n', '             shard_placement = cast(Shard, placement)\\n', '             if shard_placement.dim < 0:\\n', '-                # normalize shard dim to be positive\\n', '-                shard_placement.dim += len(tensor_shape)\\n', '             shard_dim = shard_placement.dim\\n', ' \\n', '             assert (']",
                    "hunk_fix": "@@ -145,8 +145,10 @@ def compute_global_tensor_info(\n         if placement.is_shard():\n             shard_placement = cast(Shard, placement)\n             if shard_placement.dim < 0:\n-                # normalize shard dim to be positive\n-                shard_placement.dim += len(tensor_shape)\n+                raise AssertionError(\n+                    \"Shard placements should have negative dims normalized in \"\n+                    f\"the user-facing APIs: {shard_placement}\"\n+                )\n             shard_dim = shard_placement.dim\n \n             assert ("
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 345,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/d01f8b291d437a37ec8809a18c1bb2ebfa825285",
    "date": "2023-11-08T05:27:15+00:00",
    "message": "Fix visualize_overlap for Inductor comm reordering (#113066)\n\nThe following assumptions are not always valid and need checking:\n1. `snode.node` exists\n2. `snode.node.layout.size` exists\n3. `snode.node.layout.stride` exists\n4. `snode.node.name` exists\n\nAlso there is no guarantee that there won't be two collectives running at the same time. But it's hard to visualize the overlap in that case. So disable the visualization for that case for now.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/113066\nApproved by: https://github.com/wanchaol",
    "changes": [
        {
            "name": "comms.py",
            "path": "torch/_inductor/comms.py",
            "patches": [
                {
                    "old_start": 291,
                    "old_length": 12,
                    "new_start": 291,
                    "new_length": 19,
                    "hunk_buggy": "['     detail = \"\"\\n', '     if isinstance(snode.node, ir.ExternKernelOut):\\n', '         detail = f\" ({snode.node.kernel})\"\\n', '-    out_tensor_info = (\\n', '-        f\" (size={snode.node.layout.size}, stride={snode.node.layout.stride})\"\\n', '-    )\\n', '-    return (\\n', '-        f\"{snode.node.__class__.__name__}{detail}{out_tensor_info} ({snode.node.name})\"\\n', '-    )\\n', ' \\n', ' \\n', ' def visualize_overlap(order):\\n']",
                    "hunk_fix": "@@ -291,12 +291,19 @@ def node_summary(snode):\n     detail = \"\"\n     if isinstance(snode.node, ir.ExternKernelOut):\n         detail = f\" ({snode.node.kernel})\"\n-    out_tensor_info = (\n-        f\" (size={snode.node.layout.size}, stride={snode.node.layout.stride})\"\n-    )\n-    return (\n-        f\"{snode.node.__class__.__name__}{detail}{out_tensor_info} ({snode.node.name})\"\n-    )\n+    out_tensor_info = \"\"\n+    if (\n+        hasattr(snode.node, \"layout\")\n+        and hasattr(snode.node.layout, \"size\")\n+        and hasattr(snode.node.layout, \"stride\")\n+    ):\n+        out_tensor_info = (\n+            f\" (size={snode.node.layout.size}, stride={snode.node.layout.stride})\"\n+        )\n+    node_name = \"\"\n+    if hasattr(snode.node, \"name\"):\n+        node_name = snode.node.name\n+    return f\"{snode.node.__class__.__name__}{detail}{out_tensor_info} ({node_name})\"\n \n \n def visualize_overlap(order):\n"
                },
                {
                    "old_start": 317,
                    "old_length": 7,
                    "new_start": 324,
                    "new_length": 7,
                    "hunk_buggy": "['         else:  # cur_comm_node is not None\\n', '             if isinstance(snode.node, ir.CollectiveKernel):\\n', '                 raise Exception(\\n', '-                    \"Found two collectives running at the same time, which is unexpected. \"\\n', '                     \"`visualize_overlap` needs to be updated to handle this case\"\\n', '                 )\\n', '             elif isinstance(snode.node, ir.Wait):  # end of this comm op\\n']",
                    "hunk_fix": "@@ -317,7 +324,7 @@ def visualize_overlap(order):\n         else:  # cur_comm_node is not None\n             if isinstance(snode.node, ir.CollectiveKernel):\n                 raise Exception(\n-                    \"Found two collectives running at the same time, which is unexpected. \"\n+                    \"Found two collectives running at the same time. \"\n                     \"`visualize_overlap` needs to be updated to handle this case\"\n                 )\n             elif isinstance(snode.node, ir.Wait):  # end of this comm op\n"
                },
                {
                    "old_start": 341,
                    "old_length": 11,
                    "new_start": 348,
                    "new_length": 17,
                    "hunk_buggy": "['             overlap_log.debug(\\n', '                 f\"==== Visualize overlap before reordering pass {p} ====\"  # noqa: G004\\n', '             )\\n', '-            visualize_overlap(order)\\n', '         order = p(order)  # type: ignore[operator]\\n', '         if torch.distributed.get_rank() == 0:\\n', '             overlap_log.debug(\\n', '                 f\"==== Visualize overlap after reordering pass {p} ====\"  # noqa: G004\\n', '             )\\n', '-            visualize_overlap(order)\\n', '     return order']",
                    "hunk_fix": "@@ -341,11 +348,17 @@ def reorder_compute_and_comm_for_overlap(\n             overlap_log.debug(\n                 f\"==== Visualize overlap before reordering pass {p} ====\"  # noqa: G004\n             )\n-            visualize_overlap(order)\n+            try:\n+                visualize_overlap(order)\n+            except Exception as e:\n+                overlap_log.debug(str(e))\n         order = p(order)  # type: ignore[operator]\n         if torch.distributed.get_rank() == 0:\n             overlap_log.debug(\n                 f\"==== Visualize overlap after reordering pass {p} ====\"  # noqa: G004\n             )\n-            visualize_overlap(order)\n+            try:\n+                visualize_overlap(order)\n+            except Exception as e:\n+                overlap_log.debug(str(e))\n     return order"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 346,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/93cea394dee1315c7a85ead7bb7af21363157c4f",
    "date": "2023-11-08T02:51:18+00:00",
    "message": "CMake: Loosen CUDA consistency check (#113174)\n\nCloses #108931, closes #108932, see also conda-forge/pytorch-cpu-feedstock#203\n\nCurrently we compare `CUDA_INCLUDE_DIRS` and expect exact equality\nwith `CUDAToolkit_INCLUDE_DIR` however this fails in the presense of\nsymbolic links or for split installs where there are multiple include paths.\nGiven that, it makes sense to loosen the requirement to just version\nequality under the assumption that two installs of the same version\nshould still be compatible.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/113174\nApproved by: https://github.com/malfet",
    "changes": [
        {
            "name": "cuda.cmake",
            "path": "cmake/public/cuda.cmake",
            "patches": [
                {
                    "old_start": 60,
                    "old_length": 11,
                    "new_start": 60,
                    "new_length": 10,
                    "hunk_buggy": "[' \\n', ' cmake_policy(POP)\\n', ' \\n', '-if(NOT CMAKE_CUDA_COMPILER_VERSION STREQUAL CUDAToolkit_VERSION OR\\n', '-    NOT CUDA_INCLUDE_DIRS STREQUAL CUDAToolkit_INCLUDE_DIR)\\n', '-  message(FATAL_ERROR \"Found two conflicting CUDA installs:\\\\n\"\\n', '                       \"V${CMAKE_CUDA_COMPILER_VERSION} in \\'${CUDA_INCLUDE_DIRS}\\' and\\\\n\"\\n', '-                      \"V${CUDAToolkit_VERSION} in \\'${CUDAToolkit_INCLUDE_DIR}\\'\")\\n', ' endif()\\n', ' \\n', ' if(NOT TARGET CUDA::nvToolsExt)']",
                    "hunk_fix": "@@ -60,11 +60,10 @@ find_package(CUDAToolkit REQUIRED)\n \n cmake_policy(POP)\n \n-if(NOT CMAKE_CUDA_COMPILER_VERSION STREQUAL CUDAToolkit_VERSION OR\n-    NOT CUDA_INCLUDE_DIRS STREQUAL CUDAToolkit_INCLUDE_DIR)\n-  message(FATAL_ERROR \"Found two conflicting CUDA installs:\\n\"\n+if(NOT CMAKE_CUDA_COMPILER_VERSION VERSION_EQUAL CUDAToolkit_VERSION)\n+  message(FATAL_ERROR \"Found two conflicting CUDA versions:\\n\"\n                       \"V${CMAKE_CUDA_COMPILER_VERSION} in '${CUDA_INCLUDE_DIRS}' and\\n\"\n-                      \"V${CUDAToolkit_VERSION} in '${CUDAToolkit_INCLUDE_DIR}'\")\n+                      \"V${CUDAToolkit_VERSION} in '${CUDAToolkit_INCLUDE_DIRS}'\")\n endif()\n \n if(NOT TARGET CUDA::nvToolsExt)"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 347,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/e574a8ab55b2ac4266211d30d98d32d8b849ea86",
    "date": "2023-10-25T01:18:32+00:00",
    "message": "[dynamo] Add sanity checks to ensure no double-wrapping of `FakeTensor`s produced by the current graph (#111913)\n\nPartially fixes: https://github.com/pytorch/pytorch/issues/111873\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/111913\nApproved by: https://github.com/ezyang",
    "changes": [
        {
            "name": "builder.py",
            "path": "torch/_dynamo/variables/builder.py",
            "patches": [
                {
                    "old_start": 993,
                    "old_length": 6,
                    "new_start": 993,
                    "new_length": 13,
                    "hunk_buggy": "['                 guards=self.make_guards(GuardBuilder.CONSTANT_MATCH),\\n', '             )\\n', ' \\n', '     def wrap_tensor(self, value: torch.Tensor):\\n', '         source = self.get_source()\\n', ' \\n']",
                    "hunk_fix": "@@ -993,6 +993,13 @@ class VariableBuilder:\n                 guards=self.make_guards(GuardBuilder.CONSTANT_MATCH),\n             )\n \n+    def assert_not_wrapped_by_this_graph(self, value: torch.Tensor):\n+        if is_fake(value) and maybe_get_fake_mode(value) is self.tx.fake_mode:\n+            raise InternalTorchDynamoError(\n+                \"Cannot wrap a Tensor that has already been\",\n+                \"wrapped by this instance of Dynamo\",\n+            )\n+\n     def wrap_tensor(self, value: torch.Tensor):\n         source = self.get_source()\n \n"
                },
                {
                    "old_start": 1000,
                    "old_length": 6,
                    "new_start": 1007,
                    "new_length": 7,
                    "hunk_buggy": "['             source.guard_source().is_nn_module()\\n', '             or get_static_address_type(value) is not None\\n', '         ) and not source.guard_source().is_fsdp_module():\\n', '             return self.tx.output.register_attr_or_module(\\n', '                 value,\\n', '                 self.name,\\n']",
                    "hunk_fix": "@@ -1000,6 +1007,7 @@ class VariableBuilder:\n             source.guard_source().is_nn_module()\n             or get_static_address_type(value) is not None\n         ) and not source.guard_source().is_fsdp_module():\n+            self.assert_not_wrapped_by_this_graph(value)\n             return self.tx.output.register_attr_or_module(\n                 value,\n                 self.name,\n"
                },
                {
                    "old_start": 1009,
                    "old_length": 6,
                    "new_start": 1017,
                    "new_length": 7,
                    "hunk_buggy": "['             )\\n', ' \\n', '         if is_constant_source(source):\\n', '             return self.tx.output.register_attr_or_module(\\n', '                 value,\\n', '                 re.sub(r\"[^a-zA-Z0-9]+\", \"_\", self.name),\\n']",
                    "hunk_fix": "@@ -1009,6 +1017,7 @@ class VariableBuilder:\n             )\n \n         if is_constant_source(source):\n+            self.assert_not_wrapped_by_this_graph(value)\n             return self.tx.output.register_attr_or_module(\n                 value,\n                 re.sub(r\"[^a-zA-Z0-9]+\", \"_\", self.name),\n"
                },
                {
                    "old_start": 1069,
                    "old_length": 6,
                    "new_start": 1078,
                    "new_length": 9,
                    "hunk_buggy": "['                 stored_value = stored_value.add_guards(self.make_guards(dup_guard))\\n', '             return stored_value\\n', ' \\n', \"         # tx.output has multiple tracers if we're introspecting HigherOrderOperator.\\n\", \"         # When we've discovered an untracked tensor, then we actually need\\n\", '         # to get Dynamo to track the tensor (which is what this function does)']",
                    "hunk_fix": "@@ -1069,6 +1078,9 @@ class VariableBuilder:\n                 stored_value = stored_value.add_guards(self.make_guards(dup_guard))\n             return stored_value\n \n+        # By this point, we should have deduplicated all tensors\n+        self.assert_not_wrapped_by_this_graph(value)\n+\n         # tx.output has multiple tracers if we're introspecting HigherOrderOperator.\n         # When we've discovered an untracked tensor, then we actually need\n         # to get Dynamo to track the tensor (which is what this function does)"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 348,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/097defb1608827d82b18b27adeec0a98b72a9281",
    "date": "2023-10-12T03:37:18+00:00",
    "message": "[device mesh] only check when world size > num_devices per host (#111091)\n\nas titled\nPull Request resolved: https://github.com/pytorch/pytorch/pull/111091\nApproved by: https://github.com/awgu, https://github.com/wz337\nghstack dependencies: #110898, #110900",
    "changes": [
        {
            "name": "device_mesh.py",
            "path": "torch/distributed/_tensor/device_mesh.py",
            "patches": [
                {
                    "old_start": 188,
                    "old_length": 7,
                    "new_start": 188,
                    "new_length": 10,
                    "hunk_buggy": "['             # automatically set the current cuda/cuda-like device base on num of gpu devices available in each host\\n', '             # NOTE: This device selection would only work for homogeneous hardware.\\n', '             num_devices_per_host = device_handle.device_count()\\n', '-            if world_size % num_devices_per_host != 0:\\n', '                 raise RuntimeError(\\n', '                     f\"DeviceMesh only support homogeneous hardware, but found \"\\n', '                     f\"{world_size} ranks and {num_devices_per_host} {self.device_type} devices!\"']",
                    "hunk_fix": "@@ -188,7 +188,10 @@ class DeviceMesh:\n             # automatically set the current cuda/cuda-like device base on num of gpu devices available in each host\n             # NOTE: This device selection would only work for homogeneous hardware.\n             num_devices_per_host = device_handle.device_count()\n-            if world_size % num_devices_per_host != 0:\n+            if (\n+                world_size > num_devices_per_host\n+                and world_size % num_devices_per_host != 0\n+            ):\n                 raise RuntimeError(\n                     f\"DeviceMesh only support homogeneous hardware, but found \"\n                     f\"{world_size} ranks and {num_devices_per_host} {self.device_type} devices!\""
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 349,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/07f0f383fa23e63eca164036ab58ab983e9437eb",
    "date": "2023-10-12T02:14:38+00:00",
    "message": "update tensor-like to check instance for torch function impl (#111087)\n\ntensor like should check the instance for a torch function impl, not the type\nPull Request resolved: https://github.com/pytorch/pytorch/pull/111087\nApproved by: https://github.com/ezyang",
    "changes": [
        {
            "name": "overrides.py",
            "path": "torch/overrides.py",
            "patches": [
                {
                    "old_start": 1845,
                    "old_length": 7,
                    "new_start": 1845,
                    "new_length": 7,
                    "hunk_buggy": "['     >>> is_tensor_like(TensorLike())\\n', '     True\\n', '     \"\"\"\\n', '-    return type(inp) is torch.Tensor or hasattr(type(inp), \"__torch_function__\")\\n', ' \\n', ' class TorchFunctionMode:\\n', '     \"\"\"']",
                    "hunk_fix": "@@ -1845,7 +1845,7 @@ def is_tensor_like(inp):\n     >>> is_tensor_like(TensorLike())\n     True\n     \"\"\"\n-    return type(inp) is torch.Tensor or hasattr(type(inp), \"__torch_function__\")\n+    return type(inp) is torch.Tensor or hasattr(inp, \"__torch_function__\")\n \n class TorchFunctionMode:\n     \"\"\""
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 350,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/52e76a305677dfaf26cd5d59bd1aa239375f833c",
    "date": "2023-10-11T21:26:41+00:00",
    "message": "fix ShardedTensor.gather when shard is empty (#110962)\n\nSummary:\ncurrent ShardedTensor.gather is not working as expectation when the shard is empty on any rank\n\nThe root cause is identified that when a sharded tensor has no placement on a specific rank, the metadata doesn't include that rank's placement which introduces KeyError in :                 ```shard_offset = shard_placement[shard. Metadata][1]```\n\nIt's fixed by adding an empty tensor check.\n\nTest Plan:\nbefore change:\n\nafter change:\n\nDifferential Revision: D50114085\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110962\nApproved by: https://github.com/wz337",
    "changes": [
        {
            "name": "api.py",
            "path": "torch/distributed/_shard/sharded_tensor/api.py",
            "patches": [
                {
                    "old_start": 436,
                    "old_length": 6,
                    "new_start": 436,
                    "new_length": 9,
                    "hunk_buggy": "[' \\n', '             for shard in local_shards:\\n', '                 src = shard.tensor.flatten()\\n', '                 shard_offset = shard_placement[shard.metadata][1]\\n', '                 data[shard_offset: shard_offset + src.numel()].copy_(src)\\n', ' ']",
                    "hunk_fix": "@@ -436,6 +436,9 @@ class ShardedTensor(ShardedTensorBase):\n \n             for shard in local_shards:\n                 src = shard.tensor.flatten()\n+                if src.nelement() == 0 :\n+                    warnings.warn(\"Gathering a tensor with zero elements on rank \" + str(rank))\n+                    return\n                 shard_offset = shard_placement[shard.metadata][1]\n                 data[shard_offset: shard_offset + src.numel()].copy_(src)\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 351,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/468a73f0e3527c52495c864c7d48dc26684f6c0b",
    "date": "2023-10-10T01:46:33+00:00",
    "message": "Support Numpy ints in the `torch.nn.functional.interpolate` dtype check (#110778)\n\nIn https://github.com/pytorch/pytorch/pull/99243, a check was added to ensure the `size` only contained integers.\n\nThis PR updates the check to also include numpy integers based on this comment (cc @kit1980): https://github.com/pytorch/pytorch/pull/99243#issuecomment-1646736646. Similar to the other commenter, I also ran into issues where existing software broke due to this after upgrading to PT2.1:\n\n```\n                if not torch.jit.is_scripting():\n                    if not all(_is_integer(x) for x in size):\n>                       raise TypeError(\n                            \"expected size to be one of int or Tuple[int] or Tuple[int, int] or \"\n                            f\"Tuple[int, int, int], but got size with types {[type(x) for x in size]}\"\n                        )\nE                       TypeError: expected size to be one of int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int], but got size with types [<class 'numpy.int64'>, <class 'numpy.int64'>]\n\n/conda-env/lib/python3.8/site-packages/torch/nn/functional.py:3924: TypeError\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110778\nApproved by: https://github.com/mikaylagawarecki",
    "changes": [
        {
            "name": "functional.py",
            "path": "torch/nn/functional.py",
            "patches": [
                {
                    "old_start": 4,
                    "old_length": 6,
                    "new_start": 4,
                    "new_length": 11,
                    "hunk_buggy": "[' import warnings\\n', ' import importlib\\n', ' \\n', ' import torch\\n', ' from torch import _VF\\n', ' from torch import sym_int as _sym_int\\n']",
                    "hunk_fix": "@@ -4,6 +4,11 @@ import math\n import warnings\n import importlib\n \n+try:\n+    import numpy as np\n+except ModuleNotFoundError:\n+    np = None\n+\n import torch\n from torch import _VF\n from torch import sym_int as _sym_int\n"
                },
                {
                    "old_start": 3776,
                    "old_length": 10,
                    "new_start": 3781,
                    "new_length": 12,
                    "hunk_buggy": "[' \\n', ' def _is_integer(x) -> bool:\\n', '     r\"\"\"Type check the input number is an integer.\\n', '-    Will return True for int, SymInt and Tensors with integer elements.\\n', '     \"\"\"\\n', '     if isinstance(x, (int, torch.SymInt)):\\n', '         return True\\n', '     return isinstance(x, Tensor) and not x.is_floating_point()\\n', ' \\n', ' ']",
                    "hunk_fix": "@@ -3776,10 +3781,12 @@ if upsample.__doc__:\n \n def _is_integer(x) -> bool:\n     r\"\"\"Type check the input number is an integer.\n-    Will return True for int, SymInt and Tensors with integer elements.\n+    Will return True for int, SymInt, Numpy integers and Tensors with integer elements.\n     \"\"\"\n     if isinstance(x, (int, torch.SymInt)):\n         return True\n+    if np is not None and isinstance(x, np.integer):\n+        return True\n     return isinstance(x, Tensor) and not x.is_floating_point()\n \n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 352,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/6b4c686b9a33a1503a4a4133f9067dd31e0822f7",
    "date": "2023-10-08T04:06:44+00:00",
    "message": "[aotindutor] Forward fix a performance regression (#110800)\n\nSummary: Forward fix a performance regression caused by https://github.com/pytorch/pytorch/pull/110510. When a model is run once, all those kernel pointers are initialized and removing the if-nullptr check will cause those loadKernel be unnecessarily executed again when we rerun the foward function. Another way to do this is to codegen loadKernel in the initializer, which I may do in a later PR.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110800\nApproved by: https://github.com/jansel",
    "changes": [
        {
            "name": "wrapper.py",
            "path": "torch/_inductor/codegen/wrapper.py",
            "patches": [
                {
                    "old_start": 2021,
                    "old_length": 13,
                    "new_start": 2021,
                    "new_length": 17,
                    "hunk_buggy": "['         self, name: str, mangled_name: str, cubin_path: str, shared_mem: int\\n', '     ):\\n', '         if V.graph.aot_mode:\\n', '             self.writeline(\\n', '-                f\"\"\"kernels.{name} = loadKernel(\"{cubin_path}\", \"{mangled_name}\", {shared_mem}, this->cubin_dir_);\"\"\"\\n', '             )\\n', '         else:\\n', '             self.writeline(\\n', '-                f\"\"\"{name} = loadKernel(\"{cubin_path}\", \"{mangled_name}\", {shared_mem});\"\"\"\\n', '             )\\n', ' \\n', '     def generate_args_decl(self, call_args):\\n', '         dynamic_symbols = V.graph.sizevars.free_symbols()']",
                    "hunk_fix": "@@ -2021,13 +2021,17 @@ class CudaWrapperCodeGen(CppWrapperCodeGen):\n         self, name: str, mangled_name: str, cubin_path: str, shared_mem: int\n     ):\n         if V.graph.aot_mode:\n+            self.writeline(f\"if (kernels.{name} == nullptr) {{\")\n             self.writeline(\n-                f\"\"\"kernels.{name} = loadKernel(\"{cubin_path}\", \"{mangled_name}\", {shared_mem}, this->cubin_dir_);\"\"\"\n+                f\"\"\"    kernels.{name} = loadKernel(\"{cubin_path}\", \"{mangled_name}\", {shared_mem}, this->cubin_dir_);\"\"\"\n             )\n+            self.writeline(\"}\")\n         else:\n+            self.writeline(f\"if ({name} == nullptr) {{\")\n             self.writeline(\n-                f\"\"\"{name} = loadKernel(\"{cubin_path}\", \"{mangled_name}\", {shared_mem});\"\"\"\n+                f\"\"\"    {name} = loadKernel(\"{cubin_path}\", \"{mangled_name}\", {shared_mem});\"\"\"\n             )\n+            self.writeline(\"}\")\n \n     def generate_args_decl(self, call_args):\n         dynamic_symbols = V.graph.sizevars.free_symbols()"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 353,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/a8653f35de02c7fb038e3c184dda6e67a12a39e2",
    "date": "2023-10-05T02:42:57+00:00",
    "message": "One more small Perf Tweak to fill_ (#110294)\n\n# Summary\nPerf win by check which device tensors are on\n\n## Before this PR:\n``` Shell\nCPU | CPU: 1.3328152848407626\nGPU | GPU: 6.614773320034146\nCPU | GPU: 29.027153505012393\nGPU | CPU: 17.22372299991548\n```\n## After this PR\n``` Shell\nCPU | CPU: 1.4241038949694484\nGPU | GPU: 7.060713530518115\nCPU | GPU: 15.149936103262007\nGPU | CPU: 5.774620908778161\n```\n\n#### Repro Script\n``` Python\n    a = torch.tensor([0.2, 0.5], device=\"cpu\")\n    amax = torch.tensor(0.5, device=\"cpu\")\n    print(f\"CPU | CPU: {benchmark_torch_function_in_microseconds(torch.fill_, a, amax)}\")\n\n    a = torch.tensor([0.2, 0.5], device=\"cuda\")\n    amax = torch.tensor(0.5, device=\"cuda\")\n    print(f\"GPU | GPU: {benchmark_torch_function_in_microseconds(torch.fill_, a, amax)}\")\n\n    a = torch.tensor([0.2, 0.5], device=\"cpu\")\n    amax = torch.tensor(0.5, device=\"cuda\")\n    print(f\"CPU | GPU: {benchmark_torch_function_in_microseconds(torch.fill_, a, amax)}\")\n\n    a = torch.tensor([0.2, 0.5], device=\"cuda\")\n    amax = torch.tensor(0.5, device=\"cpu\")\n    print(f\"GPU | CPU: {benchmark_torch_function_in_microseconds(torch.fill_, a, amax)}\")\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110294\nApproved by: https://github.com/mikaylagawarecki",
    "changes": [
        {
            "name": "Fill.cpp",
            "path": "aten/src/ATen/native/Fill.cpp",
            "patches": [
                {
                    "old_start": 55,
                    "old_length": 6,
                    "new_start": 55,
                    "new_length": 9,
                    "hunk_buggy": "[' \\n', ' Tensor& fill_(Tensor& self, const Tensor& value) {\\n', '   TORCH_CHECK(value.dim() == 0, \"fill_ only supports 0-dimension value tensor but got tensor with \", value.dim(), \" dimensions.\");\\n', '   // Check if value is a view of self and if it is we clone\\n', '   // it to avoid overwriting self prematurely\\n', '   if(self.is_alias_of(value)) {']",
                    "hunk_fix": "@@ -55,6 +55,9 @@ Tensor& fill_quantized_(Tensor& self, const Scalar& value) {\n \n Tensor& fill_(Tensor& self, const Tensor& value) {\n   TORCH_CHECK(value.dim() == 0, \"fill_ only supports 0-dimension value tensor but got tensor with \", value.dim(), \" dimensions.\");\n+  if (self.device() != value.device()){\n+    return fill_out(self, value.item());\n+  }\n   // Check if value is a view of self and if it is we clone\n   // it to avoid overwriting self prematurely\n   if(self.is_alias_of(value)) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 354,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/fd6c993eeaacda7ef6b83f59ad3474aed0d52eaf",
    "date": "2023-10-02T17:34:31+00:00",
    "message": "Add missing CUDA error check (#110368)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/110368\nApproved by: https://github.com/Skylion007",
    "changes": [
        {
            "name": "flash_bwd_launch_template.h",
            "path": "aten/src/ATen/native/transformers/cuda/flash_attn/flash_bwd_launch_template.h",
            "patches": [
                {
                    "old_start": 182,
                    "old_length": 6,
                    "new_start": 182,
                    "new_length": 9,
                    "hunk_buggy": "['     int max_smem_per_block;\\n', '     cudaError status_ = cudaDeviceGetAttribute(\\n', '         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\\n', '     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\\n', '         if (max_smem_per_block >= 2 * ((3 * 128 + 2 * 128) * Headdim + 2 * 128 * 128)) { // 104 KB\\n', '             if constexpr(!Is_dropout) {  // We can afford more registers to keep V in registers\\n']",
                    "hunk_fix": "@@ -182,6 +182,9 @@ void run_mha_bwd_hdim32(Flash_bwd_params &params, cudaStream_t stream, const boo\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         if (max_smem_per_block >= 2 * ((3 * 128 + 2 * 128) * Headdim + 2 * 128 * 128)) { // 104 KB\n             if constexpr(!Is_dropout) {  // We can afford more registers to keep V in registers\n"
                },
                {
                    "old_start": 203,
                    "old_length": 6,
                    "new_start": 206,
                    "new_length": 9,
                    "hunk_buggy": "['     int max_smem_per_block;\\n', '     cudaError status_ = cudaDeviceGetAttribute(\\n', '         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\\n', '     // printf(\"max_smem_per_block = %d\\\\n\", max_smem_per_block);\\n', '     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\\n', '         // Changing AtomLayoutMdQ from 2 to 4 takes the same time\\n']",
                    "hunk_fix": "@@ -203,6 +206,9 @@ void run_mha_bwd_hdim64(Flash_bwd_params &params, cudaStream_t stream, const boo\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     // printf(\"max_smem_per_block = %d\\n\", max_smem_per_block);\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         // Changing AtomLayoutMdQ from 2 to 4 takes the same time\n"
                },
                {
                    "old_start": 246,
                    "old_length": 6,
                    "new_start": 252,
                    "new_length": 9,
                    "hunk_buggy": "['     int max_smem_per_block;\\n', '     cudaError status_ = cudaDeviceGetAttribute(\\n', '         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\\n', '     // printf(\"max_smem_per_block = %d\\\\n\", max_smem_per_block);\\n', '     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\\n', '         // if (params.h == params.h_k) {\\n']",
                    "hunk_fix": "@@ -246,6 +252,9 @@ void run_mha_bwd_hdim96(Flash_bwd_params &params, cudaStream_t stream, const boo\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     // printf(\"max_smem_per_block = %d\\n\", max_smem_per_block);\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         // if (params.h == params.h_k) {\n"
                },
                {
                    "old_start": 273,
                    "old_length": 6,
                    "new_start": 282,
                    "new_length": 9,
                    "hunk_buggy": "['     int max_smem_per_block;\\n', '     cudaError status_ = cudaDeviceGetAttribute(\\n', '         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\\n', '     // printf(\"max_smem_per_block = %d\\\\n\", max_smem_per_block);\\n', '     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\\n', '         // if (params.h == params.h_k) {\\n']",
                    "hunk_fix": "@@ -273,6 +282,9 @@ void run_mha_bwd_hdim128(Flash_bwd_params &params, cudaStream_t stream, const bo\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     // printf(\"max_smem_per_block = %d\\n\", max_smem_per_block);\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         // if (params.h == params.h_k) {\n"
                },
                {
                    "old_start": 309,
                    "old_length": 6,
                    "new_start": 321,
                    "new_length": 9,
                    "hunk_buggy": "['     int max_smem_per_block;\\n', '     cudaError status_ = cudaDeviceGetAttribute(\\n', '         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\\n', '     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\\n', '         if (max_smem_per_block >= 116 * 1024) {\\n', '             run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 64, 64, 8, 4, 4, 4, false, false, T>, Is_dropout>(params, stream, configure);\\n']",
                    "hunk_fix": "@@ -309,6 +321,9 @@ void run_mha_bwd_hdim160(Flash_bwd_params &params, cudaStream_t stream, const bo\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         if (max_smem_per_block >= 116 * 1024) {\n             run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 64, 64, 8, 4, 4, 4, false, false, T>, Is_dropout>(params, stream, configure);\n"
                },
                {
                    "old_start": 326,
                    "old_length": 6,
                    "new_start": 341,
                    "new_length": 9,
                    "hunk_buggy": "['     int max_smem_per_block;\\n', '     cudaError status_ = cudaDeviceGetAttribute(\\n', '         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\\n', '     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\\n', '         if (max_smem_per_block >= 136 * 1024) {\\n', '             run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 64, 64, 8, 4, 2, 2, false, false, T>, Is_dropout>(params, stream, configure);\\n']",
                    "hunk_fix": "@@ -326,6 +341,9 @@ void run_mha_bwd_hdim192(Flash_bwd_params &params, cudaStream_t stream, const bo\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         if (max_smem_per_block >= 136 * 1024) {\n             run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 64, 64, 8, 4, 2, 2, false, false, T>, Is_dropout>(params, stream, configure);\n"
                },
                {
                    "old_start": 351,
                    "old_length": 6,
                    "new_start": 369,
                    "new_length": 9,
                    "hunk_buggy": "['     int max_smem_per_block;\\n', '     cudaError status_ = cudaDeviceGetAttribute(\\n', '         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\\n', '     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\\n', '         if (max_smem_per_block >= 176 * 1024) {  // H100\\n', '             run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 64, 64, 8, 4, 2, 2, false, false, T>, Is_dropout>(params, stream, configure);']",
                    "hunk_fix": "@@ -351,6 +369,9 @@ void run_mha_bwd_hdim256(Flash_bwd_params &params, cudaStream_t stream, const bo\n     int max_smem_per_block;\n     cudaError status_ = cudaDeviceGetAttribute(\n         &max_smem_per_block, cudaDevAttrMaxSharedMemoryPerBlockOptin, device);\n+    if (status_ != cudaSuccess) {\n+      C10_CUDA_CHECK(status_);\n+    }\n     BOOL_SWITCH(params.p_dropout < 1.f, Is_dropout, [&] {\n         if (max_smem_per_block >= 176 * 1024) {  // H100\n             run_flash_bwd<Flash_bwd_kernel_traits<Headdim, 64, 64, 8, 4, 2, 2, false, false, T>, Is_dropout>(params, stream, configure);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 355,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/175ccfc4c8443bcc65c87d9c942272d3ebf16b0b",
    "date": "2023-09-21T23:19:17+00:00",
    "message": "Verify flatbuffer module fields are initialized (#109794)\n\nFixes #109793\n\nAdd validation on flatbuffer module field to prevent segfault\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109794\nApproved by: https://github.com/malfet",
    "changes": [
        {
            "name": "flatbuffer_loader.cpp",
            "path": "torch/csrc/jit/mobile/flatbuffer_loader.cpp",
            "patches": [
                {
                    "old_start": 291,
                    "old_length": 9,
                    "new_start": 291,
                    "new_length": 11,
                    "hunk_buggy": "['   module_parsed_ = false;\\n', ' \\n', '   const auto* ivalues = module->ivalues();\\n', '-  TORCH_CHECK(ivalues != nullptr, \"Corrupted ivalues field\")\\n', '   TORCH_CHECK(\\n', '-      reinterpret_cast<const char*>(ivalues) < end, \"Corrupted ivalues field\")\\n', '   all_ivalues_.resize(ivalues->size());\\n', '   all_types_.resize(module->object_types()->size());\\n', '   storages_.resize(module->storage_data_size());']",
                    "hunk_fix": "@@ -291,9 +291,11 @@ mobile::Module FlatbufferLoader::parseModule(\n   module_parsed_ = false;\n \n   const auto* ivalues = module->ivalues();\n-  TORCH_CHECK(ivalues != nullptr, \"Corrupted ivalues field\")\n   TORCH_CHECK(\n-      reinterpret_cast<const char*>(ivalues) < end, \"Corrupted ivalues field\")\n+      ivalues && module->object_types(),\n+      \"Parsing flatbuffer module: Corrupted ivalues/object_types field\");\n+  TORCH_CHECK(\n+      reinterpret_cast<const char*>(ivalues) < end, \"Corrupted ivalues field\");\n   all_ivalues_.resize(ivalues->size());\n   all_types_.resize(module->object_types()->size());\n   storages_.resize(module->storage_data_size());"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 356,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/871b5caae76185cff141c522b3133e7543c8dabf",
    "date": "2023-09-19T00:10:51+00:00",
    "message": "Fix hpu deserialization bug (#109499)\n\n# Motivation\nfix hpu deserialization bug. It should check hpu model if and only if location start with hpu. Otherwise, it always raise an AssertError if hpu is not imported. This break the serialization/desirialization functionality abourt other third-party like IPEX.\n\n# Solution\nonly assert hpu model when start with hpu\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109499\nApproved by: https://github.com/ezyang",
    "changes": [
        {
            "name": "serialization.py",
            "path": "torch/serialization.py",
            "patches": [
                {
                    "old_start": 292,
                    "old_length": 9,
                    "new_start": 292,
                    "new_length": 9,
                    "hunk_buggy": "[' \\n', ' \\n', ' def _hpu_deserialize(obj, location):\\n', '-    hpu = getattr(torch, \"hpu\", None)\\n', '-    assert hpu is not None, \"HPU device module is not loaded\"\\n', \"     if location.startswith('hpu'):\\n\", '         device = validate_hpu_device(location)\\n', '         if getattr(obj, \"_torch_load_uninitialized\", False):\\n', '             with hpu.device(device):']",
                    "hunk_fix": "@@ -292,9 +292,9 @@ def validate_hpu_device(location):\n \n \n def _hpu_deserialize(obj, location):\n-    hpu = getattr(torch, \"hpu\", None)\n-    assert hpu is not None, \"HPU device module is not loaded\"\n     if location.startswith('hpu'):\n+        hpu = getattr(torch, \"hpu\", None)\n+        assert hpu is not None, \"HPU device module is not loaded\"\n         device = validate_hpu_device(location)\n         if getattr(obj, \"_torch_load_uninitialized\", False):\n             with hpu.device(device):"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 357,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/55c19a3c6d38a49fe34e008c4c566445c43810f0",
    "date": "2023-09-16T10:02:56+00:00",
    "message": "Inductor: Increase multiplier to 3 for Inductor AMP benchmark correctness check (#109097)\n\n**Summary**\nAs reported in https://github.com/pytorch/pytorch/issues/108333, we find some of the models have failed the benchmark's correctness check. However, the end-to-end model's accuracy ([test script](https://gist.github.com/leslie-fang-intel/aac8b3c2b450532fd0517c758bb845e0)) when comparing AMP with FP32 is within a difference of less than 0.1%. Thus, it's possible that the correctness check failures for these models are false alarms. We use multiplier of 3 instead of 2 in this PR to avoid these false alarms. Model end-to-end accuracy test results are:\n\n<html xmlns:v=\"urn:schemas-microsoft-com:vml\"\nxmlns:o=\"urn:schemas-microsoft-com:office:office\"\nxmlns:x=\"urn:schemas-microsoft-com:office:excel\"\nxmlns=\"http://www.w3.org/TR/REC-html40\">\n\n<head>\n\n<meta name=ProgId content=Excel.Sheet>\n<meta name=Generator content=\"Microsoft Excel 15\">\n<link id=Main-File rel=Main-File\nhref=\"file:///C:/Users/jiahaofa/AppData/Local/Temp/msohtmlclip1/01/clip.htm\">\n<link rel=File-List\nhref=\"file:///C:/Users/jiahaofa/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml\">\n</head>\n\n<body link=\"#0563C1\" vlink=\"#954F72\">\n\nSPR | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0 | \u00a0\n-- | -- | -- | -- | -- | -- | --\n\u00a0 | FP32 Imperative TOP1 Accuracy | FP32 Imperative TOP5 Accuracy | BF16 AMP Inductor TOP1 Accuracy | BF16 AMP Inductor TOP5 Accuracy | BF16/FP32 Relative Loss TOP1 Accuracy | BF16/FP32 Relative Loss TOP5 Accuracy\ngluon_inception_v3 | 73.262 | 90.774 | 73.256 | 90.802 | -0.01% | 0.03%\nmobilenetv2_100 | 72.89 | 90.996 | 72.826 | 90.946 | -0.09% | -0.05%\nmobilenetv3_large_100 | 75.72 | 92.55 | 75.764 | 92.554 | 0.06% | 0.00%\n\n</body>\n\n</html>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109097\nApproved by: https://github.com/jgong5, https://github.com/jansel",
    "changes": [
        {
            "name": "utils.py",
            "path": "torch/_dynamo/utils.py",
            "patches": [
                {
                    "old_start": 1062,
                    "old_length": 7,
                    "new_start": 1062,
                    "new_length": 13,
                    "hunk_buggy": "['                     )\\n', ' \\n', '                 res_error = rmse(fp64_ref, res).item()\\n', '-                multiplier = 2.0\\n', ' \\n', '                 if (\\n', '                     fp64_ref.numel() < 1000']",
                    "hunk_fix": "@@ -1062,7 +1062,13 @@ def same(\n                     )\n \n                 res_error = rmse(fp64_ref, res).item()\n-                multiplier = 2.0\n+\n+                # In the case of using AMP (Automatic Mixed Precision), certain models have\n+                # failed the benchmark's correctness check. However, the end-to-end model's\n+                # accuracy when comparing AMP with FP32 is within a difference of less than 0.1%.\n+                # Thus, it's possible that the correctness check failures for these models are\n+                # false alarms. We use multiplier of 3 instead of 2 to avoid these false alarms.\n+                multiplier = 3.0 if res.dtype == torch.bfloat16 else 2.0\n \n                 if (\n                     fp64_ref.numel() < 1000"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 358,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/62732bdcdb8b6112e01366d4ad1c2a50e61da1ed",
    "date": "2023-09-14T01:40:49+00:00",
    "message": "[ez][inductor][fx passes] quick fix for invalid nodes (#109234)\n\nSummary: As title.Need to check whether node is valid before fusion\n\nTest Plan: To add test\n\nDifferential Revision: D49241525\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/109234\nApproved by: https://github.com/yanboliang",
    "changes": [
        {
            "name": "group_batch_fusion.py",
            "path": "torch/_inductor/fx_passes/group_batch_fusion.py",
            "patches": [
                {
                    "old_start": 214,
                    "old_length": 6,
                    "new_start": 214,
                    "new_length": 8,
                    "hunk_buggy": "['     weight = get_arg_value(node, 1, \"weight\")\\n', '     return (\\n', '         is_node_meta_valid(node)\\n', '         and len(input.meta[\"example_value\"].shape) == 2\\n', '         and len(weight.meta[\"example_value\"].shape) == 2\\n', '     )']",
                    "hunk_fix": "@@ -214,6 +214,8 @@ def is_linear_node_can_be_fused(node):\n     weight = get_arg_value(node, 1, \"weight\")\n     return (\n         is_node_meta_valid(node)\n+        and is_node_meta_valid(input)\n+        and is_node_meta_valid(weight)\n         and len(input.meta[\"example_value\"].shape) == 2\n         and len(weight.meta[\"example_value\"].shape) == 2\n     )"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 359,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/d471eaeb1d2fbc7efcde6408d7d1e513b969af25",
    "date": "2023-09-06T00:02:42+00:00",
    "message": "fix inline_container.cc inplace loading (#108573)\n\nSummary:\nbypass-github-pytorch-ci-checks\nbypass-github-export-checks\nforce-merge-on-github\n\nDifferential Revision: D48971847\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/108573\nApproved by: https://github.com/wqfish",
    "changes": [
        {
            "name": "inline_container.cc",
            "path": "caffe2/serialize/inline_container.cc",
            "patches": [
                {
                    "old_start": 368,
                    "old_length": 7,
                    "new_start": 368,
                    "new_length": 11,
                    "hunk_buggy": "['       iter != nullptr,\\n', '       \"Failed to create zip reader iter: \",\\n', '       mz_zip_get_error_string(mz_zip_get_last_error(ar_.get())));\\n', '-\\n', '   for (size_t offset = 0; offset < stat.m_uncomp_size; offset += chunk_size) {\\n', '     size_t want_size =\\n', '         std::min(chunk_size, (size_t)stat.m_uncomp_size - offset);']",
                    "hunk_fix": "@@ -368,7 +368,11 @@ size_t PyTorchStreamReader::getRecord(\n       iter != nullptr,\n       \"Failed to create zip reader iter: \",\n       mz_zip_get_error_string(mz_zip_get_last_error(ar_.get())));\n-\n+  std::vector<uint8_t> buffer;\n+  if (buf == nullptr) {\n+    buffer.resize(chunk_size);\n+    buf = buffer.data();\n+  }\n   for (size_t offset = 0; offset < stat.m_uncomp_size; offset += chunk_size) {\n     size_t want_size =\n         std::min(chunk_size, (size_t)stat.m_uncomp_size - offset);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 360,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/eb8659fe81f3d4b061674bf149a6805cd292db8d",
    "date": "2023-08-31T20:21:20+00:00",
    "message": "pass inference accuracy check for detectron2_fcos_r_50_fpn (#108328)\n\nWe need a higher tolerance to pass the inference accuracy check for detectron2_fcos_r_50_fpn .\n\nCommand:\n```\npython benchmarks/dynamo/torchbench.py --backend inductor --bfloat16 --accuracy --only detectron2_fcos_r_50_fpn --disable-cudagraphs --inference\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/108328\nApproved by: https://github.com/jansel",
    "changes": [
        {
            "name": "torchbench.py",
            "path": "benchmarks/dynamo/torchbench.py",
            "patches": [
                {
                    "old_start": 145,
                    "old_length": 6,
                    "new_start": 145,
                    "new_length": 11,
                    "hunk_buggy": "['     \"drq\",\\n', ' }\\n', ' \\n', ' REQUIRE_COSINE_TOLERACE = {\\n', '     # Just keeping it here even though its empty, if we need this in future.\\n', ' }\\n']",
                    "hunk_fix": "@@ -145,6 +145,11 @@ REQUIRE_HIGHER_FP16_TOLERANCE = {\n     \"drq\",\n }\n \n+\n+REQUIRE_HIGHER_BF16_TOLERANCE = {\n+    \"detectron2_fcos_r_50_fpn\",\n+}\n+\n REQUIRE_COSINE_TOLERACE = {\n     # Just keeping it here even though its empty, if we need this in future.\n }\n"
                },
                {
                    "old_start": 440,
                    "old_length": 6,
                    "new_start": 445,
                    "new_length": 11,
                    "hunk_buggy": "['             if name in REQUIRE_HIGHER_FP16_TOLERANCE:\\n', '                 return 1e-2, cosine\\n', '             return 1e-3, cosine\\n', '         if is_training and current_device == \"cuda\":\\n', '             tolerance = 1e-3\\n', '             if name in REQUIRE_COSINE_TOLERACE:']",
                    "hunk_fix": "@@ -440,6 +445,11 @@ class TorchBenchmarkRunner(BenchmarkRunner):\n             if name in REQUIRE_HIGHER_FP16_TOLERANCE:\n                 return 1e-2, cosine\n             return 1e-3, cosine\n+\n+        if self.args.bfloat16:\n+            if name in REQUIRE_HIGHER_BF16_TOLERANCE:\n+                return 1e-2, cosine\n+\n         if is_training and current_device == \"cuda\":\n             tolerance = 1e-3\n             if name in REQUIRE_COSINE_TOLERACE:"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 361,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/e31038d574712d383fdc4c2f1bb63fc82f256ed0",
    "date": "2023-08-30T14:55:18+00:00",
    "message": "Check results dtype in index_out (#108167)\n\nThis logic exists for index_put and index_add, but for some reason not for `index.out`\nSkip testing, as this function is not technically exposed on the Python level.\n\n<!--\ncopilot:poem\n-->\n### <samp>\ud83e\udd16 Generated by Copilot at c688cfd</samp>\n\n> _`index_out` checks types_\n> _avoiding errors in autumn_\n> _complex tensors work_\n\nFixes https://github.com/pytorch/pytorch/issues/107698\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/108167\nApproved by: https://github.com/albanD",
    "changes": [
        {
            "name": "TensorAdvancedIndexing.cpp",
            "path": "aten/src/ATen/native/TensorAdvancedIndexing.cpp",
            "patches": [
                {
                    "old_start": 443,
                    "old_length": 6,
                    "new_start": 443,
                    "new_length": 9,
                    "hunk_buggy": "['   const auto& result = maybe_get_output();\\n', ' \\n', '   if (result.defined()) {\\n', '     at::assert_no_internal_overlap(result);\\n', '     at::assert_no_overlap(result, self);\\n', '     for (const at::OptionalTensorRef& index : materialized) {']",
                    "hunk_fix": "@@ -443,6 +443,9 @@ TORCH_PRECOMPUTE_META_FUNC2(index, Tensor)\n   const auto& result = maybe_get_output();\n \n   if (result.defined()) {\n+    TORCH_CHECK(self.scalar_type() == result.scalar_type(),\n+                \"index_out: self (\", self.scalar_type(), \") and result (\", result.scalar_type(),\n+                \") must have the same scalar type\");\n     at::assert_no_internal_overlap(result);\n     at::assert_no_overlap(result, self);\n     for (const at::OptionalTensorRef& index : materialized) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 362,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/db1ac4e29b0f557711190c8d49d4afb5da1844e8",
    "date": "2023-07-06T09:52:48+00:00",
    "message": "fix functional collective's allgather for gloo (#104681)\n\nSummary: We should explicitly check for the gloo backend instead of relying on the shard's device, because user might pass a GPU tensor as input and a process group gloo as the pg, and expect that should work.\n\nDifferential Revision: D47249172\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/104681\nApproved by: https://github.com/rohan-varma, https://github.com/fduwjj",
    "changes": [
        {
            "name": "_functional_collectives.py",
            "path": "torch/distributed/_functional_collectives.py",
            "patches": [
                {
                    "old_start": 461,
                    "old_length": 7,
                    "new_start": 461,
                    "new_length": 7,
                    "hunk_buggy": "['     out_tensor = shard.new_empty(out_size)\\n', '     assert out_tensor.is_contiguous()\\n', \"     # FIXME gloo doesn't support _allgather_base\\n\", '-    if shard.is_cpu:\\n', '         tensor_list = list(torch.chunk(out_tensor, group_size))\\n', '         work = dist.all_gather(tensor_list, shard, group=group, async_op=True)\\n', '     else:']",
                    "hunk_fix": "@@ -461,7 +461,7 @@ def _all_gather_into_tensor(shard, tag, ranks, group_size):\n     out_tensor = shard.new_empty(out_size)\n     assert out_tensor.is_contiguous()\n     # FIXME gloo doesn't support _allgather_base\n-    if shard.is_cpu:\n+    if dist.get_backend(group) == dist.Backend.GLOO or shard.is_cpu:\n         tensor_list = list(torch.chunk(out_tensor, group_size))\n         work = dist.all_gather(tensor_list, shard, group=group, async_op=True)\n     else:"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 363,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/a5ca445f7953711bc90c111c3cad2ec87f02e74a",
    "date": "2023-06-30T22:53:49+00:00",
    "message": "Check for corrupted ivalues. (#104243)\n\nHi! We've been fuzzing torchvision project with [sydr-fuzz](https://github.com/ispras/oss-sydr-fuzz).\nWe've found a SEGV error at address 0x0 at `vector.h:163` in pytorch third-party project flatbuffers.\n\nThe error occurs because the `ivalues` field of flatbuffer module can be null, so the corresponding check must be inserted.\n\ntorchvision version: 9d0a93eee90bf7c401b74ebf9c8be80346254f15\n\npytorch version: 0f1621df1a0a73956c7ce4e2f72f069e610e0137\n\nOS: Ubuntu 20.04\n\nHow to reproduce\n\n1. Build docker from [here](https://github.com/ispras/oss-sydr-fuzz/tree/master/projects/torchvision) and run the container:\n\n        sudo docker build -t oss-sydr-fuzz-torchvision .\n        sudo docker run --privileged --rm -v `pwd`:/fuzz -it oss-sydr-fuzz-torchvision /bin/bash\n\n2. Run the target on this input:\n[malformed-module.txt](https://github.com/pytorch/pytorch/files/11879653/malformed-module.txt)\n\n        /encode_png_fuzz malformed-module.txt\n\n3. You will see the following output:\n\n        AddressSanitizer:DEADLYSIGNAL\n        =================================================================\n        ==1154==ERROR: AddressSanitizer: SEGV on unknown address 0x000000000000 (pc 0x00000d17cc61 bp 0x7ffcbe8637f0 sp 0x7ffcbe863660 T0)\n        ==1154==The signal is caused by a READ memory access.\n        ==1154==Hint: address points to the zero page.\n            #0 0xd17cc61 in flatbuffers::Vector<flatbuffers::Offset<torch::jit::mobile::serialization::IValue> >::size() const /pytorch/third_party/flatbuffers/include/flatbuffers/vector.h:163:48\n            #1 0xd17cc61 in torch::jit::(anonymous namespace)::FlatbufferLoader::parseModule(torch::jit::mobile::serialization::Module*) /pytorch/torch/csrc/jit/mobile/flatbuffer_loader.cpp:293:32\n            #2 0xd17dd23 in torch::jit::parse_and_initialize_mobile_module_for_jit(void*, unsigned long, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >&, std::vector<c10::IValue, std::allocator<c10::IValue> >&, c10::optional<c10::Device>, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >*) /pytorch/torch/csrc/jit/mobile/flatbuffer_loader.cpp:809:29\n            #3 0xdd661b4 in torch::jit::parse_and_initialize_jit_module(std::shared_ptr<char>, unsigned long, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >&, c10::optional<c10::Device>) /pytorch/torch/csrc/jit/serialization/import.cpp:345:28\n            #4 0xdd6b24a in torch::jit::_load_jit_module_from_bytes(std::shared_ptr<char>, unsigned long, std::shared_ptr<torch::jit::CompilationUnit>, c10::optional<c10::Device>, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >&, bool) /pytorch/torch/csrc/jit/serialization/import.cpp:547:14\n            #5 0xdd6c6df in torch::jit::import_ir_module(std::shared_ptr<torch::jit::CompilationUnit>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, c10::optional<c10::Device>, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >&, bool, bool) /pytorch/torch/csrc/jit/serialization/import.cpp:443:10\n            #6 0xdd6c1c7 in torch::jit::import_ir_module(std::shared_ptr<torch::jit::CompilationUnit>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, c10::optional<c10::Device>, bool) /pytorch/torch/csrc/jit/serialization/import.cpp:421:10\n            #7 0xdd6dce4 in torch::jit::load(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, c10::optional<c10::Device>, bool) /pytorch/torch/csrc/jit/serialization/import.cpp:503:10\n            #8 0xf2d3f75 in torch::serialize::InputArchive::load_from(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, c10::optional<c10::Device>) /pytorch/torch/csrc/api/src/serialize/input-archive.cpp:97:13\n            #9 0x60509c in void torch::load<at::Tensor, char*&>(at::Tensor&, char*&) /pytorch/torch/include/torch/csrc/api/include/torch/serialize.h:107:11\n            #10 0x6036be in LLVMFuzzerTestOneInput /vision/encode_png.cc:38:5\n            #11 0x66b041 in fuzzer::Fuzzer::ExecuteCallback(unsigned char const*, unsigned long) /llvm-project-llvmorg-14.0.6/compiler-rt/lib/fuzzer/FuzzerLoop.cpp:611:15\n            #12 0x6544cc in fuzzer::RunOneTest(fuzzer::Fuzzer*, char const*, unsigned long) /llvm-project-llvmorg-14.0.6/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:324:6\n            #13 0x65a61b in fuzzer::FuzzerDriver(int*, char***, int (*)(unsigned char const*, unsigned long)) /llvm-project-llvmorg-14.0.6/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:860:9\n            #14 0x654222 in main /llvm-project-llvmorg-14.0.6/compiler-rt/lib/fuzzer/FuzzerMain.cpp:20:10\n            #15 0x7f0c87b9c082 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x24082) (BuildId: 1878e6b475720c7c51969e69ab2d276fae6d1dee)\n            #16 0x542cdd in _start (/encode_png_fuzz+0x542cdd)\n\n        AddressSanitizer can not provide additional info.\n        SUMMARY: AddressSanitizer: SEGV /pytorch/third_party/flatbuffers/include/flatbuffers/vector.h:163:48 in flatbuffers::Vector<flatbuffers::Offset<torch::jit::mobile::serialization::IValue> >::size() const\n        ==1154==ABORTING\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/104243\nApproved by: https://github.com/kit1980",
    "changes": [
        {
            "name": "flatbuffer_loader.cpp",
            "path": "torch/csrc/jit/mobile/flatbuffer_loader.cpp",
            "patches": [
                {
                    "old_start": 290,
                    "old_length": 6,
                    "new_start": 290,
                    "new_length": 7,
                    "hunk_buggy": "['   module_parsed_ = false;\\n', ' \\n', '   const auto* ivalues = module->ivalues();\\n', '   all_ivalues_.resize(ivalues->size());\\n', '   all_types_.resize(module->object_types()->size());\\n', '   storages_.resize(module->storage_data_size());']",
                    "hunk_fix": "@@ -290,6 +290,7 @@ mobile::Module FlatbufferLoader::parseModule(\n   module_parsed_ = false;\n \n   const auto* ivalues = module->ivalues();\n+  TORCH_CHECK(ivalues != nullptr, \"Corrupted ivalues field\")\n   all_ivalues_.resize(ivalues->size());\n   all_types_.resize(module->object_types()->size());\n   storages_.resize(module->storage_data_size());"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 364,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/a69f427f957a37eee9c1dd5df681f30ab38ed3e4",
    "date": "2023-06-26T22:01:27+00:00",
    "message": "aten: Ensure dim is size_t (#104201)\n\nAttempts to fix failures introduced in https://github.com/pytorch/pytorch/pull/103930 (example failures: https://github.com/pytorch/pytorch/actions/runs/5363450214/jobs/9731034104)\n\n<!--\ncopilot:all\n-->\n### <samp>\ud83e\udd16 Generated by Copilot at 67d5076</samp>\n\n### Summary\n\ud83d\udd27\ud83d\udea8\ud83d\udea6\n\n<!--\n1.  \ud83d\udd27 (wrench) - This emoji can be used to indicate a bug fix or a minor improvement to the code quality or performance.\n2.  \ud83d\udea8 (rotating light) - This emoji can be used to indicate a change that affects the error handling or validation logic of the code, or that adds or modifies a test case.\n3.  \ud83d\udea6 (vertical traffic light) - This emoji can be used to indicate a change that affects the control flow or branching logic of the code, or that adds or modifies a condition or assertion.\n-->\nFix a compiler warning in `Expand.cpp` by casting a tensor dimension to `size_t`. This improves the code quality and correctness of the `expand` function for the Vulkan backend.\n\n> _`expand` tensor_\n> _cast `dim()` to `size_t`_\n> _autumn leaves warning_\n\n### Walkthrough\n*  Cast `self.dim()` to `size_t` to avoid signed-unsigned comparison warning in `expand` function ([link](https://github.com/pytorch/pytorch/pull/104201/files?diff=unified&w=0#diff-c175e908cbcb8595b22696e672b526202ed3a4a11341603c1522397e499b5c2bL29-R29))\n\n<details>\n<summary> Fix done using chatgpt </summary>\n\n![Screenshot 2023-06-26 at 11 52 14 AM](https://github.com/pytorch/pytorch/assets/1700823/95c141e5-36b6-4916-85ca-85415bcc507f)\n\n</details>\nSigned-off-by: Eli Uriegas <eliuriegas@meta.com>\nPull Request resolved: https://github.com/pytorch/pytorch/pull/104201\nApproved by: https://github.com/lucylq, https://github.com/huydhn, https://github.com/malfet",
    "changes": [
        {
            "name": "Expand.cpp",
            "path": "aten/src/ATen/native/vulkan/ops/Expand.cpp",
            "patches": [
                {
                    "old_start": 26,
                    "old_length": 7,
                    "new_start": 26,
                    "new_length": 7,
                    "hunk_buggy": "['       self.dim() > 0 && self.dim() <= 4,\\n', '       \"Vulkan expand supports up to 4d tensors\");\\n', '   TORCH_CHECK(\\n', '-      self.dim() <= output_size.size(),\\n', '       \"Vulkan expand: the number of sizes provided (\",\\n', '       output_size.size(),\\n', '       \") must be greater or equal to the number of dimensions in the tensor (\",']",
                    "hunk_fix": "@@ -26,7 +26,7 @@ Tensor expand(\n       self.dim() > 0 && self.dim() <= 4,\n       \"Vulkan expand supports up to 4d tensors\");\n   TORCH_CHECK(\n-      self.dim() <= output_size.size(),\n+      static_cast<size_t>(self.dim()) <= output_size.size(),\n       \"Vulkan expand: the number of sizes provided (\",\n       output_size.size(),\n       \") must be greater or equal to the number of dimensions in the tensor (\","
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 365,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/b3ace213f240dc0f0f2a738f825f46e0d0dffca4",
    "date": "2023-06-24T00:49:14+00:00",
    "message": "Heap buffer overflow at `source_range_serialization.cpp:73` (#103969)\n\nHi! We've been fuzzing torchvision project with [sydr-fuzz](https://github.com/ispras/oss-sydr-fuzz).\nWe've found a heap buffer overflow error at `source_range_serialization.cpp:73` in pytorch project.\n\nThe error occurs because there is not check in `deserialize_source` that `text_table_` size can be less than `fnameIndex`. To prevent the error the corresponding check must be located.\n\ntorchvision version: 9d0a93eee90bf7c401b74ebf9c8be80346254f15\npytorch version: 0f1621df1a0a73956c7ce4e2f72f069e610e0137\n\nOS: Ubuntu 20.04\n\nHow to reproduce\n\n1. Build docker from [here](https://github.com/ispras/oss-sydr-fuzz/tree/master/projects/torchvision) and run the container:\n\n        sudo docker build -t oss-sydr-fuzz-torchvision .\n        sudo docker run --privileged --rm -v `pwd`:/fuzz -it oss-sydr-fuzz-torchvision /bin/bash\n\n2. Run the target on this input:  [serialization-crash.txt](https://github.com/pytorch/pytorch/files/11819901/serialization-crash.txt)\n\n        /encode_png_fuzz serialization-crash.txt\n\n3. You will see the following output:\n\n        =================================================================\n        ==13==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x60200055a630 at pc 0x0000010197b7 bp 0x7ffd4cfb15f0 sp 0x7ffd4cfb15e8\n        READ of size 8 at 0x60200055a630 thread T0\n            #0 0x10197b6 in std::__shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, (__gnu_cxx::_Lock_policy)2>::get() const /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/shared_ptr_base.h:1325:16\n            #1 0x10197b6 in std::__shared_ptr_access<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, (__gnu_cxx::_Lock_policy)2, false, false>::_M_get() const /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/shared_ptr_base.h:1024:66\n            #2 0x10197b6 in std::__shared_ptr_access<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, (__gnu_cxx::_Lock_policy)2, false, false>::operator*() const /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/shared_ptr_base.h:1011:10\n            #3 0xde888c2 in torch::jit::SourceRangeDeserializer::deserialize_source(c10::IValue const&) /pytorch/torch/csrc/jit/serialization/source_range_serialization.cpp:73:16\n            #4 0xde8802b in torch::jit::SourceRangeDeserializer::deserialize(c10::IValue const&) /pytorch/torch/csrc/jit/serialization/source_range_serialization.cpp:51:37\n            #5 0xde8e9c7 in torch::jit::ConcreteSourceRangeUnpickler::unpickle() /pytorch/torch/csrc/jit/serialization/source_range_serialization.cpp:224:39\n            #6 0xde8fb19 in torch::jit::ConcreteSourceRangeUnpickler::findSourceRangeThatGenerated(torch::jit::SourceRange const&) /pytorch/torch/csrc/jit/serialization/source_range_serialization.cpp:231:3\n            #7 0x10798e7 in torch::jit::Source::findSourceRangeThatGenerated(torch::jit::SourceRange const&) /pytorch/torch/csrc/jit/frontend/source_range.cpp:144:23\n            #8 0x1079d9a in torch::jit::SourceRange::findSourceRangeThatGenerated() const /pytorch/torch/csrc/jit/frontend/source_range.h:384:26\n            #9 0x1079acd in torch::jit::SourceRange::highlight(std::ostream&) const /pytorch/torch/csrc/jit/frontend/source_range.cpp:149:32\n            #10 0x1026fe2 in torch::jit::Lexer::expected(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, torch::jit::Token const&) /pytorch/torch/csrc/jit/frontend/lexer.h:461:13\n            #11 0x10417d9 in torch::jit::Lexer::expected(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /pytorch/torch/csrc/jit/frontend/lexer.h:465:5\n            #12 0x102e52c in torch::jit::Lexer::expect(int) /pytorch/torch/csrc/jit/frontend/lexer.h:471:7\n            #13 0xcee774c in torch::jit::ParserImpl::parseIdent() /pytorch/torch/csrc/jit/frontend/parser.cpp:52:16\n            #14 0xcef4ea8 in torch::jit::ParserImpl::parseBaseExp() /pytorch/torch/csrc/jit/frontend/parser.cpp:195:22\n            #15 0xcef2c1b in torch::jit::ParserImpl::parseExp(int) /pytorch/torch/csrc/jit/frontend/parser.cpp:284:16\n            #16 0xcefac6a in torch::jit::ParserImpl::parseExp() /pytorch/torch/csrc/jit/frontend/parser.cpp:262:12\n            #17 0xcefac6a in torch::jit::ParserImpl::parseSubscriptExp() /pytorch/torch/csrc/jit/frontend/parser.cpp:403:15\n            #18 0xceff39f in torch::jit::List<torch::jit::Expr> torch::jit::ParserImpl::parseList<torch::jit::Expr>(int, int, int, torch::jit::Expr (torch::jit::ParserImpl::*)())::'lambda'()::operator()() const /pytorch/torch/csrc/jit/frontend/parser.cpp:354:54\n            #19 0xceff39f in torch::jit::Expr std::__invoke_impl<void, torch::jit::List<torch::jit::Expr> torch::jit::ParserImpl::parseList<torch::jit::Expr>(int, int, int, torch::jit::Expr (torch::jit::ParserImpl::*)())::'lambda'()&>(std::__invoke_other, torch::jit::List<torch::jit::Expr> torch::jit::ParserImpl::parseList<torch::jit::Expr>(int, int, int, torch::jit::Expr (torch::jit::ParserImpl::*)())::'lambda'()&) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/invoke.h:60:14\n            #20 0xceea935 in torch::jit::ParserImpl::parseSequence(int, int, int, std::function<void ()> const&) /pytorch/torch/csrc/jit/frontend/parser.cpp:339:7\n            #21 0xceefd69 in torch::jit::List<torch::jit::Expr> torch::jit::ParserImpl::parseList<torch::jit::Expr>(int, int, int, torch::jit::Expr (torch::jit::ParserImpl::*)()) /pytorch/torch/csrc/jit/frontend/parser.cpp:353:5\n            #22 0xcef895a in torch::jit::ParserImpl::parseSubscript(c10::intrusive_ptr<torch::jit::Tree, c10::detail::intrusive_target_default_null_type<torch::jit::Tree> > const&) /pytorch/torch/csrc/jit/frontend/parser.cpp:430:9\n            #23 0xcef5e5c in torch::jit::ParserImpl::parseBaseExp() /pytorch/torch/csrc/jit/frontend/parser.cpp:206:18\n            #24 0xcef2c1b in torch::jit::ParserImpl::parseExp(int) /pytorch/torch/csrc/jit/frontend/parser.cpp:284:16\n            #25 0xceeeb9d in torch::jit::ParserImpl::parseExp() /pytorch/torch/csrc/jit/frontend/parser.cpp:262:12\n            #26 0xceeeb9d in torch::jit::ParserImpl::parseExpOrExpTuple() /pytorch/torch/csrc/jit/frontend/parser.cpp:94:19\n            #27 0xcee8a36 in torch::jit::ParserImpl::parseStmt(bool) /pytorch/torch/csrc/jit/frontend/parser.cpp:612:20\n            #28 0xcee7e72 in torch::jit::ParserImpl::parseStatements(bool, bool) /pytorch/torch/csrc/jit/frontend/parser.cpp:697:23\n            #29 0xcee56f5 in torch::jit::ParserImpl::parseClass() /pytorch/torch/csrc/jit/frontend/parser.cpp:747:9\n            #30 0xcee544a in torch::jit::Parser::parseClass() /pytorch/torch/csrc/jit/frontend/parser.cpp:812:17\n            #31 0xdddbea9 in torch::jit::SourceImporterImpl::parseSourceIfNeeded(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /pytorch/torch/csrc/jit/serialization/import_source.cpp:182:42\n            #32 0xdddadbc in torch::jit::SourceImporterImpl::findNamedType(c10::QualifiedName const&) /pytorch/torch/csrc/jit/serialization/import_source.cpp:135:3\n            #33 0xdde1d88 in torch::jit::SourceImporterImpl::resolveType(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, torch::jit::SourceRange const&) /pytorch/torch/csrc/jit/serialization/import_source.cpp:261:10\n            #34 0xcf2ba5f in torch::jit::ScriptTypeParser::parseTypeFromExpr(torch::jit::Expr const&) const /pytorch/torch/csrc/jit/frontend/script_type_parser.cpp:238:24\n            #35 0xcf2bec7 in torch::jit::ScriptTypeParser::parseType(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /pytorch/torch/csrc/jit/frontend/script_type_parser.cpp:312:10\n            #36 0xddf4284 in torch::jit::SourceImporter::loadType(c10::QualifiedName const&) const /pytorch/torch/csrc/jit/serialization/import_source.cpp:786:27\n            #37 0xdd739f7 in torch::jit::(anonymous namespace)::ScriptModuleDeserializer::readArchive(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)::$_0::operator()(c10::QualifiedName const&) const /pytorch/torch/csrc/jit/serialization/import.cpp:146:33\n            #38 0xdd739f7 in c10::StrongTypePtr std::__invoke_impl<c10::StrongTypePtr, torch::jit::(anonymous namespace)::ScriptModuleDeserializer::readArchive(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)::$_0&, c10::QualifiedName const&>(std::__invoke_other, torch::jit::(anonymous namespace)::ScriptModuleDeserializer::readArchive(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)::$_0&, c10::QualifiedName const&) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/invoke.h:60:14\n            #39 0xdd73880 in std::enable_if<is_invocable_r_v<c10::StrongTypePtr, torch::jit::(anonymous namespace)::ScriptModuleDeserializer::readArchive(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)::$_0&, c10::QualifiedName const&>, c10::StrongTypePtr>::type std::__invoke_r<c10::StrongTypePtr, torch::jit::(anonymous namespace)::ScriptModuleDeserializer::readArchive(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)::$_0&, c10::QualifiedName const&>(torch::jit::(anonymous namespace)::ScriptModuleDeserializer::readArchive(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)::$_0&, c10::QualifiedName const&) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/invoke.h:113:9\n            #40 0xdd736d6 in std::_Function_handler<c10::StrongTypePtr (c10::QualifiedName const&), torch::jit::(anonymous namespace)::ScriptModuleDeserializer::readArchive(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)::$_0>::_M_invoke(std::_Any_data const&, c10::QualifiedName const&) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/std_function.h:291:9\n            #41 0xdd76349 in std::function<c10::StrongTypePtr (c10::QualifiedName const&)>::operator()(c10::QualifiedName const&) const /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/std_function.h:622:14\n            #42 0xdeb9f48 in torch::jit::Unpickler::readGlobal(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /pytorch/torch/csrc/jit/serialization/unpickler.cpp:835:9\n            #43 0xdeb012d in torch::jit::Unpickler::readInstruction() /pytorch/torch/csrc/jit/serialization/unpickler.cpp:511:7\n            #44 0xdeae437 in torch::jit::Unpickler::run() /pytorch/torch/csrc/jit/serialization/unpickler.cpp:251:27\n            #45 0xdeae0d2 in torch::jit::Unpickler::parse_ivalue() /pytorch/torch/csrc/jit/serialization/unpickler.cpp:204:3\n            #46 0xddd6de3 in torch::jit::readArchiveAndTensors(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, c10::optional<std::function<c10::StrongTypePtr (c10::QualifiedName const&)> >, c10::optional<std::function<c10::intrusive_ptr<c10::ivalue::Object, c10::detail::intrusive_target_default_null_type<c10::ivalue::Object> > (c10::StrongTypePtr, c10::IValue)> >, c10::optional<c10::Device>, caffe2::serialize::PyTorchStreamReader&, c10::Type::SingletonOrSharedTypePtr<c10::Type> (*)(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&), std::shared_ptr<torch::jit::DeserializationStorageContext>) /pytorch/torch/csrc/jit/serialization/import_read.cpp:53:20\n            #47 0xdd732dd in torch::jit::(anonymous namespace)::ScriptModuleDeserializer::readArchive(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /pytorch/torch/csrc/jit/serialization/import.cpp:184:10\n            #48 0xdd69885 in torch::jit::(anonymous namespace)::ScriptModuleDeserializer::deserialize(c10::optional<c10::Device>, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >&, bool) /pytorch/torch/csrc/jit/serialization/import.cpp:287:19\n            #49 0xdd6c855 in torch::jit::import_ir_module(std::shared_ptr<torch::jit::CompilationUnit>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, c10::optional<c10::Device>, std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >&, bool, bool) /pytorch/torch/csrc/jit/serialization/import.cpp:438:25\n            #50 0xdd6c1c7 in torch::jit::import_ir_module(std::shared_ptr<torch::jit::CompilationUnit>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, c10::optional<c10::Device>, bool) /pytorch/torch/csrc/jit/serialization/import.cpp:421:10\n            #51 0xdd6dce4 in torch::jit::load(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, c10::optional<c10::Device>, bool) /pytorch/torch/csrc/jit/serialization/import.cpp:503:10\n            #52 0xf2d3f75 in torch::serialize::InputArchive::load_from(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, c10::optional<c10::Device>) /pytorch/torch/csrc/api/src/serialize/input-archive.cpp:97:13\n            #53 0x60509c in void torch::load<at::Tensor, char*&>(at::Tensor&, char*&) /pytorch/torch/include/torch/csrc/api/include/torch/serialize.h:107:11\n            #54 0x6036be in LLVMFuzzerTestOneInput /vision/encode_png.cc:38:5\n            #55 0x66b041 in fuzzer::Fuzzer::ExecuteCallback(unsigned char const*, unsigned long) /llvm-project-llvmorg-14.0.6/compiler-rt/lib/fuzzer/FuzzerLoop.cpp:611:15\n            #56 0x6544cc in fuzzer::RunOneTest(fuzzer::Fuzzer*, char const*, unsigned long) /llvm-project-llvmorg-14.0.6/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:324:6\n            #57 0x65a61b in fuzzer::FuzzerDriver(int*, char***, int (*)(unsigned char const*, unsigned long)) /llvm-project-llvmorg-14.0.6/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:860:9\n            #58 0x654222 in main /llvm-project-llvmorg-14.0.6/compiler-rt/lib/fuzzer/FuzzerMain.cpp:20:10\n            #59 0x7f3d12cc7082 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x24082) (BuildId: 1878e6b475720c7c51969e69ab2d276fae6d1dee)\n            #60 0x542cdd in _start (/encode_png_fuzz+0x542cdd)\n\n        0x60200055a630 is located 16 bytes to the right of 16-byte region [0x60200055a610,0x60200055a620)\n        allocated by thread T0 here:\n            #0 0x60057d in operator new(unsigned long) /llvm-project-llvmorg-14.0.6/compiler-rt/lib/asan/asan_new_delete.cpp:95:3\n            #1 0xde9185d in std::_Vector_base<std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >::_M_allocate(unsigned long) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/stl_vector.h:346:20\n            #2 0xde9185d in void std::vector<std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >::_M_realloc_insert<std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >(__gnu_cxx::__normal_iterator<std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >*, std::vector<std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > >, std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >&&) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/vector.tcc:440:33\n            #3 0xde916a1 in std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >& std::vector<std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >::emplace_back<std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >(std::shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >&&) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/vector.tcc:121:4\n            #4 0xde8f445 in torch::jit::SourceRangeDeserializer::SourceRangeDeserializer(c10::IValue) /pytorch/torch/csrc/jit/serialization/source_range_serialization.h:42:19\n            #5 0xde8e141 in torch::jit::ConcreteSourceRangeUnpickler::unpickle() /pytorch/torch/csrc/jit/serialization/source_range_serialization.cpp:215:28\n            #6 0xde8fb19 in torch::jit::ConcreteSourceRangeUnpickler::findSourceRangeThatGenerated(torch::jit::SourceRange const&) /pytorch/torch/csrc/jit/serialization/source_range_serialization.cpp:231:3\n            #7 0x10798e7 in torch::jit::Source::findSourceRangeThatGenerated(torch::jit::SourceRange const&) /pytorch/torch/csrc/jit/frontend/source_range.cpp:144:23\n            #8 0x1079d9a in torch::jit::SourceRange::findSourceRangeThatGenerated() const /pytorch/torch/csrc/jit/frontend/source_range.h:384:26\n            #9 0x1079acd in torch::jit::SourceRange::highlight(std::ostream&) const /pytorch/torch/csrc/jit/frontend/source_range.cpp:149:32\n            #10 0x1026fe2 in torch::jit::Lexer::expected(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, torch::jit::Token const&) /pytorch/torch/csrc/jit/frontend/lexer.h:461:13\n            #11 0x10417d9 in torch::jit::Lexer::expected(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /pytorch/torch/csrc/jit/frontend/lexer.h:465:5\n            #12 0xcee774c in torch::jit::ParserImpl::parseIdent() /pytorch/torch/csrc/jit/frontend/parser.cpp:52:16\n            #13 0xcef4ea8 in torch::jit::ParserImpl::parseBaseExp() /pytorch/torch/csrc/jit/frontend/parser.cpp:195:22\n            #14 0xcef2c1b in torch::jit::ParserImpl::parseExp(int) /pytorch/torch/csrc/jit/frontend/parser.cpp:284:16\n            #15 0xcefac6a in torch::jit::ParserImpl::parseExp() /pytorch/torch/csrc/jit/frontend/parser.cpp:262:12\n            #16 0xcefac6a in torch::jit::ParserImpl::parseSubscriptExp() /pytorch/torch/csrc/jit/frontend/parser.cpp:403:15\n            #17 0xceff39f in torch::jit::List<torch::jit::Expr> torch::jit::ParserImpl::parseList<torch::jit::Expr>(int, int, int, torch::jit::Expr (torch::jit::ParserImpl::*)())::'lambda'()::operator()() const /pytorch/torch/csrc/jit/frontend/parser.cpp:354:54\n            #18 0xceff39f in torch::jit::Expr std::__invoke_impl<void, torch::jit::List<torch::jit::Expr> torch::jit::ParserImpl::parseList<torch::jit::Expr>(int, int, int, torch::jit::Expr (torch::jit::ParserImpl::*)())::'lambda'()&>(std::__invoke_other, torch::jit::List<torch::jit::Expr> torch::jit::ParserImpl::parseList<torch::jit::Expr>(int, int, int, torch::jit::Expr (torch::jit::ParserImpl::*)())::'lambda'()&) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/invoke.h:60:14\n            #19 0xceea935 in torch::jit::ParserImpl::parseSequence(int, int, int, std::function<void ()> const&) /pytorch/torch/csrc/jit/frontend/parser.cpp:339:7\n            #20 0xceefd69 in torch::jit::List<torch::jit::Expr> torch::jit::ParserImpl::parseList<torch::jit::Expr>(int, int, int, torch::jit::Expr (torch::jit::ParserImpl::*)()) /pytorch/torch/csrc/jit/frontend/parser.cpp:353:5\n            #21 0xcef895a in torch::jit::ParserImpl::parseSubscript(c10::intrusive_ptr<torch::jit::Tree, c10::detail::intrusive_target_default_null_type<torch::jit::Tree> > const&) /pytorch/torch/csrc/jit/frontend/parser.cpp:430:9\n            #22 0xcef5e5c in torch::jit::ParserImpl::parseBaseExp() /pytorch/torch/csrc/jit/frontend/parser.cpp:206:18\n            #23 0xcef2c1b in torch::jit::ParserImpl::parseExp(int) /pytorch/torch/csrc/jit/frontend/parser.cpp:284:16\n            #24 0xceeeb9d in torch::jit::ParserImpl::parseExp() /pytorch/torch/csrc/jit/frontend/parser.cpp:262:12\n            #25 0xceeeb9d in torch::jit::ParserImpl::parseExpOrExpTuple() /pytorch/torch/csrc/jit/frontend/parser.cpp:94:19\n            #26 0xcee8a36 in torch::jit::ParserImpl::parseStmt(bool) /pytorch/torch/csrc/jit/frontend/parser.cpp:612:20\n            #27 0xcee7e72 in torch::jit::ParserImpl::parseStatements(bool, bool) /pytorch/torch/csrc/jit/frontend/parser.cpp:697:23\n            #28 0xcee56f5 in torch::jit::ParserImpl::parseClass() /pytorch/torch/csrc/jit/frontend/parser.cpp:747:9\n            #29 0xcee544a in torch::jit::Parser::parseClass() /pytorch/torch/csrc/jit/frontend/parser.cpp:812:17\n            #30 0xdddbea9 in torch::jit::SourceImporterImpl::parseSourceIfNeeded(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) /pytorch/torch/csrc/jit/serialization/import_source.cpp:182:42\n            #31 0xdddadbc in torch::jit::SourceImporterImpl::findNamedType(c10::QualifiedName const&) /pytorch/torch/csrc/jit/serialization/import_source.cpp:135:3\n            #32 0xdde1d88 in torch::jit::SourceImporterImpl::resolveType(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, torch::jit::SourceRange const&) /pytorch/torch/csrc/jit/serialization/import_source.cpp:261:10\n            #33 0xcf2ba5f in torch::jit::ScriptTypeParser::parseTypeFromExpr(torch::jit::Expr const&) const /pytorch/torch/csrc/jit/frontend/script_type_parser.cpp:238:24\n\n        SUMMARY: AddressSanitizer: heap-buffer-overflow /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/shared_ptr_base.h:1325:16 in std::__shared_ptr<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, (__gnu_cxx::_Lock_policy)2>::get() const\n        Shadow bytes around the buggy address:\n          0x0c04800a3470: fa fa 00 00 fa fa 00 00 fa fa fd fa fa fa 00 00\n          0x0c04800a3480: fa fa fd fa fa fa fd fd fa fa fd fd fa fa fd fa\n          0x0c04800a3490: fa fa fd fd fa fa 00 00 fa fa 00 00 fa fa 00 00\n          0x0c04800a34a0: fa fa fd fa fa fa fd fd fa fa fd fa fa fa 00 fa\n          0x0c04800a34b0: fa fa fd fd fa fa fd fd fa fa fd fa fa fa fd fd\n        =>0x0c04800a34c0: fa fa 00 00 fa fa[fa]fa fa fa fa fa fa fa fa fa\n          0x0c04800a34d0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n          0x0c04800a34e0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n          0x0c04800a34f0: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n          0x0c04800a3500: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n          0x0c04800a3510: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n        Shadow byte legend (one shadow byte represents 8 application bytes):\n          Addressable:           00\n          Partially addressable: 01 02 03 04 05 06 07\n          Heap left redzone:       fa\n          Freed heap region:       fd\n          Stack left redzone:      f1\n          Stack mid redzone:       f2\n          Stack right redzone:     f3\n          Stack after return:      f5\n          Stack use after scope:   f8\n          Global redzone:          f9\n          Global init order:       f6\n          Poisoned by user:        f7\n          Container overflow:      fc\n          Array cookie:            ac\n          Intra object redzone:    bb\n          ASan internal:           fe\n          Left alloca redzone:     ca\n          Right alloca redzone:    cb\n        ==13==ABORTING\nPull Request resolved: https://github.com/pytorch/pytorch/pull/103969\nApproved by: https://github.com/davidberard98",
    "changes": [
        {
            "name": "source_range_serialization.cpp",
            "path": "torch/csrc/jit/serialization/source_range_serialization.cpp",
            "patches": [
                {
                    "old_start": 70,
                    "old_length": 6,
                    "new_start": 70,
                    "new_length": 9,
                    "hunk_buggy": "['     int64_t starting_line_no_ = tup_elems[2].toInt();\\n', '     c10::optional<std::string> filename = c10::nullopt;\\n', ' \\n', '     filename = *text_table_[fnameIndex];\\n', ' \\n', '     std::vector<c10::string_view> pieces;']",
                    "hunk_fix": "@@ -70,6 +70,9 @@ std::shared_ptr<Source> SourceRangeDeserializer::deserialize_source(\n     int64_t starting_line_no_ = tup_elems[2].toInt();\n     c10::optional<std::string> filename = c10::nullopt;\n \n+    TORCH_CHECK(\n+        (uint64_t)fnameIndex < text_table_.size(),\n+        \"Text table index is out of range\")\n     filename = *text_table_[fnameIndex];\n \n     std::vector<c10::string_view> pieces;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 366,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/8b37821813b60a3ce2ae92e7a06057183578a450",
    "date": "2023-06-20T21:01:57+00:00",
    "message": "make balance check in DP only for cuda (#103311)\n\nFixes #103825\n1. if we want to use dp on other device ranther than \"cuda\", this balance  check will raise error, so I make the balance check only effective for `cuda`\nPull Request resolved: https://github.com/pytorch/pytorch/pull/103311\nApproved by: https://github.com/kit1980",
    "changes": [
        {
            "name": "data_parallel.py",
            "path": "torch/nn/parallel/data_parallel.py",
            "patches": [
                {
                    "old_start": 155,
                    "old_length": 7,
                    "new_start": 155,
                    "new_length": 8,
                    "hunk_buggy": "['         self.output_device = _get_device_index(output_device, True)\\n', '         self.src_device_obj = torch.device(device_type, self.device_ids[0])\\n', ' \\n', '-        _check_balance(self.device_ids)\\n', ' \\n', '         if len(self.device_ids) == 1:\\n', '             self.module.to(self.src_device_obj)']",
                    "hunk_fix": "@@ -155,7 +155,8 @@ class DataParallel(Module, Generic[T]):\n         self.output_device = _get_device_index(output_device, True)\n         self.src_device_obj = torch.device(device_type, self.device_ids[0])\n \n-        _check_balance(self.device_ids)\n+        if device_type == \"cuda\":\n+            _check_balance(self.device_ids)\n \n         if len(self.device_ids) == 1:\n             self.module.to(self.src_device_obj)"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 367,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/b287cb816c1ac52165920a121c98643c08d31ff7",
    "date": "2023-06-16T03:56:39+00:00",
    "message": "inductor: make the vec_transpose's tiling stride doesn't depend on out_idx and tiling_idex (#103651)\n\nFor TIMM swin_base_patch4_window7_224 dynamic shape path, there has an accuracy issue with horizontal reduction with vec_transpose:\n```\n#pragma omp for\nfor(long i0=static_cast<long>(0L); i0<static_cast<long>(ks0); i0+=static_cast<long>(1L))\n{\n    #pragma GCC ivdep\n    for(long i1=static_cast<long>(0L); i1<static_cast<long>(3136L); i1+=static_cast<long>(16L))\n    {\n        {\n            #pragma omp declare reduction(+:at::vec::Vectorized<float>:omp_out = omp_out + omp_in) initializer(omp_priv={{0}})\n            float tmp_acc0 = 0;\n            auto tmp_acc0_vec = at::vec::Vectorized<float>(tmp_acc0);\n            for(long i2=static_cast<long>(0L); i2<static_cast<long>(128L); i2+=static_cast<long>(16L))\n            {\n                float tmp1[16*16] __attribute__ ((aligned (16)));\n                at::vec::transpose_mxn<float,16,16>(in_ptr1 + static_cast<long>(i2 + (128L*(static_cast<long>((static_cast<long>(i1) % static_cast<long>(56L))) % static_cast<long>(7L))) + (896L*(static_cast<long>(at::native::div_floor_integer(i1, 56L)) % static_cast<long>(7L))) + (6272L*(at::native::div_floor_integer((static_cast<long>(i1) % static_cast<long>(56L)), 7L))) + (50176L*(at::native::div_floor_integer(i1, 392L))) + (401408L*i0)), static_cast<long>(((-50176L)*(at::native::div_floor_integer(i1, 392L))) + ((-6272L)*(at::native::div_floor_integer((static_cast<long>(i1) % static_cast<long>(56L)), 7L))) + ((-896L)*(static_cast<long>(at::native::div_floor_integer(i1, 56L)) % static_cast<long>(7L))) + ((-128L)*(static_cast<long>((static_cast<long>(i1) % static_cast<long>(56L))) % static_cast<long>(7L))) + (128L*(static_cast<long>((static_cast<long>((1L + i1)) % static_cast<long>(56L))) % static_cast<long>(7L))) + (896L*(static_cast<long>(at::native::div_floor_integer((1L + i1), 56L)) % static_cast<long>(7L))) + (6272L*(at::native::div_floor_integer((static_cast<long>((1L + i1)) % static_cast<long>(56L)), 7L))) + (50176L*(at::native::div_floor_integer((1L + i1), 392L)))), tmp1, 16);\n                for (long i2_inner = 0; i2_inner < 16; i2_inner++)\n                {\n                    auto tmp0 = at::vec::Vectorized<float>::loadu(in_ptr0 + static_cast<long>(i1 + (3136L*i2) + (3136L*i2_inner) + (401408L*i0)));\n                    auto tmp2 = at::vec::Vectorized<float>::loadu(tmp1 + static_cast<long>(16L*i2_inner));\n                    auto tmp3 = tmp0 + tmp2;\n                    tmp_acc0_vec = tmp_acc0_vec + tmp3;\n                }\n            }\n            tmp_acc0_vec.store(out_ptr0 + static_cast<long>(i1 + (3136L*i0)));\n        }\n    }\n}\n```\n\nThe ```transpose_mxn```'s ```ld_src``` depends on ```i1``` which is not expected. This PR will  add a check to make sure the tiling stride doesn't depend on out_idx(```i2```) and tiling_idex(```i1```)\n\nAfter this PR, the generated code will be like this:\n```\n#pragma omp for\nfor(long i0=static_cast<long>(0L); i0<static_cast<long>(ks0); i0+=static_cast<long>(1L))\n{\n    #pragma GCC ivdep\n    for(long i1=static_cast<long>(0L); i1<static_cast<long>(3136L); i1+=static_cast<long>(16L))\n    {\n        {\n            #pragma omp declare reduction(+:at::vec::Vectorized<float>:omp_out = omp_out + omp_in) initializer(omp_priv={{0}})\n            float tmp_acc0 = 0;\n            auto tmp_acc0_vec = at::vec::Vectorized<float>(tmp_acc0);\n            for(long i2=static_cast<long>(0L); i2<static_cast<long>(128L); i2+=static_cast<long>(16L))\n            {\n                for (long i2_inner = 0; i2_inner < 16; i2_inner++)\n                {\n                    auto tmp0 = at::vec::Vectorized<float>::loadu(in_ptr0 + static_cast<long>(i1 + (3136L*i2) + (3136L*i2_inner) + (401408L*i0)));\n                    auto tmp1 = ([&]() { __at_align__ float tmpbuf[16]; for (long i1_inner = 0; i1_inner < 16; i1_inner++) tmpbuf[i1_inner] = in_ptr1[static_cast<long>(i2 + i2_inner + (128L*(static_cast<long>((static_cast<long>((i1 + i1_inner)) % static_cast<long>(56L))) % static_cast<long>(7L))) + (896L*(static_cast<long>(at::native::div_floor_integer((i1 + i1_inner), 56L)) % static_cast<long>(7L))) + (6272L*(at::native::div_floor_integer((static_cast<long>((i1 + i1_inner)) % static_cast<long>(56L)), 7L))) + (50176L*(at::native::div_floor_integer((i1 + i1_inner), 392L))) + (401408L*i0))]; return at::vec::Vectorized<float>::loadu(tmpbuf); })();\n                    auto tmp2 = tmp0 + tmp1;\n                    tmp_acc0_vec = tmp_acc0_vec + tmp2;\n                }\n            }\n            tmp_acc0_vec.store(out_ptr0 + static_cast<long>(i1 + (3136L*i0)));\n        }\n    }\n}\n```\n\nHow to reproduce this issue:\n```\npython -m torch.backends.xeon.run_cpu --node_id 0 benchmarks/dynamo/timm_models.py --accuracy --float32 -dcpu --inference -n5 --inductor --dynamic-shapes --only swin_base_patch4_window7_224\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/103651\nApproved by: https://github.com/jgong5, https://github.com/jansel",
    "changes": [
        {
            "name": "cpp.py",
            "path": "torch/_inductor/codegen/cpp.py",
            "patches": [
                {
                    "old_start": 1554,
                    "old_length": 8,
                    "new_start": 1554,
                    "new_length": 15,
                    "hunk_buggy": "['         return sympy_symbol(f\"{self.itervars[self.outer_idx]}_inner\")\\n', ' \\n', '     def need_vec_transpose(self, index):\\n', '-        return stride_at(self.itervars[self.outer_idx], index) == 1 and index.has(\\n', '-            self.itervars[self.tiling_idx]\\n', '         )\\n', ' \\n', '     def gen_transposed_tile_load_store(self, name, var, index, is_store):']",
                    "hunk_fix": "@@ -1554,8 +1554,15 @@ class CppTile2DKernel(CppVecKernel):\n         return sympy_symbol(f\"{self.itervars[self.outer_idx]}_inner\")\n \n     def need_vec_transpose(self, index):\n-        return stride_at(self.itervars[self.outer_idx], index) == 1 and index.has(\n-            self.itervars[self.tiling_idx]\n+        return (\n+            stride_at(self.itervars[self.outer_idx], index) == 1\n+            and index.has(self.itervars[self.tiling_idx])\n+            and not stride_at(self.itervars[self.tiling_idx], index).has(\n+                self.itervars[self.tiling_idx]\n+            )\n+            and not stride_at(self.itervars[self.tiling_idx], index).has(\n+                self.itervars[self.outer_idx]\n+            )\n         )\n \n     def gen_transposed_tile_load_store(self, name, var, index, is_store):"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 368,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/6cc0f1c20c2f87a6c7b0e4abd5419e5007920999",
    "date": "2023-06-13T23:54:45+00:00",
    "message": "Checking for nullptr in get_model_bytecode_version (#97149)\n\nOne-liner commit to check that the ptr is not null. Just had `test_jit` that had a segfault there.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/97149\nApproved by: https://github.com/kit1980",
    "changes": [
        {
            "name": "model_compatibility.cpp",
            "path": "torch/csrc/jit/mobile/compatibility/model_compatibility.cpp",
            "patches": [
                {
                    "old_start": 108,
                    "old_length": 6,
                    "new_start": 108,
                    "new_length": 7,
                    "hunk_buggy": "[' }\\n', ' \\n', ' uint64_t _get_model_bytecode_version_from_bytes(char* data, size_t size) {\\n', '   TORCH_CHECK(size >= kFileFormatHeaderSize, \"Unrecognized data format\");\\n', '   auto format = getFileFormat(data);\\n', '   switch (format) {']",
                    "hunk_fix": "@@ -108,6 +108,7 @@ static uint64_t _get_model_bytecode_version_zip(\n }\n \n uint64_t _get_model_bytecode_version_from_bytes(char* data, size_t size) {\n+  TORCH_CHECK(data != nullptr, \"Pointer to bytes is null.\");\n   TORCH_CHECK(size >= kFileFormatHeaderSize, \"Unrecognized data format\");\n   auto format = getFileFormat(data);\n   switch (format) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 369,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/3ef4d697df5bfdbd27dfc7a79c0679da2b87e3af",
    "date": "2023-05-30T19:22:37+00:00",
    "message": "[c10d] default backend need to check for nccl availability (#102470)\n\nAs titled, we can only initialize nccl backend when NCCL is available\nPull Request resolved: https://github.com/pytorch/pytorch/pull/102470\nApproved by: https://github.com/Skylion007, https://github.com/XilunWu",
    "changes": [
        {
            "name": "distributed_c10d.py",
            "path": "torch/distributed/distributed_c10d.py",
            "patches": [
                {
                    "old_start": 255,
                    "old_length": 10,
                    "new_start": 255,
                    "new_length": 9,
                    "hunk_buggy": "['         if backend == Backend.UNDEFINED:\\n', '             # default config when backend is not specified\\n', '             # supported since PyTorch 2.0\\n', '-            self.device_backend_map = {\\n', '-                \"cpu\": Backend.GLOO,\\n', '-                \"cuda\": Backend.NCCL,\\n', '-            }\\n', '         elif backend.lower() in Backend.backend_list:\\n', '             # Cases for when backend is a single string (without device types)\\n', '             # e.g. \"nccl\", \"gloo\", \"ucc\", \"mpi\"']",
                    "hunk_fix": "@@ -255,10 +255,9 @@ class BackendConfig:\n         if backend == Backend.UNDEFINED:\n             # default config when backend is not specified\n             # supported since PyTorch 2.0\n-            self.device_backend_map = {\n-                \"cpu\": Backend.GLOO,\n-                \"cuda\": Backend.NCCL,\n-            }\n+            self.device_backend_map = {\"cpu\": Backend.GLOO}\n+            if is_nccl_available():\n+                self.device_backend_map[\"cuda\"] = Backend.NCCL\n         elif backend.lower() in Backend.backend_list:\n             # Cases for when backend is a single string (without device types)\n             # e.g. \"nccl\", \"gloo\", \"ucc\", \"mpi\""
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 370,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/faa7eb81c634492b70fcc0327622bb0aa812cacd",
    "date": "2023-05-24T08:36:43+00:00",
    "message": "change error_message for XPU Autocast data type check (#102073)\n\nXPU autocast supports bf16 and fp16 data types, we are going to change the error_message for that.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/102073\nApproved by: https://github.com/jgong5",
    "changes": [
        {
            "name": "autocast_mode.py",
            "path": "torch/amp/autocast_mode.py",
            "patches": [
                {
                    "old_start": 236,
                    "old_length": 7,
                    "new_start": 236,
                    "new_length": 7,
                    "hunk_buggy": "['             supported_dtype = [torch.bfloat16, torch.float16]\\n', '             if self.fast_dtype not in supported_dtype:\\n', \"                 error_message = 'In XPU autocast, but the target dtype is not supported. Disabling autocast.\\\\n'\\n\", \"-                error_message += 'XPU Autocast only supports dtype of torch.bfloat16 currently.'\\n\", '                 warnings.warn(error_message)\\n', '                 enabled = False\\n', \"         elif self.device == 'hpu':\"]",
                    "hunk_fix": "@@ -236,7 +236,7 @@ class autocast:\n             supported_dtype = [torch.bfloat16, torch.float16]\n             if self.fast_dtype not in supported_dtype:\n                 error_message = 'In XPU autocast, but the target dtype is not supported. Disabling autocast.\\n'\n-                error_message += 'XPU Autocast only supports dtype of torch.bfloat16 currently.'\n+                error_message += 'XPU Autocast only supports dtypes of torch.bfloat16 and torch.float16 currently.'\n                 warnings.warn(error_message)\n                 enabled = False\n         elif self.device == 'hpu':"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 371,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/bdb3fb49bc7a73cbcc5c865dda8be32888deb584",
    "date": "2023-05-22T18:37:05+00:00",
    "message": "[c10d] Fix the check message of unsupported collectives ops. (#101775)\n\n1. Fix the check message of unsupported collectives ops.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/101775\nApproved by: https://github.com/H-Huang",
    "changes": [
        {
            "name": "Backend.hpp",
            "path": "torch/csrc/distributed/c10d/Backend.hpp",
            "patches": [
                {
                    "old_start": 56,
                    "old_length": 13,
                    "new_start": 56,
                    "new_length": 13,
                    "hunk_buggy": "['   virtual void startCoalescing() {\\n', '     TORCH_CHECK(\\n', '         false,\\n', '-        c10::str(\"Backend \", getBackendName(), \"does not implement startCoalescing\"));\\n', '   }\\n', ' \\n', '   virtual c10::intrusive_ptr<Work> endCoalescing() {\\n', '     TORCH_CHECK(\\n', '         false,\\n', '-        c10::str(\"Backend \", getBackendName(), \"does not implement endCoalescing\"));\\n', '   }\\n', ' \\n', '   // Subclasses must override this method to return the backend name\\n']",
                    "hunk_fix": "@@ -56,13 +56,13 @@ class TORCH_API Backend : public torch::CustomClassHolder {\n   virtual void startCoalescing() {\n     TORCH_CHECK(\n         false,\n-        c10::str(\"Backend \", getBackendName(), \"does not implement startCoalescing\"));\n+        c10::str(\"Backend \", getBackendName(), \" does not implement startCoalescing\"));\n   }\n \n   virtual c10::intrusive_ptr<Work> endCoalescing() {\n     TORCH_CHECK(\n         false,\n-        c10::str(\"Backend \", getBackendName(), \"does not implement endCoalescing\"));\n+        c10::str(\"Backend \", getBackendName(), \" does not implement endCoalescing\"));\n   }\n \n   // Subclasses must override this method to return the backend name\n"
                },
                {
                    "old_start": 75,
                    "old_length": 7,
                    "new_start": 75,
                    "new_length": 7,
                    "hunk_buggy": "['       const BroadcastOptions& /* opts */ = BroadcastOptions()) {\\n', '     TORCH_CHECK(\\n', '         false,\\n', '-        c10::str(\"Backend \", getBackendName(), \"does not support broadcast\"));\\n', '   }\\n', ' \\n', '   virtual c10::intrusive_ptr<Work> allreduce(\\n']",
                    "hunk_fix": "@@ -75,7 +75,7 @@ class TORCH_API Backend : public torch::CustomClassHolder {\n       const BroadcastOptions& /* opts */ = BroadcastOptions()) {\n     TORCH_CHECK(\n         false,\n-        c10::str(\"Backend \", getBackendName(), \"does not support broadcast\"));\n+        c10::str(\"Backend \", getBackendName(), \" does not support broadcast\"));\n   }\n \n   virtual c10::intrusive_ptr<Work> allreduce(\n"
                },
                {
                    "old_start": 83,
                    "old_length": 7,
                    "new_start": 83,
                    "new_length": 7,
                    "hunk_buggy": "['       const AllreduceOptions& /* opts */ = AllreduceOptions()) {\\n', '     TORCH_CHECK(\\n', '         false,\\n', '-        c10::str(\"Backend \", getBackendName(), \"does not support allreduce\"));\\n', '   }\\n', ' \\n', '   virtual c10::intrusive_ptr<Work> allreduce_coalesced(\\n']",
                    "hunk_fix": "@@ -83,7 +83,7 @@ class TORCH_API Backend : public torch::CustomClassHolder {\n       const AllreduceOptions& /* opts */ = AllreduceOptions()) {\n     TORCH_CHECK(\n         false,\n-        c10::str(\"Backend \", getBackendName(), \"does not support allreduce\"));\n+        c10::str(\"Backend \", getBackendName(), \" does not support allreduce\"));\n   }\n \n   virtual c10::intrusive_ptr<Work> allreduce_coalesced(\n"
                },
                {
                    "old_start": 95,
                    "old_length": 7,
                    "new_start": 95,
                    "new_length": 7,
                    "hunk_buggy": "['         c10::str(\\n', '             \"Backend \",\\n', '             getBackendName(),\\n', '-            \"does not support allreduce_coalesced\"));\\n', '   }\\n', ' \\n', '   virtual c10::intrusive_ptr<Work> reduce(\\n']",
                    "hunk_fix": "@@ -95,7 +95,7 @@ class TORCH_API Backend : public torch::CustomClassHolder {\n         c10::str(\n             \"Backend \",\n             getBackendName(),\n-            \"does not support allreduce_coalesced\"));\n+            \" does not support allreduce_coalesced\"));\n   }\n \n   virtual c10::intrusive_ptr<Work> reduce(\n"
                },
                {
                    "old_start": 103,
                    "old_length": 7,
                    "new_start": 103,
                    "new_length": 7,
                    "hunk_buggy": "['       const ReduceOptions& /* opts */ = ReduceOptions()) {\\n', '     TORCH_CHECK(\\n', '         false,\\n', '-        c10::str(\"Backend \", getBackendName(), \"does not support reduce\"));\\n', '   }\\n', ' \\n', '   virtual c10::intrusive_ptr<Work> allgather(\\n']",
                    "hunk_fix": "@@ -103,7 +103,7 @@ class TORCH_API Backend : public torch::CustomClassHolder {\n       const ReduceOptions& /* opts */ = ReduceOptions()) {\n     TORCH_CHECK(\n         false,\n-        c10::str(\"Backend \", getBackendName(), \"does not support reduce\"));\n+        c10::str(\"Backend \", getBackendName(), \" does not support reduce\"));\n   }\n \n   virtual c10::intrusive_ptr<Work> allgather(\n"
                },
                {
                    "old_start": 112,
                    "old_length": 7,
                    "new_start": 112,
                    "new_length": 7,
                    "hunk_buggy": "['       const AllgatherOptions& /* opts */ = AllgatherOptions()) {\\n', '     TORCH_CHECK(\\n', '         false,\\n', '-        c10::str(\"Backend \", getBackendName(), \"does not support allgather\"));\\n', '   }\\n', ' \\n', '   // Gathers a single tensor inputBuffer into a single buffer outputBuffer that\\n']",
                    "hunk_fix": "@@ -112,7 +112,7 @@ class TORCH_API Backend : public torch::CustomClassHolder {\n       const AllgatherOptions& /* opts */ = AllgatherOptions()) {\n     TORCH_CHECK(\n         false,\n-        c10::str(\"Backend \", getBackendName(), \"does not support allgather\"));\n+        c10::str(\"Backend \", getBackendName(), \" does not support allgather\"));\n   }\n \n   // Gathers a single tensor inputBuffer into a single buffer outputBuffer that\n"
                },
                {
                    "old_start": 126,
                    "old_length": 7,
                    "new_start": 126,
                    "new_length": 7,
                    "hunk_buggy": "['     TORCH_CHECK(\\n', '         false,\\n', '         c10::str(\\n', '-            \"Backend \", getBackendName(), \"does not support _allgather_base\"));\\n', '   }\\n', ' \\n', '   // This function is deprecated and will be moved out of Backend to comms:\\n']",
                    "hunk_fix": "@@ -126,7 +126,7 @@ class TORCH_API Backend : public torch::CustomClassHolder {\n     TORCH_CHECK(\n         false,\n         c10::str(\n-            \"Backend \", getBackendName(), \"does not support _allgather_base\"));\n+            \"Backend \", getBackendName(), \" does not support _allgather_base\"));\n   }\n \n   // This function is deprecated and will be moved out of Backend to comms:\n"
                },
                {
                    "old_start": 142,
                    "old_length": 7,
                    "new_start": 142,
                    "new_length": 7,
                    "hunk_buggy": "['         c10::str(\\n', '             \"Backend \",\\n', '             getBackendName(),\\n', '-            \"does not support allgather_coalesced\"));\\n', '   }\\n', ' \\n', '   // This function is a coalesced version of `allgather_into_tensor` (currently\\n']",
                    "hunk_fix": "@@ -142,7 +142,7 @@ class TORCH_API Backend : public torch::CustomClassHolder {\n         c10::str(\n             \"Backend \",\n             getBackendName(),\n-            \"does not support allgather_coalesced\"));\n+            \" does not support allgather_coalesced\"));\n   }\n \n   // This function is a coalesced version of `allgather_into_tensor` (currently\n"
                },
                {
                    "old_start": 157,
                    "old_length": 7,
                    "new_start": 157,
                    "new_length": 7,
                    "hunk_buggy": "['         c10::str(\\n', '             \"Backend \",\\n', '             getBackendName(),\\n', '-            \"does not support allgather_into_tensor_coalesced\"));\\n', '   }\\n', ' \\n', '   virtual c10::intrusive_ptr<Work> gather(\\n']",
                    "hunk_fix": "@@ -157,7 +157,7 @@ class TORCH_API Backend : public torch::CustomClassHolder {\n         c10::str(\n             \"Backend \",\n             getBackendName(),\n-            \"does not support allgather_into_tensor_coalesced\"));\n+            \" does not support allgather_into_tensor_coalesced\"));\n   }\n \n   virtual c10::intrusive_ptr<Work> gather(\n"
                },
                {
                    "old_start": 166,
                    "old_length": 7,
                    "new_start": 166,
                    "new_length": 7,
                    "hunk_buggy": "['       const GatherOptions& /* opts */ = GatherOptions()) {\\n', '     TORCH_CHECK(\\n', '         false,\\n', '-        c10::str(\"Backend \", getBackendName(), \"does not support gather\"));\\n', '   }\\n', ' \\n', '   virtual c10::intrusive_ptr<Work> scatter(\\n']",
                    "hunk_fix": "@@ -166,7 +166,7 @@ class TORCH_API Backend : public torch::CustomClassHolder {\n       const GatherOptions& /* opts */ = GatherOptions()) {\n     TORCH_CHECK(\n         false,\n-        c10::str(\"Backend \", getBackendName(), \"does not support gather\"));\n+        c10::str(\"Backend \", getBackendName(), \" does not support gather\"));\n   }\n \n   virtual c10::intrusive_ptr<Work> scatter(\n"
                },
                {
                    "old_start": 175,
                    "old_length": 7,
                    "new_start": 175,
                    "new_length": 7,
                    "hunk_buggy": "['       const ScatterOptions& /* opts */ = ScatterOptions()) {\\n', '     TORCH_CHECK(\\n', '         false,\\n', '-        c10::str(\"Backend \", getBackendName(), \"does not support scatter\"));\\n', '   }\\n', ' \\n', '   virtual c10::intrusive_ptr<Work> reduce_scatter(\\n']",
                    "hunk_fix": "@@ -175,7 +175,7 @@ class TORCH_API Backend : public torch::CustomClassHolder {\n       const ScatterOptions& /* opts */ = ScatterOptions()) {\n     TORCH_CHECK(\n         false,\n-        c10::str(\"Backend \", getBackendName(), \"does not support scatter\"));\n+        c10::str(\"Backend \", getBackendName(), \" does not support scatter\"));\n   }\n \n   virtual c10::intrusive_ptr<Work> reduce_scatter(\n"
                },
                {
                    "old_start": 185,
                    "old_length": 7,
                    "new_start": 185,
                    "new_length": 7,
                    "hunk_buggy": "['     TORCH_CHECK(\\n', '         false,\\n', '         c10::str(\\n', '-            \"Backend \", getBackendName(), \"does not support reduce_scatter\"));\\n', '   }\\n', ' \\n', '   virtual c10::intrusive_ptr<Work> _reduce_scatter_base(\\n']",
                    "hunk_fix": "@@ -185,7 +185,7 @@ class TORCH_API Backend : public torch::CustomClassHolder {\n     TORCH_CHECK(\n         false,\n         c10::str(\n-            \"Backend \", getBackendName(), \"does not support reduce_scatter\"));\n+            \"Backend \", getBackendName(), \" does not support reduce_scatter\"));\n   }\n \n   virtual c10::intrusive_ptr<Work> _reduce_scatter_base(\n"
                },
                {
                    "old_start": 197,
                    "old_length": 7,
                    "new_start": 197,
                    "new_length": 7,
                    "hunk_buggy": "['         c10::str(\\n', '             \"Backend \",\\n', '             getBackendName(),\\n', '-            \"does not support _reduce_scatter_base\"));\\n', '   }\\n', ' \\n', '   virtual c10::intrusive_ptr<Work> alltoall_base(\\n']",
                    "hunk_fix": "@@ -197,7 +197,7 @@ class TORCH_API Backend : public torch::CustomClassHolder {\n         c10::str(\n             \"Backend \",\n             getBackendName(),\n-            \"does not support _reduce_scatter_base\"));\n+            \" does not support _reduce_scatter_base\"));\n   }\n \n   virtual c10::intrusive_ptr<Work> alltoall_base(\n"
                },
                {
                    "old_start": 209,
                    "old_length": 7,
                    "new_start": 209,
                    "new_length": 7,
                    "hunk_buggy": "['     TORCH_CHECK(\\n', '         false,\\n', '         c10::str(\\n', '-            \"Backend \", getBackendName(), \"does not support alltoall_base\"));\\n', '   }\\n', ' \\n', '   virtual c10::intrusive_ptr<Work> alltoall(\\n']",
                    "hunk_fix": "@@ -209,7 +209,7 @@ class TORCH_API Backend : public torch::CustomClassHolder {\n     TORCH_CHECK(\n         false,\n         c10::str(\n-            \"Backend \", getBackendName(), \"does not support alltoall_base\"));\n+            \"Backend \", getBackendName(), \" does not support alltoall_base\"));\n   }\n \n   virtual c10::intrusive_ptr<Work> alltoall(\n"
                },
                {
                    "old_start": 218,
                    "old_length": 7,
                    "new_start": 218,
                    "new_length": 7,
                    "hunk_buggy": "['       const AllToAllOptions& opts = AllToAllOptions()) {\\n', '     TORCH_CHECK(\\n', '         false,\\n', '-        c10::str(\"Backend \", getBackendName(), \"does not support alltoall\"));\\n', '   }\\n', ' \\n', '   virtual void monitoredBarrier(\\n']",
                    "hunk_fix": "@@ -218,7 +218,7 @@ class TORCH_API Backend : public torch::CustomClassHolder {\n       const AllToAllOptions& opts = AllToAllOptions()) {\n     TORCH_CHECK(\n         false,\n-        c10::str(\"Backend \", getBackendName(), \"does not support alltoall\"));\n+        c10::str(\"Backend \", getBackendName(), \" does not support alltoall\"));\n   }\n \n   virtual void monitoredBarrier(\n"
                },
                {
                    "old_start": 264,
                    "old_length": 7,
                    "new_start": 264,
                    "new_length": 7,
                    "hunk_buggy": "['       int /* dstRank */,\\n', '       int /* tag */) {\\n', '     TORCH_CHECK(\\n', '-        false, c10::str(\"Backend \", getBackendName(), \"does not support send\"));\\n', '   }\\n', ' \\n', '   virtual c10::intrusive_ptr<Work> recv(\\n']",
                    "hunk_fix": "@@ -264,7 +264,7 @@ class TORCH_API Backend : public torch::CustomClassHolder {\n       int /* dstRank */,\n       int /* tag */) {\n     TORCH_CHECK(\n-        false, c10::str(\"Backend \", getBackendName(), \"does not support send\"));\n+        false, c10::str(\"Backend \", getBackendName(), \" does not support send\"));\n   }\n \n   virtual c10::intrusive_ptr<Work> recv(\n"
                },
                {
                    "old_start": 272,
                    "old_length": 7,
                    "new_start": 272,
                    "new_length": 7,
                    "hunk_buggy": "['       int /* srcRank */,\\n', '       int /* tag */) {\\n', '     TORCH_CHECK(\\n', '-        false, c10::str(\"Backend \", getBackendName(), \"does not support recv\"));\\n', '   }\\n', ' \\n', '   virtual c10::intrusive_ptr<Work> recvAnysource(\\n']",
                    "hunk_fix": "@@ -272,7 +272,7 @@ class TORCH_API Backend : public torch::CustomClassHolder {\n       int /* srcRank */,\n       int /* tag */) {\n     TORCH_CHECK(\n-        false, c10::str(\"Backend \", getBackendName(), \"does not support recv\"));\n+        false, c10::str(\"Backend \", getBackendName(), \" does not support recv\"));\n   }\n \n   virtual c10::intrusive_ptr<Work> recvAnysource(\n"
                },
                {
                    "old_start": 281,
                    "old_length": 14,
                    "new_start": 281,
                    "new_length": 14,
                    "hunk_buggy": "['     TORCH_CHECK(\\n', '         false,\\n', '         c10::str(\\n', '-            \"Backend \", getBackendName(), \"does not support recvAnysource\"));\\n', '   }\\n', ' \\n', '   virtual c10::intrusive_ptr<Work> barrier(\\n', '       const BarrierOptions& /* opts */ = BarrierOptions()) {\\n', '     TORCH_CHECK(\\n', '         false,\\n', '-        c10::str(\"Backend \", getBackendName(), \"does not support barrier\"));\\n', '   }\\n', ' \\n', '  protected:']",
                    "hunk_fix": "@@ -281,14 +281,14 @@ class TORCH_API Backend : public torch::CustomClassHolder {\n     TORCH_CHECK(\n         false,\n         c10::str(\n-            \"Backend \", getBackendName(), \"does not support recvAnysource\"));\n+            \"Backend \", getBackendName(), \" does not support recvAnysource\"));\n   }\n \n   virtual c10::intrusive_ptr<Work> barrier(\n       const BarrierOptions& /* opts */ = BarrierOptions()) {\n     TORCH_CHECK(\n         false,\n-        c10::str(\"Backend \", getBackendName(), \"does not support barrier\"));\n+        c10::str(\"Backend \", getBackendName(), \" does not support barrier\"));\n   }\n \n  protected:"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 372,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/bde7b81f34925491fbcbb9e355697eb594e36923",
    "date": "2023-05-08T21:56:44+00:00",
    "message": "[S337714] Back out \"[PyTorch] Don't do extra numel() check in TensorImpl::data() (#98090)\" (#100893)\n\nSummary:\nOriginal commit changeset: 9875964c3b32\n\nOriginal Phabricator Diff: D44586464\n\nReviewed By: drdarshan\n\nDifferential Revision: D45664329\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/100893\nApproved by: https://github.com/xush6528",
    "changes": [
        {
            "name": "TensorImpl.h",
            "path": "c10/core/TensorImpl.h",
            "patches": [
                {
                    "old_start": 1627,
                    "old_length": 7,
                    "new_start": 1627,
                    "new_length": 7,
                    "hunk_buggy": "['     // Computing an offset into an empty tensor would be UB, since an empty\\n', \"     // tensor's storage will be nullptr, and adding a nonzero offset to nullptr\\n\", '     // is UB.  So we skip the offset computation in this case.\\n', '-    if (data == nullptr) {\\n', '       return nullptr;\\n', '     }\\n', '     return data + data_type_.itemsize() * storage_offset_;']",
                    "hunk_fix": "@@ -1627,7 +1627,7 @@ struct C10_API TensorImpl : public c10::intrusive_ptr_target {\n     // Computing an offset into an empty tensor would be UB, since an empty\n     // tensor's storage will be nullptr, and adding a nonzero offset to nullptr\n     // is UB.  So we skip the offset computation in this case.\n-    if (data == nullptr) {\n+    if (is_empty()) {\n       return nullptr;\n     }\n     return data + data_type_.itemsize() * storage_offset_;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 373,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/97245a06e14a5b87a0bca1908d7200603aff2c8c",
    "date": "2023-05-04T15:06:27+00:00",
    "message": "Turn on TORCH_CHECK for NT wrap_buffer (#100596)\n\nTORCH_INTERNAL_ASSERT_DEBUG_ONLY won't be enabled during non-debug builds, but for 1 dimension Tensors the check is cheap enough and not catching this can slow down development a lot.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/100596\nApproved by: https://github.com/drisspg",
    "changes": [
        {
            "name": "NestedTensorUtils.h",
            "path": "aten/src/ATen/native/nested/NestedTensorUtils.h",
            "patches": [
                {
                    "old_start": 32,
                    "old_length": 11,
                    "new_start": 32,
                    "new_length": 14,
                    "hunk_buggy": "[' // The following functions are used to construct nested tensors from buffers and\\n', ' // metadata.\\n', ' \\n', '-inline at::Tensor wrap_buffer(\\n', '-    at::Tensor buffer,\\n', '-    at::Tensor nested_sizes) {\\n', '-  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(\\n', '-      buffer.is_contiguous(), \"Given buffer must be contiguous.\");\\n', '   return at::detail::make_tensor<NestedTensorImpl>(\\n', '       std::move(buffer), std::move(nested_sizes));\\n', ' }']",
                    "hunk_fix": "@@ -32,11 +32,14 @@ struct NestedTensorImpl;\n // The following functions are used to construct nested tensors from buffers and\n // metadata.\n \n-inline at::Tensor wrap_buffer(\n-    at::Tensor buffer,\n-    at::Tensor nested_sizes) {\n-  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(\n-      buffer.is_contiguous(), \"Given buffer must be contiguous.\");\n+inline at::Tensor wrap_buffer(at::Tensor buffer, at::Tensor nested_sizes) {\n+  TORCH_CHECK(\n+      buffer.dim() == 1,\n+      \"Expected given buffer to be 1dim, but got \",\n+      buffer.dim(),\n+      \" instead.\");\n+  TORCH_CHECK(\n+      buffer.is_contiguous(), \"Expected given buffer to be contiguous.\");\n   return at::detail::make_tensor<NestedTensorImpl>(\n       std::move(buffer), std::move(nested_sizes));\n }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 374,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/794e3971ab90611b4a63166589368a737843c8bc",
    "date": "2023-05-02T18:50:31+00:00",
    "message": "Add size check before calling stack_.at(dict_pos) in unpickler.cpp (#94300)\n\nHi!\n\nI've been fuzzing different pytorch modules, and found a crash inside one of them.\n\nSpecifically, I'm talking about a module for unpickling and a function called `Unpickler::readInstruction()`. Running this function with provided crash file results in a crash, which occurs while calling `auto dict = stack_.at(dict_pos).toGenericDict();` [unpickler.cpp:561](https://github.com/pytorch/pytorch/blob/0e94fbc0c8ab1572c88159c1a4c397b6eb824c01/torch/csrc/jit/serialization/unpickler.cpp#L561). The crash occurs, because the index `dict_pos` is out of bounds (which itself happens because the stack size is 0).\n\nBesides this pull-request, there is another one related to unpickler hardening: https://github.com/pytorch/pytorch/pull/84343\n\nAll tests were performed on this pytorch version: [abc54f93145830b502400faa92bec86e05422fbd](https://github.com/pytorch/pytorch/tree/abc54f93145830b502400faa92bec86e05422fbd)\n\n### How to reproduce\n\n1. To reproduce the crash, use provided docker: [Dockerfile](https://github.com/ispras/oss-sydr-fuzz/tree/master/projects/pytorch)\n\n2. Build the container: `docker build -t oss-sydr-fuzz-pytorch-reproduce .`\n\n3. Copy crash file to the current directory:\n\n    - [crash-042dff5e121580425d9d34d0f293918f3c9fbf1e.zip](https://github.com/pytorch/pytorch/files/10674361/crash-042dff5e121580425d9d34d0f293918f3c9fbf1e.zip)\n\n4. Run the container: ``docker run --privileged --network host -v `pwd`:/homedir --rm -it oss-sydr-fuzz-pytorch-reproduce /bin/bash``\n\n5. And execute the binary: `/message_deserialize_sydr /homedir/crash-042dff5e121580425d9d34d0f293918f3c9fbf1e`\n\nAfter execution completes you will see this error message:\n\n```txt\nterminate called after throwing an instance of 'std::out_of_range'\n  what():  vector::_M_range_check: __n (which is 18446744073709551613) >= this->size() (which is 0)\n```\n\nAnd this stacktrace:\n\n```asan\nerminate called after throwing an instance of 'std::out_of_range'\n  what():  vector::_M_range_check: __n (which is 18446744073709551613) >= this->size() (which is 0)\n==39== ERROR: libFuzzer: deadly signal\n    #0 0x5d0df1 in __sanitizer_print_stack_trace /llvm-project/compiler-rt/lib/asan/asan_stack.cpp:87:3\n    #1 0x545727 in fuzzer::PrintStackTrace() /llvm-project/compiler-rt/lib/fuzzer/FuzzerUtil.cpp:210:5\n    #2 0x52b933 in fuzzer::Fuzzer::CrashCallback() /llvm-project/compiler-rt/lib/fuzzer/FuzzerLoop.cpp:233:3\n    #3 0x7f9118e0341f  (/lib/x86_64-linux-gnu/libpthread.so.0+0x1441f)\n    #4 0x7f9118c2300a in raise (/lib/x86_64-linux-gnu/libc.so.6+0x4300a)\n    #5 0x7f9118c02858 in abort (/lib/x86_64-linux-gnu/libc.so.6+0x22858)\n    #6 0x7f9119040910  (/lib/x86_64-linux-gnu/libstdc++.so.6+0x9e910)\n    #7 0x7f911904c38b  (/lib/x86_64-linux-gnu/libstdc++.so.6+0xaa38b)\n    #8 0x7f911904c3f6 in std::terminate() (/lib/x86_64-linux-gnu/libstdc++.so.6+0xaa3f6)\n    #9 0x7f911904c6a8 in __cxa_throw (/lib/x86_64-linux-gnu/libstdc++.so.6+0xaa6a8)\n    #10 0x7f91190433aa  (/lib/x86_64-linux-gnu/libstdc++.so.6+0xa13aa)\n    #11 0x63acdf in std::vector<c10::IValue, std::allocator<c10::IValue> >::_M_range_check(unsigned long) const /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/stl_vector.h:1073:4\n    #12 0xce8f93e in std::vector<c10::IValue, std::allocator<c10::IValue> >::at(unsigned long) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/stl_vector.h:1094:2\n    #13 0xce8f93e in torch::jit::Unpickler::readInstruction() /pytorch_fuzz/torch/csrc/jit/serialization/unpickler.cpp:546:26\n    #14 0xce8d527 in torch::jit::Unpickler::run() /pytorch_fuzz/torch/csrc/jit/serialization/unpickler.cpp:235:27\n    #15 0xce8d1c2 in torch::jit::Unpickler::parse_ivalue() /pytorch_fuzz/torch/csrc/jit/serialization/unpickler.cpp:192:3\n    #16 0xcdf0792 in torch::jit::unpickle(std::function<unsigned long (char*, unsigned long)>, std::function<c10::StrongTypePtr (c10::QualifiedName const&)>, c10::ArrayRef<at::Tensor>, c10::Type::SingletonOrSharedTypePtr<c10::Type> (*)(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)) /pytorch_fuzz/torch/csrc/jit/serialization/pickle.cpp:127:20\n    #17 0xcdf104d in torch::jit::unpickle(char const*, unsigned long, std::function<c10::StrongTypePtr (c10::QualifiedName const&)>, c10::ArrayRef<at::Tensor>, c10::Type::SingletonOrSharedTypePtr<c10::Type> (*)(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)) /pytorch_fuzz/torch/csrc/jit/serialization/pickle.cpp:137:10\n    #18 0xe0532db in torch::distributed::rpc::ScriptRemoteCall::fromMessage(torch::distributed::rpc::Message const&) /pytorch_fuzz/torch/csrc/distributed/rpc/script_remote_call.cpp:74:16\n    #19 0xe0ffa10 in torch::distributed::rpc::deserializeRequest(torch::distributed::rpc::Message const&) /pytorch_fuzz/torch/csrc/distributed/rpc/utils.cpp:108:14\n    #20 0x602a41 in LLVMFuzzerTestOneInput /message_deserialize_fuzz.cc:192:27\n    #21 0x52ce61 in fuzzer::Fuzzer::ExecuteCallback(unsigned char const*, unsigned long) /llvm-project/compiler-rt/lib/fuzzer/FuzzerLoop.cpp:611:15\n    #22 0x516d7c in fuzzer::RunOneTest(fuzzer::Fuzzer*, char const*, unsigned long) /llvm-project/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:324:6\n    #23 0x51cacb in fuzzer::FuzzerDriver(int*, char***, int (*)(unsigned char const*, unsigned long)) /llvm-project/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:860:9\n    #24 0x546062 in main /llvm-project/compiler-rt/lib/fuzzer/FuzzerMain.cpp:20:10\n    #25 0x7f9118c04082 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x24082)\n    #26 0x51169d in _start (/message_deserialize_fuzz+0x51169d)\n\nNOTE: libFuzzer has rudimentary signal handlers.\n      Combine libFuzzer with AddressSanitizer or similar for better crash reports.\nSUMMARY: libFuzzer: deadly signal\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/94300\nApproved by: https://github.com/malfet, https://github.com/apach301",
    "changes": [
        {
            "name": "unpickler.cpp",
            "path": "torch/csrc/jit/serialization/unpickler.cpp",
            "patches": [
                {
                    "old_start": 597,
                    "old_length": 10,
                    "new_start": 597,
                    "new_length": 20,
                    "hunk_buggy": "['       // | Dict         | -> (stack_size - 3)\\n', '       // | Key          | -> (stack_size - 2)\\n', '       // | Value        | -> (stack_size - 1)\\n', '       auto stack_size = stack_.size();\\n', '       auto dict_pos = stack_size - 3;\\n', '       auto key_pos = stack_size - 2;\\n', '       auto val_pos = stack_size - 1;\\n', '       auto dict = stack_.at(dict_pos).toGenericDict();\\n', '       dict.insert_or_assign(stack_.at(key_pos), stack_.at(val_pos));\\n', '       stack_.erase(stack_.begin() + (key_pos), stack_.end());']",
                    "hunk_fix": "@@ -597,10 +597,20 @@ PickleOpCode Unpickler::readInstruction() {\n       // | Dict         | -> (stack_size - 3)\n       // | Key          | -> (stack_size - 2)\n       // | Value        | -> (stack_size - 1)\n+      TORCH_CHECK(\n+          stack_.size() >= 3,\n+          \"Parsing error: stack doesn't have enough elements\");\n+\n       auto stack_size = stack_.size();\n       auto dict_pos = stack_size - 3;\n       auto key_pos = stack_size - 2;\n       auto val_pos = stack_size - 1;\n+\n+      TORCH_CHECK(\n+          (dict_pos < stack_size) && (key_pos < stack_size) &&\n+              (val_pos < stack_size),\n+          \"Parsing error: attempted out-of-bounds access while processing SETITEM opcode\");\n+\n       auto dict = stack_.at(dict_pos).toGenericDict();\n       dict.insert_or_assign(stack_.at(key_pos), stack_.at(val_pos));\n       stack_.erase(stack_.begin() + (key_pos), stack_.end());"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 375,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/7684044b713761abd4f51225dc5d83ce5869562a",
    "date": "2023-04-29T00:26:35+00:00",
    "message": "Add size check before calling .back() in rpc/script_call.cpp (#94297)\n\nHi!\n\nI've been fuzzing different pytorch modules, and found a crash inside one of them.\n\nSpecifically, I'm talking about a module that processes `script_call` rpc requests and a function `ScriptCall::fromIValues(std::vector<at::IValue>& ivalues)`.\n\nRunning this test case causes a crash that occurs when `ivalues.back()` is called [script_call.cpp:90](https://github.com/pytorch/pytorch/blob/abc54f93145830b502400faa92bec86e05422fbd/torch/csrc/distributed/rpc/script_call.cpp#L90). The crash occurs because the vector `ivalues` is empty.\n\nAll tests were performed on this pytorch version: [abc54f93145830b502400faa92bec86e05422fbd](https://github.com/pytorch/pytorch/tree/abc54f93145830b502400faa92bec86e05422fbd)\n\nThe provided patch checks if there are enough elements in the ivalues vector.\n\n### How to reproduce\n\n1. To reproduce the crash, use provided docker: [Dockerfile](https://github.com/ispras/oss-sydr-fuzz/tree/master/projects/pytorch)\n\n2. Build the container: `docker build -t oss-sydr-fuzz-pytorch-reproduce .`\n\n3. Copy crash file to the current directory:\n\n    - [crash-9f76d4e37a2391136a4ce07d47269db1e063e4b4.zip](https://github.com/pytorch/pytorch/files/10674059/crash-9f76d4e37a2391136a4ce07d47269db1e063e4b4.zip)\n\n4. Run the container: ``docker run --privileged --network host -v `pwd`:/homedir --rm -it oss-sydr-fuzz-pytorch-reproduce /bin/bash``\n\n5. And execute the binary: `/message_deserialize_fuzz /homedir/crash-9f76d4e37a2391136a4ce07d47269db1e063e4b4`\n\nAfter execution completes you will see this stacktrace:\n\n```asan\nAddressSanitizer:DEADLYSIGNAL\n=================================================================\n==57==ERROR: AddressSanitizer: SEGV on unknown address (pc 0x0000008e7b19 bp 0x7ffd2fdded70 sp 0x7ffd2fddec40 T0)\n==57==The signal is caused by a READ memory access.\n==57==Hint: this fault was caused by a dereference of a high value address (see register values below).  Disassemble the provided pc to learn which register was used.\n    #0 0x8e7b19 in c10::IValue::isString() const /pytorch_fuzz/aten/src/ATen/core/ivalue.h:639:27\n    #1 0x8e7b19 in c10::IValue::toStringRef[abi:cxx11]() const /pytorch_fuzz/aten/src/ATen/core/ivalue_inl.h:2179:3\n    #2 0xe04fb58 in torch::distributed::rpc::ScriptCall::fromIValues(std::vector<c10::IValue, std::allocator<c10::IValue> >&) /pytorch_fuzz/torch/csrc/distributed/rpc/script_call.cpp:90:53\n    #3 0xe0511f0 in torch::distributed::rpc::ScriptCall::fromMessage(torch::distributed::rpc::Message const&) /pytorch_fuzz/torch/csrc/distributed/rpc/script_call.cpp:133:10\n    #4 0xe0ff71e in torch::distributed::rpc::deserializeRequest(torch::distributed::rpc::Message const&) /pytorch_fuzz/torch/csrc/distributed/rpc/utils.cpp:102:14\n    #5 0x602a41 in LLVMFuzzerTestOneInput /message_deserialize_fuzz.cc:192:27\n    #6 0x52ce61 in fuzzer::Fuzzer::ExecuteCallback(unsigned char const*, unsigned long) /llvm-project/compiler-rt/lib/fuzzer/FuzzerLoop.cpp:611:15\n    #7 0x516d7c in fuzzer::RunOneTest(fuzzer::Fuzzer*, char const*, unsigned long) /llvm-project/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:324:6\n    #8 0x51cacb in fuzzer::FuzzerDriver(int*, char***, int (*)(unsigned char const*, unsigned long)) /llvm-project/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:860:9\n    #9 0x546062 in main /llvm-project/compiler-rt/lib/fuzzer/FuzzerMain.cpp:20:10\n    #10 0x7f41e42a8082 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x24082)\n    #11 0x51169d in _start (/message_deserialize_fuzz+0x51169d)\n\nAddressSanitizer can not provide additional info.\nSUMMARY: AddressSanitizer: SEGV /pytorch_fuzz/aten/src/ATen/core/ivalue.h:639:27 in c10::IValue::isString() const\n==57==ABORTING\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/94297\nApproved by: https://github.com/ezyang",
    "changes": [
        {
            "name": "script_call.cpp",
            "path": "torch/csrc/distributed/rpc/script_call.cpp",
            "patches": [
                {
                    "old_start": 83,
                    "old_length": 6,
                    "new_start": 83,
                    "new_length": 10,
                    "hunk_buggy": "[' \\n', ' std::unique_ptr<ScriptCall> ScriptCall::fromIValues(\\n', '     std::vector<at::IValue>& ivalues) {\\n', '   // Last element in the vector is always qualifiedName for both\\n', '   // builitin operator and TorchScript function\\n', '   // If the qualifiedName is not a builtin operator name, then treat it']",
                    "hunk_fix": "@@ -83,6 +83,10 @@ void ScriptCall::toIValues(std::vector<at::IValue>& ivalues) const {\n \n std::unique_ptr<ScriptCall> ScriptCall::fromIValues(\n     std::vector<at::IValue>& ivalues) {\n+  TORCH_INTERNAL_ASSERT(\n+      ivalues.size() > 1,\n+      \"At least 2 IValues are required to build a ScriptCall.\");\n+\n   // Last element in the vector is always qualifiedName for both\n   // builitin operator and TorchScript function\n   // If the qualifiedName is not a builtin operator name, then treat it"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 376,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/8ee59280d78a4fefc4de0da04b287e067c28de0d",
    "date": "2023-04-21T06:40:09+00:00",
    "message": "Bug - check config for dynamic (#99676)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/99676\nApproved by: https://github.com/ezyang",
    "changes": [
        {
            "name": "builder.py",
            "path": "torch/_dynamo/variables/builder.py",
            "patches": [
                {
                    "old_start": 1185,
                    "old_length": 8,
                    "new_start": 1185,
                    "new_length": 9,
                    "hunk_buggy": "['                 marked_static = i in getattr(e, \"_dynamo_static_indices\", set())\\n', ' \\n', '                 # NB: both static and dynamic have precedence over\\n', '-                automatic_dynamic = curr_sizes is None or curr_sizes[i] is None\\n', '-\\n', '                 # We will process constraints first, as they will imply that we\\n', '                 # have a dynamic dimension\\n', '                 # Precedence: export constraints > eager constraints']",
                    "hunk_fix": "@@ -1185,8 +1185,9 @@ def wrap_to_fake_tensor_and_record(\n                 marked_static = i in getattr(e, \"_dynamo_static_indices\", set())\n \n                 # NB: both static and dynamic have precedence over\n-                automatic_dynamic = curr_sizes is None or curr_sizes[i] is None\n-\n+                automatic_dynamic = config.automatic_dynamic_shapes and (\n+                    curr_sizes is None or curr_sizes[i] is None\n+                )\n                 # We will process constraints first, as they will imply that we\n                 # have a dynamic dimension\n                 # Precedence: export constraints > eager constraints"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 377,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/a3701b674046bcefb5927a6643364b186f77dbcf",
    "date": "2023-04-10T15:56:38+00:00",
    "message": "fix backward bug for custom device (#98586)\n\nFixes #ISSUE_NUMBER\nIn the backward on some device , it may get an error to get device index because of exchange a new thread.\nSo just set_device and check the device index in `setDevice`  func may be better for some many kinds of devices.\nFor CUDA, the device index check is also included in `setDevice`  func.https://github.com/pytorch/pytorch/blob/master/c10/cuda/impl/CUDAGuardImpl.h#:~:text=%7D-,void%20setDevice(Device%20d)%20const%20override%20%7B,%7D,-void%20uncheckedSetDevice(Device\n```\nvoid setDevice(Device d) const override {\n    TORCH_INTERNAL_ASSERT(d.is_cuda());\n    Device current_device = getDevice();\n    if (current_device != d) {\n      C10_CUDA_CHECK(cudaSetDevice(d.index()));\n    }\n  }\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/98586\nApproved by: https://github.com/albanD",
    "changes": [
        {
            "name": "engine.cpp",
            "path": "torch/csrc/autograd/engine.cpp",
            "patches": [
                {
                    "old_start": 782,
                    "old_length": 8,
                    "new_start": 782,
                    "new_length": 7,
                    "hunk_buggy": "['     for (const auto i : c10::irange(static_cast<size_t>(\\n', '              c10::DeviceType::COMPILE_TIME_MAX_DEVICE_TYPES))) {\\n', '       auto* impl = c10::impl::device_guard_impl_registry[i].load();\\n', '-      if (impl && device < impl->deviceCount() &&\\n', '-          impl->getDevice().index() != device) {\\n', '         impl->setDevice(at::Device(static_cast<c10::DeviceType>(i), device));\\n', '       }\\n', '     }']",
                    "hunk_fix": "@@ -782,8 +782,7 @@ void set_device(int device) {\n     for (const auto i : c10::irange(static_cast<size_t>(\n              c10::DeviceType::COMPILE_TIME_MAX_DEVICE_TYPES))) {\n       auto* impl = c10::impl::device_guard_impl_registry[i].load();\n-      if (impl && device < impl->deviceCount() &&\n-          impl->getDevice().index() != device) {\n+      if (impl && device < impl->deviceCount()) {\n         impl->setDevice(at::Device(static_cast<c10::DeviceType>(i), device));\n       }\n     }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 378,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/f386312ec936a94bfb1abe44acdd61d498f4272b",
    "date": "2023-04-04T00:59:52+00:00",
    "message": "[PyTorch] Don't do extra numel() check in TensorImpl::data() (#98090)\n\n`is_empty()` checks `numel() == 0`, but we don't need to access `numel_` at all (or the policy that `numel()` checks) in our happy path -- we just need the data pointer from `storage_`. Let's do the check we need to do using only the data we strictly need, rather than adding instructions loading other pieces of data.\n\nDifferential Revision: [D44586464](https://our.internmc.facebook.com/intern/diff/D44586464/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/98090\nApproved by: https://github.com/Skylion007",
    "changes": [
        {
            "name": "TensorImpl.h",
            "path": "c10/core/TensorImpl.h",
            "patches": [
                {
                    "old_start": 1541,
                    "old_length": 12,
                    "new_start": 1541,
                    "new_length": 11,
                    "hunk_buggy": "['     // Computing an offset into an empty tensor would be UB, since an empty\\n', \"     // tensor's storage will be nullptr, and adding a nonzero offset to nullptr\\n\", '     // is UB.  So we skip the offset computation in this case.\\n', '-    if (is_empty()) {\\n', '       return nullptr;\\n', '     }\\n', '-    return static_cast<void*>(\\n', '-        static_cast<char*>(storage_.data()) +\\n', '-        data_type_.itemsize() * storage_offset_);\\n', '   }\\n', ' \\n', '   /**']",
                    "hunk_fix": "@@ -1541,12 +1541,11 @@ struct C10_API TensorImpl : public c10::intrusive_ptr_target {\n     // Computing an offset into an empty tensor would be UB, since an empty\n     // tensor's storage will be nullptr, and adding a nonzero offset to nullptr\n     // is UB.  So we skip the offset computation in this case.\n-    if (is_empty()) {\n+    char* const data = static_cast<char*>(storage_.data());\n+    if (data == nullptr) {\n       return nullptr;\n     }\n-    return static_cast<void*>(\n-        static_cast<char*>(storage_.data()) +\n-        data_type_.itemsize() * storage_offset_);\n+    return static_cast<void*>(data + data_type_.itemsize() * storage_offset_);\n   }\n \n   /**"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 379,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/35be57970143236d74661f2415d66d496aab5476",
    "date": "2023-03-29T23:08:03+00:00",
    "message": "Refactor TENSOR_MATCH guards to check dim (for NT support) (#97896)\n\nTweaks the TENSOR_MATCH guard logic to avoid saving sizes / strides for the case of dynamic shapes. Instead, the dim() is stored, which is enough for both dense tensors and NTs.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/97896\nApproved by: https://github.com/ezyang",
    "changes": [
        {
            "name": "guards.cpp",
            "path": "torch/csrc/dynamo/guards.cpp",
            "patches": [
                {
                    "old_start": 33,
                    "old_length": 14,
                    "new_start": 33,
                    "new_length": 16,
                    "hunk_buggy": "['         device_index_(v.device().index()),\\n', '         requires_grad_(state.grad_mode_enabled && v.requires_grad()),\\n', '         dynamic_shapes_(dynamic_shapes) {\\n', '-    auto ndim = v.ndimension();\\n', '-    const auto& sizes = v.sizes();\\n', '-    const auto& strides = v.strides();\\n', '-    sizes_.reserve(ndim);\\n', '-    strides_.reserve(ndim);\\n', '-    for (auto i : c10::irange(ndim)) {\\n', '-      sizes_.emplace_back(sizes[i]);\\n', '-      strides_.emplace_back(strides[i]);\\n', '     }\\n', '   }\\n', ' \\n']",
                    "hunk_fix": "@@ -33,14 +33,16 @@ class TensorCheck {\n         device_index_(v.device().index()),\n         requires_grad_(state.grad_mode_enabled && v.requires_grad()),\n         dynamic_shapes_(dynamic_shapes) {\n-    auto ndim = v.ndimension();\n-    const auto& sizes = v.sizes();\n-    const auto& strides = v.strides();\n-    sizes_.reserve(ndim);\n-    strides_.reserve(ndim);\n-    for (auto i : c10::irange(ndim)) {\n-      sizes_.emplace_back(sizes[i]);\n-      strides_.emplace_back(strides[i]);\n+    dim_ = v.ndimension();\n+    if (!dynamic_shapes_) {\n+      const auto& sizes = v.sizes();\n+      const auto& strides = v.strides();\n+      sizes_.reserve(dim_);\n+      strides_.reserve(dim_);\n+      for (auto i : c10::irange(dim_)) {\n+        sizes_.emplace_back(sizes[i]);\n+        strides_.emplace_back(strides[i]);\n+      }\n     }\n   }\n \n"
                },
                {
                    "old_start": 53,
                    "old_length": 8,
                    "new_start": 55,
                    "new_length": 8,
                    "hunk_buggy": "['         requires_grad_ != (state.grad_mode_enabled && v.requires_grad())) {\\n', '       return false;\\n', '     }\\n', '-    auto ndim = static_cast<size_t>(v.ndimension());\\n', '-    if (ndim != sizes_.size()) {\\n', '       return false;\\n', '     }\\n', '     if (!dynamic_shapes_) {\\n']",
                    "hunk_fix": "@@ -53,8 +55,8 @@ class TensorCheck {\n         requires_grad_ != (state.grad_mode_enabled && v.requires_grad())) {\n       return false;\n     }\n-    auto ndim = static_cast<size_t>(v.ndimension());\n-    if (ndim != sizes_.size()) {\n+    auto ndim = v.ndimension();\n+    if (ndim != dim_) {\n       return false;\n     }\n     if (!dynamic_shapes_) {\n"
                },
                {
                    "old_start": 102,
                    "old_length": 8,
                    "new_start": 104,
                    "new_length": 8,
                    "hunk_buggy": "['                   << requires_grad_;\\n', '       return fail_reason.str();\\n', '     }\\n', '-    size_t ndim = static_cast<size_t>(v.ndimension());\\n', '-    if (ndim != sizes_.size()) {\\n', '       // return fmt::format(\"tensor rank mismatch. expected {}, actual {}\",\\n', '       // sizes_.size(), ndim);\\n', '       fail_reason << \"rank mismatch. expected \" << sizes_.size() << \", actual \"\\n']",
                    "hunk_fix": "@@ -102,8 +104,8 @@ class TensorCheck {\n                   << requires_grad_;\n       return fail_reason.str();\n     }\n-    size_t ndim = static_cast<size_t>(v.ndimension());\n-    if (ndim != sizes_.size()) {\n+    auto ndim = v.ndimension();\n+    if (ndim != dim_) {\n       // return fmt::format(\"tensor rank mismatch. expected {}, actual {}\",\n       // sizes_.size(), ndim);\n       fail_reason << \"rank mismatch. expected \" << sizes_.size() << \", actual \"\n"
                },
                {
                    "old_start": 143,
                    "old_length": 8,
                    "new_start": 145,
                    "new_length": 11,
                    "hunk_buggy": "['   at::DeviceIndex device_index_;\\n', '   bool requires_grad_;\\n', '   bool dynamic_shapes_;\\n', '   std::vector<int64_t> sizes_;\\n', '   std::vector<int64_t> strides_;\\n', ' };\\n', ' \\n', ' typedef std::vector<TensorCheck> ChecksList;']",
                    "hunk_fix": "@@ -143,8 +145,11 @@ class TensorCheck {\n   at::DeviceIndex device_index_;\n   bool requires_grad_;\n   bool dynamic_shapes_;\n+  // NB: These are unset if dynamic shapes is enabled.\n   std::vector<int64_t> sizes_;\n   std::vector<int64_t> strides_;\n+  // Not strictly required for dense tensors, but nested tensors need it.\n+  int64_t dim_;\n };\n \n typedef std::vector<TensorCheck> ChecksList;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 380,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/4ab1588d9919bc1a62219a5c2393e0784ddaae70",
    "date": "2023-03-22T08:42:48+00:00",
    "message": "Enhance error message for dependency check (#96642)\n\nIf python development library is missing when building pytorch from source, cmake will raise the error like:\n```\nCMake Error at cmake/Dependencies.cmake:1079 (if):\n  if given arguments:\n\n    \"VERSION_LESS\" \"3\"\n\n  Unknown arguments specified\n```\n\nit's quite a misleading information that user would consider it's a syntax error or cmake version problem.\n\nThis PR add a check to ensure `PYTHONLIBS_VERSION_STRING` exist before using.\n\nRelated  #87993\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/96642\nApproved by: https://github.com/kit1980",
    "changes": [
        {
            "name": "Dependencies.cmake",
            "path": "cmake/Dependencies.cmake",
            "patches": [
                {
                    "old_start": 1069,
                    "old_length": 6,
                    "new_start": 1069,
                    "new_length": 11,
                    "hunk_buggy": "['   find_package(PythonInterp 3.0)\\n', '   find_package(PythonLibs 3.0)\\n', ' \\n', '   if(${PYTHONLIBS_VERSION_STRING} VERSION_LESS 3)\\n', '     message(FATAL_ERROR\\n', '       \"Found Python libraries version ${PYTHONLIBS_VERSION_STRING}. Python 2 has reached end-of-life and is no longer supported by PyTorch.\")']",
                    "hunk_fix": "@@ -1069,6 +1069,11 @@ if(BUILD_PYTHON)\n   find_package(PythonInterp 3.0)\n   find_package(PythonLibs 3.0)\n \n+  if(NOT PYTHONLIBS_VERSION_STRING)\n+    message(FATAL_ERROR\n+      \"Python development libraries could not be found.\")\n+  endif()\n+\n   if(${PYTHONLIBS_VERSION_STRING} VERSION_LESS 3)\n     message(FATAL_ERROR\n       \"Found Python libraries version ${PYTHONLIBS_VERSION_STRING}. Python 2 has reached end-of-life and is no longer supported by PyTorch.\")"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 381,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/45296f87ec865a7a500a6fd98353035c040d0cb7",
    "date": "2023-03-21T03:19:31+00:00",
    "message": "Fix for verify_dynamo on ROCm (#97013)\n\nPrior to this change ROCm was not exiting check_cuda, causing an exception at packaging.version.parse(torch.version.cuda), this change exits check_cuda if torch.version.cuda is None\n\n```\npython verify_dynamo.py\n\nPython version: 3.9.16\n`torch` version: 2.1.0a0+git2b2f10c\nCUDA version: None\nROCM version: 5.4\n\nAll required checks passed\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/97013\nApproved by: https://github.com/jithunnair-amd, https://github.com/malfet, https://github.com/kit1980",
    "changes": [
        {
            "name": "verify_dynamo.py",
            "path": "tools/dynamo/verify_dynamo.py",
            "patches": [
                {
                    "old_start": 81,
                    "old_length": 7,
                    "new_start": 81,
                    "new_length": 7,
                    "hunk_buggy": "[' def check_cuda():\\n', '     import torch\\n', ' \\n', '-    if not torch.cuda.is_available():\\n', '         return None\\n', ' \\n', '     torch_cuda_ver = packaging.version.parse(torch.version.cuda)']",
                    "hunk_fix": "@@ -81,7 +81,7 @@ def get_rocm_version():\n def check_cuda():\n     import torch\n \n-    if not torch.cuda.is_available():\n+    if not torch.cuda.is_available() or torch.version.hip is not None:\n         return None\n \n     torch_cuda_ver = packaging.version.parse(torch.version.cuda)"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 382,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/c69b3b8d4f484cf537d98974a3a4143b77edf3c8",
    "date": "2023-03-13T20:04:12+00:00",
    "message": "[CUDA12] Autograd engine use current device only (#92354)\n\nThis is a device agnostic version #91191.\nThe reason of existence of this PR is device agnostic policy of autograd engine. Hence, the compile time `USE_CUDA` is not supported, so doing something like:\nhttps://github.com/pytorch/pytorch/blob/fa1ea9f9bcaa77c1370468059be95ad9b421f500/torch/csrc/autograd/engine.cpp#L351-L357\nis not effective.\n\nIn this PR a check upon CUDA devices in device registry is added such that threads set the same CUDA device.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/92354\nApproved by: https://github.com/albanD, https://github.com/ngimel",
    "changes": [
        {
            "name": "engine.cpp",
            "path": "torch/csrc/autograd/engine.cpp",
            "patches": [
                {
                    "old_start": 347,
                    "old_length": 14,
                    "new_start": 347,
                    "new_length": 7,
                    "hunk_buggy": "[\"   // We don't have any good reason to prefer one or the other, so we've\\n\", '   // arbitrarily picked to colocate devices.  Maybe the other approach is\\n', '   // better.\\n', '-\\n', '-#if defined(USE_CUDA)\\n', '-  if (at::detail::getCUDAHooks().hasPrimaryContext(device)) {\\n', '-    set_device(device);\\n', '-  }\\n', '-#else\\n', '-  set_device(device);\\n', '-#endif\\n', ' \\n', \"   // initialize each device thread's thread local ready queue with the ready\\n\", '   // queue that is created before the thread initialization\\n']",
                    "hunk_fix": "@@ -347,14 +347,7 @@ void Engine::thread_init(\n   // We don't have any good reason to prefer one or the other, so we've\n   // arbitrarily picked to colocate devices.  Maybe the other approach is\n   // better.\n-\n-#if defined(USE_CUDA)\n-  if (at::detail::getCUDAHooks().hasPrimaryContext(device)) {\n-    set_device(device);\n-  }\n-#else\n-  set_device(device);\n-#endif\n+  worker_device = device;\n \n   // initialize each device thread's thread local ready queue with the ready\n   // queue that is created before the thread initialization\n"
                },
                {
                    "old_start": 526,
                    "old_length": 6,
                    "new_start": 519,
                    "new_length": 8,
                    "hunk_buggy": "['         continue;\\n', '       }\\n', ' \\n', '       if (task.fn_ && !local_graph_task->has_error_.load()) {\\n', '         // Set the ThreadLocalState before calling the function.\\n', \"         // NB: The ThreadLocalStateGuard doesn't set the grad_mode because\\n\"]",
                    "hunk_fix": "@@ -526,6 +519,8 @@ auto Engine::thread_main(const std::shared_ptr<GraphTask>& graph_task) -> void {\n         continue;\n       }\n \n+      set_device(worker_device);\n+\n       if (task.fn_ && !local_graph_task->has_error_.load()) {\n         // Set the ThreadLocalState before calling the function.\n         // NB: The ThreadLocalStateGuard doesn't set the grad_mode because\n"
                },
                {
                    "old_start": 787,
                    "old_length": 7,
                    "new_start": 782,
                    "new_length": 8,
                    "hunk_buggy": "['     for (const auto i : c10::irange(static_cast<size_t>(\\n', '              c10::DeviceType::COMPILE_TIME_MAX_DEVICE_TYPES))) {\\n', '       auto* impl = c10::impl::device_guard_impl_registry[i].load();\\n', '-      if (impl && device < impl->deviceCount()) {\\n', '         impl->setDevice(at::Device(static_cast<c10::DeviceType>(i), device));\\n', '       }\\n', '     }']",
                    "hunk_fix": "@@ -787,7 +782,8 @@ void set_device(int device) {\n     for (const auto i : c10::irange(static_cast<size_t>(\n              c10::DeviceType::COMPILE_TIME_MAX_DEVICE_TYPES))) {\n       auto* impl = c10::impl::device_guard_impl_registry[i].load();\n-      if (impl && device < impl->deviceCount()) {\n+      if (impl && device < impl->deviceCount() &&\n+          impl->getDevice().index() != device) {\n         impl->setDevice(at::Device(static_cast<c10::DeviceType>(i), device));\n       }\n     }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 383,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/a5aceba61fc290236af955e2c4fff4513476c9ff",
    "date": "2023-03-07T19:16:27+00:00",
    "message": "[static-runtime] a pass through checks throwing exceptions (#95983)\n\nSummary: increasing verbosity where possible\n\nTest Plan: CI\n\nDifferential Revision: D43761268\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/95983\nApproved by: https://github.com/davidberard98",
    "changes": [
        {
            "name": "impl.cpp",
            "path": "torch/csrc/jit/runtime/static/impl.cpp",
            "patches": [
                {
                    "old_start": 679,
                    "old_length": 7,
                    "new_start": 679,
                    "new_length": 12,
                    "hunk_buggy": "[' \\n', '     if (node->kind() == prim::Constant) {\\n', '       auto* v = node->output();\\n', '-      TORCH_CHECK(v->type()->kind() != FunctionType::Kind);\\n', '       value_to_index.emplace(v, constants_.size());\\n', '       constants_.emplace_back(toIValue(v).value());\\n', '       continue;\\n']",
                    "hunk_fix": "@@ -679,7 +679,12 @@ void StaticModule::prepareFunctionsAndConstants(\n \n     if (node->kind() == prim::Constant) {\n       auto* v = node->output();\n-      TORCH_CHECK(v->type()->kind() != FunctionType::Kind);\n+      TORCH_CHECK(\n+          v->type()->kind() != FunctionType::Kind,\n+          \"got \",\n+          typeKindToString(v->type()->kind()),\n+          \" instead of \",\n+          typeKindToString(FunctionType::Kind));\n       value_to_index.emplace(v, constants_.size());\n       constants_.emplace_back(toIValue(v).value());\n       continue;\n"
                },
                {
                    "old_start": 948,
                    "old_length": 10,
                    "new_start": 953,
                    "new_length": 24,
                    "hunk_buggy": "[' \\n', '   if (!is_root_block_ || C10_UNLIKELY(!schema)) {\\n', '     TORCH_CHECK(\\n', '-        kwargs.empty(), \"Schema is not available, but BlockRunner got kwargs.\");\\n', ' \\n', '     const auto total_num_inputs = args.size() + first_input_is_self_;\\n', '-    TORCH_CHECK(total_num_inputs == block_info_.num_inputs());\\n', ' \\n', '     for (size_t i = 0; i < args.size(); ++i) {\\n', '       set_arg(i, std::forward<IValueList>(args));\\n']",
                    "hunk_fix": "@@ -948,10 +953,24 @@ void BlockRunner::set_inputs(\n \n   if (!is_root_block_ || C10_UNLIKELY(!schema)) {\n     TORCH_CHECK(\n-        kwargs.empty(), \"Schema is not available, but BlockRunner got kwargs.\");\n+        kwargs.empty(),\n+        \"BlockRunner got kwargs; is_root_block: \",\n+        std::to_string(is_root_block_),\n+        \"schema: \",\n+        schema ? schema->name() : \"(not available)\");\n \n     const auto total_num_inputs = args.size() + first_input_is_self_;\n-    TORCH_CHECK(total_num_inputs == block_info_.num_inputs());\n+    TORCH_CHECK(\n+        total_num_inputs == block_info_.num_inputs(),\n+        \"Block runner got \",\n+        std::to_string(total_num_inputs),\n+        \" inputs; \",\n+        \" first_input_is_self: \",\n+        std::to_string(first_input_is_self_),\n+        \"; SR block expects \",\n+        std::to_string(block_info_.num_inputs()),\n+        \" inputs for schema \",\n+        schema ? schema->name() : \"(not available)\");\n \n     for (size_t i = 0; i < args.size(); ++i) {\n       set_arg(i, std::forward<IValueList>(args));\n"
                },
                {
                    "old_start": 964,
                    "old_length": 7,
                    "new_start": 983,
                    "new_length": 12,
                    "hunk_buggy": "['   DCHECK(!schema_args.empty());\\n', '   TORCH_CHECK(\\n', '       args.size() < schema_args.size(),\\n', '-      \"Static runtime got too many arguments\");\\n', '   for (size_t i = 0; i < schema_args.size() - 1; ++i) {\\n', '     // Start at 1 since the schema always contains `self`.\\n', '     const auto& schema_arg = schema_args[i + 1];\\n']",
                    "hunk_fix": "@@ -964,7 +983,12 @@ void BlockRunner::set_inputs(\n   DCHECK(!schema_args.empty());\n   TORCH_CHECK(\n       args.size() < schema_args.size(),\n-      \"Static runtime got too many arguments\");\n+      \"Static runtime got \",\n+      std::to_string(args.size()),\n+      \" arguments, expects \",\n+      std::to_string(schema_args.size() - 1),\n+      \" for schema \",\n+      schema->name());\n   for (size_t i = 0; i < schema_args.size() - 1; ++i) {\n     // Start at 1 since the schema always contains `self`.\n     const auto& schema_arg = schema_args[i + 1];\n"
                },
                {
                    "old_start": 990,
                    "old_length": 9,
                    "new_start": 1014,
                    "new_length": 20,
                    "hunk_buggy": "['     }\\n', ' \\n', '     TORCH_CHECK(\\n', '-        false, \"Static runtime is missing required kwarg \", schema_arg.name());\\n', '   }\\n', '-  TORCH_CHECK(consumed_kwargs == kwargs.size());\\n', ' }\\n', ' \\n', ' void BlockRunner::create_memory_planner() {\\n']",
                    "hunk_fix": "@@ -990,9 +1014,20 @@ void BlockRunner::set_inputs(\n     }\n \n     TORCH_CHECK(\n-        false, \"Static runtime is missing required kwarg \", schema_arg.name());\n+        false,\n+        \"Static runtime is missing required kwarg \",\n+        schema_arg.name(),\n+        \" for schema \",\n+        schema->name());\n   }\n-  TORCH_CHECK(consumed_kwargs == kwargs.size());\n+  TORCH_CHECK(\n+      consumed_kwargs == kwargs.size(),\n+      \"kwargs size mismatch (consumed \",\n+      std::to_string(consumed_kwargs),\n+      \", expected \",\n+      std::to_string(kwargs.size()),\n+      \" for schema \",\n+      schema->name());\n }\n \n void BlockRunner::create_memory_planner() {\n"
                },
                {
                    "old_start": 1958,
                    "old_length": 7,
                    "new_start": 1993,
                    "new_length": 14,
                    "hunk_buggy": "['       fn_(fn),\\n', '       inputs_(std::move(inputs)),\\n', '       outputs_offset_(outputs_offset) {\\n', '-  TORCH_CHECK(num_outputs() == node->outputs().size());\\n', ' }\\n', ' \\n', ' std::vector<IValue> ProcessedNode::inputs_ivalue_vec() const {']",
                    "hunk_fix": "@@ -1958,7 +1993,14 @@ StaticNodeInfo::StaticNodeInfo(\n       fn_(fn),\n       inputs_(std::move(inputs)),\n       outputs_offset_(outputs_offset) {\n-  TORCH_CHECK(num_outputs() == node->outputs().size());\n+  TORCH_CHECK(\n+      num_outputs() == node->outputs().size(),\n+      \"Node \",\n+      node->kind().toQualString(),\n+      \" has \",\n+      std::to_string(num_outputs()),\n+      \" outputs, expected \",\n+      std::to_string(node->outputs().size()));\n }\n \n std::vector<IValue> ProcessedNode::inputs_ivalue_vec() const {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 384,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/1359d16fe8ca0cb7041674c455f2f99a9636fec0",
    "date": "2023-03-05T14:53:02+00:00",
    "message": "[CI] Further tighten the checking of two eager runs (#95902)\n\nSummary: To catch nondeterminism in eager if there is any.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/95902\nApproved by: https://github.com/jansel",
    "changes": [
        {
            "name": "common.py",
            "path": "benchmarks/dynamo/common.py",
            "patches": [
                {
                    "old_start": 1269,
                    "old_length": 10,
                    "new_start": 1269,
                    "new_length": 13,
                    "hunk_buggy": "['             correct_rerun_result = self.run_n_iterations(\\n', '                 model_copy, clone_inputs(example_inputs)\\n', '             )\\n', '             if not same(\\n', '                 correct_result,\\n', '                 correct_rerun_result,\\n', '-                fp64_ref=None,  # Two eager runs should be the same without comparing against fp64_output\\n', '                 equal_nan=self.equal_nan,\\n', '             ):\\n', '                 accuracy_status = \"eager_variation\"\\n']",
                    "hunk_fix": "@@ -1269,10 +1269,13 @@ class BenchmarkRunner:\n             correct_rerun_result = self.run_n_iterations(\n                 model_copy, clone_inputs(example_inputs)\n             )\n+            # Two eager runs should have exactly same result\n             if not same(\n                 correct_result,\n                 correct_rerun_result,\n-                fp64_ref=None,  # Two eager runs should be the same without comparing against fp64_output\n+                fp64_ref=None,\n+                cos_similarity=False,\n+                tol=0,\n                 equal_nan=self.equal_nan,\n             ):\n                 accuracy_status = \"eager_variation\"\n"
                },
                {
                    "old_start": 1956,
                    "old_length": 9,
                    "new_start": 1959,
                    "new_length": 12,
                    "hunk_buggy": "['             # TODO - Using train mode for timm_models. Move to train mode for HF and Torchbench as well.\\n', '             args.use_eval_mode = True\\n', '         inductor_config.fallback_random = True\\n', '         torch.backends.cudnn.allow_tf32 = False\\n', '         torch.backends.cudnn.benchmark = False\\n', '-        torch.backends.cudnn.deterministic = True\\n', ' \\n', '         # Remove randomeness when torch manual seed is called\\n', '         patch_torch_manual_seed()']",
                    "hunk_fix": "@@ -1956,9 +1959,12 @@ def run(runner, args, original_dir=None):\n             # TODO - Using train mode for timm_models. Move to train mode for HF and Torchbench as well.\n             args.use_eval_mode = True\n         inductor_config.fallback_random = True\n+        torch.use_deterministic_algorithms(True)\n+        os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n+        torch.backends.cudnn.deterministic = True\n         torch.backends.cudnn.allow_tf32 = False\n         torch.backends.cudnn.benchmark = False\n-        torch.backends.cudnn.deterministic = True\n+        torch.backends.cuda.matmul.allow_tf32 = False\n \n         # Remove randomeness when torch manual seed is called\n         patch_torch_manual_seed()"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 385,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/a6a433aecd0da3ac3c8d49cb36091623f1b5ec9e",
    "date": "2023-02-13T21:00:00+00:00",
    "message": "Add stack emptiness checks inside interpreter.cpp (#94298)\n\nHi!\n\nI've been fuzzing different pytorch modules, and found a few crashes inside one of them.\n\nSpecifically, I'm talking about a module for interpreting the JIT code and a function called `InterpreterState::run()`. Running this function with provided crash file results in a crash, which occurs while calling `dim()` on a `stack` with 0 elements ([line-686](https://github.com/pytorch/pytorch/blob/abc54f93145830b502400faa92bec86e05422fbd/torch/csrc/jit/runtime/interpreter.cpp#L686)). The crash itself occurs later, when std::move is called with incorrect value of type `IValue`.\n\nThe second crash is similar and occurs on [line 328](https://github.com/pytorch/pytorch/blob/abc54f93145830b502400faa92bec86e05422fbd/torch/csrc/jit/runtime/interpreter.cpp#LL328C15-L328C48), where `reg(inst.X + i - 1) = pop(stack);` is executed. The error here is the same, `Stack stack` might not contain enough elements.\n\nThe third crash occurs on [line 681](https://github.com/pytorch/pytorch/blob/abc54f93145830b502400faa92bec86e05422fbd/torch/csrc/jit/runtime/interpreter.cpp#L681). The problem here is the same as for previous crashes. There are not enough elements in the stack.\n\nIn addition to these places, there are many others (in the same function) where border checking is also missing. I am not sure what is the best way to fix these problems, however I suggest adding a boundary check inside each of these case statement.\n\nAll tests were performed on this pytorch version: [abc54f93145830b502400faa92bec86e05422fbd](https://github.com/pytorch/pytorch/tree/abc54f93145830b502400faa92bec86e05422fbd)\n\n### How to reproduce\n\n1. To reproduce the crash, use provided docker: [Dockerfile](https://github.com/ispras/oss-sydr-fuzz/tree/master/projects/pytorch)\n\n2. Build the container: `docker build -t oss-sydr-fuzz-pytorch-reproduce .`\n\n3. Copy these crash files to the current directory:\n\n    - [crash-4f18c5128c9a5a94343fcbbd543d7d6b02964471.zip](https://github.com/pytorch/pytorch/files/10674143/crash-4f18c5128c9a5a94343fcbbd543d7d6b02964471.zip)\n    - [crash-55384dd7c9689ed7b94ac6697cc43db4e0dd905a.zip](https://github.com/pytorch/pytorch/files/10674147/crash-55384dd7c9689ed7b94ac6697cc43db4e0dd905a.zip)\n    - [crash-06b6125d01c5f91fae112a1aa7dcc76d71b66576.zip](https://github.com/pytorch/pytorch/files/10674152/crash-06b6125d01c5f91fae112a1aa7dcc76d71b66576.zip)\n\n4. Run the container: ``docker run --privileged --network host -v `pwd`:/homedir --rm -it oss-sydr-fuzz-pytorch-reproduce /bin/bash``\n\n5. And execute the binary: `/jit_differential_fuzz /homedir/crash-4f18c5128c9a5a94343fcbbd543d7d6b02964471`\n\nAfter execution completes you will see this stacktrace:\n\n```asan\n=36==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x6060001657f8 at pc 0x00000060bc91 bp 0x7fff00b33380 sp 0x7fff00b33378\nREAD of size 4 at 0x6060001657f8 thread T0\n    #0 0x60bc90 in c10::IValue::IValue(c10::IValue&&) /pytorch_fuzz/torch/include/ATen/core/ivalue.h:214:43\n    #1 0xc20e7cd in torch::jit::pop(std::vector<c10::IValue, std::allocator<c10::IValue> >&) /pytorch_fuzz/aten/src/ATen/core/stack.h:102:12\n    #2 0xc20e7cd in torch::jit::dim(std::vector<c10::IValue, std::allocator<c10::IValue> >&) /pytorch_fuzz/torch/csrc/jit/mobile/promoted_prim_ops.cpp:119:20\n    #3 0xc893060 in torch::jit::InterpreterStateImpl::runImpl(std::vector<c10::IValue, std::allocator<c10::IValue> >&) /pytorch_fuzz/torch/csrc/jit/runtime/interpreter.cpp:686:13\n    #4 0xc85c47b in torch::jit::InterpreterStateImpl::run(std::vector<c10::IValue, std::allocator<c10::IValue> >&) /pytorch_fuzz/torch/csrc/jit/runtime/interpreter.cpp:1010:9\n    #5 0x600598 in runGraph(std::shared_ptr<torch::jit::Graph>, std::vector<at::Tensor, std::allocator<at::Tensor> > const&) /jit_differential_fuzz.cc:66:38\n    #6 0x601d99 in LLVMFuzzerTestOneInput /jit_differential_fuzz.cc:107:25\n    #7 0x52ccf1 in fuzzer::Fuzzer::ExecuteCallback(unsigned char const*, unsigned long) /llvm-project/compiler-rt/lib/fuzzer/FuzzerLoop.cpp:611:15\n    #8 0x516c0c in fuzzer::RunOneTest(fuzzer::Fuzzer*, char const*, unsigned long) /llvm-project/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:324:6\n    #9 0x51c95b in fuzzer::FuzzerDriver(int*, char***, int (*)(unsigned char const*, unsigned long)) /llvm-project/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:860:9\n    #10 0x545ef2 in main /llvm-project/compiler-rt/lib/fuzzer/FuzzerMain.cpp:20:10\n    #11 0x7f9ec069a082 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x24082)\n    #12 0x51152d in _start (/jit_differential_fuzz+0x51152d)\n\n0x6060001657f8 is located 8 bytes to the left of 64-byte region [0x606000165800,0x606000165840)\nallocated by thread T0 here:\n    #0 0x5fd42d in operator new(unsigned long) /llvm-project/compiler-rt/lib/asan/asan_new_delete.cpp:95:3\n    #1 0xa16ab5 in void std::vector<c10::IValue, std::allocator<c10::IValue> >::_M_realloc_insert<c10::IValue&>(__gnu_cxx::__normal_iterator<c10::IValue*, std::vector<c10::IValue, std::allocator<c10::IValue> > >, c10::IValue&) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/vector.tcc:440:33\n    #2 0xa168f1 in c10::IValue& std::vector<c10::IValue, std::allocator<c10::IValue> >::emplace_back<c10::IValue&>(c10::IValue&) /usr/bin/../lib/gcc/x86_64-linux-gnu/10/../../../../include/c++/10/bits/vector.tcc:121:4\n    #3 0xc89b53c in torch::jit::InterpreterStateImpl::runImpl(std::vector<c10::IValue, std::allocator<c10::IValue> >&) /pytorch_fuzz/torch/csrc/jit/runtime/interpreter.cpp:344:19\n    #4 0xc85c47b in torch::jit::InterpreterStateImpl::run(std::vector<c10::IValue, std::allocator<c10::IValue> >&) /pytorch_fuzz/torch/csrc/jit/runtime/interpreter.cpp:1010:9\n    #5 0x600598 in runGraph(std::shared_ptr<torch::jit::Graph>, std::vector<at::Tensor, std::allocator<at::Tensor> > const&) /jit_differential_fuzz.cc:66:38\n    #6 0x601d99 in LLVMFuzzerTestOneInput /jit_differential_fuzz.cc:107:25\n    #7 0x52ccf1 in fuzzer::Fuzzer::ExecuteCallback(unsigned char const*, unsigned long) /llvm-project/compiler-rt/lib/fuzzer/FuzzerLoop.cpp:611:15\n    #8 0x516c0c in fuzzer::RunOneTest(fuzzer::Fuzzer*, char const*, unsigned long) /llvm-project/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:324:6\n    #9 0x51c95b in fuzzer::FuzzerDriver(int*, char***, int (*)(unsigned char const*, unsigned long)) /llvm-project/compiler-rt/lib/fuzzer/FuzzerDriver.cpp:860:9\n    #10 0x545ef2 in main /llvm-project/compiler-rt/lib/fuzzer/FuzzerMain.cpp:20:10\n    #11 0x7f9ec069a082 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x24082)\n\nSUMMARY: AddressSanitizer: heap-buffer-overflow /pytorch_fuzz/torch/include/ATen/core/ivalue.h:214:43 in c10::IValue::IValue(c10::IValue&&)\nShadow bytes around the buggy address:\n  0x0c0c80024aa0: fd fd fd fd fd fd fd fa fa fa fa fa 00 00 00 00\n  0x0c0c80024ab0: 00 00 00 fa fa fa fa fa fd fd fd fd fd fd fd fd\n  0x0c0c80024ac0: fa fa fa fa fd fd fd fd fd fd fd fd fa fa fa fa\n  0x0c0c80024ad0: fd fd fd fd fd fd fd fd fa fa fa fa fd fd fd fd\n  0x0c0c80024ae0: fd fd fd fd fa fa fa fa 00 00 00 00 00 00 00 00\n=>0x0c0c80024af0: fa fa fa fa fd fd fd fd fd fd fd fd fa fa fa[fa]\n  0x0c0c80024b00: 00 00 00 00 00 00 00 00 fa fa fa fa fa fa fa fa\n  0x0c0c80024b10: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x0c0c80024b20: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x0c0c80024b30: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x0c0c80024b40: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\nShadow byte legend (one shadow byte represents 8 application bytes):\n  Addressable:           00\n  Partially addressable: 01 02 03 04 05 06 07\n  Heap left redzone:       fa\n  Freed heap region:       fd\n  Stack left redzone:      f1\n  Stack mid redzone:       f2\n  Stack right redzone:     f3\n  Stack after return:      f5\n  Stack use after scope:   f8\n  Global redzone:          f9\n  Global init order:       f6\n  Poisoned by user:        f7\n  Container overflow:      fc\n  Array cookie:            ac\n  Intra object redzone:    bb\n  ASan internal:           fe\n  Left alloca redzone:     ca\n  Right alloca redzone:    cb\n==36==ABORTING\n```\n\n6. Executing the remaining crashes gives similar crash reports\nPull Request resolved: https://github.com/pytorch/pytorch/pull/94298\nApproved by: https://github.com/davidberard98",
    "changes": [
        {
            "name": "interpreter.cpp",
            "path": "torch/csrc/jit/runtime/interpreter.cpp",
            "patches": [
                {
                    "old_start": 324,
                    "old_length": 6,
                    "new_start": 324,
                    "new_length": 7,
                    "hunk_buggy": "['             INST_NEXT;\\n', '           case INST(STOREN): {\\n', '             INST_GUARD;\\n', '             for (size_t i = inst.N; i > 0; --i) {\\n', '               reg(inst.X + i - 1) = pop(stack);\\n', '             }\\n']",
                    "hunk_fix": "@@ -324,6 +324,7 @@ struct InterpreterStateImpl : c10::intrusive_ptr_target {\n             INST_NEXT;\n           case INST(STOREN): {\n             INST_GUARD;\n+            TORCH_INTERNAL_ASSERT_DEBUG_ONLY(stack.size() >= inst.N);\n             for (size_t i = inst.N; i > 0; --i) {\n               reg(inst.X + i - 1) = pop(stack);\n             }\n"
                },
                {
                    "old_start": 678,
                    "old_length": 11,
                    "new_start": 679,
                    "new_length": 13,
                    "hunk_buggy": "['             INST_NEXT;\\n', '           case INST(DTYPE): {\\n', '             INST_GUARD;\\n', '             dtype(stack);\\n', '           }\\n', '             INST_NEXT;\\n', '           case INST(DIM): {\\n', '             INST_GUARD;\\n', '             dim(stack);\\n', '           }\\n', '             INST_NEXT;']",
                    "hunk_fix": "@@ -678,11 +679,13 @@ struct InterpreterStateImpl : c10::intrusive_ptr_target {\n             INST_NEXT;\n           case INST(DTYPE): {\n             INST_GUARD;\n+            TORCH_INTERNAL_ASSERT_DEBUG_ONLY(!stack.empty());\n             dtype(stack);\n           }\n             INST_NEXT;\n           case INST(DIM): {\n             INST_GUARD;\n+            TORCH_INTERNAL_ASSERT_DEBUG_ONLY(!stack.empty());\n             dim(stack);\n           }\n             INST_NEXT;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 386,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/4e1bd4abe7691f460cb021e5b314168caa42ef92",
    "date": "2023-02-09T15:22:02+00:00",
    "message": "Fix scalar type resolution for optional tensor (#94427)\n\nWhen TorchScript Value has an optional tensor, `dtype()` or `scalarType()` is not available and raise (by design).\n\nThe symbolic `_op_with_optional_float_cast` must check whether the tensor is otpional or not before calling the scalar type resolution API. This PR fixes that\nPull Request resolved: https://github.com/pytorch/pytorch/pull/94427\nApproved by: https://github.com/abock, https://github.com/shubhambhokare1",
    "changes": [
        {
            "name": "symbolic_opset9.py",
            "path": "torch/onnx/symbolic_opset9.py",
            "patches": [
                {
                    "old_start": 1323,
                    "old_length": 12,
                    "new_start": 1323,
                    "new_length": 15,
                    "hunk_buggy": "[' \\n', '     if require_cast:\\n', '         for input in inputs:\\n', '-            input_scalar_type = _type_utils.JitScalarType.from_value(input)\\n', '-            if input.isCompleteTensor() and input_scalar_type != dtype_0:\\n', '-                raise errors.SymbolicValueError(\\n', '-                    f\"Inputs of {op_name} must have same dtype. Got {dtype_0.scalar_name()} and {input_scalar_type.scalar_name()}\",\\n', '-                    input,\\n', '-                )\\n', '         for i, input in enumerate(inputs):\\n', '             if input.isCompleteTensor() and not symbolic_helper._is_fp(input):\\n', '                 inputs[i] = g.op(\\n']",
                    "hunk_fix": "@@ -1323,12 +1323,15 @@ def _op_with_optional_float_cast(g: jit_utils.GraphContext, op_name, *args, **kw\n \n     if require_cast:\n         for input in inputs:\n-            input_scalar_type = _type_utils.JitScalarType.from_value(input)\n-            if input.isCompleteTensor() and input_scalar_type != dtype_0:\n-                raise errors.SymbolicValueError(\n-                    f\"Inputs of {op_name} must have same dtype. Got {dtype_0.scalar_name()} and {input_scalar_type.scalar_name()}\",\n-                    input,\n-                )\n+\n+            if input.isCompleteTensor():\n+                input_scalar_type = _type_utils.JitScalarType.from_value(input)\n+                if input_scalar_type != dtype_0:\n+                    raise errors.SymbolicValueError(\n+                        f\"Inputs of {op_name} must have same dtype.\"\n+                        f\"Got {dtype_0.scalar_name()} and {input_scalar_type.scalar_name()}\",\n+                        input,\n+                    )\n         for i, input in enumerate(inputs):\n             if input.isCompleteTensor() and not symbolic_helper._is_fp(input):\n                 inputs[i] = g.op(\n"
                },
                {
                    "old_start": 3617,
                    "old_length": 7,
                    "new_start": 3620,
                    "new_length": 7,
                    "hunk_buggy": "['         for t in symbolic_helper._unpack_list(data):\\n', '             shape_reference = g.op(\"Constant\", value_t=torch.LongTensor([1]))\\n', '             t = symbolic_helper._reshape_helper(g, t, shape_reference)\\n', '-            t = g.op(\"Cast\", t, to_i=dtype.onnx_type())\\n', '             input_list.append(t)\\n', '         return g.op(\"Concat\", *input_list, axis_i=0)\\n', '     else:']",
                    "hunk_fix": "@@ -3617,7 +3620,7 @@ def tensor(\n         for t in symbolic_helper._unpack_list(data):\n             shape_reference = g.op(\"Constant\", value_t=torch.LongTensor([1]))\n             t = symbolic_helper._reshape_helper(g, t, shape_reference)\n-            t = g.op(\"Cast\", t, to_i=dtype.onnx_type())\n+            t = g.op(\"Cast\", t, to_i=_type_utils.JitScalarType(dtype).onnx_type())\n             input_list.append(t)\n         return g.op(\"Concat\", *input_list, axis_i=0)\n     else:"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 387,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/f77f88fbc7511b405c4e493bdd74634b633f63d1",
    "date": "2023-02-01T08:12:39+00:00",
    "message": "[Quant] X86 qengine always uses fbgemm kernels on OS other than Linux (#93218)\n\n**Summary**\nX86 quantization backend (qengine) with oneDNN kernels has not been validated on OS other than Linux. So, let it fall back to fbgemm if OS is not Linux. This makes sure the behavior is the same on Windows/Mac as the previous default fbgemm qengine on x86 CPUs.\n\n**Test plan**\nCI checks.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/93218\nApproved by: https://github.com/jgong5, https://github.com/jerryzh168",
    "changes": [
        {
            "name": "OnednnUtils.h",
            "path": "aten/src/ATen/native/quantized/cpu/OnednnUtils.h",
            "patches": [
                {
                    "old_start": 391,
                    "old_length": 18,
                    "new_start": 391,
                    "new_length": 27,
                    "hunk_buggy": "['   return is_symmetric;\\n', ' }\\n', ' \\n', '-// Check if onednn should be used w.r.t fbgemm\\n', ' static bool should_use_onednn_quant(\\n', '     const at::Tensor& weight,\\n', '     bool is_transposed_conv,\\n', '     int groups,\\n', '     torch::List<int64_t> output_padding) {\\n', '   bool vnni_available = cpuinfo_has_x86_avx512vnni();\\n', '   bool w_sym_quant =\\n', '       is_weight_symmetric_quant(weight, is_transposed_conv);\\n', '   bool opad_all_zero =\\n', '       std::all_of(output_padding.begin(), output_padding.end(), [](int i) { return i==0; });\\n', '   return vnni_available && (groups <= 100) && w_sym_quant && opad_all_zero;\\n', ' }\\n', ' \\n', ' } // onednn_utils']",
                    "hunk_fix": "@@ -391,18 +391,27 @@ static bool is_weight_symmetric_quant(\n   return is_symmetric;\n }\n \n-// Check if onednn should be used w.r.t fbgemm\n+// When qengine is x86, use this util func to check if onednn kernel\n+// is preferred than fbgemm's to get better performance.\n static bool should_use_onednn_quant(\n     const at::Tensor& weight,\n     bool is_transposed_conv,\n     int groups,\n     torch::List<int64_t> output_padding) {\n+  // Performance of onednn is only validated on Linux right now.\n+  // Also, the heuristics for dispatching are based on perf data on Linux.\n+  // So, for x86 qengine, we always use fbgemm kernels if OS is not Linux.\n+  // TODO Support more OSs.\n+#if !defined(__linux__)\n+  return false;\n+#else\n   bool vnni_available = cpuinfo_has_x86_avx512vnni();\n   bool w_sym_quant =\n       is_weight_symmetric_quant(weight, is_transposed_conv);\n   bool opad_all_zero =\n       std::all_of(output_padding.begin(), output_padding.end(), [](int i) { return i==0; });\n   return vnni_available && (groups <= 100) && w_sym_quant && opad_all_zero;\n+#endif\n }\n \n } // onednn_utils"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 388,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/63e47c68a692c70bc64c49d687f85f7f5cd02ce3",
    "date": "2023-01-25T20:36:44+00:00",
    "message": "[cpp] remove checks from embedding bag impl (#92982)\n\nThese checks incur an H2D sync on every embedding bag forward. Also, the equivalent python code for embedding_bag does not have them. Kill!\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/92982\nApproved by: https://github.com/ezyang",
    "changes": [
        {
            "name": "embedding.h",
            "path": "torch/csrc/api/include/torch/nn/functional/embedding.h",
            "patches": [
                {
                    "old_start": 126,
                    "old_length": 17,
                    "new_start": 126,
                    "new_length": 6,
                    "hunk_buggy": "['     TORCH_CHECK(\\n', '         offsets_.defined(), \"offsets has to be a 1D Tensor but got null\");\\n', '     TORCH_CHECK(offsets_.dim() == 1, \"offsets has to be a 1D Tensor\");\\n', '-    TORCH_CHECK(\\n', '-        offsets_[0].item<int64_t>() == 0,\\n', '-        \"offsets[0] has to be 0, i.e., the first sequence in the mini-batch has to start from position 0. However, got \",\\n', '-        offsets_[0].item<int64_t>());\\n', '-    TORCH_CHECK(\\n', '-        offsets_[-1].item<int64_t>() <= input_.size(0),\\n', '-        \"offsets[-1] can not be greater than input\\'s length({\",\\n', '-        input_.size(0),\\n', '-        \"}), but got offsets[-1] of {\",\\n', '-        offsets_[-1].item<int64_t>(),\\n', '-        \"}\");\\n', '   } else {\\n', '     TORCH_CHECK(\\n', '         false,']",
                    "hunk_fix": "@@ -126,17 +126,6 @@ inline Tensor embedding_bag(\n     TORCH_CHECK(\n         offsets_.defined(), \"offsets has to be a 1D Tensor but got null\");\n     TORCH_CHECK(offsets_.dim() == 1, \"offsets has to be a 1D Tensor\");\n-    TORCH_CHECK(\n-        offsets_[0].item<int64_t>() == 0,\n-        \"offsets[0] has to be 0, i.e., the first sequence in the mini-batch has to start from position 0. However, got \",\n-        offsets_[0].item<int64_t>());\n-    TORCH_CHECK(\n-        offsets_[-1].item<int64_t>() <= input_.size(0),\n-        \"offsets[-1] can not be greater than input's length({\",\n-        input_.size(0),\n-        \"}), but got offsets[-1] of {\",\n-        offsets_[-1].item<int64_t>(),\n-        \"}\");\n   } else {\n     TORCH_CHECK(\n         false,"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 389,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/40d6f2a02027023216607adb892d3b9c7493904c",
    "date": "2023-01-17T23:14:21+00:00",
    "message": "Update sdp_utils to check gradmode and subclassed tensors (#92323)\n\n# Summary\nFix up the grad check test to check for subclassed tensors and gradmode\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/92323\nApproved by: https://github.com/soulitzer",
    "changes": [
        {
            "name": "sdp_utils.h",
            "path": "aten/src/ATen/native/transformers/cuda/sdp_utils.h",
            "patches": [
                {
                    "old_start": 2,
                    "old_length": 6,
                    "new_start": 2,
                    "new_length": 7,
                    "hunk_buggy": "[' \\n', ' #include <ATen/Context.h>\\n', ' #include <ATen/TensorUtils.h>\\n', ' #include <ATen/core/Tensor.h>\\n', ' #include <ATen/cuda/CUDAContext.h>\\n', ' #include <ATen/detail/CUDAHooksInterface.h>\\n']",
                    "hunk_fix": "@@ -2,6 +2,7 @@\n \n #include <ATen/Context.h>\n #include <ATen/TensorUtils.h>\n+#include <ATen/TensorSubclassLikeUtils.h>\n #include <ATen/core/Tensor.h>\n #include <ATen/cuda/CUDAContext.h>\n #include <ATen/detail/CUDAHooksInterface.h>\n"
                },
                {
                    "old_start": 152,
                    "old_length": 7,
                    "new_start": 153,
                    "new_length": 12,
                    "hunk_buggy": "[' }\\n', ' \\n', ' inline bool check_requires_grad(sdp_params params, bool debug) {\\n', '-  if (params.query.requires_grad() || params.key.requires_grad() || params.value.requires_grad()) {\\n', '     if (debug) {\\n', '       TORCH_WARN(\"Flash Attention does not currently support training.\");\\n', '     }']",
                    "hunk_fix": "@@ -152,7 +153,12 @@ inline bool check_for_nested_inputs(sdp_params params, bool debug){\n }\n \n inline bool check_requires_grad(sdp_params params, bool debug) {\n-  if (params.query.requires_grad() || params.key.requires_grad() || params.value.requires_grad()) {\n+  bool any_tensors_are_subclass =\n+      at::areAnyTensorSubclassLike({params.query, params.key, params.value});\n+  const bool any_inputs_require_grad = params.query.requires_grad() ||\n+      params.key.requires_grad() || params.value.requires_grad();\n+  const bool gradmode_enabled = at::GradMode::is_enabled();\n+  if ((any_inputs_require_grad && gradmode_enabled) || any_tensors_are_subclass) {\n     if (debug) {\n       TORCH_WARN(\"Flash Attention does not currently support training.\");\n     }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 390,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/aab55d6d0d7b958e32cfdbb69794e107cfceb6bc",
    "date": "2023-01-05T23:58:45+00:00",
    "message": "[Quant] Remove all the dequant nodes when the ref module has multi input args (#90157)\n\n**Summary**:\nWhen converting a ref module into a quant module, `_lower_static_weighted_ref_module` pass assumes the `ref_node` only has 1 input node, and only remove the first `dequant` node. We add a check in this PR to ensure this is the case for `_lower_static_weighted_ref_module` pass.\n\n**Test Plan**:\nWe only add a check in this PR, there is no new added test case.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/90157\nApproved by: https://github.com/Xia-Weiwen, https://github.com/jgong5, https://github.com/jerryzh168",
    "changes": [
        {
            "name": "_lower_to_native_backend.py",
            "path": "torch/ao/quantization/fx/_lower_to_native_backend.py",
            "patches": [
                {
                    "old_start": 511,
                    "old_length": 6,
                    "new_start": 511,
                    "new_length": 7,
                    "hunk_buggy": "['         setattr(modules[parent_name], module_name, q_module)\\n', ' \\n', '         # Step 2: Reroute around dq_node, and remove q_node and its args\\n', '         dq_node = ref_node.args[0]\\n', '         assert(isinstance(dq_node, Node))\\n', '         ref_node.replace_input_with(dq_node, dq_node.args[0])']",
                    "hunk_fix": "@@ -511,6 +511,7 @@ def _lower_static_weighted_ref_module(\n         setattr(modules[parent_name], module_name, q_module)\n \n         # Step 2: Reroute around dq_node, and remove q_node and its args\n+        assert(len(ref_node.args) == 1)\n         dq_node = ref_node.args[0]\n         assert(isinstance(dq_node, Node))\n         ref_node.replace_input_with(dq_node, dq_node.args[0])"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 391,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/6bf0e3b697ce688bc8325440dea3b51fea571c3d",
    "date": "2023-01-03T22:38:29+00:00",
    "message": "[inductor] Check for BackendCompilerFailed on CI (#91634)\n\nSummary: https://github.com/pytorch/pytorch/pull/91283/ skips certain\nrandom triton failure on CI, but we need to check against the\nBackendCompilerFailed exception type.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/91634\nApproved by: https://github.com/ngimel",
    "changes": [
        {
            "name": "common.py",
            "path": "benchmarks/dynamo/common.py",
            "patches": [
                {
                    "old_start": 24,
                    "old_length": 6,
                    "new_start": 24,
                    "new_length": 7,
                    "hunk_buggy": "[' import torch._dynamo.utils\\n', ' import torch.distributed\\n', ' from scipy.stats import gmean, ttest_ind\\n', ' from torch._dynamo.optimizations import backends\\n', ' from torch._dynamo.optimizations.log_args import conv_args_analysis\\n', ' from torch._dynamo.profiler import fx_insert_profiling, Profiler\\n']",
                    "hunk_fix": "@@ -24,6 +24,7 @@ import torch._dynamo\n import torch._dynamo.utils\n import torch.distributed\n from scipy.stats import gmean, ttest_ind\n+from torch._dynamo.exc import BackendCompilerFailed\n from torch._dynamo.optimizations import backends\n from torch._dynamo.optimizations.log_args import conv_args_analysis\n from torch._dynamo.profiler import fx_insert_profiling, Profiler\n"
                },
                {
                    "old_start": 1185,
                    "old_length": 12,
                    "new_start": 1186,
                    "new_length": 13,
                    "hunk_buggy": "['                 new_result = optimized_model_iter_fn(model_copy, example_inputs)\\n', '             except Exception as e:\\n', '                 log.exception(e)\\n', '-                if self.args.ci and (\\n', '-                    (\\n', '-                        isinstance(e, RuntimeError)\\n', '-                        and \"Internal Triton PTX codegen error\" in str(e)\\n', '                     )\\n', '-                    or (isinstance(e, KeyError) and \"cubin\" in str(e))\\n', '                 ):\\n', '                     accuracy_status = \"pass_due_to_skip\"\\n', '                     return record_status(accuracy_status)']",
                    "hunk_fix": "@@ -1185,12 +1186,13 @@ class BenchmarkRunner:\n                 new_result = optimized_model_iter_fn(model_copy, example_inputs)\n             except Exception as e:\n                 log.exception(e)\n-                if self.args.ci and (\n-                    (\n-                        isinstance(e, RuntimeError)\n-                        and \"Internal Triton PTX codegen error\" in str(e)\n+                if (\n+                    self.args.ci\n+                    and isinstance(e, BackendCompilerFailed)\n+                    and (\n+                        \"Internal Triton PTX codegen error\" in str(e)\n+                        or \"cubin\" in str(e)\n                     )\n-                    or (isinstance(e, KeyError) and \"cubin\" in str(e))\n                 ):\n                     accuracy_status = \"pass_due_to_skip\"\n                     return record_status(accuracy_status)"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 392,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/7e9bf2ed860b8b60d252eead4cc457c3fe5f1667",
    "date": "2022-12-19T23:43:28+00:00",
    "message": "When nopython=True, Dynamo can't allow graph breaks. (#90970)\n\nI count the number of sub-graphs (for tiny-GPT2 in huggingface) by\n```\n    class GraphCaptureCompiler:\n        def __init__(self):\n            self.captured_graphs = []\n        def compile(self, gm, example_inputs):\n            self.captured_graphs.append(gm)\n            return gm\n    compiler = GraphCaptureCompiler()\n    torch._dynamo.optimize(compiler, nopython=True)(Wrapper(fn))(*args)\n```\n\nAlthough `len(compiler.captured_graphs)` is 2, no error was thrown during the compilation. This observation conflicts with `nopython=True`. After some digging, I found a check is missed before making graph break. This PR adds it.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/90970\nApproved by: https://github.com/ezyang, https://github.com/jansel",
    "changes": [
        {
            "name": "symbolic_convert.py",
            "path": "torch/_dynamo/symbolic_convert.py",
            "patches": [
                {
                    "old_start": 306,
                    "old_length": 7,
                    "new_start": 306,
                    "new_length": 7,
                    "hunk_buggy": "['             try:\\n', '                 return inner_fn(self, inst)\\n', '             except Unsupported as excp:\\n', '-                if self.has_backedge():\\n', '                     msg = \"Skipping frame because there is a graph break in a for/while loop\"\\n', '                     log.debug(msg)\\n', '                     raise exc.SkipFrame(msg) from excp']",
                    "hunk_fix": "@@ -306,7 +306,7 @@ def break_graph_if_unsupported(*, push):\n             try:\n                 return inner_fn(self, inst)\n             except Unsupported as excp:\n-                if self.has_backedge():\n+                if self.has_backedge() and self.should_compile_partial_graph():\n                     msg = \"Skipping frame because there is a graph break in a for/while loop\"\n                     log.debug(msg)\n                     raise exc.SkipFrame(msg) from excp"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 393,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/9e314bd8224f93b4ba1f9e4c065150e47a2de2cc",
    "date": "2022-12-06T18:17:20+00:00",
    "message": "[dtensor] handle the case where output of op is Optional[Tensor] (#90241)\n\nObserved by @aazzolini, some op might have Optional[Tensor] returns\nwhere it return None (i.e. native_layer_norm_backward), it's a mismatch\nbetween C++ aten op signature and python None, but we need to handle it\nin the python side\nPull Request resolved: https://github.com/pytorch/pytorch/pull/90241\nApproved by: https://github.com/aazzolini",
    "changes": [
        {
            "name": "utils.py",
            "path": "torch/distributed/_tensor/utils.py",
            "patches": [
                {
                    "old_start": 10,
                    "old_length": 7,
                    "new_start": 10,
                    "new_length": 7,
                    "hunk_buggy": "[' ArgKwargsType = Union[Tuple[object, ...], Dict[str, object]]\\n', ' # ATen op schemas could have Tensor, Tuple[Tensor] and List[Tensor], so output type sould\\n', ' # be the same set of possiblities.\\n', '-OutputSpecType = Optional[Union[DTensorSpec, Sequence[DTensorSpec]]]\\n', ' \\n', ' \\n', ' def unwrap_local_tensor(e: \"dtensor.DTensor\") -> torch.Tensor:\\n']",
                    "hunk_fix": "@@ -10,7 +10,7 @@ from torch.distributed._tensor.placement_types import DTensorSpec\n ArgKwargsType = Union[Tuple[object, ...], Dict[str, object]]\n # ATen op schemas could have Tensor, Tuple[Tensor] and List[Tensor], so output type sould\n # be the same set of possiblities.\n-OutputSpecType = Optional[Union[DTensorSpec, Sequence[DTensorSpec]]]\n+OutputSpecType = Optional[Union[DTensorSpec, Sequence[Optional[DTensorSpec]]]]\n \n \n def unwrap_local_tensor(e: \"dtensor.DTensor\") -> torch.Tensor:\n"
                },
                {
                    "old_start": 45,
                    "old_length": 8,
                    "new_start": 45,
                    "new_length": 13,
                    "hunk_buggy": "['         assert spec is not None and isinstance(\\n', '             spec, tuple\\n', '         ), f\"output spec does not match with output! Expected tuple, got {spec}\"\\n', '         return tuple(\\n', '             dtensor.DTensor(e, s.mesh, s.placements, size=s.shape)\\n', '             for e, s in zip(res, spec)\\n', '         )\\n', '     else:']",
                    "hunk_fix": "@@ -45,8 +45,13 @@ def wrap(res: object, spec: OutputSpecType) -> object:\n         assert spec is not None and isinstance(\n             spec, tuple\n         ), f\"output spec does not match with output! Expected tuple, got {spec}\"\n+\n+        # NOTE: local results might return Optional Tensor from ATen op, so we need to\n+        # handle that case and make sure we don't wrap None with DTensor.\n+        # (i.e. native_layer_norm.backward)\n         return tuple(\n             dtensor.DTensor(e, s.mesh, s.placements, size=s.shape)\n+            if e is not None and s is not None else None\n             for e, s in zip(res, spec)\n         )\n     else:"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 394,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/2597d5d72272d196b4cb5442ffc8cde376d1f785",
    "date": "2022-12-06T03:07:53+00:00",
    "message": "TorchDynamo: always convert flexiblelayout to be FixedLayout when given a stride_order (#89904)\n\nFor convolution, we always call **require_stride_order** to convert the input to the target stride order,  if the original input's layout is flexiblelayout, there always have a memory copy because the **is_stride_order_storage_and_layout** only checks the init stride order,  I think for flexiblelayout, means it's layout can be changed, if the user gives a stride order, I think we always need to convert the flexiblelayout to be FixedLayout using given strider order.\n\nGiven a CV user case, the max_pooling's output is used by two convolutions, there has two memory copies:\n\n```\nkernel_cpp_0 = async_compile.cpp('''\n#include \"/tmp/torchinductor_xiaobing/77/c7773nj5pwikpmm2pwa62rcudlf7p3if7eyqb5k4sjsvewwje4le.h\"\nextern \"C\" void kernel(const float* __restrict__ in_ptr0,\n                       float* __restrict__ out_ptr0,\n                       float* __restrict__ out_ptr1,\n                       float* __restrict__ out_ptr2)\n{\n    #pragma GCC ivdep\n    for(long i0=0; i0<128; i0+=1)\n    {\n        #pragma GCC ivdep\n        for(long i1=0; i1<3; i1+=1)\n        {\n            #pragma GCC ivdep\n            for(long i2=0; i2<3; i2+=1)\n            {\n                #pragma GCC ivdep\n                for(long i3=0; i3<3; i3+=1)\n                {\n                    {\n                        {\n                            auto tmp0 = in_ptr0[i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp1 = in_ptr0[3 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp3 = in_ptr0[6 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp5 = in_ptr0[21 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp7 = in_ptr0[24 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp9 = in_ptr0[27 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp11 = in_ptr0[42 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp13 = in_ptr0[45 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp15 = in_ptr0[48 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp2 = (tmp0 != tmp0) ? tmp0 : std::max(tmp1, tmp0);\n                            auto tmp4 = (tmp2 != tmp2) ? tmp2 : std::max(tmp3, tmp2);\n                            auto tmp6 = (tmp4 != tmp4) ? tmp4 : std::max(tmp5, tmp4);\n                            auto tmp8 = (tmp6 != tmp6) ? tmp6 : std::max(tmp7, tmp6);\n                            auto tmp10 = (tmp8 != tmp8) ? tmp8 : std::max(tmp9, tmp8);\n                            auto tmp12 = (tmp10 != tmp10) ? tmp10 : std::max(tmp11, tmp10);\n                            auto tmp14 = (tmp12 != tmp12) ? tmp12 : std::max(tmp13, tmp12);\n                            auto tmp16 = (tmp14 != tmp14) ? tmp14 : std::max(tmp15, tmp14);\n                            out_ptr0[i3 + (3*i2) + (9*i1) + (27*i0)] = tmp16;\n                        }\n                    }\n                }\n            }\n        }\n    }\n    #pragma GCC ivdep\n    for(long i0=0; i0<128; i0+=1)\n    {\n        #pragma GCC ivdep\n        for(long i1=0; i1<3; i1+=1)\n        {\n            #pragma GCC ivdep\n            for(long i2=0; i2<9; i2+=1)\n            {\n                {\n                    {\n                        auto tmp0 = out_ptr0[i1 + (3*i2) + (27*i0)];\n                        out_ptr1[i1 + (3*i2) + (27*i0)] = tmp0;\n                        out_ptr2[i1 + (3*i2) + (27*i0)] = tmp0;\n                    }\n                }\n            }\n        }\n    }\n}\n''')\n\nasync_compile.wait(globals())\ndel async_compile\n\ndef call(args):\n    arg0_1, arg1_1, arg2_1, arg3_1, arg4_1 = args\n    args.clear()\n    buf0 = empty_strided((128, 3, 3, 3), (27, 1, 9, 3), device='cpu', dtype=torch.float32)\n    buf2 = empty_strided((128, 3, 3, 3), (27, 1, 9, 3), device='cpu', dtype=torch.float32)\n    buf4 = empty_strided((128, 3, 3, 3), (27, 1, 9, 3), device='cpu', dtype=torch.float32)\n    kernel_cpp_0(c_void_p(arg4_1.data_ptr()), c_void_p(buf0.data_ptr()), c_void_p(buf2.data_ptr()), c_void_p(buf4.data_ptr()))\n    del arg4_1\n    del buf0\n    buf3 = torch.ops.mkldnn._convolution_pointwise(buf2, arg0_1, arg1_1, (0, 0), (1, 1), (1, 1), 1, 'none', [], '')\n    assert_size_stride(buf3, (128, 3, 3, 3), (27, 1, 9, 3))\n    del arg0_1\n    del arg1_1\n    del buf2\n    buf5 = torch.ops.mkldnn._convolution_pointwise(buf4, arg2_1, arg3_1, (0, 0), (1, 1), (1, 1), 1, 'none', [], '')\n    assert_size_stride(buf5, (128, 3, 3, 3), (27, 1, 9, 3))\n    del arg2_1\n    del arg3_1\n    return (buf3, buf5, )\n```\n\nAfter this PR, the generated  code will remove the redundant memory copy:\n\n```\nkernel_cpp_0 = async_compile.cpp('''\n#include \"/tmp/torchinductor_xiaobing/77/c7773nj5pwikpmm2pwa62rcudlf7p3if7eyqb5k4sjsvewwje4le.h\"\nextern \"C\" void kernel(const float* __restrict__ in_ptr0,\n                       float* __restrict__ out_ptr0)\n{\n    #pragma GCC ivdep\n    for(long i0=0; i0<128; i0+=1)\n    {\n        #pragma GCC ivdep\n        for(long i1=0; i1<3; i1+=1)\n        {\n            #pragma GCC ivdep\n            for(long i2=0; i2<3; i2+=1)\n            {\n                #pragma GCC ivdep\n                for(long i3=0; i3<3; i3+=1)\n                {\n                    {\n                        {\n                            auto tmp0 = in_ptr0[i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp1 = in_ptr0[3 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp3 = in_ptr0[6 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp5 = in_ptr0[21 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp7 = in_ptr0[24 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp9 = in_ptr0[27 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp11 = in_ptr0[42 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp13 = in_ptr0[45 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp15 = in_ptr0[48 + i3 + (6*i2) + (42*i1) + (147*i0)];\n                            auto tmp2 = (tmp0 != tmp0) ? tmp0 : std::max(tmp1, tmp0);\n                            auto tmp4 = (tmp2 != tmp2) ? tmp2 : std::max(tmp3, tmp2);\n                            auto tmp6 = (tmp4 != tmp4) ? tmp4 : std::max(tmp5, tmp4);\n                            auto tmp8 = (tmp6 != tmp6) ? tmp6 : std::max(tmp7, tmp6);\n                            auto tmp10 = (tmp8 != tmp8) ? tmp8 : std::max(tmp9, tmp8);\n                            auto tmp12 = (tmp10 != tmp10) ? tmp10 : std::max(tmp11, tmp10);\n                            auto tmp14 = (tmp12 != tmp12) ? tmp12 : std::max(tmp13, tmp12);\n                            auto tmp16 = (tmp14 != tmp14) ? tmp14 : std::max(tmp15, tmp14);\n                            out_ptr0[i3 + (3*i2) + (9*i1) + (27*i0)] = tmp16;\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n''')\n\nasync_compile.wait(globals())\ndel async_compile\n\ndef call(args):\n    arg0_1, arg1_1, arg2_1, arg3_1, arg4_1 = args\n    args.clear()\n    buf0 = empty_strided((128, 3, 3, 3), (27, 1, 9, 3), device='cpu', dtype=torch.float32)\n    kernel_cpp_0(c_void_p(arg4_1.data_ptr()), c_void_p(buf0.data_ptr()))\n    del arg4_1\n    buf2 = torch.ops.mkldnn._convolution_pointwise(buf0, arg0_1, arg1_1, (0, 0), (1, 1), (1, 1), 1, 'none', [], '')\n    assert_size_stride(buf2, (128, 3, 3, 3), (27, 1, 9, 3))\n    del arg0_1\n    del arg1_1\n    buf3 = torch.ops.mkldnn._convolution_pointwise(buf0, arg2_1, arg3_1, (0, 0), (1, 1), (1, 1), 1, 'none', [], '')\n    assert_size_stride(buf3, (128, 3, 3, 3), (27, 1, 9, 3))\n    del arg2_1\n    del arg3_1\n    return (buf2, buf3, )\n\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/89904\nApproved by: https://github.com/jansel",
    "changes": [
        {
            "name": "ir.py",
            "path": "torch/_inductor/ir.py",
            "patches": [
                {
                    "old_start": 2497,
                    "old_length": 9,
                    "new_start": 2497,
                    "new_length": 7,
                    "hunk_buggy": "[' \\n', '         # require x to have the layout as strided_ordered as order\\n', '         if is_storage_and_layout(x):\\n', '-            if isinstance(\\n', '-                x.get_layout(), FlexibleLayout\\n', '-            ) and is_stride_order_storage_and_layout(x, order):\\n', '                 # fix flexiblelayout to be FixedLayout with stride_order\\n', '                 as_storage_and_layout(\\n', '                     x, freeze=True, want_contiguous=False, stride_order=order']",
                    "hunk_fix": "@@ -2497,9 +2497,7 @@ class ExternKernel(InputsKernel):\n \n         # require x to have the layout as strided_ordered as order\n         if is_storage_and_layout(x):\n-            if isinstance(\n-                x.get_layout(), FlexibleLayout\n-            ) and is_stride_order_storage_and_layout(x, order):\n+            if isinstance(x.get_layout(), FlexibleLayout):\n                 # fix flexiblelayout to be FixedLayout with stride_order\n                 as_storage_and_layout(\n                     x, freeze=True, want_contiguous=False, stride_order=order"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 395,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/6f5945e4bb1258d39a2878a08a910fcc8f659d5e",
    "date": "2022-12-01T22:01:41+00:00",
    "message": "triton supports devices < 7.0, not 6.0 (#90020)\n\ntriton is still buggy with Pascal devices, so make the error checker reflect that.\n\nAlso, this < 6.0 never worked, as the `has_triton` definition in utils.py was checking >= 7.0.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/90020\nApproved by: https://github.com/yanboliang, https://github.com/anijain2305",
    "changes": [
        {
            "name": "scheduler.py",
            "path": "torch/_inductor/scheduler.py",
            "patches": [
                {
                    "old_start": 1080,
                    "old_length": 9,
                    "new_start": 1080,
                    "new_length": 9,
                    "hunk_buggy": "['         else:\\n', '             if not has_triton():\\n', '                 device_props = torch.cuda.get_device_properties(device)\\n', '-                if device_props.major < 6:\\n', '                     raise RuntimeError(\\n', '-                        f\"Found {device_props.name} which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 6.0, but your device is of CUDA capability {device_props.major}.{device_props.minor}\"  # noqa: B950\\n', '                     )\\n', '                 else:\\n', '                     raise RuntimeError(']",
                    "hunk_fix": "@@ -1080,9 +1080,9 @@ class Scheduler:\n         else:\n             if not has_triton():\n                 device_props = torch.cuda.get_device_properties(device)\n-                if device_props.major < 6:\n+                if device_props.major < 7:\n                     raise RuntimeError(\n-                        f\"Found {device_props.name} which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 6.0, but your device is of CUDA capability {device_props.major}.{device_props.minor}\"  # noqa: B950\n+                        f\"Found {device_props.name} which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability {device_props.major}.{device_props.minor}\"  # noqa: B950\n                     )\n                 else:\n                     raise RuntimeError("
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 396,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/5b51ca6808191e9f3dcea1d43fa731488cc688bb",
    "date": "2022-11-23T03:07:22+00:00",
    "message": "Update CUDA compiler matrix (#86360)\n\nSwitch GCC/Clang max versions to be exclusive as the `include/crt/host_config.h` checks the major version only for the upper bound. This allows to be less restrictive and match the checks in the aforementioned header.\nAlso update the versions using that header in the CUDA SDKs.\n\nFollow up to #82860\n\nI noticed this as PyTorch 1.12.1 with CUDA 11.3.1 and GCC 10.3 was failing in the `test_cpp_extensions*` tests.\n\nExample for CUDA 11.3.1 from the SDK header:\n\n```\n#if __GNUC__ > 11\n// Error out\n...\n#if (__clang_major__ >= 12) || (__clang_major__ < 3) || ((__clang_major__ == 3) &&  (__clang_minor__ < 3))\n// Error out\n```\nPull Request resolved: https://github.com/pytorch/pytorch/pull/86360\nApproved by: https://github.com/ezyang",
    "changes": [
        {
            "name": "cpp_extension.py",
            "path": "torch/utils/cpp_extension.py",
            "patches": [
                {
                    "old_start": 18,
                    "old_length": 7,
                    "new_start": 18,
                    "new_length": 7,
                    "hunk_buggy": "[' from ._cpp_extension_versioner import ExtensionVersioner\\n', ' from .hipify import hipify_python\\n', ' from .hipify.hipify_python import GeneratedFileCleaner\\n', '-from typing import List, Optional, Union, Tuple\\n', ' from torch.torch_version import TorchVersion\\n', ' \\n', ' from setuptools.command.build_ext import build_ext\\n']",
                    "hunk_fix": "@@ -18,7 +18,7 @@ from .file_baton import FileBaton\n from ._cpp_extension_versioner import ExtensionVersioner\n from .hipify import hipify_python\n from .hipify.hipify_python import GeneratedFileCleaner\n-from typing import List, Optional, Union, Tuple\n+from typing import Dict, List, Optional, Union, Tuple\n from torch.torch_version import TorchVersion\n \n from setuptools.command.build_ext import build_ext\n"
                },
                {
                    "old_start": 42,
                    "old_length": 29,
                    "new_start": 42,
                    "new_length": 35,
                    "hunk_buggy": "[' MINIMUM_GCC_VERSION = (5, 0, 0)\\n', ' MINIMUM_MSVC_VERSION = (19, 0, 24215)\\n', ' \\n', ' # The following values were taken from the following GitHub gist that\\n', ' # summarizes the minimum valid major versions of g++/clang++ for each supported\\n', ' # CUDA version: https://gist.github.com/ax3l/9489132\\n', '-CUDA_GCC_VERSIONS = {\\n', \"-    '10.2': (MINIMUM_GCC_VERSION, (8, 0, 0)),\\n\", \"-    '11.1': (MINIMUM_GCC_VERSION, (10, 0, 0)),\\n\", \"-    '11.2': (MINIMUM_GCC_VERSION, (10, 2, 1)),\\n\", \"-    '11.3': (MINIMUM_GCC_VERSION, (10, 2, 1)),\\n\", \"-    '11.4': ((6, 0, 0), (11, 5, 0)),\\n\", \"-    '11.5': ((6, 0, 0), (11, 5, 0)),\\n\", \"-    '11.6': ((6, 0, 0), (11, 5, 0)),\\n\", \"-    '11.7': ((6, 0, 0), (11, 5, 0)),\\n\", ' }\\n', ' \\n', '-CUDA_CLANG_VERSIONS = {\\n', \"-    '10.2': ((3, 3, 0), (8, 0, 0)),\\n\", \"-    '11.1': ((6, 0, 0), (9, 0, 0)),\\n\", \"-    '11.2': ((6, 0, 0), (9, 0, 0)),\\n\", \"-    '11.3': ((6, 0, 0), (11, 0, 0)),\\n\", \"-    '11.4': ((6, 0, 0), (11, 0, 0)),\\n\", \"-    '11.5': ((6, 0, 0), (12, 0, 0)),\\n\", \"-    '11.6': ((6, 0, 0), (12, 0, 0)),\\n\", \"-    '11.7': ((6, 0, 0), (13, 0, 0)),\\n\", ' }\\n', ' \\n', ' __all__ = [\"get_default_build_root\", \"check_compiler_ok_for_platform\", \"get_compiler_abi_compatibility_and_version\", \"BuildExtension\",\\n']",
                    "hunk_fix": "@@ -42,29 +42,35 @@ SUBPROCESS_DECODE_ARGS = ('oem',) if IS_WINDOWS else ()\n MINIMUM_GCC_VERSION = (5, 0, 0)\n MINIMUM_MSVC_VERSION = (19, 0, 24215)\n \n+VersionRange = Tuple[Tuple[int, ...], Tuple[int, ...]]\n+VersionMap = Dict[str, VersionRange]\n # The following values were taken from the following GitHub gist that\n # summarizes the minimum valid major versions of g++/clang++ for each supported\n # CUDA version: https://gist.github.com/ax3l/9489132\n-CUDA_GCC_VERSIONS = {\n-    '10.2': (MINIMUM_GCC_VERSION, (8, 0, 0)),\n-    '11.1': (MINIMUM_GCC_VERSION, (10, 0, 0)),\n-    '11.2': (MINIMUM_GCC_VERSION, (10, 2, 1)),\n-    '11.3': (MINIMUM_GCC_VERSION, (10, 2, 1)),\n-    '11.4': ((6, 0, 0), (11, 5, 0)),\n-    '11.5': ((6, 0, 0), (11, 5, 0)),\n-    '11.6': ((6, 0, 0), (11, 5, 0)),\n-    '11.7': ((6, 0, 0), (11, 5, 0)),\n+# Or from include/crt/host_config.h in the CUDA SDK\n+# The second value is the exclusive(!) upper bound, i.e. min <= version < max\n+CUDA_GCC_VERSIONS: VersionMap = {\n+    '10.2': (MINIMUM_GCC_VERSION, (9, 0)),\n+    '11.0': (MINIMUM_GCC_VERSION, (10, 0)),\n+    '11.1': (MINIMUM_GCC_VERSION, (11, 0)),\n+    '11.2': (MINIMUM_GCC_VERSION, (11, 0)),\n+    '11.3': (MINIMUM_GCC_VERSION, (11, 0)),\n+    '11.4': ((6, 0, 0), (12, 0)),\n+    '11.5': ((6, 0, 0), (12, 0)),\n+    '11.6': ((6, 0, 0), (12, 0)),\n+    '11.7': ((6, 0, 0), (12, 0)),\n }\n \n-CUDA_CLANG_VERSIONS = {\n-    '10.2': ((3, 3, 0), (8, 0, 0)),\n-    '11.1': ((6, 0, 0), (9, 0, 0)),\n-    '11.2': ((6, 0, 0), (9, 0, 0)),\n-    '11.3': ((6, 0, 0), (11, 0, 0)),\n-    '11.4': ((6, 0, 0), (11, 0, 0)),\n-    '11.5': ((6, 0, 0), (12, 0, 0)),\n-    '11.6': ((6, 0, 0), (12, 0, 0)),\n-    '11.7': ((6, 0, 0), (13, 0, 0)),\n+MINIMUM_CLANG_VERSION = (3, 3, 0)\n+CUDA_CLANG_VERSIONS: VersionMap = {\n+    '10.2': (MINIMUM_CLANG_VERSION, (9, 0)),\n+    '11.1': (MINIMUM_CLANG_VERSION, (11, 0)),\n+    '11.2': (MINIMUM_CLANG_VERSION, (12, 0)),\n+    '11.3': (MINIMUM_CLANG_VERSION, (12, 0)),\n+    '11.4': (MINIMUM_CLANG_VERSION, (13, 0)),\n+    '11.5': (MINIMUM_CLANG_VERSION, (13, 0)),\n+    '11.6': (MINIMUM_CLANG_VERSION, (14, 0)),\n+    '11.7': (MINIMUM_CLANG_VERSION, (14, 0)),\n }\n \n __all__ = [\"get_default_build_root\", \"check_compiler_ok_for_platform\", \"get_compiler_abi_compatibility_and_version\", \"BuildExtension\",\n"
                },
                {
                    "old_start": 388,
                    "old_length": 20,
                    "new_start": 394,
                    "new_length": 19,
                    "hunk_buggy": "['             _is_binary_build()):\\n', '         return\\n', ' \\n', \"-    cuda_compiler_bounds = CUDA_CLANG_VERSIONS if compiler_name.startswith('clang') else CUDA_GCC_VERSIONS\\n\", ' \\n', '     if cuda_str_version not in cuda_compiler_bounds:\\n', \"         warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\\n\", '     else:\\n', '-        min_compiler_version, max_compiler_version = cuda_compiler_bounds[cuda_str_version]\\n', '-        # Special case for 11.4.0, which has lower compiler bounds that 11.4.1\\n', '         if \"V11.4.48\" in cuda_version_str and cuda_compiler_bounds == CUDA_GCC_VERSIONS:\\n', '-            max_compiler_version = (10, 0, 0)\\n', \"         min_compiler_version_str = '.'.join(map(str, min_compiler_version))\\n\", \"-        max_compiler_version_str = '.'.join(map(str, max_compiler_version))\\n\", ' \\n', \"-        version_bound_str = f'>={min_compiler_version_str}'\\n\", \"-        version_bound_str = f'{version_bound_str}, <={max_compiler_version_str}'\\n\", ' \\n', '         if compiler_version < TorchVersion(min_compiler_version_str):\\n', '             raise RuntimeError(\\n']",
                    "hunk_fix": "@@ -388,20 +394,19 @@ def _check_cuda_version(compiler_name: str, compiler_version: TorchVersion) -> N\n             _is_binary_build()):\n         return\n \n-    cuda_compiler_bounds = CUDA_CLANG_VERSIONS if compiler_name.startswith('clang') else CUDA_GCC_VERSIONS\n+    cuda_compiler_bounds: VersionMap = CUDA_CLANG_VERSIONS if compiler_name.startswith('clang') else CUDA_GCC_VERSIONS\n \n     if cuda_str_version not in cuda_compiler_bounds:\n         warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n     else:\n-        min_compiler_version, max_compiler_version = cuda_compiler_bounds[cuda_str_version]\n-        # Special case for 11.4.0, which has lower compiler bounds that 11.4.1\n+        min_compiler_version, max_excl_compiler_version = cuda_compiler_bounds[cuda_str_version]\n+        # Special case for 11.4.0, which has lower compiler bounds than 11.4.1\n         if \"V11.4.48\" in cuda_version_str and cuda_compiler_bounds == CUDA_GCC_VERSIONS:\n-            max_compiler_version = (10, 0, 0)\n+            max_excl_compiler_version = (11, 0)\n         min_compiler_version_str = '.'.join(map(str, min_compiler_version))\n-        max_compiler_version_str = '.'.join(map(str, max_compiler_version))\n+        max_excl_compiler_version_str = '.'.join(map(str, max_excl_compiler_version))\n \n-        version_bound_str = f'>={min_compiler_version_str}'\n-        version_bound_str = f'{version_bound_str}, <={max_compiler_version_str}'\n+        version_bound_str = f'>={min_compiler_version_str}, <{max_excl_compiler_version_str}'\n \n         if compiler_version < TorchVersion(min_compiler_version_str):\n             raise RuntimeError(\n"
                },
                {
                    "old_start": 409,
                    "old_length": 10,
                    "new_start": 414,
                    "new_length": 10,
                    "hunk_buggy": "[\"                 f'than the minimum required version by CUDA {cuda_str_version} ({min_compiler_version_str}). '\\n\", \"                 f'Please make sure to use an adequate version of {compiler_name} ({version_bound_str}).'\\n\", '             )\\n', '-        if compiler_version > TorchVersion(max_compiler_version_str):\\n', '             raise RuntimeError(\\n', \"                 f'The current installed version of {compiler_name} ({compiler_version}) is greater '\\n\", \"-                f'than the maximum required version by CUDA {cuda_str_version} ({max_compiler_version_str}). '\\n\", \"                 f'Please make sure to use an adequate version of {compiler_name} ({version_bound_str}).'\\n\", '             )\\n', ' ']",
                    "hunk_fix": "@@ -409,10 +414,10 @@ def _check_cuda_version(compiler_name: str, compiler_version: TorchVersion) -> N\n                 f'than the minimum required version by CUDA {cuda_str_version} ({min_compiler_version_str}). '\n                 f'Please make sure to use an adequate version of {compiler_name} ({version_bound_str}).'\n             )\n-        if compiler_version > TorchVersion(max_compiler_version_str):\n+        if compiler_version >= TorchVersion(max_excl_compiler_version_str):\n             raise RuntimeError(\n                 f'The current installed version of {compiler_name} ({compiler_version}) is greater '\n-                f'than the maximum required version by CUDA {cuda_str_version} ({max_compiler_version_str}). '\n+                f'than the maximum required version by CUDA {cuda_str_version}. '\n                 f'Please make sure to use an adequate version of {compiler_name} ({version_bound_str}).'\n             )\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 397,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/e856a4d66bead8997a83f8714547c09fcbcdc263",
    "date": "2022-11-17T20:10:52+00:00",
    "message": "Add an env var to skip cudnn version compatibility check (#89184)\n\nskip the check by setting `PYTORCH_SKIP_CUDNN_COMPATIBILITY_CHECK=1`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/89184\nApproved by: https://github.com/ngimel",
    "changes": [
        {
            "name": "__init__.py",
            "path": "torch/backends/cudnn/__init__.py",
            "patches": [
                {
                    "old_start": 37,
                    "old_length": 6,
                    "new_start": 37,
                    "new_length": 8,
                    "hunk_buggy": "['             else:\\n', '                 cudnn_compatible = runtime_minor >= compile_minor\\n', '             if not cudnn_compatible:\\n', \"                 base_error_msg = (f'cuDNN version incompatibility: '\\n\", \"                                   f'PyTorch was compiled  against {compile_version} '\\n\", \"                                   f'but found runtime version {runtime_version}. '\"]",
                    "hunk_fix": "@@ -37,6 +37,8 @@ if _cudnn is not None:\n             else:\n                 cudnn_compatible = runtime_minor >= compile_minor\n             if not cudnn_compatible:\n+                if os.environ.get('PYTORCH_SKIP_CUDNN_COMPATIBILITY_CHECK', '0') == '1':\n+                    return True\n                 base_error_msg = (f'cuDNN version incompatibility: '\n                                   f'PyTorch was compiled  against {compile_version} '\n                                   f'but found runtime version {runtime_version}. '"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 398,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/ee91c328da5739ce03b3127cd7c542ce505212b8",
    "date": "2022-11-11T12:19:31+00:00",
    "message": "Fix cuda/cpu check on NoneType (#88854)\n\nSummary: Fix cuda/cpu check on NoneType\n\nTest Plan: sabdcastle/ github CI/CD\n\nDifferential Revision: D41203955\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/88854\nApproved by: https://github.com/drisspg, https://github.com/ngimel",
    "changes": [
        {
            "name": "activation.py",
            "path": "torch/nn/modules/activation.py",
            "patches": [
                {
                    "old_start": 1111,
                    "old_length": 7,
                    "new_start": 1111,
                    "new_length": 7,
                    "hunk_buggy": "['             # generator expressions.\\n', '             if torch.overrides.has_torch_function(tensor_args):\\n', '                 why_not_fast_path = \"some Tensor argument has_torch_function\"\\n', \"-            elif not all([(x.is_cuda or 'cpu' in str(x.device)) for x in tensor_args]):\\n\", '                 why_not_fast_path = \"some Tensor argument is neither CUDA nor CPU\"\\n', '             elif torch.is_grad_enabled() and any([x.requires_grad for x in tensor_args]):\\n', '                 why_not_fast_path = (\"grad is enabled and at least one of query or the \"']",
                    "hunk_fix": "@@ -1111,7 +1111,7 @@ class MultiheadAttention(Module):\n             # generator expressions.\n             if torch.overrides.has_torch_function(tensor_args):\n                 why_not_fast_path = \"some Tensor argument has_torch_function\"\n-            elif not all([(x.is_cuda or 'cpu' in str(x.device)) for x in tensor_args]):\n+            elif not all([(x is None or x.is_cuda or 'cpu' in str(x.device)) for x in tensor_args]):\n                 why_not_fast_path = \"some Tensor argument is neither CUDA nor CPU\"\n             elif torch.is_grad_enabled() and any([x.requires_grad for x in tensor_args]):\n                 why_not_fast_path = (\"grad is enabled and at least one of query or the \""
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 399,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/b90db4a78f8d760377a81a5a64d03ab4b67599de",
    "date": "2022-10-20T05:05:56+00:00",
    "message": "[DataPipe] Fix type checking to accept both Iter and Map DataPipe (#87285)\n\nFixes https://github.com/pytorch/data/issues/841\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/87285\nApproved by: https://github.com/NivekT",
    "changes": [
        {
            "name": "dataloader.py",
            "path": "torch/utils/data/dataloader.py",
            "patches": [
                {
                    "old_start": 109,
                    "old_length": 7,
                    "new_start": 109,
                    "new_length": 7,
                    "hunk_buggy": "['     assert info is not None\\n', '     total_workers = info.num_workers\\n', '     datapipe = info.dataset\\n', '-    assert isinstance(datapipe, IterDataPipe)\\n', '     # To distribute elements across distributed process evenly, we should shard data on distributed\\n', '     # processes first then shard on worker processes\\n', '     total_workers *= world_size']",
                    "hunk_fix": "@@ -109,7 +109,7 @@ def _sharding_worker_init_fn(worker_init_fn, world_size, rank_id, worker_id):\n     assert info is not None\n     total_workers = info.num_workers\n     datapipe = info.dataset\n-    assert isinstance(datapipe, IterDataPipe)\n+    assert isinstance(datapipe, (IterDataPipe, MapDataPipe))\n     # To distribute elements across distributed process evenly, we should shard data on distributed\n     # processes first then shard on worker processes\n     total_workers *= world_size"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 400,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/232fbd90ff6d93362120d955befeeb297179ddad",
    "date": "2022-10-19T07:13:38+00:00",
    "message": "[TorchDynamo]: fused bias for cpu convolution path (#87050)\n\nFor aten.convolution CPU path, the bias always can be fused, so this PR adds a device check: if inputs' device is CPU, we will fuse it for a good performance.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/87050\nApproved by: https://github.com/jgong5, https://github.com/jansel",
    "changes": [
        {
            "name": "lowering.py",
            "path": "torch/_inductor/lowering.py",
            "patches": [
                {
                    "old_start": 1091,
                    "old_length": 11,
                    "new_start": 1091,
                    "new_length": 16,
                    "hunk_buggy": "['     output_padding: List[int],\\n', '     groups: int,\\n', ' ):\\n', '     result = TensorBox.create(\\n', '         ir.Convolution.create(\\n', '             x,\\n', '             weight,\\n', '-            None,  # bias handled below\\n', '             stride,\\n', '             padding,\\n', '             dilation,\\n']",
                    "hunk_fix": "@@ -1091,11 +1091,16 @@ def convolution(\n     output_padding: List[int],\n     groups: int,\n ):\n+    is_cpu = all(\n+        input.get_device().type == \"cpu\"\n+        for input in (x, weight, bias)\n+        if input is not None\n+    )\n     result = TensorBox.create(\n         ir.Convolution.create(\n             x,\n             weight,\n-            None,  # bias handled below\n+            bias if is_cpu else None,  # For cpu path, bias can always be fused\n             stride,\n             padding,\n             dilation,\n"
                },
                {
                    "old_start": 1104,
                    "old_length": 7,
                    "new_start": 1109,
                    "new_length": 7,
                    "hunk_buggy": "['             groups,\\n', '         )\\n', '     )\\n', '-    if bias is not None:\\n', '         kernel_dims = len(weight.get_size()) - 2\\n', '         out_chan = result.get_size()[-1 - kernel_dims]\\n', '         bias = view(bias, [out_chan] + kernel_dims * [1])']",
                    "hunk_fix": "@@ -1104,7 +1109,7 @@ def convolution(\n             groups,\n         )\n     )\n-    if bias is not None:\n+    if not is_cpu and bias is not None:\n         kernel_dims = len(weight.get_size()) - 2\n         out_chan = result.get_size()[-1 - kernel_dims]\n         bias = view(bias, [out_chan] + kernel_dims * [1])"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 401,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/4839f73f329b38819e6f69a8662d61dc36558e52",
    "date": "2022-10-13T17:54:28+00:00",
    "message": "Fix incorrect tensor storage check  (#86845)\n\nFix incorrect tensor storage check\n\nThis change contains an incorrect check for storage: https://github.com/pytorch/pytorch/pull/86557\n**self.storage is not None**\nshould have been:\n**not torch._C._has_storage(self)**\n\nThese fixes were run through the DirectML test suite, and confirm the check is now working correctly.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/86845\nApproved by: https://github.com/martinb35, https://github.com/bdhirsh",
    "changes": [
        {
            "name": "_tensor.py",
            "path": "torch/_tensor.py",
            "patches": [
                {
                    "old_start": 116,
                    "old_length": 7,
                    "new_start": 115,
                    "new_length": 10,
                    "hunk_buggy": "['             if (\\n', '                 self.is_sparse\\n', '                 or self.device.type in [\"lazy\", \"xla\", \"mps\", \"ort\", \"meta\", \"hpu\"]\\n', '-                or (self.storage is None and self.device.type == \"privateuseone\")\\n', '                 or (type(self) is not Tensor and self.data_ptr() == 0)\\n', '             ):\\n', '                 new_tensor = self.clone()\\n']",
                    "hunk_fix": "@@ -116,7 +115,10 @@ class Tensor(torch._C._TensorBase):\n             if (\n                 self.is_sparse\n                 or self.device.type in [\"lazy\", \"xla\", \"mps\", \"ort\", \"meta\", \"hpu\"]\n-                or (self.storage is None and self.device.type == \"privateuseone\")\n+                or (\n+                    not torch._C._has_storage(self)\n+                    and self.device.type == \"privateuseone\"\n+                )\n                 or (type(self) is not Tensor and self.data_ptr() == 0)\n             ):\n                 new_tensor = self.clone()\n"
                },
                {
                    "old_start": 273,
                    "old_length": 7,
                    "new_start": 275,
                    "new_length": 7,
                    "hunk_buggy": "['         #    `tolist()` converts every single element in the tensor into python objects\\n', '         #    and serialize them one by one.\\n', '         if self.device.type in [\"xla\", \"ort\", \"hpu\"] or (\\n', '-            self.storage is None and self.device.type == \"privateuseone\"\\n', '         ):\\n', \"             # Convert BFloat16 tesors to Float32 before conversion to numpy, as numpy doesn't\\n\", '             # support BFloat16. The rebuild tensor from numpy takes in the original self.dtype,']",
                    "hunk_fix": "@@ -273,7 +275,7 @@ class Tensor(torch._C._TensorBase):\n         #    `tolist()` converts every single element in the tensor into python objects\n         #    and serialize them one by one.\n         if self.device.type in [\"xla\", \"ort\", \"hpu\"] or (\n-            self.storage is None and self.device.type == \"privateuseone\"\n+            not torch._C._has_storage(self) and self.device.type == \"privateuseone\"\n         ):\n             # Convert BFloat16 tesors to Float32 before conversion to numpy, as numpy doesn't\n             # support BFloat16. The rebuild tensor from numpy takes in the original self.dtype,"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 402,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/61b9d8fccd3361f21e1f3548c2a9538b62cc7525",
    "date": "2022-08-26T20:03:24+00:00",
    "message": "[Profiler][Trivial] Add null handling to `AppendOnlyList::copy` memcpy path. (#83963)\n\nIt is apparently undefined behavior to do pointer arithmetic on nullptr. In the case of AppendOnlyList, `next_` will only be null if `end_` is also null and thus the `memcpy` path will only be triggered if `n == 0`. Nonetheless, it is UB to `memcpy(0, 0, 0)`\n\nThe extra null check is in a `C10_LIKELY` block so the extra cost should be negligible, and indeed after dusting off the component microbenchmarks there's no observable difference.\n\nDifferential Revision: [D38969443](https://our.internmc.facebook.com/intern/diff/D38969443/)\nPull Request resolved: https://github.com/pytorch/pytorch/pull/83963\nApproved by: https://github.com/slgong-fb",
    "changes": [
        {
            "name": "containers.h",
            "path": "torch/csrc/profiler/containers.h",
            "patches": [
                {
                    "old_start": 71,
                    "old_length": 8,
                    "new_start": 71,
                    "new_length": 8,
                    "hunk_buggy": "['   typename std::enable_if<\\n', '       std::is_same<T0, T>::value && std::is_trivially_copyable<T>::value>::type\\n', '   copy(c10::ArrayRef<T0> src) {\\n', '-    int n = src.size();\\n', '-    if (C10_LIKELY(next_ + n <= end_)) {\\n', '       std::memcpy((void*)next_, (void*)src.begin(), n * sizeof(T0));\\n', '       next_ += n;\\n', '     } else {']",
                    "hunk_fix": "@@ -71,8 +71,8 @@ class AppendOnlyList {\n   typename std::enable_if<\n       std::is_same<T0, T>::value && std::is_trivially_copyable<T>::value>::type\n   copy(c10::ArrayRef<T0> src) {\n-    int n = src.size();\n-    if (C10_LIKELY(next_ + n <= end_)) {\n+    size_t n = src.size();\n+    if (C10_LIKELY(next_ && (next_ + n <= end_))) {\n       std::memcpy((void*)next_, (void*)src.begin(), n * sizeof(T0));\n       next_ += n;\n     } else {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 403,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/63cbdc92a750a667ffdcfbdac563d02db6fd9559",
    "date": "2022-08-25T08:28:40+00:00",
    "message": "switching the exact check to isinstance check (#84023)\n\nSimplifying a type check if an object is a SymIntNode in `is_symint_node`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/84023\nApproved by: https://github.com/ezyang",
    "changes": [
        {
            "name": "python_arg_parser.h",
            "path": "torch/csrc/utils/python_arg_parser.h",
            "patches": [
                {
                    "old_start": 79,
                    "old_length": 8,
                    "new_start": 79,
                    "new_length": 7,
                    "hunk_buggy": "[' namespace torch {\\n', ' inline bool is_symint_node(py::handle obj) {\\n', '   auto static tp_symn = py::type::of<c10::SymIntNodeImpl>();\\n', '-  // TODO: switch this to `isinstance`\\n', '-  if (obj.get_type().equal(tp_symn)) {\\n', '     TORCH_CHECK(\\n', '         !jit::tracer::isTracing(), \"JIT tracing of SymInts isn\\'t supported!\");\\n', '     return true;']",
                    "hunk_fix": "@@ -79,8 +79,7 @@\n namespace torch {\n inline bool is_symint_node(py::handle obj) {\n   auto static tp_symn = py::type::of<c10::SymIntNodeImpl>();\n-  // TODO: switch this to `isinstance`\n-  if (obj.get_type().equal(tp_symn)) {\n+  if (py::isinstance(obj, tp_symn)) {\n     TORCH_CHECK(\n         !jit::tracer::isTracing(), \"JIT tracing of SymInts isn't supported!\");\n     return true;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 404,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/6592259ea52f45e1fc9a633ccb5b154ba5099334",
    "date": "2022-08-01T09:28:44+00:00",
    "message": "[HPU] Enable torch.jit.load for HPU (#81759)\n\nAs per torch.jit.load documentation, all previously saved modules,\nirrespective of their device, are first loaded onto CPU, and then\nare moved to the devices they were saved from. So far, supported\ndevices included CPU and CUDA only. To enable torch.jit.load for\nHPU, additional check for HPU is introduced.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/81759\nApproved by: https://github.com/eellison",
    "changes": [
        {
            "name": "unpickler.cpp",
            "path": "torch/csrc/jit/serialization/unpickler.cpp",
            "patches": [
                {
                    "old_start": 505,
                    "old_length": 11,
                    "new_start": 505,
                    "new_length": 12,
                    "hunk_buggy": "['         tensor = at::empty({0}, options).set_(storage);\\n', '       }\\n', ' \\n', '-      if (device.is_cuda() || device.is_xpu() || device.is_meta()) {\\n', '         tensor = tensor.to(device, tensor.scalar_type());\\n', '       } else if (device.type() != DeviceType::CPU) {\\n', '         AT_ERROR(\\n', '-            \"supported devices include CPU and CUDA, however got \",\\n', '             DeviceTypeName(device.type(), false));\\n', '       }\\n', '       stack_.emplace_back(std::move(tensor));']",
                    "hunk_fix": "@@ -505,11 +505,12 @@ PickleOpCode Unpickler::readInstruction() {\n         tensor = at::empty({0}, options).set_(storage);\n       }\n \n-      if (device.is_cuda() || device.is_xpu() || device.is_meta()) {\n+      if (device.is_cuda() || device.is_xpu() || device.is_meta() ||\n+          device.is_hpu()) {\n         tensor = tensor.to(device, tensor.scalar_type());\n       } else if (device.type() != DeviceType::CPU) {\n         AT_ERROR(\n-            \"supported devices include CPU and CUDA, however got \",\n+            \"supported devices include CPU, CUDA and HPU, however got \",\n             DeviceTypeName(device.type(), false));\n       }\n       stack_.emplace_back(std::move(tensor));"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 405,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/577e90ae9bf257040acb68da3626d9a64d07bf7a",
    "date": "2022-06-23T09:04:34+00:00",
    "message": "Improve error message for missing ops (#80005)\n\nThe current error message is ill formed. Example\n\nerror: Following ops cannot be found. Please check if the operator library is included in the build. If built with selected ops, check if these ops are in the list. If you are a Meta employee, please see fburl.com/missing_ops for a fix. Or post it in https://discuss.pytorch.org/aten::to.prim_dtype ()\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/80005\nApproved by: https://github.com/cccclai",
    "changes": [
        {
            "name": "function.cpp",
            "path": "torch/csrc/jit/mobile/function.cpp",
            "patches": [
                {
                    "old_start": 85,
                    "old_length": 8,
                    "new_start": 85,
                    "new_length": 9,
                    "hunk_buggy": "['   if (should_check_operators) {\\n', '     TORCH_CHECK(\\n', '         unsupported_op_names.empty(),\\n', '-        \"Following ops cannot be found. Please check if the operator library is included in the build. If built with selected ops, check if these ops are in the list. If you are a Meta employee, please see fburl.com/missing_ops for a fix. Or post it in https://discuss.pytorch.org/\",\\n', '-        c10::Join(\", \", unsupported_op_names));\\n', '   }\\n', '   code_.initialized = all_ops_supported;\\n', '   return all_ops_supported;']",
                    "hunk_fix": "@@ -85,8 +85,9 @@ bool Function::initialize_operators(bool should_check_operators) {\n   if (should_check_operators) {\n     TORCH_CHECK(\n         unsupported_op_names.empty(),\n-        \"Following ops cannot be found. Please check if the operator library is included in the build. If built with selected ops, check if these ops are in the list. If you are a Meta employee, please see fburl.com/missing_ops for a fix. Or post it in https://discuss.pytorch.org/\",\n-        c10::Join(\", \", unsupported_op_names));\n+        \"Following ops cannot be found: [\",\n+        c10::Join(\", \", unsupported_op_names),\n+        \"]. Please check if the operator library is included in the build. If built with selected ops, check if these ops are in the list. If you are a Meta employee, please see fburl.com/missing_ops for a fix. Or post it in https://discuss.pytorch.org/c/mobile/\");\n   }\n   code_.initialized = all_ops_supported;\n   return all_ops_supported;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 406,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/157d478a30f27fd9d866c1235841721a559c8d0b",
    "date": "2022-06-05T23:10:55+00:00",
    "message": "Fix omission of shape in size check in index.\n\nSigned-off-by: Edward Z. Yang <ezyangfb.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/78897\n\nApproved by: https://github.com/Lezcano, https://github.com/anjali411",
    "changes": [
        {
            "name": "_meta_registrations.py",
            "path": "torch/_meta_registrations.py",
            "patches": [
                {
                    "old_start": 193,
                    "old_length": 7,
                    "new_start": 193,
                    "new_length": 7,
                    "hunk_buggy": "['                 )\\n', '                 for j in range(index.ndim):\\n', '                     check(\\n', '-                        index[j] <= self.shape[k + j],\\n', '                         lambda: f\"The shape of the mask {index.shape} at index {i} \"\\n', '                                 f\"does not match the shape of the indexed tensor {self.shape} at index {k + j}\",\\n', '                         IndexError']",
                    "hunk_fix": "@@ -193,7 +193,7 @@ def meta_index_Tensor(self, indices):\n                 )\n                 for j in range(index.ndim):\n                     check(\n-                        index[j] <= self.shape[k + j],\n+                        index.shape[j] == self.shape[k + j],\n                         lambda: f\"The shape of the mask {index.shape} at index {i} \"\n                                 f\"does not match the shape of the indexed tensor {self.shape} at index {k + j}\",\n                         IndexError"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 407,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/1f819ee965894b8332cb364a67c91855c91c9dcc",
    "date": "2022-06-03T20:14:32+00:00",
    "message": "Add check for no grad in transformer encoder nestedtensor conversion (#78832)\n\nBefore, we allowed inputs with grad to be converted to NestedTensors. Autograd attempts to find the size of the NestedTensor, but NestedTensor throws an exception for its size function. This causes all calls to nn.TransformerEncoder with grad enabled to fail.\n\nFix: we add a check for no grad in transformer encoder so we do not convert tensor with grad to nestedtensor.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/78832\nApproved by: https://github.com/cpuhrsch, https://github.com/jbschlosser",
    "changes": [
        {
            "name": "transformer.py",
            "path": "torch/nn/modules/transformer.py",
            "patches": [
                {
                    "old_start": 226,
                    "old_length": 9,
                    "new_start": 226,
                    "new_length": 10,
                    "hunk_buggy": "['                         first_layer.linear2.bias,\\n', '                     )\\n', '                     if not torch.overrides.has_torch_function(tensor_args):\\n', \"-                        if output.is_cuda or 'cpu' in str(output.device):\\n\", '-                            convert_to_nested = True\\n', '-                            output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not())\\n', ' \\n', '         for mod in self.layers:\\n', '             if convert_to_nested:']",
                    "hunk_fix": "@@ -226,9 +226,10 @@ class TransformerEncoder(Module):\n                         first_layer.linear2.bias,\n                     )\n                     if not torch.overrides.has_torch_function(tensor_args):\n-                        if output.is_cuda or 'cpu' in str(output.device):\n-                            convert_to_nested = True\n-                            output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not())\n+                        if not torch.is_grad_enabled() or all([not x.requires_grad for x in tensor_args]):\n+                            if output.is_cuda or 'cpu' in str(output.device):\n+                                convert_to_nested = True\n+                                output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not())\n \n         for mod in self.layers:\n             if convert_to_nested:"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 408,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/b6920405da340bbd3397b80bf16d9c360b0c48d2",
    "date": "2022-05-26T02:27:33+00:00",
    "message": "reorder checks to shave 1 us off no-op dispatch time\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/78316\n\nApproved by: https://github.com/Chillee, https://github.com/ezyang",
    "changes": [
        {
            "name": "python_variable.cpp",
            "path": "torch/csrc/autograd/python_variable.cpp",
            "patches": [
                {
                    "old_start": 262,
                    "old_length": 12,
                    "new_start": 262,
                    "new_length": 12,
                    "hunk_buggy": "[' \\n', ' static bool check_has_torch_dispatch(PyObject *obj) {\\n', '   PyTypeObject *tp = Py_TYPE(obj);\\n', '   py::object attr = PyObject_FastGetAttrString(obj, \"__torch_dispatch__\");\\n', '-  return (\\n', '-    !THPVariable_CheckTypeExact(tp) &&\\n', '-    // TODO: test if Python key is disabled\\n', '-    attr.ptr() != nullptr &&\\n', '-    attr.ptr() != torch::disabled_torch_dispatch_impl()\\n', '   );\\n', ' }\\n', ' ']",
                    "hunk_fix": "@@ -262,12 +262,12 @@ static const char* VOLATILE_WARNING =\n \n static bool check_has_torch_dispatch(PyObject *obj) {\n   PyTypeObject *tp = Py_TYPE(obj);\n+  if (THPVariable_CheckTypeExact(tp)) {\n+    return false;\n+  }\n   py::object attr = PyObject_FastGetAttrString(obj, \"__torch_dispatch__\");\n-  return (\n-    !THPVariable_CheckTypeExact(tp) &&\n-    // TODO: test if Python key is disabled\n-    attr.ptr() != nullptr &&\n-    attr.ptr() != torch::disabled_torch_dispatch_impl()\n+  return (attr.ptr() != nullptr &&\n+          attr.ptr() != torch::disabled_torch_dispatch_impl()\n   );\n }\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 409,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/fe6aa0844466e5dd2669092eac5edde153108b28",
    "date": "2022-05-13T20:49:40+00:00",
    "message": "[PyTorch] IValue(const c10::Scalar&) improvements\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/72551\n\nDon't delegate to `operator=` for construction. Catch hypothetical addition of a new Scalar type via debug assertion rather than checking in prod.\n\nDifferential Revision: [D34090560](https://our.internmc.facebook.com/intern/diff/D34090560/)\n\nApproved by: https://github.com/malfet",
    "changes": [
        {
            "name": "ivalue.h",
            "path": "aten/src/ATen/core/ivalue.h",
            "patches": [
                {
                    "old_start": 750,
                    "old_length": 15,
                    "new_start": 750,
                    "new_length": 17,
                    "hunk_buggy": "['   // Scalar, which gets encoded as either an Int, a Double or a ComplexDouble\\n', '   IValue(const at::Scalar& s) : IValue() {\\n', '     if (s.isFloatingPoint()) {\\n', '-      *this = s.toDouble();\\n', '     } else if (s.isComplex()) {\\n', '       *this = s.toComplexDouble();\\n', '     } else if (s.isBoolean()) {\\n', '-      *this = s.toBool();\\n', '-    } else if (s.isIntegral(false)) {\\n', '-      *this = s.toLong();\\n', '     } else {\\n', '-      TORCH_CHECK(false, \"Unknown type in Scalar\");\\n', '     }\\n', '   }\\n', ' ']",
                    "hunk_fix": "@@ -750,15 +750,17 @@ public:\n   // Scalar, which gets encoded as either an Int, a Double or a ComplexDouble\n   IValue(const at::Scalar& s) : IValue() {\n     if (s.isFloatingPoint()) {\n-      *this = s.toDouble();\n+      tag = Tag::Double;\n+      payload.u.as_double = s.toDouble();\n     } else if (s.isComplex()) {\n       *this = s.toComplexDouble();\n     } else if (s.isBoolean()) {\n-      *this = s.toBool();\n-    } else if (s.isIntegral(false)) {\n-      *this = s.toLong();\n+      tag = Tag::Bool;\n+      payload.u.as_bool = s.toBool();\n     } else {\n-      TORCH_CHECK(false, \"Unknown type in Scalar\");\n+      TORCH_INTERNAL_ASSERT_DEBUG_ONLY(s.isIntegral(false), \"Unknown type in Scalar\");\n+      tag  = Tag::Int;\n+      payload.u.as_int = s.toLong();\n     }\n   }\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 410,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/29b702144bf5bb96dfd8fcbd04b6562a27ca5385",
    "date": "2022-05-07T10:35:29+00:00",
    "message": "Fix issue in s_addmm_out_sparse_dense_cpu only supporting CUDA device checking (#77018)\n\n## Motivation\nThe at::native::s_addmm_out_sparse_dense_cpu only supports the CPU tensors. But it only checks whether the tensor is on CUDA device which is not enough.\n\n## Solution\nChange the tensor device type checkging from is_cuda to !is_cpu to protect other backends than the CUDA.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/77018\nApproved by: https://github.com/IvanYashchuk",
    "changes": [
        {
            "name": "SparseTensorMath.cpp",
            "path": "aten/src/ATen/native/sparse/SparseTensorMath.cpp",
            "patches": [
                {
                    "old_start": 895,
                    "old_length": 10,
                    "new_start": 895,
                    "new_length": 22,
                    "hunk_buggy": "['     const Scalar& alpha\\n', ' ) {\\n', '   // TODO: This error message seems awfully opaque\\n', '-  TORCH_CHECK(!t.is_cuda(),  \"Expected all tensors to be on the same device. addmm expected \\'t\\' to be CPU tensor, but got CUDA tensor\");\\n', '-  TORCH_CHECK(!r.is_cuda(), \"Expected all tensors to be on the same device. addmm: expected \\'out\\' to be CPU tensor, but got CUDA tensor\");\\n', '-  TORCH_CHECK(!sparse_.is_cuda(), \"Expected all tensors to be on the same device. addmm: expected \\'mat1\\' to be a CPU tensor, but got a CUDA tensor\");\\n', '-  TORCH_CHECK(!dense.is_cuda(), \"Expected all tensors to be on the same device. addmm: expected \\'mat2\\' to be a CPU tensor, but got a CUDA tensor\");\\n', ' \\n', '   TORCH_CHECK(sparse_.sparse_dim() == 2, \"addmm: matrices expected, got \", sparse_.sparse_dim(), \"D tensor\");\\n', '   TORCH_CHECK(sparse_.dense_dim() == 0, \"addmm: scalar values expected, got \", sparse_.dense_dim(), \"D values\");']",
                    "hunk_fix": "@@ -895,10 +895,22 @@ Tensor& s_addmm_out_sparse_dense_cpu(\n     const Scalar& alpha\n ) {\n   // TODO: This error message seems awfully opaque\n-  TORCH_CHECK(!t.is_cuda(),  \"Expected all tensors to be on the same device. addmm expected 't' to be CPU tensor, but got CUDA tensor\");\n-  TORCH_CHECK(!r.is_cuda(), \"Expected all tensors to be on the same device. addmm: expected 'out' to be CPU tensor, but got CUDA tensor\");\n-  TORCH_CHECK(!sparse_.is_cuda(), \"Expected all tensors to be on the same device. addmm: expected 'mat1' to be a CPU tensor, but got a CUDA tensor\");\n-  TORCH_CHECK(!dense.is_cuda(), \"Expected all tensors to be on the same device. addmm: expected 'mat2' to be a CPU tensor, but got a CUDA tensor\");\n+  TORCH_CHECK(\n+      t.is_cpu(),\n+      \"Expected all tensors to be on the same device. addmm expected 't' to be CPU tensor, but got tensor on \",\n+      t.device());\n+  TORCH_CHECK(\n+      r.is_cpu(),\n+      \"Expected all tensors to be on the same device. addmm: expected 'out' to be CPU tensor, but got tensor on \",\n+      t.device());\n+  TORCH_CHECK(\n+      sparse_.is_cpu(),\n+      \"Expected all tensors to be on the same device. addmm: expected 'mat1' to be a CPU tensor, but got tensor on \",\n+      t.device());\n+  TORCH_CHECK(\n+      dense.is_cpu(),\n+      \"Expected all tensors to be on the same device. addmm: expected 'mat2' to be a CPU tensor, but got tensor on \",\n+      t.device());\n \n   TORCH_CHECK(sparse_.sparse_dim() == 2, \"addmm: matrices expected, got \", sparse_.sparse_dim(), \"D tensor\");\n   TORCH_CHECK(sparse_.dense_dim() == 0, \"addmm: scalar values expected, got \", sparse_.dense_dim(), \"D values\");"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 411,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/9bcb4de16878073896d8743fbd70d5abe28b595a",
    "date": "2022-05-03T11:50:36+00:00",
    "message": "check parameter k and l\n\nFixes #76715\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/76719\nApproved by: https://github.com/ezyang",
    "changes": [
        {
            "name": "TensorImpl.h",
            "path": "c10/core/TensorImpl.h",
            "patches": [
                {
                    "old_start": 87,
                    "old_length": 7,
                    "new_start": 87,
                    "new_length": 7,
                    "hunk_buggy": "[' \\n', ' // Product of all dims between k and l (not including dims[k] and dims[l])\\n', ' inline int64_t size_between_dim_(int k, int l, IntArrayRef dims) {\\n', '-  TORCH_CHECK((unsigned)l < dims.size());\\n', '   int64_t r = 1;\\n', '   if (k < l) {\\n', '     for (int i = k + 1; i < l; ++i) {']",
                    "hunk_fix": "@@ -87,7 +87,7 @@ inline int64_t size_to_dim_(int k, IntArrayRef dims) {\n \n // Product of all dims between k and l (not including dims[k] and dims[l])\n inline int64_t size_between_dim_(int k, int l, IntArrayRef dims) {\n-  TORCH_CHECK((unsigned)l < dims.size());\n+  TORCH_CHECK((unsigned)l < dims.size() && (unsigned)k < dims.size());\n   int64_t r = 1;\n   if (k < l) {\n     for (int i = k + 1; i < l; ++i) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 412,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/75be4f9cdb503d6eff189b2bc5c05d96bff66653",
    "date": "2022-04-26T17:52:45+00:00",
    "message": "check tensor has storage before refer to tensor data ptr\n\nIn the exporter dedupe initializers passes, check the tensor has storage before reference to tensor's data_ptr, otherwise it will result in a crash.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/76342\nApproved by: https://github.com/BowenBao",
    "changes": [
        {
            "name": "deduplicate_initializers.cpp",
            "path": "torch/csrc/jit/passes/onnx/deduplicate_initializers.cpp",
            "patches": [
                {
                    "old_start": 65,
                    "old_length": 7,
                    "new_start": 65,
                    "new_length": 7,
                    "hunk_buggy": "[' \\n', ' bool DeduplicateInitializersByDataPtr(at::Tensor& t1, at::Tensor& t2) {\\n', '   return t1.sizes().equals(t2.sizes()) && t1.strides().equals(t2.strides()) &&\\n', '-      (t1.data_ptr() == t2.data_ptr());\\n', ' }\\n', ' \\n', ' bool DeduplicateInitializersByValue(at::Tensor& t1, at::Tensor& t2) {']",
                    "hunk_fix": "@@ -65,7 +65,7 @@ void DeduplicateInitializers(\n \n bool DeduplicateInitializersByDataPtr(at::Tensor& t1, at::Tensor& t2) {\n   return t1.sizes().equals(t2.sizes()) && t1.strides().equals(t2.strides()) &&\n-      (t1.data_ptr() == t2.data_ptr());\n+      (t1.has_storage() && t2.has_storage() && t1.data_ptr() == t2.data_ptr());\n }\n \n bool DeduplicateInitializersByValue(at::Tensor& t1, at::Tensor& t2) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 413,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/f02b7a9c36dd6182da694bc47a5c345285dfd951",
    "date": "2022-04-26T13:07:56+00:00",
    "message": "Pad: don't error when unused fill value is zero\n\nFixes pytorch/vision#5873\n\nIn the python version of `F.pad`, checking that the fill value was\nleft as default was done by comparing against zero:\nhttps://github.com/pytorch/pytorch/blob/bc2c6edaf163b1a1330e37a6e34caf8c553e4755/torch/nn/functional.py#L4366\n\nSo if someone does explicitly pass in a zero-value, then this\n`TORCH_CHECK` was an accidental BC-break.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/76307\n\nApproved by: https://github.com/albanD, https://github.com/jbschlosser, https://github.com/datumbox",
    "changes": [
        {
            "name": "PadNd.cpp",
            "path": "aten/src/ATen/native/PadNd.cpp",
            "patches": [
                {
                    "old_start": 163,
                    "old_length": 10,
                    "new_start": 163,
                    "new_length": 9,
                    "hunk_buggy": "['   if (mode == at::padding_mode::constant) {\\n', '     return at::constant_pad_nd(self, pad, value.value_or(0.0));\\n', '   }\\n', '-  TORCH_CHECK(\\n', '-      !value.has_value(), \"Padding mode \\\\\"\",\\n', '-      padding_mode_string(mode),\\n', '-      \"\\\\\" doesn\\'t take in value argument\");\\n', ' \\n', '   if (pad.size() == 2 && (input_dim == 2 || input_dim == 3)) {\\n', '     switch (mode) {']",
                    "hunk_fix": "@@ -163,10 +163,9 @@ Tensor _pad_enum(const Tensor &self, IntArrayRef pad, int64_t mode_int, c10::opt\n   if (mode == at::padding_mode::constant) {\n     return at::constant_pad_nd(self, pad, value.value_or(0.0));\n   }\n-  TORCH_CHECK(\n-      !value.has_value(), \"Padding mode \\\"\",\n-      padding_mode_string(mode),\n-      \"\\\" doesn't take in value argument\");\n+  TORCH_CHECK(!value.has_value() || *value == 0,\n+              \"Padding mode \\\"\", padding_mode_string(mode),\n+              \"\\\" doesn't take in value argument\");\n \n   if (pad.size() == 2 && (input_dim == 2 || input_dim == 3)) {\n     switch (mode) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 414,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/a9deda5469a6ef73692a9dd796cc4eeba4436d6c",
    "date": "2022-04-26T00:56:10+00:00",
    "message": "Fix issue in sparce_coo_tensor only supporting CUDA device.\n\n## Motivation\nThe at::native::_validate_sparse_coo_tensor_args only supports checking the indices on CUDA device and CPU device. To extend the function to support more device type.\n\n## Solution\nCopy  the indices to the CPU to validate the correctness.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/76293\nApproved by: https://github.com/mrshenli",
    "changes": [
        {
            "name": "SparseTensor.cpp",
            "path": "aten/src/ATen/native/sparse/SparseTensor.cpp",
            "patches": [
                {
                    "old_start": 345,
                    "old_length": 7,
                    "new_start": 345,
                    "new_length": 7,
                    "hunk_buggy": "['     Tensor max_indices =\\n', '         std::get</* values */ 0>(indices.max(/* dim */ 1, /* keepdim */ false));\\n', '     Tensor cpu_min_indices, cpu_max_indices;\\n', '-    if (indices.is_cuda()) {\\n', '       cpu_min_indices = min_indices.to(at::DeviceType::CPU);\\n', '       cpu_max_indices = max_indices.to(at::DeviceType::CPU);\\n', '     } else {']",
                    "hunk_fix": "@@ -345,7 +345,7 @@ void _validate_sparse_coo_tensor_args(\n     Tensor max_indices =\n         std::get</* values */ 0>(indices.max(/* dim */ 1, /* keepdim */ false));\n     Tensor cpu_min_indices, cpu_max_indices;\n-    if (indices.is_cuda()) {\n+    if (!indices.is_cpu()) {\n       cpu_min_indices = min_indices.to(at::DeviceType::CPU);\n       cpu_max_indices = max_indices.to(at::DeviceType::CPU);\n     } else {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 415,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/bc371a2cd03ce573f3ad4f7be141364136028905",
    "date": "2022-04-13T00:33:49+00:00",
    "message": "[quant][fx][fix] Add additional checks when tracing back during maybe share output observer function\n\nSummary:\nCurrently in `maybe_make_input_output_share_observers`  we trace back from a node to find the activation_post_process\nof the input node, we have internal use case which would error out during tracing back, this PR is adding a guard\nduring this process to return False early when the node doesn't have any input\n\nTest Plan:\nnot sure when this would happen, verify within the internal test case\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/75650\n\nApproved by: https://github.com/vkuzo",
    "changes": [
        {
            "name": "prepare.py",
            "path": "torch/ao/quantization/fx/prepare.py",
            "patches": [
                {
                    "old_start": 976,
                    "old_length": 6,
                    "new_start": 976,
                    "new_length": 9,
                    "hunk_buggy": "['                 continue\\n', '             iteration_guard = 0\\n', '             while not is_activation_post_process_node(input_arg, modules):\\n', '                 input_arg = input_arg.args[0]\\n', '                 iteration_guard += 1\\n', '                 if iteration_guard > 10000:']",
                    "hunk_fix": "@@ -976,6 +976,9 @@ def maybe_make_input_output_share_observers(\n                 continue\n             iteration_guard = 0\n             while not is_activation_post_process_node(input_arg, modules):\n+                # failed to trace back since no input arg for the current node\n+                if len(input_arg.args) < 1:\n+                    return False\n                 input_arg = input_arg.args[0]\n                 iteration_guard += 1\n                 if iteration_guard > 10000:"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 416,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/37dea0454dd310cfe443859f717862657df6b753",
    "date": "2022-04-08T03:56:03+00:00",
    "message": "[quant] add checking number of args when checking observer in same graph (#75460)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/75460\n\nadd checking for number of args checking observer in same graph\n\nTest Plan:\npython3 test/test_quantization.py TestQuantizeFxOps\n\nImported from OSS\n\nReviewed By: malfet\n\nDifferential Revision: D35479504\n\nfbshipit-source-id: d7dc38a27fdf8e0b236b6976d484b0701c61184c\n(cherry picked from commit 45542f796f5e6f6259f3ec647dbd2a9fa69ababc)",
    "changes": [
        {
            "name": "prepare.py",
            "path": "torch/ao/quantization/fx/prepare.py",
            "patches": [
                {
                    "old_start": 175,
                    "old_length": 7,
                    "new_start": 175,
                    "new_length": 7,
                    "hunk_buggy": "['     in a different place rather than not observed.\\n', '     \"\"\"\\n', '     node_output_dtype = get_arg_target_dtype_as_output(node, modules, node_name_to_target_dtype)\\n', '-    if isinstance(node.args[0], Node):\\n', \"         if node_output_dtype == torch.quint8 and node.args[0].op == 'placeholder':\\n\", '             return False\\n', '     return True']",
                    "hunk_fix": "@@ -175,7 +175,7 @@ def is_observer_in_same_graph(node, modules, node_name_to_target_dtype):\n     in a different place rather than not observed.\n     \"\"\"\n     node_output_dtype = get_arg_target_dtype_as_output(node, modules, node_name_to_target_dtype)\n-    if isinstance(node.args[0], Node):\n+    if len(node.args) > 0 and isinstance(node.args[0], Node):\n         if node_output_dtype == torch.quint8 and node.args[0].op == 'placeholder':\n             return False\n     return True"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 417,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/1c5a8125798392f8d7c57e88735f43a14ae0beca",
    "date": "2022-03-26T14:49:11+00:00",
    "message": "Better type checking in disable_torch_function/dispatch\n\nA follow up fix for PR #74509\nOriginal issue: #73933\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/74720\nApproved by: https://github.com/gchanan",
    "changes": [
        {
            "name": "disable_torch_function.cpp",
            "path": "torch/csrc/utils/disable_torch_function.cpp",
            "patches": [
                {
                    "old_start": 119,
                    "old_length": 10,
                    "new_start": 119,
                    "new_length": 12,
                    "hunk_buggy": "['   py::tuple py_args;\\n', '   if (args == nullptr) {\\n', '     py_args = py::make_tuple();\\n', '-  } else if (PyList_CheckExact(args)) {\\n', '     py_args = py::reinterpret_steal<py::tuple>(PyList_AsTuple(args));\\n', '-  } else {\\n', '     py_args = py::reinterpret_borrow<py::tuple>(args);\\n', '   }\\n', ' \\n', '   // These are all C-API calls so no exceptions will be raised\\n']",
                    "hunk_fix": "@@ -119,10 +119,12 @@ PyObject* THPModule_disable_torch_function(PyObject *self, PyObject *a) {\n   py::tuple py_args;\n   if (args == nullptr) {\n     py_args = py::make_tuple();\n-  } else if (PyList_CheckExact(args)) {\n+  } else if (PyList_Check(args)) {\n     py_args = py::reinterpret_steal<py::tuple>(PyList_AsTuple(args));\n-  } else {\n+  } else if (PyTuple_Check(args)) {\n     py_args = py::reinterpret_borrow<py::tuple>(args);\n+  } else {\n+    throw torch::TypeError(\"expected List or Tuple (got %s)\", Py_TYPE(args)->tp_name);\n   }\n \n   // These are all C-API calls so no exceptions will be raised\n"
                },
                {
                    "old_start": 146,
                    "old_length": 10,
                    "new_start": 148,
                    "new_length": 12,
                    "hunk_buggy": "['   py::tuple py_args;\\n', '   if (args == nullptr) {\\n', '     py_args = py::make_tuple();\\n', '-  } else if (PyList_CheckExact(args)) {\\n', '     py_args = py::reinterpret_steal<py::tuple>(PyList_AsTuple(args));\\n', '-  } else {\\n', '     py_args = py::reinterpret_borrow<py::tuple>(args);\\n', '   }\\n', ' \\n', '   // This implementation is not completely correct.  The moral']",
                    "hunk_fix": "@@ -146,10 +148,12 @@ PyObject* THPModule_disable_torch_dispatch(PyObject *self, PyObject *a) {\n   py::tuple py_args;\n   if (args == nullptr) {\n     py_args = py::make_tuple();\n-  } else if (PyList_CheckExact(args)) {\n+  } else if (PyList_Check(args)) {\n     py_args = py::reinterpret_steal<py::tuple>(PyList_AsTuple(args));\n-  } else {\n+  } else if (PyTuple_Check(args)) {\n     py_args = py::reinterpret_borrow<py::tuple>(args);\n+  } else {\n+    throw torch::TypeError(\"expected List or Tuple (got %s)\", Py_TYPE(args)->tp_name);\n   }\n \n   // This implementation is not completely correct.  The moral"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 418,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/f810d96806d0e767aeca9fe9cf50e0bdcaab7d52",
    "date": "2022-03-14T20:29:58+00:00",
    "message": "remove redundant index check for index_select_out_cpu_dim1_ (#74093)\n\nSummary:\nFor  **index_select_out_cpu_dim1_**, there has a redundant idex check, **check_indexarray_range** has checked  **the index>=0 and  index < slect_dim**, we don't need re-check it at copy step.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/74093\n\nReviewed By: ezyang\n\nDifferential Revision: D34823673\n\nPulled By: ngimel\n\nfbshipit-source-id: c723b5bd6254c36588063da0175470268bffca5d\n(cherry picked from commit 1791fa4e488cdf77f0d6850f364a74a7a92fedee)",
    "changes": [
        {
            "name": "TensorAdvancedIndexing.cpp",
            "path": "aten/src/ATen/native/TensorAdvancedIndexing.cpp",
            "patches": [
                {
                    "old_start": 880,
                    "old_length": 9,
                    "new_start": 880,
                    "new_length": 6,
                    "hunk_buggy": "[' \\n', '           for (const auto i : c10::irange(N)) {\\n', '             auto idx = idxs[i];\\n', '-            if (idx < 0) {\\n', '-              idx = idx + src_indexing_axis_dim;\\n', '-            }\\n', '             dst_floats[i] = src_floats[idx];\\n', '           }\\n', '         }\\n']",
                    "hunk_fix": "@@ -880,9 +880,6 @@ Tensor & index_select_out_cpu_dim1_(\n \n           for (const auto i : c10::irange(N)) {\n             auto idx = idxs[i];\n-            if (idx < 0) {\n-              idx = idx + src_indexing_axis_dim;\n-            }\n             dst_floats[i] = src_floats[idx];\n           }\n         }\n"
                },
                {
                    "old_start": 892,
                    "old_length": 10,
                    "new_start": 889,
                    "new_length": 6,
                    "hunk_buggy": "['         for (const auto batch : c10::irange(outer_dims_product)) {\\n', '           for (const auto i : c10::irange(N)) {\\n', '             auto idx = idxs[i];\\n', '-            if (idx < 0) {\\n', '-              idx = idx + src_indexing_axis_dim;\\n', '-            }\\n', '-\\n', '             auto src = src_base + batch * src_batch_bytesize + idx * block_bytesize;\\n', '             auto dst = out + batch * gathered_batch_bytesize + i * block_bytesize;\\n', '             memcpy(dst, src, block_bytesize);']",
                    "hunk_fix": "@@ -892,10 +889,6 @@ Tensor & index_select_out_cpu_dim1_(\n         for (const auto batch : c10::irange(outer_dims_product)) {\n           for (const auto i : c10::irange(N)) {\n             auto idx = idxs[i];\n-            if (idx < 0) {\n-              idx = idx + src_indexing_axis_dim;\n-            }\n-\n             auto src = src_base + batch * src_batch_bytesize + idx * block_bytesize;\n             auto dst = out + batch * gathered_batch_bytesize + i * block_bytesize;\n             memcpy(dst, src, block_bytesize);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 419,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/012829eb3657aff2d58cead0bd166089c6e90c7f",
    "date": "2022-03-09T16:44:56+00:00",
    "message": "[Lazy][JIT] Do not crash when target device is unsupported by fuser (#73820)\n\nSummary:\nThe `canFuseOnDevice` function now crashes when the device is not covered (i.e., CPU, GPU, XPU). However, now we have some devices, such as XLA and Lazy, that could perform fusion by themselves. This checker then prevents these devices from working on the models partially implemented in `jit.script`.\n\nThis PR proposes to remove this checker and simply return false for all uncovered cases. Another alternative is adding the following logic if it is unsafe to simply remove the checker:\n```\nelse if (device-> type() == DeviceType::XLA || device-> type() == DeviceType::Lazy) {\n  return false;\n} else {\n  TORCH_CHECK_NOT_IMPLEMENTED(false, \"Unknown device for tensorexpr fuser\")\n}\n```\n\ncc wconstab\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/73820\n\nReviewed By: navahgar\n\nDifferential Revision: D34731314\n\nPulled By: wconstab\n\nfbshipit-source-id: 1c0a90dcd6c67803a27fa2f305d78b2a539f3604\n(cherry picked from commit 94c61d6c0a9c2ef6e0d046f7f06a7158b43d4d61)",
    "changes": [
        {
            "name": "tensorexpr_fuser.cpp",
            "path": "torch/csrc/jit/passes/tensorexpr_fuser.cpp",
            "patches": [
                {
                    "old_start": 840,
                    "old_length": 9,
                    "new_start": 840,
                    "new_length": 8,
                    "hunk_buggy": "['       return canFuseOnGPU();\\n', '     } else if (device->is_xpu()) {\\n', '       return false;\\n', '-    } else {\\n', '-      TORCH_CHECK_NOT_IMPLEMENTED(false, \"Unknown device for tensorexpr fuser\")\\n', '     }\\n', '   }\\n', ' \\n', '   bool isFusableOnDevice(Node* node) {']",
                    "hunk_fix": "@@ -840,9 +840,8 @@ class TensorExprFuser {\n       return canFuseOnGPU();\n     } else if (device->is_xpu()) {\n       return false;\n-    } else {\n-      TORCH_CHECK_NOT_IMPLEMENTED(false, \"Unknown device for tensorexpr fuser\")\n     }\n+    return false;\n   }\n \n   bool isFusableOnDevice(Node* node) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 420,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/be253b8ee8a104997773d11ed28928a48193217d",
    "date": "2022-03-01T06:31:52+00:00",
    "message": "Fix overflow check in `geometry_is_contiguous` (#73162)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/73162\n\nThe existing check isn't safe for 32-bit `size_t` because the max\n64-bit int will overflow.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D34524229\n\nPulled By: malfet\n\nfbshipit-source-id: 58b1bb2fa605e9972644723ba8b37643ba024f38\n(cherry picked from commit b15aa0fbd49153bcdf61bc7676449f3ee4a5bb51)",
    "changes": [
        {
            "name": "TensorGeometry.cpp",
            "path": "aten/src/ATen/TensorGeometry.cpp",
            "patches": [
                {
                    "old_start": 7,
                    "old_length": 7,
                    "new_start": 7,
                    "new_length": 7,
                    "hunk_buggy": "[' \\n', ' // See TensorGeometry.h on why this is useful now that we cache is_contiguous.\\n', ' bool geometry_is_contiguous(IntArrayRef sizes, IntArrayRef strides) {\\n', '-  assert(sizes.size() < static_cast<std::size_t>(std::numeric_limits<std::int64_t>::max()));\\n', '   auto dim = static_cast<std::int64_t>(sizes.size());\\n', '   int64_t expected_stride = 1;\\n', '   bool contig_if_nonempty = true;']",
                    "hunk_fix": "@@ -7,7 +7,7 @@ namespace at {\n \n // See TensorGeometry.h on why this is useful now that we cache is_contiguous.\n bool geometry_is_contiguous(IntArrayRef sizes, IntArrayRef strides) {\n-  assert(sizes.size() < static_cast<std::size_t>(std::numeric_limits<std::int64_t>::max()));\n+  assert(!overflows<std::int64_t>(sizes.size()));\n   auto dim = static_cast<std::int64_t>(sizes.size());\n   int64_t expected_stride = 1;\n   bool contig_if_nonempty = true;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 421,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/cafd0f33042f5344a27ccde33b352eab676a0bdd",
    "date": "2022-02-24T19:39:32+00:00",
    "message": "[jit][edge] Fix array index checking in mobile interpreter. (#73241)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/73241\n\nStop using non-portable out-of-range indexing in mobile interpreter, also change code types indexing to use vector.at() to catch out-of-range bugs earlier.\n\nTest Plan: buck test mode/dbg mode/no-gpu -c fbcode.platform=platform010 //caffe2/test/cpp/jit:jit -- BackendTest.TestCompiler\n\nReviewed By: dhruvbird, r-barnes\n\nDifferential Revision: D34370237\n\nfbshipit-source-id: 1827f75ed00ecc10bbcece48329b0ac87189b079\n(cherry picked from commit ab943ef414c8d109bd766f672def63be28af2571)",
    "changes": [
        {
            "name": "interpreter.cpp",
            "path": "torch/csrc/jit/mobile/interpreter.cpp",
            "patches": [
                {
                    "old_start": 233,
                    "old_length": 7,
                    "new_start": 233,
                    "new_length": 7,
                    "hunk_buggy": "['           }\\n', '           return false;\\n', '         case LIST_CONSTRUCT: {\\n', '-          listConstruct(stack, *code.types_[inst.X], inst.N);\\n', '           frame.step();\\n', '         } break;\\n', '         case LIST_UNPACK: {\\n']",
                    "hunk_fix": "@@ -233,7 +233,7 @@ bool InterpreterState::run(Stack& stack) {\n           }\n           return false;\n         case LIST_CONSTRUCT: {\n-          listConstruct(stack, *code.types_[inst.X], inst.N);\n+          listConstruct(stack, *code.types_.at(inst.X), inst.N);\n           frame.step();\n         } break;\n         case LIST_UNPACK: {\n"
                },
                {
                    "old_start": 305,
                    "old_length": 21,
                    "new_start": 305,
                    "new_length": 20,
                    "hunk_buggy": "['           frame.step();\\n', '         } break;\\n', '         case DICT_CONSTRUCT: {\\n', '-          dictConstruct(stack, *code.types_[inst.X], inst.N);\\n', '           frame.step();\\n', '         } break;\\n', '         case NAMED_TUPLE_CONSTRUCT: {\\n', '-          namedTupleConstruct(stack, code.types_[inst.X], inst.N);\\n', '           frame.step();\\n', '         } break;\\n', '         case CREATE_OBJECT: {\\n', '-          auto type = code.types_[inst.X]->expect<c10::ClassType>();\\n', '           createObject(stack, type);\\n', '           frame.step();\\n', '         } break;\\n', '         case ISINSTANCE: {\\n', '-          at::ArrayRef<TypePtr> types(\\n', '-              &(code.types_[inst.X]), &(code.types_[inst.X + inst.N]));\\n', '           isinstance(stack, types);\\n', '           frame.step();\\n', '         } break;']",
                    "hunk_fix": "@@ -305,21 +305,20 @@ bool InterpreterState::run(Stack& stack) {\n           frame.step();\n         } break;\n         case DICT_CONSTRUCT: {\n-          dictConstruct(stack, *code.types_[inst.X], inst.N);\n+          dictConstruct(stack, *code.types_.at(inst.X), inst.N);\n           frame.step();\n         } break;\n         case NAMED_TUPLE_CONSTRUCT: {\n-          namedTupleConstruct(stack, code.types_[inst.X], inst.N);\n+          namedTupleConstruct(stack, code.types_.at(inst.X), inst.N);\n           frame.step();\n         } break;\n         case CREATE_OBJECT: {\n-          auto type = code.types_[inst.X]->expect<c10::ClassType>();\n+          auto type = code.types_.at(inst.X)->expect<c10::ClassType>();\n           createObject(stack, type);\n           frame.step();\n         } break;\n         case ISINSTANCE: {\n-          at::ArrayRef<TypePtr> types(\n-              &(code.types_[inst.X]), &(code.types_[inst.X + inst.N]));\n+          at::ArrayRef<TypePtr> types(&code.types_.at(inst.X), inst.N);\n           isinstance(stack, types);\n           frame.step();\n         } break;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 422,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/bdc8b3f3e828ca7202879baa379fda6df5b078d2",
    "date": "2022-02-19T01:33:51+00:00",
    "message": "[vulkan] Re-route arithmetic ops to scalar versions when second arg is zero-dim (#73108)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/73108\n\nWhen arithmetic ops are invoked from torchscript the scalar argument will sometimes be wrapped in a zero-dimensional tensor, which will cause the Vulkan implementation to complain as all input tensors are expected to have the same number of channels. The solution is to have the Tensor implementations of the op check if the second argument is zero-dimensional and re-route it to the Scalar implementation if that's the case.\n\nTest Plan: Imported from OSS\n\nReviewed By: mikaylagawarecki\n\nDifferential Revision: D34354840\n\nPulled By: SS-JIA\n\nfbshipit-source-id: b24799bb3dd4336791a39bea9382c14243ad58e4\n(cherry picked from commit c6dd8eb13b9be3800405c64a3a81e5c68da64355)",
    "changes": [
        {
            "name": "Arithmetic.cpp",
            "path": "aten/src/ATen/native/vulkan/ops/Arithmetic.cpp",
            "patches": [
                {
                    "old_start": 322,
                    "old_length": 6,
                    "new_start": 322,
                    "new_length": 13,
                    "hunk_buggy": "['     const Tensor& self_arg,\\n', '     const Tensor& other_arg,\\n', '     const Scalar& alpha) {\\n', '   return arithmetic_tensor(\\n', '       self_arg, other_arg, c10::optional<Scalar>(alpha), VK_KERNEL(add));\\n', ' }\\n']",
                    "hunk_fix": "@@ -322,6 +322,13 @@ Tensor add_tensor(\n     const Tensor& self_arg,\n     const Tensor& other_arg,\n     const Scalar& alpha) {\n+  if (other_arg.sizes().size() == 0) {\n+    return arithmetic_scalar(\n+        self_arg,\n+        other_arg.item<float>(),\n+        c10::optional<Scalar>(alpha.to<float>()),\n+        VK_KERNEL(add_scalar));\n+  }\n   return arithmetic_tensor(\n       self_arg, other_arg, c10::optional<Scalar>(alpha), VK_KERNEL(add));\n }\n"
                },
                {
                    "old_start": 354,
                    "old_length": 6,
                    "new_start": 361,
                    "new_length": 13,
                    "hunk_buggy": "['     const Tensor& self_arg,\\n', '     const Tensor& other_arg,\\n', '     const Scalar& alpha) {\\n', '   return arithmetic_tensor(\\n', '       self_arg, other_arg, c10::optional<Scalar>(alpha), VK_KERNEL(sub));\\n', ' }\\n']",
                    "hunk_fix": "@@ -354,6 +361,13 @@ Tensor sub_tensor(\n     const Tensor& self_arg,\n     const Tensor& other_arg,\n     const Scalar& alpha) {\n+  if (other_arg.sizes().size() == 0) {\n+    return arithmetic_scalar(\n+        self_arg,\n+        other_arg.item<float>(),\n+        c10::optional<Scalar>(-1 * alpha.to<float>()),\n+        VK_KERNEL(add_scalar));\n+  }\n   return arithmetic_tensor(\n       self_arg, other_arg, c10::optional<Scalar>(alpha), VK_KERNEL(sub));\n }\n"
                },
                {
                    "old_start": 374,
                    "old_length": 6,
                    "new_start": 388,
                    "new_length": 13,
                    "hunk_buggy": "[' }\\n', ' \\n', ' Tensor mul_tensor(const Tensor& self_arg, const Tensor& other_arg) {\\n', '   return arithmetic_tensor(\\n', '       self_arg, other_arg, c10::optional<Scalar>(), VK_KERNEL(mul));\\n', ' }\\n']",
                    "hunk_fix": "@@ -374,6 +388,13 @@ Tensor& mul_scalar_(Tensor& self, const Scalar& other) {\n }\n \n Tensor mul_tensor(const Tensor& self_arg, const Tensor& other_arg) {\n+  if (other_arg.sizes().size() == 0) {\n+    return arithmetic_scalar(\n+        self_arg,\n+        other_arg.item<float>(),\n+        c10::optional<Scalar>(),\n+        VK_KERNEL(mul_scalar));\n+  }\n   return arithmetic_tensor(\n       self_arg, other_arg, c10::optional<Scalar>(), VK_KERNEL(mul));\n }\n"
                },
                {
                    "old_start": 400,
                    "old_length": 6,
                    "new_start": 421,
                    "new_length": 13,
                    "hunk_buggy": "[' }\\n', ' \\n', ' Tensor div_tensor(const Tensor& self_arg, const Tensor& other_arg) {\\n', '   return arithmetic_tensor(\\n', '       self_arg, other_arg, c10::optional<Scalar>(), VK_KERNEL(div));\\n', ' }']",
                    "hunk_fix": "@@ -400,6 +421,13 @@ Tensor& div_scalar_(Tensor& self, const Scalar& other) {\n }\n \n Tensor div_tensor(const Tensor& self_arg, const Tensor& other_arg) {\n+  if (other_arg.sizes().size() == 0) {\n+    return arithmetic_scalar(\n+        self_arg,\n+        1.0 / other_arg.item<float>(),\n+        c10::optional<Scalar>(),\n+        VK_KERNEL(mul_scalar));\n+  }\n   return arithmetic_tensor(\n       self_arg, other_arg, c10::optional<Scalar>(), VK_KERNEL(div));\n }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 423,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/41ad221751e57c2d2ccc82b431f56d6ed62e1741",
    "date": "2022-02-16T18:36:21+00:00",
    "message": "[PyTorch] MHA: fix contiguity assumption in transform_bias_rescale_qkv (#72465)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/72465\n\nThis code path incorrectly assumed input tensors were contiguous. Now we check that.\nghstack-source-id: 149201476\n\nTest Plan: CI\n\nReviewed By: ngimel\n\nDifferential Revision: D34007665\n\nfbshipit-source-id: c43438f2495e32304ea3f7846e01eceb4a9448f7\n(cherry picked from commit 0767b225f23846c1636ac3622f46b0c5ec071d96)",
    "changes": [
        {
            "name": "attention.cpp",
            "path": "aten/src/ATen/native/attention.cpp",
            "patches": [
                {
                    "old_start": 113,
                    "old_length": 15,
                    "new_start": 113,
                    "new_length": 18,
                    "hunk_buggy": "['   TORCH_CHECK(_3D % 3 == 0);\\n', '   const auto dim_per_head = D / num_head;\\n', '   auto q_k_v = at::empty({3, B, num_head, T, dim_per_head}, qkv.options());\\n', ' \\n', '-  AT_DISPATCH_FLOATING_TYPES_AND2(\\n', '       ScalarType::Half,\\n', '       ScalarType::BFloat16,\\n', '       qkv.scalar_type(),\\n', '       \"transform_bias_rescale_qkv\",\\n', '       [&] {\\n', '-        scalar_t* qkv_data = qkv.data_ptr<scalar_t>();\\n', '-        scalar_t* qkv_bias_data = qkv_bias.data_ptr<scalar_t>();\\n', '         scalar_t* q_k_v_data = q_k_v.data_ptr<scalar_t>();\\n', '         const scalar_t sqrt_dim_per_head = std::sqrt(static_cast<scalar_t>(dim_per_head));\\n', ' \\n']",
                    "hunk_fix": "@@ -113,15 +113,18 @@ std::tuple<Tensor, Tensor, Tensor> transform_bias_rescale_qkv(\n   TORCH_CHECK(_3D % 3 == 0);\n   const auto dim_per_head = D / num_head;\n   auto q_k_v = at::empty({3, B, num_head, T, dim_per_head}, qkv.options());\n+  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(q_k_v.is_contiguous());\n \n-  AT_DISPATCH_FLOATING_TYPES_AND2(\n+  const auto qkv_contig = qkv.expect_contiguous();\n+  const auto qkv_bias_contig = qkv_bias.expect_contiguous();\n+ AT_DISPATCH_FLOATING_TYPES_AND2(\n       ScalarType::Half,\n       ScalarType::BFloat16,\n       qkv.scalar_type(),\n       \"transform_bias_rescale_qkv\",\n       [&] {\n-        scalar_t* qkv_data = qkv.data_ptr<scalar_t>();\n-        scalar_t* qkv_bias_data = qkv_bias.data_ptr<scalar_t>();\n+        scalar_t* qkv_data = qkv_contig->data_ptr<scalar_t>();\n+        scalar_t* qkv_bias_data = qkv_bias_contig->data_ptr<scalar_t>();\n         scalar_t* q_k_v_data = q_k_v.data_ptr<scalar_t>();\n         const scalar_t sqrt_dim_per_head = std::sqrt(static_cast<scalar_t>(dim_per_head));\n \n"
                },
                {
                    "old_start": 134,
                    "old_length": 6,
                    "new_start": 137,
                    "new_length": 7,
                    "hunk_buggy": "['       });\\n', '   auto q_k_v_s =\\n', '       at::native::split(q_k_v.view({3 * B, num_head, T, dim_per_head}), B, 0);\\n', '   return std::make_tuple(q_k_v_s[0], q_k_v_s[1], q_k_v_s[2]);\\n', ' }\\n', ' ']",
                    "hunk_fix": "@@ -134,6 +137,7 @@ std::tuple<Tensor, Tensor, Tensor> transform_bias_rescale_qkv(\n       });\n   auto q_k_v_s =\n       at::native::split(q_k_v.view({3 * B, num_head, T, dim_per_head}), B, 0);\n+  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(q_k_v_s.size() == 3);\n   return std::make_tuple(q_k_v_s[0], q_k_v_s[1], q_k_v_s[2]);\n }\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 424,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/678c08bb55eef0c2e707a17d0cd6e50f5b9bd427",
    "date": "2022-02-11T15:59:13+00:00",
    "message": "[PG Wrapper] Small fix (#72657)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/72657\n\n_ProcessGroupWrapper check needs to be gated on Gloo availability,\nthis fails when gloo is not avail_ProcessGroupWrapper check needs to be gated\non Gloo availability, this fails when gloo is not avail.\nghstack-source-id: 148837056\n\nTest Plan: CI\n\nReviewed By: zhaojuanmao\n\nDifferential Revision: D34144848\n\nfbshipit-source-id: 42a04918b968247f3259cd2cde5438e1265b04fe\n(cherry picked from commit ba5de989396cb621dffc6ef424938df494eb1be6)",
    "changes": [
        {
            "name": "distributed_c10d.py",
            "path": "torch/distributed/distributed_c10d.py",
            "patches": [
                {
                    "old_start": 1566,
                    "old_length": 10,
                    "new_start": 1566,
                    "new_length": 12,
                    "hunk_buggy": "[' \\n', ' def _check_for_nccl_backend(group):\\n', '     pg = group or _get_default_group()\\n', '-    # It is not expected for PG to be wrapped many times, but support it just\\n', '-    # in case\\n', '-    while isinstance(pg, _ProcessGroupWrapper):\\n', '-        pg = pg.wrapped_pg\\n', ' \\n', '     return (\\n', '         is_nccl_available() and']",
                    "hunk_fix": "@@ -1566,10 +1566,12 @@ def _tensor_to_object(tensor, tensor_size):\n \n def _check_for_nccl_backend(group):\n     pg = group or _get_default_group()\n-    # It is not expected for PG to be wrapped many times, but support it just\n-    # in case\n-    while isinstance(pg, _ProcessGroupWrapper):\n-        pg = pg.wrapped_pg\n+    # Gate PG wrapper check on Gloo availability.\n+    if _GLOO_AVAILABLE:\n+        # It is not expected for PG to be wrapped many times, but support it just\n+        # in case\n+        while isinstance(pg, _ProcessGroupWrapper):\n+            pg = pg.wrapped_pg\n \n     return (\n         is_nccl_available() and"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 425,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/5b7c72101ca8e9d4edba1d16b6121ad900ca3936",
    "date": "2022-02-03T15:36:15+00:00",
    "message": "[Quant][devs] Removed check for is_quantized in dequantize_cpu_or_cuda (#71958)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71958\n\nThis PR is part of a series of PRs addressing https://github.com/pytorch/pytorch/issues/54150,\nrelated to using dispatcher for calls to quantized backends as opposed to if/else conditionals.\nThis particular PR isn't dispatcher related but does remove the extraneous torch check for a quant tensor\nsince the dispatcher already handles a quantized backend for this particular function\n\nDifferential Revision:\nD33833765\nD33833765\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nPulled By: dzdang\n\nfbshipit-source-id: c3bb531a5c09326bdf724b5185a19ea0a379bba7\n(cherry picked from commit f053b8248f895446f6a9d352de4038df6c6d4b2d)",
    "changes": [
        {
            "name": "QTensor.cpp",
            "path": "aten/src/ATen/native/quantized/QTensor.cpp",
            "patches": [
                {
                    "old_start": 94,
                    "old_length": 7,
                    "new_start": 94,
                    "new_length": 6,
                    "hunk_buggy": "[' }\\n', ' \\n', ' Tensor dequantize_cpu_or_cuda(const Tensor& self) {\\n', '-  TORCH_CHECK(!self.is_quantized());\\n', '   return self.to(at::kFloat);\\n', ' }\\n', ' ']",
                    "hunk_fix": "@@ -94,7 +94,6 @@ Tensor quantize_per_channel(\n }\n \n Tensor dequantize_cpu_or_cuda(const Tensor& self) {\n-  TORCH_CHECK(!self.is_quantized());\n   return self.to(at::kFloat);\n }\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 426,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/d23231fd8cd50e4eb657eb7c3cf102475634f9c6",
    "date": "2022-02-03T00:41:03+00:00",
    "message": "Fix upgrader codegen when constant list is 0 (#72199)\n\nSummary:\nWhen the constant list is empty, previous codegen will generate something like\n```\nstd::vector<c10::IValue>({\n\n}), // constants list,\n```\nHowever it will fail quick-check, because it includes trailing spaces. This pr will generate the following instead.\n```\nstd::vector<c10::IValue>(), // constants list,\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/72199\n\nghstack-source-id: 148231023\n\nTest Plan: CI\n\nReviewed By: tugsbayasgalan\n\nDifferential Revision: D33952046\n\nfbshipit-source-id: 359b8a418928c89bbeb446b44774b312c94f03bc\n(cherry picked from commit 060490f66724e418a43548c2eaffa3244e780557)",
    "changes": [
        {
            "name": "gen_mobile_upgraders.py",
            "path": "tools/codegen/operator_versions/gen_mobile_upgraders.py",
            "patches": [
                {
                    "old_start": 38,
                    "old_length": 6,
                    "new_start": 38,
                    "new_length": 8,
                    "hunk_buggy": "['         ${constant_list}\\n', '     }), // constants list\"\"\")\\n', ' \\n', ' ONE_TYPE = CodeTemplate(\"\"\"c10::parseType(\"${type_str}\"),\"\"\")\\n', ' \\n', ' TYPE_LIST = CodeTemplate(\"\"\"std::vector<c10::TypePtr>({\\n']",
                    "hunk_fix": "@@ -38,6 +38,8 @@ CONSTANT_LIST = CodeTemplate(\"\"\"std::vector<c10::IValue>({\n         ${constant_list}\n     }), // constants list\"\"\")\n \n+CONSTANTS_LIST_EMPTY = \"\"\"std::vector<c10::IValue>(), // constants list\"\"\"\n+\n ONE_TYPE = CodeTemplate(\"\"\"c10::parseType(\"${type_str}\"),\"\"\")\n \n TYPE_LIST = CodeTemplate(\"\"\"std::vector<c10::TypePtr>({\n"
                },
                {
                    "old_start": 181,
                    "old_length": 6,
                    "new_start": 183,
                    "new_length": 8,
                    "hunk_buggy": "['                 constant=convert_constant\\n', '             )\\n', '         )\\n', '     return CONSTANT_LIST.substitute(constant_list=\"\".join(constants_list_part).lstrip(\"\\\\n\"))\\n', ' \\n', ' def construct_operators(operator_list_from_yaml: List[Any]) -> str:']",
                    "hunk_fix": "@@ -181,6 +183,8 @@ def construct_constants(constants_list_from_yaml: List[Any]) -> str:\n                 constant=convert_constant\n             )\n         )\n+    if len(constants_list_part) == 0:\n+        return CONSTANTS_LIST_EMPTY\n     return CONSTANT_LIST.substitute(constant_list=\"\".join(constants_list_part).lstrip(\"\\n\"))\n \n def construct_operators(operator_list_from_yaml: List[Any]) -> str:"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 427,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/871e240e6367f94966a3e2f9deefbfa98e314d6d",
    "date": "2022-01-31T20:50:42+00:00",
    "message": "Improved error message for interpolation (#72066)\n\nSummary:\nDescription:\n- Improved error message for CUDA interpolation with antialiasing\n\njbschlosser could you please check this PR and the wording if the error message is more clear now ? Thank.\nI'm skipping all the tests now and once we are agreed on the wording if any updates are required, I update and restart the tests to ensure nothing is broken.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/72066\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D33892729\n\nPulled By: jbschlosser\n\nfbshipit-source-id: 6249c7a1c51aa2e242f4bb8bfbe3f2abab17a8e8\n(cherry picked from commit 44eb5391cf4fed54b379e96dfa9f23ef6ab1ecfa)",
    "changes": [
        {
            "name": "UpSampleBilinear2d.cu",
            "path": "aten/src/ATen/native/cuda/UpSampleBilinear2d.cu",
            "patches": [
                {
                    "old_start": 727,
                    "old_length": 7,
                    "new_start": 727,
                    "new_length": 9,
                    "hunk_buggy": "['         size_t shmem_size = weights_per_block * sizeof(scalar_t);\\n', '         TORCH_CHECK(\\n', '             shmem_size <= sharedMemPerBlock,\\n', '-            \"Too much shared memory required: \", shmem_size, \" vs \", sharedMemPerBlock);\\n', ' \\n', '         upsample_gen2d_aa_out_frame<scalar_t, accscalar_t>\\n', '             <<<grid,\\n']",
                    "hunk_fix": "@@ -727,7 +727,9 @@ static void upsample_gen2d_aa_out_cuda_template(\n         size_t shmem_size = weights_per_block * sizeof(scalar_t);\n         TORCH_CHECK(\n             shmem_size <= sharedMemPerBlock,\n-            \"Too much shared memory required: \", shmem_size, \" vs \", sharedMemPerBlock);\n+            \"Provided interpolation parameters can not be handled with current algorithm implementation. \",\n+            \"Please reduce the scale factor. Too much shared memory required: \",\n+            shmem_size, \" vs \", sharedMemPerBlock);\n \n         upsample_gen2d_aa_out_frame<scalar_t, accscalar_t>\n             <<<grid,\n"
                },
                {
                    "old_start": 809,
                    "old_length": 7,
                    "new_start": 811,
                    "new_length": 9,
                    "hunk_buggy": "['         size_t sharedMemPerBlock = at::cuda::getCurrentDeviceProperties()->sharedMemPerBlock;\\n', '         TORCH_CHECK(\\n', '             shmem_size <= sharedMemPerBlock,\\n', '-            \"Too much shared memory required: \", shmem_size, \" vs \", sharedMemPerBlock);\\n', ' \\n', '         upsample_gen2d_aa_backward_out_frame<scalar_t, accscalar_t>\\n', '             <<<grid,']",
                    "hunk_fix": "@@ -809,7 +811,9 @@ static void upsample_gen2d_aa_backward_out_cuda_template(\n         size_t sharedMemPerBlock = at::cuda::getCurrentDeviceProperties()->sharedMemPerBlock;\n         TORCH_CHECK(\n             shmem_size <= sharedMemPerBlock,\n-            \"Too much shared memory required: \", shmem_size, \" vs \", sharedMemPerBlock);\n+            \"Provided interpolation parameters can not be handled with current algorithm implementation. \",\n+            \"Please reduce the scale factor. Too much shared memory required: \",\n+            shmem_size, \" vs \", sharedMemPerBlock);\n \n         upsample_gen2d_aa_backward_out_frame<scalar_t, accscalar_t>\n             <<<grid,"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 428,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/027c0d7f8e37e583c02b372df5331d73793c06b1",
    "date": "2022-01-27T02:28:19+00:00",
    "message": "fixed compilations on xla tensor print (#71147)\n\nSummary:\nFixes multiple compilation on xla tensor print. Please check the conversation here: https://github.com/pytorch/xla/pull/3253\n\nThis is done to avoid compilations during tensor printing. Torch performs some tensor operations like slicing to make the tensor readable. These operations result in compilations. Hence to avoid the compilations, copying the tensor to cpu before printing.\n\nexample:\n\n```\ndev = xm.xla_device()\ndef test_linear(input_shape=(8, 1024)):\n    import pdb\n    pdb.set_trace()\n    linear = torch.nn.Linear(in_features=1024, out_features=4096, bias=True).to(dev)\n    inp = torch.randn(*input_shape).to(dev)\n    output = linear(inp)\n    xm.mark_step()\n    return output\n```\nReturning from this function would have resulted in 63 compiles, since PDB prints the value of the return output. In this case it is a xla tensor.\n\nNow with the current change, there is no compilation.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71147\n\nReviewed By: shunting314\n\nDifferential Revision: D33795177\n\nPulled By: wconstab\n\nfbshipit-source-id: 74b53d9a1cb7ef67f9d8b0a32064f3896be449b5\n(cherry picked from commit a9e0687fc5c9981fb55ea4dc406c283c80fa20c9)",
    "changes": [
        {
            "name": "_tensor_str.py",
            "path": "torch/_tensor_str.py",
            "patches": [
                {
                    "old_start": 318,
                    "old_length": 6,
                    "new_start": 318,
                    "new_length": 12,
                    "hunk_buggy": "[\"             or (self.device.type == 'cuda' and torch.cuda.current_device() != self.device.index):\\n\", \"         suffixes.append('device=\\\\'' + str(self.device) + '\\\\'')\\n\", ' \\n', '     # TODO: add an API to map real -> complex dtypes\\n', '     _default_complex_dtype = torch.cdouble if torch.get_default_dtype() == torch.double else torch.cfloat\\n', '     has_default_dtype = self.dtype in (torch.get_default_dtype(), _default_complex_dtype, torch.int64, torch.bool)']",
                    "hunk_fix": "@@ -318,6 +318,12 @@ def _str_intern(inp):\n             or (self.device.type == 'cuda' and torch.cuda.current_device() != self.device.index):\n         suffixes.append('device=\\'' + str(self.device) + '\\'')\n \n+    # Tensor printing performs tensor operations like slice, indexing, etc to make it in a\n+    # representable format. These operations on xla/lazy tensor results in compilations. Hence,\n+    # to avoid compilations, copying the tensor to cpu before printing.\n+    if self.device.type == 'xla' or self.device.type == 'lazy':\n+        self = self.to('cpu')\n+\n     # TODO: add an API to map real -> complex dtypes\n     _default_complex_dtype = torch.cdouble if torch.get_default_dtype() == torch.double else torch.cfloat\n     has_default_dtype = self.dtype in (torch.get_default_dtype(), _default_complex_dtype, torch.int64, torch.bool)"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 429,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/666ff0ae220e1a5c406b0bc5cd43283e1b18b38e",
    "date": "2022-01-26T22:29:33+00:00",
    "message": "Update _create_c10d_store to check port value (#71863)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71863\n\nPort number is int in python, but needs to be uint16_t when called for TCPStore constructor.\n\nRelated to #67172\n\nTest Plan: Imported from OSS\n\nReviewed By: cbalioglu\n\nDifferential Revision: D33793270\n\nPulled By: H-Huang\n\nfbshipit-source-id: 89ab47ec8bd7518f9ecbf7d01871fe059b0e77b1\n(cherry picked from commit 84bff1f5bb11029ff3fcf7a04faa3b9c7b25286a)",
    "changes": [
        {
            "name": "rendezvous.py",
            "path": "torch/distributed/rendezvous.py",
            "patches": [
                {
                    "old_start": 147,
                    "old_length": 6,
                    "new_start": 147,
                    "new_length": 9,
                    "hunk_buggy": "['     and port are correctly passed via ``hostname`` and ``port``. All\\n', '     non-zero ranks will create and return a TCPStore client.\\n', '     \"\"\"\\n', ' \\n', '     if _torchelastic_use_agent_store():\\n', '         attempt = os.environ[\"TORCHELASTIC_RESTART_COUNT\"]']",
                    "hunk_fix": "@@ -147,6 +147,9 @@ def _create_c10d_store(hostname, port, rank, world_size, timeout) -> Store:\n     and port are correctly passed via ``hostname`` and ``port``. All\n     non-zero ranks will create and return a TCPStore client.\n     \"\"\"\n+    # check if port is uint16_t\n+    if not 0 <= port < 2**16:\n+        raise ValueError(f\"port must have value from 0 to 65535 but was {port}.\")\n \n     if _torchelastic_use_agent_store():\n         attempt = os.environ[\"TORCHELASTIC_RESTART_COUNT\"]"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 430,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/442d7d72def17dba46f0b95c55c6a028428be0bc",
    "date": "2021-11-09T11:42:34-08:00",
    "message": "fixed type checking errors in options.py (#68056)\n\nSummary:\nFixes [issue#64](https://github.com/MLH-Fellowship/pyre-check/issues/64)\nThis PR fixes the type checking errors in torch/distributed/rpc/options.py.\nThe variable types in 84:8 and 85:8 were  declared to have type `List`  but were sometimes assigned a value of  `None`. This caused an incompatitble variable type error. Therefore, I changed the type from `List` to `Optional[List]` . Hence, this fixes the incompatitble variable type error.\n\nSigned-off-by: Onyemowo  Agbo\nonionymous\n0xedward\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68056\n\nReviewed By: zou3519\n\nDifferential Revision: D32282289\n\nPulled By: mrshenli\n\nfbshipit-source-id: ee410165e623834b4f5f3da8d44bd5a29306daae",
    "changes": [
        {
            "name": "options.py",
            "path": "torch/distributed/rpc/options.py",
            "patches": [
                {
                    "old_start": 1,
                    "old_length": 9,
                    "new_start": 1,
                    "new_length": 8,
                    "hunk_buggy": "['-from torch._C._distributed_rpc import _TensorPipeRpcBackendOptionsBase\\n', '-from . import constants as rpc_contants\\n', ' \\n', ' import torch\\n', '-\\n', '-from typing import Dict, List, Optional, Union\\n', ' \\n', ' \\n', ' DeviceType = Union[int, str, torch.device]\\n']",
                    "hunk_fix": "@@ -1,9 +1,8 @@\n-from torch._C._distributed_rpc import _TensorPipeRpcBackendOptionsBase\n-from . import constants as rpc_contants\n+from typing import Dict, List, Optional, Union\n \n import torch\n-\n-from typing import Dict, List, Optional, Union\n+from torch._C._distributed_rpc import _TensorPipeRpcBackendOptionsBase\n+from . import constants as rpc_contants\n \n \n DeviceType = Union[int, str, torch.device]\n"
                },
                {
                    "old_start": 19,
                    "old_length": 9,
                    "new_start": 18,
                    "new_length": 11,
                    "hunk_buggy": "['     return device\\n', ' \\n', ' \\n', '-def _to_device_map(device_map: Dict[DeviceType, DeviceType]) -> Dict[torch.device, torch.device]:\\n', '-    full_device_map : Dict[torch.device, torch.device] = {}\\n', '-    reverse_map : Dict[torch.device, torch.device] = {}\\n', '     for k in device_map:\\n', '         v = device_map[k]\\n', '         k, v = torch.device(k), torch.device(v)\\n']",
                    "hunk_fix": "@@ -19,9 +18,11 @@ def _to_device(device: DeviceType) -> torch.device:\n     return device\n \n \n-def _to_device_map(device_map: Dict[DeviceType, DeviceType]) -> Dict[torch.device, torch.device]:\n-    full_device_map : Dict[torch.device, torch.device] = {}\n-    reverse_map : Dict[torch.device, torch.device] = {}\n+def _to_device_map(\n+    device_map: Dict[DeviceType, DeviceType]\n+) -> Dict[torch.device, torch.device]:\n+    full_device_map: Dict[torch.device, torch.device] = {}\n+    reverse_map: Dict[torch.device, torch.device] = {}\n     for k in device_map:\n         v = device_map[k]\n         k, v = torch.device(k), torch.device(v)\n"
                },
                {
                    "old_start": 81,
                    "old_length": 17,
                    "new_start": 82,
                    "new_length": 15,
                    "hunk_buggy": "['         init_method: str = rpc_contants.DEFAULT_INIT_METHOD,\\n', '         device_maps: Optional[Dict[str, Dict[DeviceType, DeviceType]]] = None,\\n', '         devices: Optional[List[DeviceType]] = None,\\n', '-        _transports: List = None,\\n', '-        _channels: List = None,\\n', '     ):\\n', '         full_device_maps = (\\n', '-            {} if device_maps is None else\\n', '-            {k : _to_device_map(v) for k, v in device_maps.items()}\\n', '-        )\\n', '-        full_device_list = (\\n', '-            [] if devices is None else\\n', '-            _to_device_list(devices)\\n', '         )\\n', '         super().__init__(\\n', '             num_worker_threads,\\n', '             _transports,']",
                    "hunk_fix": "@@ -81,17 +82,15 @@ class TensorPipeRpcBackendOptions(_TensorPipeRpcBackendOptionsBase):\n         init_method: str = rpc_contants.DEFAULT_INIT_METHOD,\n         device_maps: Optional[Dict[str, Dict[DeviceType, DeviceType]]] = None,\n         devices: Optional[List[DeviceType]] = None,\n-        _transports: List = None,\n-        _channels: List = None,\n+        _transports: Optional[List] = None,\n+        _channels: Optional[List] = None,\n     ):\n         full_device_maps = (\n-            {} if device_maps is None else\n-            {k : _to_device_map(v) for k, v in device_maps.items()}\n-        )\n-        full_device_list = (\n-            [] if devices is None else\n-            _to_device_list(devices)\n+            {}\n+            if device_maps is None\n+            else {k: _to_device_map(v) for k, v in device_maps.items()}\n         )\n+        full_device_list = [] if devices is None else _to_device_list(devices)\n         super().__init__(\n             num_worker_threads,\n             _transports,"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 431,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/152f665dee05377f7459d985d60dc1edb782d40e",
    "date": "2021-11-01T16:53:54-07:00",
    "message": "Inserted check for PyObject_IsInstance in THPVariableCheck (#67588)\n\nSummary:\nInserted check for the return of PyObject_IsInstance to capture the case in which it raises an exception and return -1. When this happen THPVariable_Check now throws a python_error to signal the exception.\n\nFixes https://github.com/pytorch/pytorch/issues/65084\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67588\n\nReviewed By: mruberry\n\nDifferential Revision: D32064776\n\nPulled By: albanD\n\nfbshipit-source-id: 895c7682e0991ca257e27f9638a7462d83707320",
    "changes": [
        {
            "name": "python_variable.h",
            "path": "torch/csrc/autograd/python_variable.h",
            "patches": [
                {
                    "old_start": 6,
                    "old_length": 6,
                    "new_start": 6,
                    "new_length": 7,
                    "hunk_buggy": "[' \\n', ' #include <torch/csrc/autograd/variable.h>\\n', ' #include <torch/csrc/THP_export.h>\\n', ' \\n', ' // Python object that backs torch.autograd.Variable\\n', ' // NOLINTNEXTLINE(cppcoreguidelines-pro-type-member-init)\\n']",
                    "hunk_fix": "@@ -6,6 +6,7 @@\n \n #include <torch/csrc/autograd/variable.h>\n #include <torch/csrc/THP_export.h>\n+#include <torch/csrc/Exceptions.h>\n \n // Python object that backs torch.autograd.Variable\n // NOLINTNEXTLINE(cppcoreguidelines-pro-type-member-init)\n"
                },
                {
                    "old_start": 43,
                    "old_length": 7,
                    "new_start": 44,
                    "new_length": 13,
                    "hunk_buggy": "[' \\n', ' inline bool THPVariable_Check(PyObject *obj)\\n', ' {\\n', '-  return THPVariableClass && PyObject_IsInstance(obj, THPVariableClass);\\n', ' }\\n', ' \\n', ' inline const at::Tensor& THPVariable_Unpack(THPVariable* var) {']",
                    "hunk_fix": "@@ -43,7 +44,13 @@ static inline bool THPVariable_CheckExact(PyObject *obj) {\n \n inline bool THPVariable_Check(PyObject *obj)\n {\n-  return THPVariableClass && PyObject_IsInstance(obj, THPVariableClass);\n+  if (!THPVariableClass)\n+      return false;\n+\n+  const auto result = PyObject_IsInstance(obj, THPVariableClass);\n+  if (result == -1)\n+      throw python_error();\n+  return result;\n }\n \n inline const at::Tensor& THPVariable_Unpack(THPVariable* var) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 432,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/45d5b3248b430aca70111316accd165954464589",
    "date": "2021-11-01T14:45:33-07:00",
    "message": "Fixed C++ BatchNorm pretty_print() with optional momentum (#67335)\n\nSummary:\nSummary : Inserted a check for the momentum and print  \"None\" in case is not defined. See  https://github.com/pytorch/pytorch/issues/65143\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67335\n\nTest Plan:\nThe code below now prints `torch::nn::BatchNorm2d(128, eps=1e-05, momentum=None, affine=true, track_running_stats=true)` without generating errors.\n```\ntorch::nn::BatchNorm2d m(torch::nn::BatchNormOptions(128).momentum(c10::nullopt));\nstd::cerr << *m << \"\\n\";\n```\nFixes https://github.com/pytorch/pytorch/issues/65143\n\nReviewed By: mruberry\n\nDifferential Revision: D32067820\n\nPulled By: ngimel\n\nfbshipit-source-id: f40f9bbe090aa78e00f6c3a57deae393d946b88d",
    "changes": [
        {
            "name": "batchnorm.cpp",
            "path": "torch/csrc/api/src/nn/modules/batchnorm.cpp",
            "patches": [
                {
                    "old_start": 22,
                    "old_length": 7,
                    "new_start": 22,
                    "new_length": 15,
                    "hunk_buggy": "['          << \"torch::nn::BatchNorm\" << D << \"d(\"\\n', '          << this->options.num_features() << \", \"\\n', '          << \"eps=\" << this->options.eps() << \", \"\\n', '-         << \"momentum=\" << this->options.momentum().value() << \", \"\\n', '          << \"affine=\" << this->options.affine() << \", \"\\n', '          << \"track_running_stats=\" << this->options.track_running_stats() << \")\";\\n', ' }']",
                    "hunk_fix": "@@ -22,7 +22,15 @@ void BatchNormImplBase<D, Derived>::pretty_print(std::ostream& stream) const {\n          << \"torch::nn::BatchNorm\" << D << \"d(\"\n          << this->options.num_features() << \", \"\n          << \"eps=\" << this->options.eps() << \", \"\n-         << \"momentum=\" << this->options.momentum().value() << \", \"\n+         << \"momentum=\";\n+\n+  if (this->options.momentum().has_value()) {\n+      stream << this->options.momentum().value();\n+  } else {\n+      stream << \"None\";\n+  }\n+\n+   stream << \", \"\n          << \"affine=\" << this->options.affine() << \", \"\n          << \"track_running_stats=\" << this->options.track_running_stats() << \")\";\n }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 433,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/df475aa1dc4310abc273cf26b14b6ac1cdb7dfa4",
    "date": "2021-10-05T07:59:56-07:00",
    "message": "Update Vulkan runner in benchmark binary to handle non-tensor inputs (#66123)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66123\n\nSome models may take in a list of tensors as inputs, thus the bundled inputs will contain `IValues` that are of the type `c10::List`. For Vulkan models, every tensor in the `IValue` list has to be converted to a vulkan tensor first, and this case is not currently handled by the Vulkan model wrapper in the benchmark binary.\n\nThis diff introduces `IValue` type checking to the input processor of the Vulkan model wrapper, and adds support for Tensor and List types.\n\nTest Plan:\n```\n# Build the binary\ncd ~/fbsource\nbuck build -c ndk.custom_libcxx=false -c pt.enable_qpl=0 //xplat/caffe2:ptmobile_compareAndroid\\#android-arm64 --show-output\n# Push it to the device\nadb push buck-out/gen/xplat/caffe2/ptmobile_compareAndroid\\#android-arm64 /data/local/tmp/compare_models\n\n# Run the benchmark binary\nBENCH_CMD=\"/data/local/tmp/compare_models\"\nBENCH_CMD+=\" --model=$PATH_TO_MODEL\"\nBENCH_CMD+=\" --refmodel=$PATH_TO_REFERENCE_MODEL\"\nBENCH_CMD+=\" --input_type=float --input_dims=$MODEL_INPUT_SIZE\"\nBENCH_CMD+=\" --iter=100\"\nBENCH_CMD+=\" --tolerance 1e-5\"\n```\n\nReviewed By: beback4u\n\nDifferential Revision: D31276862\n\nfbshipit-source-id: 1d9abf958963da6ecad641202f0458402bee5ced",
    "changes": [
        {
            "name": "speed_benchmark_torch.cc",
            "path": "binaries/speed_benchmark_torch.cc",
            "patches": [
                {
                    "old_start": 184,
                    "old_length": 7,
                    "new_start": 184,
                    "new_length": 27,
                    "hunk_buggy": "['     inputs_.clear();\\n', '     inputs_.reserve(inputs.size());\\n', '     for (const auto& input : inputs) {\\n', '-      inputs_.emplace_back(input.toTensor().vulkan());\\n', '     }\\n', ' \\n', '     // Run, and download the output tensor to system memory.']",
                    "hunk_fix": "@@ -184,7 +184,27 @@ class vkRunner final : public Runner<T> {\n     inputs_.clear();\n     inputs_.reserve(inputs.size());\n     for (const auto& input : inputs) {\n-      inputs_.emplace_back(input.toTensor().vulkan());\n+      if (input.isTensor()) {\n+        inputs_.emplace_back(input.toTensor().vulkan());\n+      }\n+      else if (input.isList()) {\n+        const c10::List<c10::IValue> input_as_list = input.toList();\n+        c10::List<at::Tensor> input_vk_list;\n+        input_vk_list.reserve(input_as_list.size());\n+        for (int i=0; i < input_as_list.size(); ++i) {\n+          const c10::IValue element = input_as_list.get(i);\n+          if (element.isTensor()) {\n+            input_vk_list.emplace_back(element.toTensor().vulkan());\n+          }\n+          else {\n+            CAFFE_THROW(\"Input of type c10::List must only contain Tensors!\");\n+          }\n+        }\n+        inputs_.emplace_back(c10::IValue(input_vk_list));\n+      }\n+      else {\n+        CAFFE_THROW(\"Inputs must only contain IValues of type c10::Tensor or c10::List!\");\n+      }\n     }\n \n     // Run, and download the output tensor to system memory."
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 434,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/c99277e177cf16736262251c7e92ea5e9ba2c5c2",
    "date": "2021-09-13T14:24:16-07:00",
    "message": "handle the case in acc_ops.sum when dim == 0, differentiating it from the case when dim is None (#64869)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/64869\n\nhandle the case in acc_ops.sum when dim == 0, differentiating it from the case when dim is None\n\nReviewed By: 842974287\n\nDifferential Revision: D30872739\n\nfbshipit-source-id: 2755d3230804a16ef1c9289f804138c6dd7766b3",
    "changes": [
        {
            "name": "acc_ops.py",
            "path": "torch/fx/experimental/fx_acc/acc_ops.py",
            "patches": [
                {
                    "old_start": 567,
                    "old_length": 7,
                    "new_start": 567,
                    "new_length": 7,
                    "hunk_buggy": "[' \\n', ' @register_acc_op\\n', ' def sum(*, input, dim=None, keepdim=False, dtype=None):\\n', '-    if dim:\\n', '         return torch.sum(**locals())\\n', '     else:\\n', '         return input.sum(dtype=dtype)']",
                    "hunk_fix": "@@ -567,7 +567,7 @@ def add_sum_mapper(node: torch.fx.Node, mod: torch.fx.GraphModule) -> torch.fx.N\n \n @register_acc_op\n def sum(*, input, dim=None, keepdim=False, dtype=None):\n-    if dim:\n+    if dim is not None:\n         return torch.sum(**locals())\n     else:\n         return input.sum(dtype=dtype)"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 435,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/c06dfd7c26102ac2436ca25609c92fa794e972ca",
    "date": "2021-08-25T10:25:34-07:00",
    "message": "[fx2trt] Check input device in TRTModule (#63893)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63893\n\nAdd a check to ensure all the inputs are on cuda device.\n\nTest Plan: CI\n\nReviewed By: kflu, houseroad\n\nDifferential Revision: D30525265\n\nfbshipit-source-id: 6e50b70fd535defc1f802d51e8bb991b2dd73741",
    "changes": [
        {
            "name": "fx2trt.py",
            "path": "torch/fx/experimental/fx2trt/fx2trt.py",
            "patches": [
                {
                    "old_start": 86,
                    "old_length": 6,
                    "new_start": 86,
                    "new_length": 7,
                    "hunk_buggy": "['         bindings: List[Any] = [None] * (len(self.input_names) + len(self.output_names))\\n', ' \\n', '         for i, input_name in enumerate(self.input_names):\\n', '             idx = self.engine.get_binding_index(input_name)\\n', '             bindings[idx] = contiguous_inputs[i].data_ptr()\\n', ' ']",
                    "hunk_fix": "@@ -86,6 +86,7 @@ class TRTModule(torch.nn.Module):\n         bindings: List[Any] = [None] * (len(self.input_names) + len(self.output_names))\n \n         for i, input_name in enumerate(self.input_names):\n+            assert inputs[i].is_cuda, f\"{i}th input is not on cuda device.\"\n             idx = self.engine.get_binding_index(input_name)\n             bindings[idx] = contiguous_inputs[i].data_ptr()\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 436,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/af3cbfed9510747c776418c260c5116f662c6452",
    "date": "2021-08-18T10:41:10-07:00",
    "message": "Add validation check in fx2trt interpreter (#63424)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63424\n\nAdd validation check in fx2trt for missing converter operators. If any op missing, interpreter init will report missing operators.\n\nTest Plan:\nfor call_function and call_method:\nmanual test with feeds benchmark and verify init failed with expected message.\n{F642390780}\n\nfor call_module:\nspecify a module as leaf node and make acc_tracer trace it as a node; then in fx2trt.py, in CONVERTER initialize stage make it skip recording all modules; initialize interpreter and call validator function, verify the output includes the missing module name, return value print as screenshot below.\n\n{F643458718}\n\nReviewed By: 842974287\n\nDifferential Revision: D30294832\n\nfbshipit-source-id: 243dca3fdfc6a174ded65248938e2a234aec19c6",
    "changes": [
        {
            "name": "fx2trt.py",
            "path": "torch/fx/experimental/fx2trt/fx2trt.py",
            "patches": [
                {
                    "old_start": 229,
                    "old_length": 6,
                    "new_start": 229,
                    "new_length": 10,
                    "hunk_buggy": "['         self.input_specs = input_specs\\n', '         self.input_specs_iter = 0\\n', '         self.validate_input_specs()\\n', '         self._cur_node_name: Optional[str] = None\\n', '         self._input_names: List[str] = []\\n', '         self._output_names: List[str] = []\\n']",
                    "hunk_fix": "@@ -229,6 +229,10 @@ class TRTInterpreter(torch.fx.Interpreter):\n         self.input_specs = input_specs\n         self.input_specs_iter = 0\n         self.validate_input_specs()\n+        missing_ops = self.validate_conversion\n+        if not missing_ops:\n+            warnings.warn(\"Interpretation may fail due to missing operations \\n\"\n+                          + \"\\n\".join(f\"{i}\" for i in missing_ops))\n         self._cur_node_name: Optional[str] = None\n         self._input_names: List[str] = []\n         self._output_names: List[str] = []\n"
                },
                {
                    "old_start": 290,
                    "old_length": 6,
                    "new_start": 294,
                    "new_length": 19,
                    "hunk_buggy": "['                     len(shape_ranges) == 0\\n', '                 ), \"shape_ranges are provided for input that doesn\\'t have dynamic dim.\"\\n', ' \\n', '     def run(\\n', '         self,\\n', '         max_batch_size=64,']",
                    "hunk_fix": "@@ -290,6 +294,19 @@ class TRTInterpreter(torch.fx.Interpreter):\n                     len(shape_ranges) == 0\n                 ), \"shape_ranges are provided for input that doesn't have dynamic dim.\"\n \n+    def validate_conversion(self):\n+        missing_converter = set()\n+\n+        for node in self.module.graph.nodes:\n+            if node.op in [\"call_function\", \"call_method\"] and not CONVERTERS.get(node.target):\n+                missing_converter.add(f\"{node.op} {node.target}\")\n+            elif node.op == \"call_module\":\n+                submod = self.fetch_attr(node.target)\n+                if not CONVERTERS.get(type(submod)):\n+                    missing_converter.add(f\"{node.op} {type(submod)}\")\n+\n+        return missing_converter\n+\n     def run(\n         self,\n         max_batch_size=64,"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 437,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/30e1c74dc19ae2b622b46ebcdb7972c42775ac80",
    "date": "2021-08-18T06:44:10-07:00",
    "message": "Update cuda amp to also check xla device (#63413)\n\nSummary:\nFixes https://github.com/pytorch/xla/issues/3086. Pytorch/XLA:GPU also use cuda amp. I verified the pt/xla `test_autocast` with this fix and all test passed.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63413\n\nReviewed By: ngimel\n\nDifferential Revision: D30380785\n\nPulled By: bdhirsh\n\nfbshipit-source-id: fd1a1de7d224c616fc3fa90b80a688a21f6b1ecc",
    "changes": [
        {
            "name": "autocast_mode.py",
            "path": "torch/autocast_mode.py",
            "patches": [
                {
                    "old_start": 135,
                    "old_length": 7,
                    "new_start": 135,
                    "new_length": 7,
                    "hunk_buggy": "['             self.fast_dtype = torch.get_autocast_cpu_dtype()\\n', '         else:\\n', \"             raise RuntimeError('User specified autocast device_type must be \\\\'cuda\\\\' or \\\\'cpu\\\\'')\\n\", \"-        if not torch.cuda.is_available() and self.device == 'cuda':\\n\", \"             warnings.warn('User provided device_type of \\\\'cuda\\\\', but CUDA is not available. Disabling')\\n\", '             enabled = False\\n', '         for key, value in kwargs.items():']",
                    "hunk_fix": "@@ -135,7 +135,7 @@ class autocast(object):\n             self.fast_dtype = torch.get_autocast_cpu_dtype()\n         else:\n             raise RuntimeError('User specified autocast device_type must be \\'cuda\\' or \\'cpu\\'')\n-        if not torch.cuda.is_available() and self.device == 'cuda':\n+        if torch.cuda.amp.common.amp_definitely_not_available() and self.device == 'cuda':\n             warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n             enabled = False\n         for key, value in kwargs.items():"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 438,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/18eeccc7e8cba16d71efdd2eca831983c4abde15",
    "date": "2021-08-03T16:00:55-07:00",
    "message": "[mypy] Fix Optional type check (#62668)\n\nSummary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/62668\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet, 842974287\n\nDifferential Revision: D30077960\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 5e423bfb65a65974ed848caa177330d6e61452e6",
    "changes": [
        {
            "name": "fx2trt.py",
            "path": "torch/fx/experimental/fx2trt/fx2trt.py",
            "patches": [
                {
                    "old_start": 296,
                    "old_length": 6,
                    "new_start": 296,
                    "new_length": 7,
                    "hunk_buggy": "['                 shape = shape[1:]\\n', '         else:\\n', '             for i, shape_range in enumerate(shape_ranges):\\n', '                 self.optimization_profiles[i].set_shape(target, *shape_range)\\n', ' \\n', '         return self.network.add_input(name=target, shape=tuple(shape), dtype=torch_dtype_to_trt(dtype))']",
                    "hunk_fix": "@@ -296,6 +296,7 @@ class BaseTRTInterpreter(torch.fx.Interpreter):\n                 shape = shape[1:]\n         else:\n             for i, shape_range in enumerate(shape_ranges):\n+                assert self.optimization_profiles\n                 self.optimization_profiles[i].set_shape(target, *shape_range)\n \n         return self.network.add_input(name=target, shape=tuple(shape), dtype=torch_dtype_to_trt(dtype))"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 439,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/5a20c56ebce3426397210e91693fbbeade8b46ba",
    "date": "2021-07-12T10:09:33-07:00",
    "message": "[static runtime] Remove hasOperation() check (#61496)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/61496\n\nglow::FusionGroup is JitOnlyOperator that produces an Operation when passed a Node* https://fburl.com/ybwfn3bl\n\nhasOperation doesn't return true in that case https://fburl.com/19wd10aw\n\nby removing the hasOperation() check, the Operation gets successfully materialized, and static runtime enables successfully and runs ok. Will check that the outputs match with jit interpreter\n\nTest Plan:\nTest with 281805158_2\n```\n./buck-out/gen/admarket/lib/ranking/prediction_replayer/replayer --model_inference_type_target=DISAGG_ACCELERATOR --prediction_replayer_force_model_type=inline_cvr_post_imp_model --prediction_replayer_force_model=281805158_2 --prediction_replayer_target_tier=127.0.0.1:7447 --prediction_replayer_input_stream_filename=/data/users/ansha/tmp/adfinder/filter_requests_inline_cvr_post_imp_model_1000_2021_04_29 --ignore_model_id_mismatch --check_performance --fully_remote_sr_connection_options=\"overall_timeout:10000000,processing_timeout:10000000\" --use_new_encoding_for_ads_services --use_new_encoding_from_model_id_to_shard_id --sigrid_force_model_dir=/data/users/ansha/tmp/adfinder/281805158_2/ --sigrid_predictor_model_suffix=.predictor.disagg.local \u2014use_new_encoding_from_model_id_to_shard_id=true --prediction_replayer_force_model_kind=19 --pytorch_predictor_static_runtime_enable=true --prediction_replayer_target_qps=1\n```\n\n```\nNNPI_LOG_LEVEL=0 USE_INF_API=1 ./buck-out/gen/sigrid/predictor/sigrid_remote_predictor_glow_nnpi \\\n  --force_models=281805158_2 \\\n  --sigrid_predictor_model_suffix=.predictor.disagg.remote_other \\\n  --gflags_config_path=sigrid/predictor/gflags/predictor_gflags_ads_perf_glow_nnpi_pyper_v1 \\\n  --smc_server_port=7447 \\\n  --sigrid_predictor_tier_name=sigrid.predictor.perf.dianshi_staticruntime_debug_0604.test.storage \\\n  --predictor_storage_smc_tier=sigrid.predictor.perf.dianshi_staticruntime_debug_0604.test.storage \\\n  --predictor_storage_smc_tier_v2=sigrid.predictor.perf.dianshi_staticruntime_debug_0604.test.storage \\\n  --torch_glow_min_fusion_group_size=30 \\\n  --glow_enable_sanitize_inputs=100 \\\n  --sigrid_force_model_dir=/data/users/ansha/tmp/adfinder/281805158_2/ \\\n  --pytorch_predictor_static_runtime_enable=true \\\n  --pytorch_predictor_glow_enable=true \\\n  --pytorch_predictor_enable_loading_xl_format_on_cpu=false \\\n  --pytorch_disagg_acc_input_dump_path=/tmp/\n```\n\nReviewed By: hlu1\n\nDifferential Revision: D29647043\n\nfbshipit-source-id: 8ce6dc0f4f0464b65ca6a8c9d42e3d8bb392e66e",
    "changes": [
        {
            "name": "impl.cpp",
            "path": "torch/csrc/jit/runtime/static/impl.cpp",
            "patches": [
                {
                    "old_start": 1357,
                    "old_length": 7,
                    "new_start": 1357,
                    "new_length": 6,
                    "hunk_buggy": "['   }\\n', '   {\\n', '     const Operator& op = node->getOperator();\\n', '-    TORCH_CHECK(op.hasOperation());\\n', '     op_ = op.getOperation(node);\\n', '     VLOG(1) << \"Fallback interpreter for node: \" << PrintNode(node);\\n', '   }']",
                    "hunk_fix": "@@ -1357,7 +1357,6 @@ ProcessedNode::ProcessedNode(\n   }\n   {\n     const Operator& op = node->getOperator();\n-    TORCH_CHECK(op.hasOperation());\n     op_ = op.getOperation(node);\n     VLOG(1) << \"Fallback interpreter for node: \" << PrintNode(node);\n   }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 440,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/0fc110cdd19363f2eb5de68b6eeb82dadc933be0",
    "date": "2021-07-08T13:26:07-07:00",
    "message": "[CUDA graphs] Don't sync between replays for cuda driver version 11.4+ (#61063)\n\nSummary:\nThe bug in libcuda.so that required https://github.com/pytorch/pytorch/pull/57556 is fixed for libcuda.so versions >= 11.4.\n\nThis PR changes replay() to sync after each launch only if the process's in-use libcuda.so is < 11.4.\n\nWith all the \"enhanced\" and \"forward\" compatibility promises flying around, and the fact that \"driver\" sometimes means kernel-mode driver and sometimes means user-mode driver (libcuda.so), I wasn't sure if this PR's check suffices to trigger the sync iff the in-use libcuda.so is < 11.4, but Cuda people say what I wrote is reasonable.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/61063\n\nReviewed By: mruberry\n\nDifferential Revision: D29600907\n\nPulled By: ngimel\n\nfbshipit-source-id: 71bf0bcbde43091e29f3812440abeb7a95d161e2",
    "changes": [
        {
            "name": "CUDAGraph.cpp",
            "path": "aten/src/ATen/cuda/CUDAGraph.cpp",
            "patches": [
                {
                    "old_start": 180,
                    "old_length": 11,
                    "new_start": 180,
                    "new_length": 15,
                    "hunk_buggy": "['   // graph_exec_ may be replayed in any stream.\\n', '   AT_CUDA_CHECK(cudaGraphLaunch(graph_exec_, at::cuda::getCurrentCUDAStream()));\\n', ' \\n', '-  // Temporary workaround for bug in libcuda.so that causes replayed graphs\\n', '-  // with certain topologies to be corrupted (kernels elided, internal syncs\\n', '-  // ignored) when replayed back to back without a sync in between.\\n', \"-  // I hate to use a hard sync, but it's the only surefire workaround at the moment.\\n\", '-  cudaDeviceSynchronize();\\n', ' #else\\n', '   TORCH_CHECK(false, \"CUDA graphs may only be used in Pytorch built with CUDA >= 11.0\");\\n', ' #endif']",
                    "hunk_fix": "@@ -180,11 +180,15 @@ void CUDAGraph::replay() {\n   // graph_exec_ may be replayed in any stream.\n   AT_CUDA_CHECK(cudaGraphLaunch(graph_exec_, at::cuda::getCurrentCUDAStream()));\n \n-  // Temporary workaround for bug in libcuda.so that causes replayed graphs\n-  // with certain topologies to be corrupted (kernels elided, internal syncs\n-  // ignored) when replayed back to back without a sync in between.\n-  // I hate to use a hard sync, but it's the only surefire workaround at the moment.\n-  cudaDeviceSynchronize();\n+  int version;\n+  AT_CUDA_CHECK(cudaDriverGetVersion(&version));\n+  if (version < 11040) {\n+    // Workaround for bug in libcuda.so that causes replayed graphs with\n+    // certain topologies to be corrupted (kernels elided, internal syncs\n+    // ignored) when replayed back to back without a sync in between.\n+    // The bug is fixed in CUDA 11.4+.\n+    cudaDeviceSynchronize();\n+  }\n #else\n   TORCH_CHECK(false, \"CUDA graphs may only be used in Pytorch built with CUDA >= 11.0\");\n #endif"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 441,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/d3de37609f2f052a7efb098ab69540458ebaaa6c",
    "date": "2021-06-29T14:20:17-07:00",
    "message": "Support fused_dropout with XPU backend (#60231)\n\nSummary:\n## Motivation\nEnable the fused dropout optimization on XPU devices.\n\n## Solution\nAdd XPU device in the fused dropout acceptable checking.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/60231\n\nReviewed By: jbschlosser\n\nDifferential Revision: D29437659\n\nPulled By: ezyang\n\nfbshipit-source-id: b77245bb53d3ac93ab30a2a85994376ae5928c34",
    "changes": [
        {
            "name": "Dropout.cpp",
            "path": "aten/src/ATen/native/Dropout.cpp",
            "patches": [
                {
                    "old_start": 22,
                    "old_length": 7,
                    "new_start": 22,
                    "new_length": 7,
                    "hunk_buggy": "[' }\\n', ' \\n', ' bool is_fused_kernel_acceptable(const Tensor& input, double p) {\\n', '-  return input.is_cuda() && p > 0 && p < 1 && input.numel() > 0;\\n', ' }\\n', ' \\n', ' // NB: sure, we could have used different overloads here, but I would feel insecure']",
                    "hunk_fix": "@@ -22,7 +22,7 @@ Tensor make_feature_noise(const Tensor& input) {\n }\n \n bool is_fused_kernel_acceptable(const Tensor& input, double p) {\n-  return input.is_cuda() && p > 0 && p < 1 && input.numel() > 0;\n+  return (input.is_cuda() || input.is_xpu()) && p > 0 && p < 1 && input.numel() > 0;\n }\n \n // NB: sure, we could have used different overloads here, but I would feel insecure"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 442,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/f118d20bea9188db1bd053dd1d1af1b32479183e",
    "date": "2021-06-28T10:40:30-07:00",
    "message": "Make requires grad check run only when grad mode is enabled (#60740)\n\nSummary:\nAs per title.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/60740\n\nReviewed By: ngimel\n\nDifferential Revision: D29405934\n\nPulled By: albanD\n\nfbshipit-source-id: 35c537939a3871f5a0d2146543506e4d07465724",
    "changes": [
        {
            "name": "VariableTypeUtils.h",
            "path": "torch/csrc/autograd/VariableTypeUtils.h",
            "patches": [
                {
                    "old_start": 292,
                    "old_length": 9,
                    "new_start": 292,
                    "new_length": 11,
                    "hunk_buggy": "['   return tensors;\\n', ' }\\n', ' \\n', '-inline void check_no_requires_grad(const Tensor& tensor, const char* name, const char* fn_name=\"\") {\\n', '-  TORCH_CHECK(!(tensor.defined() && tensor.requires_grad()), \"The function \\'\", fn_name, \"\\' is not differentiable \"\\n', '-              \"with respect to argument \\'\", name, \"\\'. This input cannot have requires_grad True.\");\\n', ' }\\n', ' \\n', ' inline void check_no_requires_grad(const c10::optional<Tensor>& tensor, const char* name, const char* fn_name=\"\") {\\n']",
                    "hunk_fix": "@@ -292,9 +292,11 @@ inline std::vector<Tensor> as_view(const Tensor & base, std::vector<Tensor>& ten\n   return tensors;\n }\n \n-inline void check_no_requires_grad(const Tensor& tensor, const char* name, const char* fn_name=\"\") {\n-  TORCH_CHECK(!(tensor.defined() && tensor.requires_grad()), \"The function '\", fn_name, \"' is not differentiable \"\n-              \"with respect to argument '\", name, \"'. This input cannot have requires_grad True.\");\n+inline void check_no_requires_grad(const Tensor& tensor, const char* name,\n+                                   const char* fn_name=\"\", bool check_grad_mode=true) {\n+  TORCH_CHECK(!(tensor.defined() && tensor.requires_grad()) || !(check_grad_mode && GradMode::is_enabled()),\n+              \"The function '\", fn_name, \"' is not differentiable with respect to argument '\", name,\n+              \"'. This input cannot have requires_grad True.\");\n }\n \n inline void check_no_requires_grad(const c10::optional<Tensor>& tensor, const char* name, const char* fn_name=\"\") {\n"
                },
                {
                    "old_start": 304,
                    "old_length": 15,
                    "new_start": 306,
                    "new_length": 23,
                    "hunk_buggy": "[' }\\n', ' \\n', ' inline void check_no_requires_grad(TensorList tensors, const char* name, const char* fn_name=\"\") {\\n', '   for (auto& tensor : tensors) {\\n', '-    check_no_requires_grad(tensor, name, fn_name);\\n', '   }\\n', ' }\\n', ' \\n', ' inline void check_no_requires_grad(const c10::List<c10::optional<Tensor>>& tensors, const char* name, const char* fn_name=\"\") {\\n', '   for (c10::optional<Tensor> tensor : tensors) {\\n', '     if (tensor.has_value()) {\\n', '-      check_no_requires_grad(*tensor, name, fn_name);\\n', '     }\\n', '   }\\n', ' }']",
                    "hunk_fix": "@@ -304,15 +306,23 @@ inline void check_no_requires_grad(const c10::optional<Tensor>& tensor, const ch\n }\n \n inline void check_no_requires_grad(TensorList tensors, const char* name, const char* fn_name=\"\") {\n+  // GradMode check is expensive, so check it only once for TensorLists\n+  if (!GradMode::is_enabled()) {\n+    return;\n+  }\n   for (auto& tensor : tensors) {\n-    check_no_requires_grad(tensor, name, fn_name);\n+    check_no_requires_grad(tensor, name, fn_name, /*check_grad_mode*/ false);\n   }\n }\n \n inline void check_no_requires_grad(const c10::List<c10::optional<Tensor>>& tensors, const char* name, const char* fn_name=\"\") {\n+  // GradMode check is expensive, so check it only once for TensorLists\n+  if (!GradMode::is_enabled()) {\n+    return;\n+  }\n   for (c10::optional<Tensor> tensor : tensors) {\n     if (tensor.has_value()) {\n-      check_no_requires_grad(*tensor, name, fn_name);\n+      check_no_requires_grad(*tensor, name, fn_name, /*check_grad_mode*/ false);\n     }\n   }\n }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 443,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/7bf195f3608e0f28c30ffb6e2fecd74a1d4ee50a",
    "date": "2021-06-23T11:47:50-07:00",
    "message": "fix kernel launch check in cross kernel\n\nSummary: per title\n\nTest Plan: buck test mode/opt //caffe2/test:kernel_launch_checks -- --exact 'caffe2/test:kernel_launch_checks - test_check_cuda_launches (test_kernel_launch_checks.AlwaysCheckCudaLaunchTest)' --run-disabled\n\nReviewed By: r-barnes\n\nDifferential Revision: D29335739\n\nfbshipit-source-id: 385c66b1806886deba35f7fd83e29e0885999119",
    "changes": [
        {
            "name": "CrossKernel.cu",
            "path": "aten/src/ATen/native/cuda/CrossKernel.cu",
            "patches": [
                {
                    "old_start": 47,
                    "old_length": 15,
                    "new_start": 47,
                    "new_length": 16,
                    "hunk_buggy": "['     if (ostride * 2 > int_max || x1stride * 2 > int_max || x2stride * 2 > int_max) {\\n', '       cross_kernel<<<grid, num_threads, 0, stream>>>(\\n', '           N, out, x1, x2, offset_calculator, ostride, x1stride, x2stride);\\n', '     } else {\\n', '       cross_kernel<<<grid, num_threads, 0, stream>>>(\\n', '           N, out, x1, x2, offset_calculator,\\n', '           static_cast<int>(ostride),\\n', '           static_cast<int>(x1stride),\\n', '           static_cast<int>(x2stride));\\n', '     }\\n', '   });\\n', '-  C10_CUDA_KERNEL_LAUNCH_CHECK();\\n', ' }\\n', ' \\n', ' void cross_impl(Tensor& result, const Tensor& x1, const Tensor& x2, int64_t dim) {']",
                    "hunk_fix": "@@ -47,15 +47,16 @@ void launch_cross_kernel(const TensorIteratorBase& iter, int64_t ostride,\n     if (ostride * 2 > int_max || x1stride * 2 > int_max || x2stride * 2 > int_max) {\n       cross_kernel<<<grid, num_threads, 0, stream>>>(\n           N, out, x1, x2, offset_calculator, ostride, x1stride, x2stride);\n+      C10_CUDA_KERNEL_LAUNCH_CHECK();\n     } else {\n       cross_kernel<<<grid, num_threads, 0, stream>>>(\n           N, out, x1, x2, offset_calculator,\n           static_cast<int>(ostride),\n           static_cast<int>(x1stride),\n           static_cast<int>(x2stride));\n+      C10_CUDA_KERNEL_LAUNCH_CHECK();\n     }\n   });\n-  C10_CUDA_KERNEL_LAUNCH_CHECK();\n }\n \n void cross_impl(Tensor& result, const Tensor& x1, const Tensor& x2, int64_t dim) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 444,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/15dbc566c57eedbd0245e786912e94586eba0fd2",
    "date": "2021-06-16T12:19:12-07:00",
    "message": "[torch][segment_reduce] Add missing cuda kernel launch check (#60114)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/60114\n\nSame as title.\n\nTest Plan: Unit test (test_kernel_launch_checks.py) is passing.\n\nReviewed By: ngimel\n\nDifferential Revision: D29169538\n\nfbshipit-source-id: ba4518dcb1a4713144d92faec2bb5bdf656ff7c5",
    "changes": [
        {
            "name": "SegmentReduce.cu",
            "path": "aten/src/ATen/native/cuda/SegmentReduce.cu",
            "patches": [
                {
                    "old_start": 138,
                    "old_length": 6,
                    "new_start": 138,
                    "new_length": 7,
                    "hunk_buggy": "['                   segment_count,\\n', '                   initial.has_value(),\\n', '                   initial_value);\\n', '         }\\n', '       });\\n', ' ']",
                    "hunk_fix": "@@ -138,6 +138,7 @@ Tensor _segment_reduce_cuda_kernel(\n                   segment_count,\n                   initial.has_value(),\n                   initial_value);\n+          C10_CUDA_KERNEL_LAUNCH_CHECK();\n         }\n       });\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 445,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/e24dee00d40d01bd83b7a08fbcf9cdd51a05b04b",
    "date": "2021-05-17T18:03:19-07:00",
    "message": "add kernel launch checks after each kernel launch to silence the check (#58432)\n\nSummary:\nT90898552\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58432\n\nReviewed By: r-barnes\n\nDifferential Revision: D28487446\n\nPulled By: ngimel\n\nfbshipit-source-id: 3a756ffa3cd68720e132af27cd5ae36f7fd4a2d8",
    "changes": [
        {
            "name": "Normalization.cuh",
            "path": "aten/src/ATen/native/cuda/Normalization.cuh",
            "patches": [
                {
                    "old_start": 1664,
                    "old_length": 6,
                    "new_start": 1664,
                    "new_length": 7,
                    "hunk_buggy": "['           static_cast<accscalar_t>(norm_fct),\\n', '           reduction_size,\\n', '           stride);\\n', '     } else {\\n', '       batch_norm_backward_elemt_channels_last_kernel<ELEMENTS_PER_ITER>\\n', '           <<<grid, block, 0, stream>>>(\\n']",
                    "hunk_fix": "@@ -1664,6 +1664,7 @@ at::Tensor batch_norm_backward_elemt_channels_last_cuda_template(\n           static_cast<accscalar_t>(norm_fct),\n           reduction_size,\n           stride);\n+          C10_CUDA_KERNEL_LAUNCH_CHECK();\n     } else {\n       batch_norm_backward_elemt_channels_last_kernel<ELEMENTS_PER_ITER>\n           <<<grid, block, 0, stream>>>(\n"
                },
                {
                    "old_start": 1678,
                    "old_length": 8,
                    "new_start": 1679,
                    "new_length": 8,
                    "hunk_buggy": "['           static_cast<accscalar_t>(norm_fct),\\n', '           reduction_size,\\n', '           stride);\\n', '     }\\n', '-    C10_CUDA_KERNEL_LAUNCH_CHECK();\\n', '   });\\n', ' \\n', '   return grad_input;']",
                    "hunk_fix": "@@ -1678,8 +1679,8 @@ at::Tensor batch_norm_backward_elemt_channels_last_cuda_template(\n           static_cast<accscalar_t>(norm_fct),\n           reduction_size,\n           stride);\n+          C10_CUDA_KERNEL_LAUNCH_CHECK();\n     }\n-    C10_CUDA_KERNEL_LAUNCH_CHECK();\n   });\n \n   return grad_input;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 446,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/e9e125475e94aabfb34ee239fadc760615eef429",
    "date": "2021-05-11T22:07:21-07:00",
    "message": "[Static Runtime] Add schema check to aten::repeat and fb::fast_gather (#58106)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58106\n\nFollowup for D28047955 (https://github.com/pytorch/pytorch/commit/1f83d8eec25c7339bd3e2862baf9b389e6a738a4).\n\nReviewed By: ajyu\n\nDifferential Revision: D28369472\n\nfbshipit-source-id: 36aa10082589f4b6f0cc2d79f032fe72a19cda57",
    "changes": [
        {
            "name": "ops.cpp",
            "path": "torch/csrc/jit/runtime/static/ops.cpp",
            "patches": [
                {
                    "old_start": 1285,
                    "old_length": 6,
                    "new_start": 1285,
                    "new_length": 9,
                    "hunk_buggy": "[' \\n', ' // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)\\n', ' REGISTER_OPERATOR_FUNCTOR(aten::repeat, aten_repeat, [](Node* n) -> SROperator {\\n', '   return [](ProcessedNode* p_node) {\\n', '     const auto& self = p_node->Input(0).toTensor();\\n', '     const auto repeats = p_node->Input(1).toIntVector();']",
                    "hunk_fix": "@@ -1285,6 +1285,9 @@ REGISTER_OPERATOR_FUNCTOR(\n \n // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)\n REGISTER_OPERATOR_FUNCTOR(aten::repeat, aten_repeat, [](Node* n) -> SROperator {\n+  if (n->inputs().size() != 2) {\n+    return nullptr;\n+  }\n   return [](ProcessedNode* p_node) {\n     const auto& self = p_node->Input(0).toTensor();\n     const auto repeats = p_node->Input(1).toIntVector();"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 447,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/1f83d8eec25c7339bd3e2862baf9b389e6a738a4",
    "date": "2021-05-11T16:30:45-07:00",
    "message": "[Static Runtime] Return nullptr if the number of input args doesn't match (#58018)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58018\n\n- Add checks for the number of input args and return nullptr if it doesn't match. This is intended to make Static Runtime more robust so that op schema change is less likely to break things. Imagine that a new arg is added to an op or a new overload is added that has this added arg, SR would simply ignore this added arg. If this arg has a default value, SR would run the model with the default value and give you wrong results, which can be hard to track down.\n\nReviewed By: ajyu\n\nDifferential Revision: D28047955\n\nfbshipit-source-id: 01067059edd5cfea80c4ee121829f7733b11f601",
    "changes": [
        {
            "name": "ops.cpp",
            "path": "torch/csrc/jit/runtime/static/ops.cpp",
            "patches": [
                {
                    "old_start": 272,
                    "old_length": 6,
                    "new_start": 272,
                    "new_length": 9,
                    "hunk_buggy": "[' \\n', ' // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)\\n', ' REGISTER_OPERATOR_FUNCTOR(aten::mul, aten_mul, [](Node* n) -> SROperator {\\n', '   return [](ProcessedNode* p_node) {\\n', '     const auto& in0_t = p_node->Input(0).toTensor();\\n', '     const auto& in1_t = p_node->Input(1).toTensor();\\n']",
                    "hunk_fix": "@@ -272,6 +272,9 @@ REGISTER_OPERATOR_FUNCTOR(\n \n // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)\n REGISTER_OPERATOR_FUNCTOR(aten::mul, aten_mul, [](Node* n) -> SROperator {\n+  if (n->inputs().size() != 2) {\n+    return nullptr;\n+  }\n   return [](ProcessedNode* p_node) {\n     const auto& in0_t = p_node->Input(0).toTensor();\n     const auto& in1_t = p_node->Input(1).toTensor();\n"
                },
                {
                    "old_start": 286,
                    "old_length": 6,
                    "new_start": 289,
                    "new_length": 9,
                    "hunk_buggy": "[' \\n', ' // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)\\n', ' REGISTER_OPERATOR_FUNCTOR(aten::addmm, aten_addmm, [](Node* n) -> SROperator {\\n', '   return [](ProcessedNode* p_node) {\\n', '     const auto& in0_t = p_node->Input(0).toTensor();\\n', '     const auto& in1_t = p_node->Input(1).toTensor();\\n']",
                    "hunk_fix": "@@ -286,6 +289,9 @@ REGISTER_OPERATOR_FUNCTOR(aten::mul, aten_mul, [](Node* n) -> SROperator {\n \n // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)\n REGISTER_OPERATOR_FUNCTOR(aten::addmm, aten_addmm, [](Node* n) -> SROperator {\n+  if (n->inputs().size() != 5) {\n+    return nullptr;\n+  }\n   return [](ProcessedNode* p_node) {\n     const auto& in0_t = p_node->Input(0).toTensor();\n     const auto& in1_t = p_node->Input(1).toTensor();\n"
                },
                {
                    "old_start": 301,
                    "old_length": 8,
                    "new_start": 307,
                    "new_length": 13,
                    "hunk_buggy": "['   };\\n', ' });\\n', ' \\n', ' // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)\\n', ' REGISTER_OPERATOR_FUNCTOR(aten::clamp, aten_clamp, [](Node* n) -> SROperator {\\n', '   return [](ProcessedNode* p_node) {\\n', '     const auto& in0_t = p_node->Input(0).toTensor();\\n', '     const auto in1_s = p_node->Input(1).toOptional<at::Scalar>();\\n']",
                    "hunk_fix": "@@ -301,8 +307,13 @@ REGISTER_OPERATOR_FUNCTOR(aten::addmm, aten_addmm, [](Node* n) -> SROperator {\n   };\n });\n \n+// TODO: support\n+// clamp.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -> Tensor\n // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)\n REGISTER_OPERATOR_FUNCTOR(aten::clamp, aten_clamp, [](Node* n) -> SROperator {\n+  if (n->inputs().size() != 3) {\n+    return nullptr;\n+  }\n   return [](ProcessedNode* p_node) {\n     const auto& in0_t = p_node->Input(0).toTensor();\n     const auto in1_s = p_node->Input(1).toOptional<at::Scalar>();\n"
                },
                {
                    "old_start": 318,
                    "old_length": 6,
                    "new_start": 329,
                    "new_length": 9,
                    "hunk_buggy": "[' \\n', ' // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)\\n', ' REGISTER_OPERATOR_FUNCTOR(aten::bmm, aten_bmm, [](Node* n) -> SROperator {\\n', '   return [](ProcessedNode* p_node) {\\n', '     const auto& in0_t = p_node->Input(0).toTensor();\\n', '     const auto& in1_t = p_node->Input(1).toTensor();\\n']",
                    "hunk_fix": "@@ -318,6 +329,9 @@ REGISTER_OPERATOR_FUNCTOR(aten::clamp, aten_clamp, [](Node* n) -> SROperator {\n \n // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)\n REGISTER_OPERATOR_FUNCTOR(aten::bmm, aten_bmm, [](Node* n) -> SROperator {\n+  if (n->inputs().size() != 2) {\n+    return nullptr;\n+  }\n   return [](ProcessedNode* p_node) {\n     const auto& in0_t = p_node->Input(0).toTensor();\n     const auto& in1_t = p_node->Input(1).toTensor();\n"
                },
                {
                    "old_start": 335,
                    "old_length": 6,
                    "new_start": 349,
                    "new_length": 9,
                    "hunk_buggy": "['     aten::nan_to_num,\\n', '     aten_nan_to_num,\\n', '     [](Node* n) -> SROperator {\\n', '       return [](ProcessedNode* p_node) {\\n', '         const auto& in0_t = p_node->Input(0).toTensor();\\n', '         const auto in1_d = p_node->Input(1).toOptional<double>();\\n']",
                    "hunk_fix": "@@ -335,6 +349,9 @@ REGISTER_OPERATOR_FUNCTOR(\n     aten::nan_to_num,\n     aten_nan_to_num,\n     [](Node* n) -> SROperator {\n+      if (n->inputs().size() != 4) {\n+        return nullptr;\n+      }\n       return [](ProcessedNode* p_node) {\n         const auto& in0_t = p_node->Input(0).toTensor();\n         const auto in1_d = p_node->Input(1).toOptional<double>();\n"
                },
                {
                    "old_start": 350,
                    "old_length": 6,
                    "new_start": 367,
                    "new_length": 9,
                    "hunk_buggy": "['     });\\n', ' // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)\\n', ' REGISTER_OPERATOR_FUNCTOR(aten::cat, aten_cat, [](Node* n) -> SROperator {\\n', '   return [](ProcessedNode* p_node) {\\n', '     const auto in0_tl = p_node->Input(0).toTensorVector();\\n', '     const auto in1_i = p_node->Input(1).toInt();\\n']",
                    "hunk_fix": "@@ -350,6 +367,9 @@ REGISTER_OPERATOR_FUNCTOR(\n     });\n // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)\n REGISTER_OPERATOR_FUNCTOR(aten::cat, aten_cat, [](Node* n) -> SROperator {\n+  if (n->inputs().size() != 2) {\n+    return nullptr;\n+  }\n   return [](ProcessedNode* p_node) {\n     const auto in0_tl = p_node->Input(0).toTensorVector();\n     const auto in1_i = p_node->Input(1).toInt();\n"
                },
                {
                    "old_start": 364,
                    "old_length": 6,
                    "new_start": 384,
                    "new_length": 9,
                    "hunk_buggy": "[' \\n', \" // Split out into a function to appease MSVC's pre-processor\\n\", ' SROperator aten_stack(Node* n) {\\n', '   return [](ProcessedNode* p_node) {\\n', '     const auto inputs = p_node->Input(0).toTensorVector();\\n', '     const auto dim = p_node->Input(1).toInt();\\n']",
                    "hunk_fix": "@@ -364,6 +384,9 @@ REGISTER_OPERATOR_FUNCTOR(aten::cat, aten_cat, [](Node* n) -> SROperator {\n \n // Split out into a function to appease MSVC's pre-processor\n SROperator aten_stack(Node* n) {\n+  if (n->inputs().size() != 2) {\n+    return nullptr;\n+  }\n   return [](ProcessedNode* p_node) {\n     const auto inputs = p_node->Input(0).toTensorVector();\n     const auto dim = p_node->Input(1).toInt();\n"
                },
                {
                    "old_start": 384,
                    "old_length": 6,
                    "new_start": 407,
                    "new_length": 9,
                    "hunk_buggy": "['     aten::leaky_relu,\\n', '     aten_leaky_relu,\\n', '     [](Node* n) -> SROperator {\\n', '       return [](ProcessedNode* p_node) {\\n', '         const auto& in0_t = p_node->Input(0).toTensor();\\n', '         const auto in1_s = p_node->Input(1).toScalar();\\n']",
                    "hunk_fix": "@@ -384,6 +407,9 @@ REGISTER_OPERATOR_FUNCTOR(\n     aten::leaky_relu,\n     aten_leaky_relu,\n     [](Node* n) -> SROperator {\n+      if (n->inputs().size() != 2) {\n+        return nullptr;\n+      }\n       return [](ProcessedNode* p_node) {\n         const auto& in0_t = p_node->Input(0).toTensor();\n         const auto in1_s = p_node->Input(1).toScalar();\n"
                },
                {
                    "old_start": 548,
                    "old_length": 6,
                    "new_start": 574,
                    "new_length": 9,
                    "hunk_buggy": "[' \\n', ' // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)\\n', ' REGISTER_OPERATOR_FUNCTOR(aten::relu, aten_relu, [](Node* n) -> SROperator {\\n', '   auto te = createRelu();\\n', '   return [te](ProcessedNode* p_node) {\\n', '     const auto& in0_t = p_node->Input(0).toTensor();\\n']",
                    "hunk_fix": "@@ -548,6 +574,9 @@ std::shared_ptr<TEWrapper> createSigmoid() {\n \n // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)\n REGISTER_OPERATOR_FUNCTOR(aten::relu, aten_relu, [](Node* n) -> SROperator {\n+  if (n->inputs().size() != 1) {\n+    return nullptr;\n+  }\n   auto te = createRelu();\n   return [te](ProcessedNode* p_node) {\n     const auto& in0_t = p_node->Input(0).toTensor();\n"
                },
                {
                    "old_start": 568,
                    "old_length": 6,
                    "new_start": 597,
                    "new_length": 9,
                    "hunk_buggy": "[' \\n', ' // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)\\n', ' REGISTER_OPERATOR_FUNCTOR(aten::tanh, aten_tanh, [](Node* n) -> SROperator {\\n', '   auto te = createTanh();\\n', '   return [te](ProcessedNode* p_node) {\\n', '     const auto& in0_t = p_node->Input(0).toTensor();\\n']",
                    "hunk_fix": "@@ -568,6 +597,9 @@ REGISTER_OPERATOR_FUNCTOR(aten::relu, aten_relu, [](Node* n) -> SROperator {\n \n // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)\n REGISTER_OPERATOR_FUNCTOR(aten::tanh, aten_tanh, [](Node* n) -> SROperator {\n+  if (n->inputs().size() != 1) {\n+    return nullptr;\n+  }\n   auto te = createTanh();\n   return [te](ProcessedNode* p_node) {\n     const auto& in0_t = p_node->Input(0).toTensor();\n"
                },
                {
                    "old_start": 591,
                    "old_length": 6,
                    "new_start": 623,
                    "new_length": 9,
                    "hunk_buggy": "['     aten::sigmoid,\\n', '     aten_sigmoid,\\n', '     [](Node* n) -> SROperator {\\n', '       auto te = createSigmoid();\\n', '       return [te](ProcessedNode* p_node) {\\n', '         const auto& in0_t = p_node->Input(0).toTensor();\\n']",
                    "hunk_fix": "@@ -591,6 +623,9 @@ REGISTER_OPERATOR_FUNCTOR(\n     aten::sigmoid,\n     aten_sigmoid,\n     [](Node* n) -> SROperator {\n+      if (n->inputs().size() != 1) {\n+        return nullptr;\n+      }\n       auto te = createSigmoid();\n       return [te](ProcessedNode* p_node) {\n         const auto& in0_t = p_node->Input(0).toTensor();\n"
                },
                {
                    "old_start": 641,
                    "old_length": 8,
                    "new_start": 676,
                    "new_length": 13,
                    "hunk_buggy": "['   };\\n', ' });\\n', ' \\n', ' // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)\\n', ' REGISTER_OPERATOR_FUNCTOR(aten::clone, aten_clone, [](Node* n) -> SROperator {\\n', '   return [](ProcessedNode* p_node) {\\n', '     const auto& in0_t = p_node->Input(0).toTensor();\\n', '     if (p_node->Output(0).isNone()) {\\n']",
                    "hunk_fix": "@@ -641,8 +676,13 @@ REGISTER_OPERATOR_FUNCTOR(aten::logit, aten_logit, [](Node* n) -> SROperator {\n   };\n });\n \n+// TODO: fix clone\n+// clone(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor\n // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)\n REGISTER_OPERATOR_FUNCTOR(aten::clone, aten_clone, [](Node* n) -> SROperator {\n+  if (n->inputs().size() != 2) {\n+    return nullptr;\n+  }\n   return [](ProcessedNode* p_node) {\n     const auto& in0_t = p_node->Input(0).toTensor();\n     if (p_node->Output(0).isNone()) {\n"
                },
                {
                    "old_start": 658,
                    "old_length": 6,
                    "new_start": 698,
                    "new_length": 9,
                    "hunk_buggy": "['     quantized::embedding_bag_byte_rowwise_offsets,\\n', '     quantized_embedding_bag_byte_rowwise_offsets,\\n', '     [](Node* n) -> SROperator {\\n', '       return [](ProcessedNode* p_node) {\\n', '         const auto& weight = p_node->Input(0).toTensor();\\n', '         const auto& indices = p_node->Input(1).toTensor();\\n']",
                    "hunk_fix": "@@ -658,6 +698,9 @@ REGISTER_OPERATOR_FUNCTOR(\n     quantized::embedding_bag_byte_rowwise_offsets,\n     quantized_embedding_bag_byte_rowwise_offsets,\n     [](Node* n) -> SROperator {\n+      if (n->inputs().size() != 9) {\n+        return nullptr;\n+      }\n       return [](ProcessedNode* p_node) {\n         const auto& weight = p_node->Input(0).toTensor();\n         const auto& indices = p_node->Input(1).toTensor();\n"
                },
                {
                    "old_start": 691,
                    "old_length": 6,
                    "new_start": 734,
                    "new_length": 9,
                    "hunk_buggy": "['     quantized::embedding_bag_4bit_rowwise_offsets,\\n', '     embedding_bag_4bit_rowwise_offsets,\\n', '     [](Node* n) -> SROperator {\\n', '       return [](ProcessedNode* p_node) {\\n', '         const auto& weight = p_node->Input(0).toTensor();\\n', '         const auto& indices = p_node->Input(1).toTensor();\\n']",
                    "hunk_fix": "@@ -691,6 +734,9 @@ REGISTER_OPERATOR_FUNCTOR(\n     quantized::embedding_bag_4bit_rowwise_offsets,\n     embedding_bag_4bit_rowwise_offsets,\n     [](Node* n) -> SROperator {\n+      if (n->inputs().size() != 9) {\n+        return nullptr;\n+      }\n       return [](ProcessedNode* p_node) {\n         const auto& weight = p_node->Input(0).toTensor();\n         const auto& indices = p_node->Input(1).toTensor();\n"
                },
                {
                    "old_start": 726,
                    "old_length": 6,
                    "new_start": 772,
                    "new_length": 9,
                    "hunk_buggy": "['     aten::narrow_copy,\\n', '     aten_narrow_copy,\\n', '     [](Node* n) -> SROperator {\\n', '       return [](ProcessedNode* p_node) {\\n', '         const auto& self = p_node->Input(0).toTensor(); // self\\n', '         const auto dim = p_node->Input(1).toInt(); // dim\\n']",
                    "hunk_fix": "@@ -726,6 +772,9 @@ REGISTER_OPERATOR_FUNCTOR(\n     aten::narrow_copy,\n     aten_narrow_copy,\n     [](Node* n) -> SROperator {\n+      if (n->inputs().size() != 4) {\n+        return nullptr;\n+      }\n       return [](ProcessedNode* p_node) {\n         const auto& self = p_node->Input(0).toTensor(); // self\n         const auto dim = p_node->Input(1).toInt(); // dim\n"
                },
                {
                    "old_start": 748,
                    "old_length": 6,
                    "new_start": 797,
                    "new_length": 9,
                    "hunk_buggy": "['     });\\n', ' // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)\\n', ' REGISTER_OPERATOR_FUNCTOR(aten::index, aten_index, [](Node* n) -> SROperator {\\n', '   return [](ProcessedNode* p_node) {\\n', '     const auto& in0_t = p_node->Input(0).toTensor();\\n', '     const auto in1_l =\\n']",
                    "hunk_fix": "@@ -748,6 +797,9 @@ REGISTER_OPERATOR_FUNCTOR(\n     });\n // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)\n REGISTER_OPERATOR_FUNCTOR(aten::index, aten_index, [](Node* n) -> SROperator {\n+  if (n->inputs().size() != 2) {\n+    return nullptr;\n+  }\n   return [](ProcessedNode* p_node) {\n     const auto& in0_t = p_node->Input(0).toTensor();\n     const auto in1_l =\n"
                },
                {
                    "old_start": 762,
                    "old_length": 6,
                    "new_start": 814,
                    "new_length": 9,
                    "hunk_buggy": "[' });\\n', ' // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)\\n', ' REGISTER_OPERATOR_FUNCTOR(aten::pow, aten_pow, [](Node* n) -> SROperator {\\n', '   return [](ProcessedNode* p_node) {\\n', '     if (p_node->Output(0).isNone()) {\\n', '       c10::ScalarType dtype;\\n']",
                    "hunk_fix": "@@ -762,6 +814,9 @@ REGISTER_OPERATOR_FUNCTOR(aten::index, aten_index, [](Node* n) -> SROperator {\n });\n // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)\n REGISTER_OPERATOR_FUNCTOR(aten::pow, aten_pow, [](Node* n) -> SROperator {\n+  if (n->inputs().size() != 2) {\n+    return nullptr;\n+  }\n   return [](ProcessedNode* p_node) {\n     if (p_node->Output(0).isNone()) {\n       c10::ScalarType dtype;\n"
                },
                {
                    "old_start": 815,
                    "old_length": 6,
                    "new_start": 870,
                    "new_length": 7,
                    "hunk_buggy": "['     aten_to_copy,\\n', '     [](Node* n) -> SROperator {\\n', '       // support 4- or 5-arg for adindexer/adfinder models\\n', '       TORCH_CHECK(n->inputs().size() == 4 || n->inputs().size() == 5);\\n', '       return [](ProcessedNode* p_node) {\\n', '         const auto& self = p_node->Input(0).toTensor();\\n']",
                    "hunk_fix": "@@ -815,6 +870,7 @@ REGISTER_OPERATOR_FUNCTOR(\n     aten_to_copy,\n     [](Node* n) -> SROperator {\n       // support 4- or 5-arg for adindexer/adfinder models\n+      // Keep TORCH_CHECK here because there is no alternative for fallback\n       TORCH_CHECK(n->inputs().size() == 4 || n->inputs().size() == 5);\n       return [](ProcessedNode* p_node) {\n         const auto& self = p_node->Input(0).toTensor();\n"
                },
                {
                    "old_start": 864,
                    "old_length": 6,
                    "new_start": 920,
                    "new_length": 7,
                    "hunk_buggy": "['     static_runtime::reshape_copy,\\n', '     aten_reshape,\\n', '     [](Node* n) -> SROperator {\\n', '       return [](ProcessedNode* p_node) {\\n', '         const auto& self = p_node->Input(0).toTensor(); // self\\n', '         const auto proposed_shape = p_node->Input(1).toIntVector(); // shape\\n']",
                    "hunk_fix": "@@ -864,6 +920,7 @@ REGISTER_OPERATOR_FUNCTOR(\n     static_runtime::reshape_copy,\n     aten_reshape,\n     [](Node* n) -> SROperator {\n+      TORCH_CHECK(n->inputs().size() == 2);\n       return [](ProcessedNode* p_node) {\n         const auto& self = p_node->Input(0).toTensor(); // self\n         const auto proposed_shape = p_node->Input(1).toIntVector(); // shape\n"
                },
                {
                    "old_start": 897,
                    "old_length": 6,
                    "new_start": 954,
                    "new_length": 9,
                    "hunk_buggy": "[' \\n', ' // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)\\n', ' REGISTER_OPERATOR_FUNCTOR(aten::sum, aten_sum, [](Node* n) -> SROperator {\\n', '   return [](ProcessedNode* p_node) {\\n', '     const at::Tensor& self = p_node->Input(0).toTensor();\\n', ' \\n']",
                    "hunk_fix": "@@ -897,6 +954,9 @@ REGISTER_OPERATOR_FUNCTOR(\n \n // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)\n REGISTER_OPERATOR_FUNCTOR(aten::sum, aten_sum, [](Node* n) -> SROperator {\n+  if (n->inputs().size() != 2 && n->inputs().size() != 4) {\n+    return nullptr;\n+  }\n   return [](ProcessedNode* p_node) {\n     const at::Tensor& self = p_node->Input(0).toTensor();\n \n"
                },
                {
                    "old_start": 927,
                    "old_length": 6,
                    "new_start": 987,
                    "new_length": 9,
                    "hunk_buggy": "[' \\n', ' std::function<void(ProcessedNode*)> getNativeOperation(Node* n) {\\n', '   if (n->kind() == c10::Symbol::fromQualString(\"aten::transpose\")) {\\n', '     return [](ProcessedNode* p_node) {\\n', '       const auto& in0_t = p_node->Input(0).toTensor();\\n', '       const auto in1_i = p_node->Input(1).toInt();\\n']",
                    "hunk_fix": "@@ -927,6 +987,9 @@ REGISTER_OPERATOR_FUNCTOR(aten::sum, aten_sum, [](Node* n) -> SROperator {\n \n std::function<void(ProcessedNode*)> getNativeOperation(Node* n) {\n   if (n->kind() == c10::Symbol::fromQualString(\"aten::transpose\")) {\n+    if (n->inputs().size() != 3) {\n+      return nullptr;\n+    }\n     return [](ProcessedNode* p_node) {\n       const auto& in0_t = p_node->Input(0).toTensor();\n       const auto in1_i = p_node->Input(1).toInt();\n"
                },
                {
                    "old_start": 934,
                    "old_length": 8,
                    "new_start": 997,
                    "new_length": 10,
                    "hunk_buggy": "['       p_node->Output(0) = at::native::transpose(in0_t, in1_i, in2_i);\\n', '     };\\n', '   } else if (n->kind() == c10::Symbol::fromQualString(\"aten::flatten\")) {\\n', '     return [](ProcessedNode* p_node) {\\n', '-      DCHECK(p_node->inputs().size() == 3);\\n', '       const auto& in0_t = p_node->Input(0).toTensor();\\n', '       const auto in1_i = p_node->Input(1).toInt();\\n', '       const auto in2_i = p_node->Input(2).toInt();\\n']",
                    "hunk_fix": "@@ -934,8 +997,10 @@ std::function<void(ProcessedNode*)> getNativeOperation(Node* n) {\n       p_node->Output(0) = at::native::transpose(in0_t, in1_i, in2_i);\n     };\n   } else if (n->kind() == c10::Symbol::fromQualString(\"aten::flatten\")) {\n+    if (n->inputs().size() != 3) {\n+      return nullptr;\n+    }\n     return [](ProcessedNode* p_node) {\n-      DCHECK(p_node->inputs().size() == 3);\n       const auto& in0_t = p_node->Input(0).toTensor();\n       const auto in1_i = p_node->Input(1).toInt();\n       const auto in2_i = p_node->Input(2).toInt();\n"
                },
                {
                    "old_start": 980,
                    "old_length": 6,
                    "new_start": 1045,
                    "new_length": 9,
                    "hunk_buggy": "['       p_node->Output(0) = std::move(stack[0]);\\n', '     };\\n', '   } else if (n->kind() == c10::Symbol::fromQualString(\"aten::__getitem__\")) {\\n', '     return [](ProcessedNode* p_node) {\\n', '       auto dict = p_node->Input(0).toGenericDict();\\n', '       auto key = p_node->Input(1);\\n']",
                    "hunk_fix": "@@ -980,6 +1045,9 @@ std::function<void(ProcessedNode*)> getNativeOperation(Node* n) {\n       p_node->Output(0) = std::move(stack[0]);\n     };\n   } else if (n->kind() == c10::Symbol::fromQualString(\"aten::__getitem__\")) {\n+    if (n->inputs().size() != 2) {\n+      return nullptr;\n+    }\n     return [](ProcessedNode* p_node) {\n       auto dict = p_node->Input(0).toGenericDict();\n       auto key = p_node->Input(1);\n"
                },
                {
                    "old_start": 1024,
                    "old_length": 18,
                    "new_start": 1092,
                    "new_length": 27,
                    "hunk_buggy": "['       }\\n', '     };\\n', '   } else if (n->kind() == c10::Symbol::fromQualString(\"aten::permute\")) {\\n', '     return [](ProcessedNode* p_node) {\\n', '       const auto& in0_t = p_node->Input(0).toTensor();\\n', '       const auto in1_iv = p_node->Input(1).toIntVector();\\n', '       p_node->Output(0) = at::native::permute(in0_t, in1_iv);\\n', '     };\\n', '   } else if (n->kind() == c10::Symbol::fromQualString(\"aten::reshape\")) {\\n', '     return [](ProcessedNode* p_node) {\\n', '       const auto& in0_t = p_node->Input(0).toTensor();\\n', '       const auto in1_iv = p_node->Input(1).toIntVector();\\n', '       p_node->Output(0) = at::native::reshape(in0_t, in1_iv);\\n', '     };\\n', '   } else if (n->kind() == c10::Symbol::fromQualString(\"aten::slice\")) {\\n', '     return [](ProcessedNode* p_node) {\\n', '       const auto& in0_t = p_node->Input(0).toTensor();\\n', '       const auto in1_i = p_node->Input(1).toInt();\\n']",
                    "hunk_fix": "@@ -1024,18 +1092,27 @@ std::function<void(ProcessedNode*)> getNativeOperation(Node* n) {\n       }\n     };\n   } else if (n->kind() == c10::Symbol::fromQualString(\"aten::permute\")) {\n+    if (n->inputs().size() != 2) {\n+      return nullptr;\n+    }\n     return [](ProcessedNode* p_node) {\n       const auto& in0_t = p_node->Input(0).toTensor();\n       const auto in1_iv = p_node->Input(1).toIntVector();\n       p_node->Output(0) = at::native::permute(in0_t, in1_iv);\n     };\n   } else if (n->kind() == c10::Symbol::fromQualString(\"aten::reshape\")) {\n+    if (n->inputs().size() != 2) {\n+      return nullptr;\n+    }\n     return [](ProcessedNode* p_node) {\n       const auto& in0_t = p_node->Input(0).toTensor();\n       const auto in1_iv = p_node->Input(1).toIntVector();\n       p_node->Output(0) = at::native::reshape(in0_t, in1_iv);\n     };\n   } else if (n->kind() == c10::Symbol::fromQualString(\"aten::slice\")) {\n+    if (n->inputs().size() != 5) {\n+      return nullptr;\n+    }\n     return [](ProcessedNode* p_node) {\n       const auto& in0_t = p_node->Input(0).toTensor();\n       const auto in1_i = p_node->Input(1).toInt();\n"
                },
                {
                    "old_start": 1045,
                    "old_length": 6,
                    "new_start": 1122,
                    "new_length": 9,
                    "hunk_buggy": "['       p_node->Output(0) = at::native::slice(in0_t, in1_i, in2_i, in3_i, in4_i);\\n', '     };\\n', '   } else if (n->kind() == c10::Symbol::fromQualString(\"aten::narrow\")) {\\n', '     return [](ProcessedNode* p_node) {\\n', '       const auto& self = p_node->Input(0).toTensor(); // self\\n', '       const auto dim = p_node->Input(1).toInt(); // dim\\n']",
                    "hunk_fix": "@@ -1045,6 +1122,9 @@ std::function<void(ProcessedNode*)> getNativeOperation(Node* n) {\n       p_node->Output(0) = at::native::slice(in0_t, in1_i, in2_i, in3_i, in4_i);\n     };\n   } else if (n->kind() == c10::Symbol::fromQualString(\"aten::narrow\")) {\n+    if (n->inputs().size() != 4) {\n+      return nullptr;\n+    }\n     return [](ProcessedNode* p_node) {\n       const auto& self = p_node->Input(0).toTensor(); // self\n       const auto dim = p_node->Input(1).toInt(); // dim\n"
                },
                {
                    "old_start": 1105,
                    "old_length": 13,
                    "new_start": 1185,
                    "new_length": 11,
                    "hunk_buggy": "['     aten::embedding_bag,\\n', '     aten_embedding_bag,\\n', '     [](Node* n) -> SROperator {\\n', '       return [](ProcessedNode* p_node) {\\n', '-        // TODO: Support only 9 args once the old signature has been removed.\\n', '-        TORCH_CHECK(\\n', '-            p_node->inputs().size() == 8 || p_node->inputs().size() == 9,\\n', '-            \"Expected number of inputs is 8 or 9, but got \" +\\n', '-                std::to_string(p_node->inputs().size()));\\n', '-\\n', '         const auto& weight = p_node->Input(0).toTensor();\\n', '         const auto& indices = p_node->Input(1).toTensor();\\n', '         const auto& offsets = p_node->Input(2).toTensor();\\n']",
                    "hunk_fix": "@@ -1105,13 +1185,11 @@ REGISTER_OPERATOR_FUNCTOR(\n     aten::embedding_bag,\n     aten_embedding_bag,\n     [](Node* n) -> SROperator {\n+      // TODO: Support only 9 args once the old signature has been removed.\n+      if (n->inputs().size() != 8 && n->inputs().size() != 9) {\n+        return nullptr;\n+      }\n       return [](ProcessedNode* p_node) {\n-        // TODO: Support only 9 args once the old signature has been removed.\n-        TORCH_CHECK(\n-            p_node->inputs().size() == 8 || p_node->inputs().size() == 9,\n-            \"Expected number of inputs is 8 or 9, but got \" +\n-                std::to_string(p_node->inputs().size()));\n-\n         const auto& weight = p_node->Input(0).toTensor();\n         const auto& indices = p_node->Input(1).toTensor();\n         const auto& offsets = p_node->Input(2).toTensor();\n"
                },
                {
                    "old_start": 1220,
                    "old_length": 6,
                    "new_start": 1298,
                    "new_length": 9,
                    "hunk_buggy": "[' });\\n', ' \\n', ' REGISTER_OPERATOR_FUNCTOR(aten::div, aten_div, [](Node* n) -> SROperator {\\n', '   return [](ProcessedNode* p_node) {\\n', '     const auto& in0_t = p_node->Input(0).toTensor();\\n', '     c10::optional<std::string> rounding_mode = c10::nullopt;\\n']",
                    "hunk_fix": "@@ -1220,6 +1298,9 @@ REGISTER_OPERATOR_FUNCTOR(aten::repeat, aten_repeat, [](Node* n) -> SROperator {\n });\n \n REGISTER_OPERATOR_FUNCTOR(aten::div, aten_div, [](Node* n) -> SROperator {\n+  if (n->inputs().size() != 2 && n->inputs().size() != 3) {\n+    return nullptr;\n+  }\n   return [](ProcessedNode* p_node) {\n     const auto& in0_t = p_node->Input(0).toTensor();\n     c10::optional<std::string> rounding_mode = c10::nullopt;\n"
                },
                {
                    "old_start": 1242,
                    "old_length": 6,
                    "new_start": 1323,
                    "new_length": 9,
                    "hunk_buggy": "[' \\n', ' // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)\\n', ' REGISTER_OPERATOR_FUNCTOR(aten::sub, aten_sub, [](Node* n) -> SROperator {\\n', '   return [](ProcessedNode* p_node) {\\n', '     const auto& in0_t = p_node->Input(0).toTensor();\\n', '     const auto alpha = p_node->Input(2).toScalar();\\n']",
                    "hunk_fix": "@@ -1242,6 +1323,9 @@ REGISTER_OPERATOR_FUNCTOR(aten::div, aten_div, [](Node* n) -> SROperator {\n \n // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)\n REGISTER_OPERATOR_FUNCTOR(aten::sub, aten_sub, [](Node* n) -> SROperator {\n+  if (n->inputs().size() != 3) {\n+    return nullptr;\n+  }\n   return [](ProcessedNode* p_node) {\n     const auto& in0_t = p_node->Input(0).toTensor();\n     const auto alpha = p_node->Input(2).toScalar();\n"
                },
                {
                    "old_start": 1264,
                    "old_length": 6,
                    "new_start": 1348,
                    "new_length": 9,
                    "hunk_buggy": "['     aten::clamp_min,\\n', '     aten_clamp_min,\\n', '     [](Node* n) -> SROperator {\\n', '       return [](ProcessedNode* p_node) {\\n', '         const auto& in0_t = p_node->Input(0).toTensor();\\n', '         const auto in1_s = p_node->Input(1).toScalar();\\n']",
                    "hunk_fix": "@@ -1264,6 +1348,9 @@ REGISTER_OPERATOR_FUNCTOR(\n     aten::clamp_min,\n     aten_clamp_min,\n     [](Node* n) -> SROperator {\n+      if (n->inputs().size() != 2) {\n+        return nullptr;\n+      }\n       return [](ProcessedNode* p_node) {\n         const auto& in0_t = p_node->Input(0).toTensor();\n         const auto in1_s = p_node->Input(1).toScalar();\n"
                },
                {
                    "old_start": 1278,
                    "old_length": 6,
                    "new_start": 1365,
                    "new_length": 9,
                    "hunk_buggy": "[' \\n', ' // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)\\n', ' REGISTER_OPERATOR_FUNCTOR(aten::argmin, aten_argmin, [](Node* n) -> SROperator {\\n', '   return [](ProcessedNode* p_node) {\\n', '     const auto& in0_t = p_node->Input(0).toTensor();\\n', '     const auto dim = p_node->Input(1).toOptional<int64_t>();\\n']",
                    "hunk_fix": "@@ -1278,6 +1365,9 @@ REGISTER_OPERATOR_FUNCTOR(\n \n // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)\n REGISTER_OPERATOR_FUNCTOR(aten::argmin, aten_argmin, [](Node* n) -> SROperator {\n+  if (n->inputs().size() != 3) {\n+    return nullptr;\n+  }\n   return [](ProcessedNode* p_node) {\n     const auto& in0_t = p_node->Input(0).toTensor();\n     const auto dim = p_node->Input(1).toOptional<int64_t>();\n"
                },
                {
                    "old_start": 1295,
                    "old_length": 7,
                    "new_start": 1385,
                    "new_length": 11,
                    "hunk_buggy": "['     aten::layer_norm,\\n', '     aten_layer_norm,\\n', '     [](Node* n) -> SROperator {\\n', '       return [](ProcessedNode* p_node) {\\n', '         const auto& input = p_node->Input(0).toTensor();\\n', '         const auto normalized_shape = p_node->Input(1).toIntVector();\\n', '         auto weight_opt = p_node->Input(2).toOptional<at::Tensor>();\\n']",
                    "hunk_fix": "@@ -1295,7 +1385,11 @@ REGISTER_OPERATOR_FUNCTOR(\n     aten::layer_norm,\n     aten_layer_norm,\n     [](Node* n) -> SROperator {\n+      if (n->inputs().size() != 6) {\n+        return nullptr;\n+      }\n       return [](ProcessedNode* p_node) {\n+        // ignore Input(5): `bool cudnn_enable=True`\n         const auto& input = p_node->Input(0).toTensor();\n         const auto normalized_shape = p_node->Input(1).toIntVector();\n         auto weight_opt = p_node->Input(2).toOptional<at::Tensor>();\n"
                },
                {
                    "old_start": 1410,
                    "old_length": 6,
                    "new_start": 1504,
                    "new_length": 9,
                    "hunk_buggy": "[' \\n', ' // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)\\n', ' REGISTER_OPERATOR_FUNCTOR(aten::matmul, aten_matmul, [](Node* n) -> SROperator {\\n', '   return [](ProcessedNode* p_node) {\\n', '     const auto& in0_t = p_node->Input(0).toTensor();\\n', '     const auto& in1_t = p_node->Input(1).toTensor();']",
                    "hunk_fix": "@@ -1410,6 +1504,9 @@ REGISTER_OPERATOR_FUNCTOR(aten::norm, aten_norm, [](Node* n) -> SROperator {\n \n // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)\n REGISTER_OPERATOR_FUNCTOR(aten::matmul, aten_matmul, [](Node* n) -> SROperator {\n+  if (n->inputs().size() != 2) {\n+    return nullptr;\n+  }\n   return [](ProcessedNode* p_node) {\n     const auto& in0_t = p_node->Input(0).toTensor();\n     const auto& in1_t = p_node->Input(1).toTensor();"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 448,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/603097be18824a33069addec7b8f14ba5c3bc67a",
    "date": "2021-03-15T21:53:05-07:00",
    "message": "OneDNN MaxPooling: reduce memory use for inference path (#52728)\n\nSummary:\nFor OneDNN MaxPooling training, it will save indices as a workspace for backward, but for inference, indices are not necessary, this PR will make check to avoid saving indices to reduce memory use for inference path.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52728\n\nReviewed By: jbschlosser\n\nDifferential Revision: D27062435\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 9e70268a8ba491a7914b980079c0945d753cd4f3",
    "changes": [
        {
            "name": "Pooling.cpp",
            "path": "aten/src/ATen/native/mkldnn/Pooling.cpp",
            "patches": [
                {
                    "old_start": 1,
                    "old_length": 6,
                    "new_start": 1,
                    "new_length": 7,
                    "hunk_buggy": "[' #include <ATen/ATen.h>\\n', ' #include <ATen/Config.h>\\n', ' #include <ATen/NativeFunctions.h>\\n', ' #include <ATen/native/utils/ParamUtils.h>\\n', ' #include <tuple>\\n', ' \\n']",
                    "hunk_fix": "@@ -1,6 +1,7 @@\n #include <ATen/ATen.h>\n #include <ATen/Config.h>\n #include <ATen/NativeFunctions.h>\n+#include <ATen/core/grad_mode.h>\n #include <ATen/native/utils/ParamUtils.h>\n #include <tuple>\n \n"
                },
                {
                    "old_start": 249,
                    "old_length": 6,
                    "new_start": 250,
                    "new_length": 15,
                    "hunk_buggy": "['         false /*ceil_mode */);\\n', '   }\\n', ' \\n', '   ideep::tensor y;\\n', '   ideep::pooling_forward::compute(\\n', '       x,\\n']",
                    "hunk_fix": "@@ -249,6 +250,15 @@ static Tensor _mkldnn_pooling(\n         false /*ceil_mode */);\n   }\n \n+  auto aprop_kind = ideep::prop_kind::forward;\n+  // for max_pool, prop_kind::forward will save indices as workspace for backward use,\n+  // for inference, don't need the indices, set aprop_kind to prop_kind::forward_inference\n+  // can reduce the memory use.\n+  if (ideep::algorithm::pooling_max == algo\n+      && !(input.requires_grad() && at::GradMode::is_enabled())) {\n+    aprop_kind = ideep::prop_kind::forward_inference;\n+  }\n+\n   ideep::tensor y;\n   ideep::pooling_forward::compute(\n       x,\n"
                },
                {
                    "old_start": 259,
                    "old_length": 7,
                    "new_start": 269,
                    "new_length": 7,
                    "hunk_buggy": "['       {padding_vec_l.cbegin(), padding_vec_l.cend()},\\n', '       {padding_vec_r.cbegin(), padding_vec_r.cend()},\\n', '       algo,\\n', '-      ideep::prop_kind::forward);\\n', ' \\n', '   return new_with_itensor_mkldnn(std::move(y), optTypeMetaToScalarType(input.options().dtype_opt()), input.options().device_opt());\\n', ' }']",
                    "hunk_fix": "@@ -259,7 +269,7 @@ static Tensor _mkldnn_pooling(\n       {padding_vec_l.cbegin(), padding_vec_l.cend()},\n       {padding_vec_r.cbegin(), padding_vec_r.cend()},\n       algo,\n-      ideep::prop_kind::forward);\n+      aprop_kind);\n \n   return new_with_itensor_mkldnn(std::move(y), optTypeMetaToScalarType(input.options().dtype_opt()), input.options().device_opt());\n }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 449,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/0584fd9339af7c939ab7d955db05743ba58ff86d",
    "date": "2021-03-10T16:51:36-08:00",
    "message": "[quant][fx][graphmode][fix] Only insert observers for fixed qparam ops (#53330)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/53330\n\nFixed a condition check for fixed qparam ops, previously we were including CopyNodes as well\n\nTest Plan:\npython test/test_quantization.py TestQuantizeFxOps.test_fixed_qparams_ops_fp16\n\nImported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D26836867\n\nfbshipit-source-id: 8c486155244f852e675a938c3f4237f26505671c",
    "changes": [
        {
            "name": "quantize.py",
            "path": "torch/quantization/fx/quantize.py",
            "patches": [
                {
                    "old_start": 207,
                    "old_length": 7,
                    "new_start": 207,
                    "new_length": 10,
                    "hunk_buggy": "['                 elif isinstance(input_arg, list):\\n', '                     return all(map(is_observed, input_arg))\\n', ' \\n', '-            if activation_dtype(qconfig) == torch.float16:\\n', '                 insert_observer(\\n', '                     node, qconfig.activation(),\\n', '                     model, activation_post_process_map, env, observed_graph,']",
                    "hunk_fix": "@@ -207,7 +207,10 @@ def insert_observer_for_output_of_the_node(\n                 elif isinstance(input_arg, list):\n                     return all(map(is_observed, input_arg))\n \n-            if activation_dtype(qconfig) == torch.float16:\n+            # insert observers for fixedqparams ops like sigmoid, since\n+            # it supports fp16 static quantization\n+            if isinstance(quantize_handler, FixedQParamsOpQuantizeHandler) and \\\n+               activation_dtype(qconfig) == torch.float16:\n                 insert_observer(\n                     node, qconfig.activation(),\n                     model, activation_post_process_map, env, observed_graph,"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 450,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/13121598efc7d86cb7ae6e05322bb95c1d0f16bc",
    "date": "2021-02-25T08:44:30-08:00",
    "message": "[Pytorch, sparsity] Bug fix to update requantization and zp parameters of input (#52797)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52797\n\nAlso sneaking in change to check for realloc failure for packed activation buffer\n\nFB:\nIn dynamic quantization input's quantization scale and zero point can be\ndifferent on every iterations. Thus requantization scale needs to be\nrecomputed.\n\nEarlier bug that calculated those only at op creation time results in wrong\nresults on subsequent runs.\n\nThis diff fixes that.\n\nTest Plan:\nFB:\nbuck test caffe2/torch/fb/model_optimization:sparsity_test\n\nReviewed By: z-a-f, jiatongzhou\n\nDifferential Revision: D26651968\n\nfbshipit-source-id: e5b9acef03fc45f31c43d88a175f3a64f7dbf4bd",
    "changes": [
        {
            "name": "operator-run.c",
            "path": "aten/src/ATen/native/quantized/cpu/qnnpack/src/operator-run.c",
            "patches": [
                {
                    "old_start": 1045,
                    "old_length": 6,
                    "new_start": 1045,
                    "new_length": 12,
                    "hunk_buggy": "['       const size_t m_stride = (output_size + (mr - 1)) & -mr;\\n', '       op->prepacked_a =\\n', '         (uint8_t*)realloc((void*)op->prepacked_a, k_stride * m_stride);\\n', ' \\n', '       struct q8gemm_prepackA_sparse_dq_context\\n', '         q8gemm_prepack_sparse_dq_context = {']",
                    "hunk_fix": "@@ -1045,6 +1045,12 @@ enum pytorch_qnnp_status pytorch_qnnp_run_operator(\n       const size_t m_stride = (output_size + (mr - 1)) & -mr;\n       op->prepacked_a =\n         (uint8_t*)realloc((void*)op->prepacked_a, k_stride * m_stride);\n+      if (op->prepacked_a == NULL) {\n+        pytorch_qnnp_log_error(\n+            \"failed to allocate %zu bytes for packed activation buffer\",\n+            (k_stride * m_stride));\n+        return pytorch_qnnp_status_out_of_memory;\n+      }\n \n       struct q8gemm_prepackA_sparse_dq_context\n         q8gemm_prepack_sparse_dq_context = {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 451,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/c9548176965557a76526ba0db23ff5c9facd3e97",
    "date": "2021-02-24T20:09:25-08:00",
    "message": "print matrix dims in torch cuda matrix multiply error (#52780)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/52780\n\ntrying to improve the error message for torch matrix multiply dimension mismatch\n\nTest Plan: check if code compiles\n\nReviewed By: akyrola\n\nDifferential Revision: D26617036\n\nfbshipit-source-id: de23e551af985a00384fb1cccd04120b9d2728b3",
    "changes": [
        {
            "name": "LinearAlgebra.cu",
            "path": "aten/src/ATen/native/cuda/LinearAlgebra.cu",
            "patches": [
                {
                    "old_start": 78,
                    "old_length": 7,
                    "new_start": 78,
                    "new_length": 13,
                    "hunk_buggy": "['   IntArrayRef mat1_sizes = mat1.sizes();\\n', '   IntArrayRef mat2_sizes = mat2.sizes();\\n', '   IntArrayRef self__sizes = self_.sizes();\\n', '-  TORCH_CHECK(mat1_sizes[1] == mat2_sizes[0], \"mat1 dim 1 must match mat2 dim 0\");\\n', '   TORCH_CHECK(self__sizes[0] == mat1_sizes[0], \"self_ dim 0 must match mat1 dim 0\");\\n', '   TORCH_CHECK(self__sizes[1] == mat2_sizes[1], \"self_ dim 1 must match mat2 dim 1\");\\n', ' ']",
                    "hunk_fix": "@@ -78,7 +78,13 @@ Tensor& addmm_out_cuda_impl(Tensor& result, const Tensor& self, const Tensor& ma\n   IntArrayRef mat1_sizes = mat1.sizes();\n   IntArrayRef mat2_sizes = mat2.sizes();\n   IntArrayRef self__sizes = self_.sizes();\n-  TORCH_CHECK(mat1_sizes[1] == mat2_sizes[0], \"mat1 dim 1 must match mat2 dim 0\");\n+  TORCH_CHECK(\n+      mat1_sizes[1] == mat2_sizes[0],\n+      \"mat1 dim 1 must match mat2 dim 0\",\n+      \" mat1 dim1:\",\n+      mat1_sizes[1],\n+      \" mat2 dim0: \",\n+      mat2_sizes[0]);\n   TORCH_CHECK(self__sizes[0] == mat1_sizes[0], \"self_ dim 0 must match mat1 dim 0\");\n   TORCH_CHECK(self__sizes[1] == mat2_sizes[1], \"self_ dim 1 must match mat2 dim 1\");\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 452,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/5a962369e2b527b36a737723df1fe9c180aa2925",
    "date": "2021-02-05T09:59:12-08:00",
    "message": "[Gradient Compression] Check if the backend is NCCL when a DDP communication hook is registered (#51759)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/51759\n\nSome unit tests actually register a comm hook on other backends like GLOO. Example: `test_ddp_comm_hook_future_passing_cpu`\n\nTherefore, only do the check on `register_builtin_comm_hook`.\n\nCurrently DDP communication hook can only be supported on NCCL. Add a check in the registration methods.\nghstack-source-id: 121115814\n\nTest Plan: unit tests.\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D26268581\n\nfbshipit-source-id: c739fa4dca6d320202dc6689d790c2761c834c30",
    "changes": [
        {
            "name": "reducer.cpp",
            "path": "torch/lib/c10d/reducer.cpp",
            "patches": [
                {
                    "old_start": 1041,
                    "old_length": 14,
                    "new_start": 1041,
                    "new_length": 13,
                    "hunk_buggy": "['   // Warn user about unnecessary perf hit if all parameters were used.\\n', '   if (unused_parameters_.empty()) {\\n', '     TORCH_WARN_ONCE(\\n', '-      \"find_unused_parameters=True was specified in DDP constructor, \"\\n', '-      \"but did not find any unused parameters. This flag results in an extra \"\\n', '-      \"traversal of the autograd graph every iteration, which can adversely \"\\n', '-      \"affect performance. If your model indeed never has any unused \"\\n', '-      \"parameters, consider turning this flag off. Note that this warning may \"\\n', '-      \"be a false positive your model has flow control causing later iterations \"\\n', '-      \"to have unused parameters.\"\\n', '-    );\\n', '   }\\n', ' }\\n', ' \\n']",
                    "hunk_fix": "@@ -1041,14 +1041,13 @@ void Reducer::prepare_for_backward(\n   // Warn user about unnecessary perf hit if all parameters were used.\n   if (unused_parameters_.empty()) {\n     TORCH_WARN_ONCE(\n-      \"find_unused_parameters=True was specified in DDP constructor, \"\n-      \"but did not find any unused parameters. This flag results in an extra \"\n-      \"traversal of the autograd graph every iteration, which can adversely \"\n-      \"affect performance. If your model indeed never has any unused \"\n-      \"parameters, consider turning this flag off. Note that this warning may \"\n-      \"be a false positive your model has flow control causing later iterations \"\n-      \"to have unused parameters.\"\n-    );\n+        \"find_unused_parameters=True was specified in DDP constructor, \"\n+        \"but did not find any unused parameters. This flag results in an extra \"\n+        \"traversal of the autograd graph every iteration, which can adversely \"\n+        \"affect performance. If your model indeed never has any unused \"\n+        \"parameters, consider turning this flag off. Note that this warning may \"\n+        \"be a false positive your model has flow control causing later iterations \"\n+        \"to have unused parameters.\");\n   }\n }\n \n"
                },
                {
                    "old_start": 1403,
                    "old_length": 8,
                    "new_start": 1402,
                    "new_length": 8,
                    "hunk_buggy": "['   TORCH_CHECK(\\n', '       comm_hook_ == nullptr,\\n', '       \"register_comm_hook or register_builtin_comm_hook can only be called once.\");\\n', '-  // TODO(@sinannasir): Single-process multiple-device mode support for DDP\\n', '-  // communication hook. Related to GH Issue #42542.\\n', '   TORCH_CHECK(\\n', '       replicas_.size() == 1,\\n', '       \"Communication hook does not support single-process multiple-device mode.\");\\n']",
                    "hunk_fix": "@@ -1403,8 +1402,8 @@ void Reducer::register_comm_hook(std::unique_ptr<CommHookInterface> iface) {\n   TORCH_CHECK(\n       comm_hook_ == nullptr,\n       \"register_comm_hook or register_builtin_comm_hook can only be called once.\");\n-  // TODO(@sinannasir): Single-process multiple-device mode support for DDP\n-  // communication hook. Related to GH Issue #42542.\n+  // TODO(#42542): Single-process multiple-device mode support for DDP\n+  // communication hook.\n   TORCH_CHECK(\n       replicas_.size() == 1,\n       \"Communication hook does not support single-process multiple-device mode.\");\n"
                },
                {
                    "old_start": 1421,
                    "old_length": 6,
                    "new_start": 1420,
                    "new_length": 11,
                    "hunk_buggy": "['   TORCH_CHECK(\\n', '       replicas_.size() == 1,\\n', '       \"Communication hook does not support single-process multiple-device mode.\");\\n', ' \\n', '   switch (comm_hook_type) {\\n', '     case c10d::BuiltinCommHookType::ALLREDUCE:\\n']",
                    "hunk_fix": "@@ -1421,6 +1420,11 @@ void Reducer::register_builtin_comm_hook(\n   TORCH_CHECK(\n       replicas_.size() == 1,\n       \"Communication hook does not support single-process multiple-device mode.\");\n+  // TODO: Support GLOO and MPI backends for DDP communication hook.\n+  TORCH_CHECK(\n+      process_group_->getBackendName() == \"nccl\",\n+      \"register_builtin_comm_hook currently can only support NCCL backend, but the current backend is %s.\",\n+      process_group_->getBackendName());\n \n   switch (comm_hook_type) {\n     case c10d::BuiltinCommHookType::ALLREDUCE:\n"
                },
                {
                    "old_start": 1467,
                    "old_length": 11,
                    "new_start": 1471,
                    "new_length": 10,
                    "hunk_buggy": "[' }\\n', ' \\n', ' void Reducer::set_construction_logging_data(\\n', '-  const std::string& module_name,\\n', '-  const std::vector<int>& device_ids,\\n', '-  int output_device,\\n', '-  bool broadcast_buffers\\n', '-) {\\n', '   ddp_logging_data_->module_name = module_name;\\n', '   ddp_logging_data_->device_ids = device_ids;\\n', '   ddp_logging_data_->output_device = output_device;']",
                    "hunk_fix": "@@ -1467,11 +1471,10 @@ void Reducer::ensure_prior_reduction_finished() {\n }\n \n void Reducer::set_construction_logging_data(\n-  const std::string& module_name,\n-  const std::vector<int>& device_ids,\n-  int output_device,\n-  bool broadcast_buffers\n-) {\n+    const std::string& module_name,\n+    const std::vector<int>& device_ids,\n+    int output_device,\n+    bool broadcast_buffers) {\n   ddp_logging_data_->module_name = module_name;\n   ddp_logging_data_->device_ids = device_ids;\n   ddp_logging_data_->output_device = output_device;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 453,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/6420071b43dc9f2679c22952b5051b0c28f42da2",
    "date": "2021-01-12T07:55:18-08:00",
    "message": "Disable complex dispatch on min/max functions (#50347)\n\nSummary:\nFixes https://github.com/pytorch/pytorch/issues/50064\n\n**PROBLEM:**\nIn issue https://github.com/pytorch/pytorch/issues/36377, min/max functions were disabled for complex inputs (via dtype checks).\nHowever, min/max kernels are still being compiled and dispatched for complex.\n\n**FIX:**\nThe aforementioned dispatch has been disabled & we now rely on errors produced\nby dispatch macro to not run those ops on complex, instead of doing redundant dtype checks.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/50347\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D25870385\n\nPulled By: anjali411\n\nfbshipit-source-id: 921541d421c509b7a945ac75f53718cd44e77df1",
    "changes": [
        {
            "name": "ReduceAllOpsKernel.cpp",
            "path": "aten/src/ATen/native/cpu/ReduceAllOpsKernel.cpp",
            "patches": [
                {
                    "old_start": 74,
                    "old_length": 7,
                    "new_start": 74,
                    "new_length": 7,
                    "hunk_buggy": "['     reduce_all_impl<int64_t>(result, input, upper_bound<int64_t>(),\\n', '       [=](int64_t a, int64_t b) -> int64_t { return min_impl(a, b); });\\n', '   } else {\\n', '-    AT_DISPATCH_ALL_TYPES_AND_COMPLEX(input.scalar_type(), \"min_all\", [&] {\\n', '       using Vec = vec256::Vec256<scalar_t>;\\n', '       reduce_all_impl_vec<scalar_t>(result, input, upper_bound<scalar_t>(),\\n', '         [=] (scalar_t a , scalar_t b) -> scalar_t { return min_impl(a, b); },\\n']",
                    "hunk_fix": "@@ -74,7 +74,7 @@ static void min_all_kernel_impl(Tensor& result, const Tensor& input) {\n     reduce_all_impl<int64_t>(result, input, upper_bound<int64_t>(),\n       [=](int64_t a, int64_t b) -> int64_t { return min_impl(a, b); });\n   } else {\n-    AT_DISPATCH_ALL_TYPES_AND_COMPLEX(input.scalar_type(), \"min_all\", [&] {\n+    AT_DISPATCH_ALL_TYPES(input.scalar_type(), \"min_all\", [&] {\n       using Vec = vec256::Vec256<scalar_t>;\n       reduce_all_impl_vec<scalar_t>(result, input, upper_bound<scalar_t>(),\n         [=] (scalar_t a , scalar_t b) -> scalar_t { return min_impl(a, b); },\n"
                },
                {
                    "old_start": 99,
                    "old_length": 7,
                    "new_start": 99,
                    "new_length": 7,
                    "hunk_buggy": "['     reduce_all_impl<int64_t>(result, input, lower_bound<int64_t>(),\\n', '       [=](int64_t a, int64_t b) -> int64_t { return max_impl(a, b); });\\n', '   } else {\\n', '-    AT_DISPATCH_ALL_TYPES_AND_COMPLEX(input.scalar_type(), \"max_all\", [&] {\\n', '       using Vec = vec256::Vec256<scalar_t>;\\n', '       reduce_all_impl_vec<scalar_t>(result, input, lower_bound<scalar_t>(),\\n', '         [=] (scalar_t a , scalar_t b) -> scalar_t { return max_impl(a, b); },\\n']",
                    "hunk_fix": "@@ -99,7 +99,7 @@ static void max_all_kernel_impl(Tensor& result, const Tensor& input) {\n     reduce_all_impl<int64_t>(result, input, lower_bound<int64_t>(),\n       [=](int64_t a, int64_t b) -> int64_t { return max_impl(a, b); });\n   } else {\n-    AT_DISPATCH_ALL_TYPES_AND_COMPLEX(input.scalar_type(), \"max_all\", [&] {\n+    AT_DISPATCH_ALL_TYPES(input.scalar_type(), \"max_all\", [&] {\n       using Vec = vec256::Vec256<scalar_t>;\n       reduce_all_impl_vec<scalar_t>(result, input, lower_bound<scalar_t>(),\n         [=] (scalar_t a , scalar_t b) -> scalar_t { return max_impl(a, b); },\n"
                },
                {
                    "old_start": 193,
                    "old_length": 7,
                    "new_start": 193,
                    "new_length": 7,
                    "hunk_buggy": "['       }\\n', '     );\\n', '   } else {\\n', '-    AT_DISPATCH_ALL_TYPES_AND_COMPLEX(input.scalar_type(), \"_aminmax_all_all\", [&] {\\n', '       using Vec = vec256::Vec256<scalar_t>;\\n', '       using scalar_t_pair = std::pair<scalar_t, scalar_t>;\\n', '       reduce_all_impl_vec_two_outputs<scalar_t>(']",
                    "hunk_fix": "@@ -193,7 +193,7 @@ static void _aminmax_all_kernel_impl(Tensor& min_result, Tensor& max_result,\n       }\n     );\n   } else {\n-    AT_DISPATCH_ALL_TYPES_AND_COMPLEX(input.scalar_type(), \"_aminmax_all_all\", [&] {\n+    AT_DISPATCH_ALL_TYPES(input.scalar_type(), \"_aminmax_all_all\", [&] {\n       using Vec = vec256::Vec256<scalar_t>;\n       using scalar_t_pair = std::pair<scalar_t, scalar_t>;\n       reduce_all_impl_vec_two_outputs<scalar_t>("
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 454,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/5a63c452e638dad8e077887ad8d2c94ff0e23917",
    "date": "2021-01-07T21:20:21-08:00",
    "message": "Disable cuDNN persistent RNN on sm_86 devices (#49534)\n\nSummary:\nExcludes sm_86 GPU devices from using cuDNN persistent RNN.\n\nThis is because there are some hard-to-detect edge cases that will throw exceptions with cudnn 8.0.5 on Nvidia A40 GPU.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/49534\n\nReviewed By: mruberry\n\nDifferential Revision: D25632378\n\nPulled By: mrshenli\n\nfbshipit-source-id: cbe78236d85d4d0c2e4ca63a3fc2c4e2de662d9e",
    "changes": [
        {
            "name": "RNN.cpp",
            "path": "aten/src/ATen/native/cudnn/RNN.cpp",
            "patches": [
                {
                    "old_start": 722,
                    "old_length": 6,
                    "new_start": 722,
                    "new_length": 11,
                    "hunk_buggy": "['                 (tensors.seq_length >=10 && bsize <=32));\\n', '       }\\n', '     } else if (prop->major >= 8) {\\n', '       // Based on tests by Vasily Volkov and xwang233.  Vasily only tried bsize <= 128,\\n', '       // so conservatively enable persistence for bsize <= 128 only.\\n', '       // TODO:  Run more tests for bsize > 128.']",
                    "hunk_fix": "@@ -722,6 +722,11 @@ namespace {\n                 (tensors.seq_length >=10 && bsize <=32));\n       }\n     } else if (prop->major >= 8) {\n+      if (prop->minor == 6) {\n+        // Excludes sm_86 GPU devices from using persistent rnn.\n+        // This is because there are some edge cases that will throw exceptions with cudnn 8.0.5 on Nvidia A40 GPU.\n+        return false;\n+      }\n       // Based on tests by Vasily Volkov and xwang233.  Vasily only tried bsize <= 128,\n       // so conservatively enable persistence for bsize <= 128 only.\n       // TODO:  Run more tests for bsize > 128."
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 455,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/e3542d2c12d8aaaccf8a53873e480c20dc6b7338",
    "date": "2020-12-11T13:57:57-08:00",
    "message": "[PyTorch] avoid unnecessary call to empty_tensor_restride in empty() (#48211)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/48211\n\nOur empty benchmark makes this call unconditionally. If\nMemoryFormat::Contiguous is indeed a common case (or if workloads are\nlikely to use a consistent-ish memory format), then I'd expect\nchecking first to be a win.\nghstack-source-id: 118224990\n\nTest Plan:\nProfiled empty benchmark with perf, saw time spent in empty_tensor_restride go down.\n\nRan framework overhead benchmarks. ~7% win on empty(), 0.5-1.5% regression on InPlace, ~2% win on OutOfPlace. Seems like both the In/Out of place ones are likely to be noise because they don't exercise empty?\n\nReviewed By: bhosmer\n\nDifferential Revision: D24914706\n\nfbshipit-source-id: 916771b335143f9b4ec9fae0d8118222ab6e8659",
    "changes": [
        {
            "name": "Utils.cpp",
            "path": "aten/src/ATen/Utils.cpp",
            "patches": [
                {
                    "old_start": 57,
                    "old_length": 8,
                    "new_start": 57,
                    "new_length": 12,
                    "hunk_buggy": "['     tensor.unsafeGetTensorImpl()->set_sizes_contiguous(size);\\n', '   }\\n', ' \\n', '-  auto memory_format = memory_format_opt.value_or(MemoryFormat::Contiguous);\\n', '-  tensor.unsafeGetTensorImpl()->empty_tensor_restride(memory_format);\\n', ' \\n', '   return tensor;\\n', ' }']",
                    "hunk_fix": "@@ -57,8 +57,12 @@ Tensor empty_cpu(\n     tensor.unsafeGetTensorImpl()->set_sizes_contiguous(size);\n   }\n \n-  auto memory_format = memory_format_opt.value_or(MemoryFormat::Contiguous);\n-  tensor.unsafeGetTensorImpl()->empty_tensor_restride(memory_format);\n+  if (memory_format_opt.has_value()) {\n+    // Restriding a just-created empty contiguous tensor does nothing.\n+    if (*memory_format_opt != MemoryFormat::Contiguous) {\n+      tensor.unsafeGetTensorImpl()->empty_tensor_restride(*memory_format_opt);\n+    }\n+  }\n \n   return tensor;\n }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 456,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/1c02be1b6a0f6d02d3a0ae19c13d51a3e59a55ae",
    "date": "2020-12-01T14:55:18-08:00",
    "message": "Fix AttributeError in _get_device_attr (#48406)\n\nSummary:\nIn PyTorch 1.5, when running `torch.cuda.reset_peak_memory_stats()` on a machine where `torch.cuda.is_available() is False`, I would get:\n```\nAssertionError:\nFound no NVIDIA driver on your system. Please check that you\nhave an NVIDIA GPU and installed a driver from\nhttp://www.nvidia.com/Download/index.aspx\n```\n\nIn PyTorch 1.7, the same gets me a worse error (and a user warning about missing NVIDIA drivers if you look for it):\n```\n...\n  File \"/opt/conda/lib/python3.7/site-packages/torch/_utils.py\", line 440, in _get_device_attr\n    if device_type.lower() == \"cuda\":\nAttributeError: 'NoneType' object has no attribute 'lower'\n```\n\nThe formerly raised AssertionError is depended on by libraries like pytorch_memlab: https://github.com/Stonesjtu/pytorch_memlab/blob/ec9a72fc302981ddc3ee56d6e16694610d646c36/pytorch_memlab/line_profiler/line_profiler.py#L90\nIt would be pretty gross if pytorch_memlab had to change that to catch an AttributeError.\n\nWith this patch, we get a more sensible:\n```\n...\n  File \"/opt/conda/lib/python3.7/site-packages/torch/cuda/memory.py\", line 209, in reset_peak_memory_stats\n    return torch._C._cuda_resetPeakMemoryStats(device)\nRuntimeError: invalid argument to reset_peak_memory_stats\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/48406\n\nReviewed By: mrshenli\n\nDifferential Revision: D25205630\n\nPulled By: ngimel\n\nfbshipit-source-id: 7c505a6500d730f3a2da348020e2a7a5e1306dcb",
    "changes": [
        {
            "name": "_utils.py",
            "path": "torch/_utils.py",
            "patches": [
                {
                    "old_start": 437,
                    "old_length": 7,
                    "new_start": 437,
                    "new_length": 7,
                    "hunk_buggy": "[' \\n', ' def _get_device_attr(get_member):\\n', '     device_type = _get_available_device_type()\\n', '-    if device_type.lower() == \"cuda\":\\n', '         return get_member(torch.cuda)\\n', '     # add more available device types here\\n', '     return None']",
                    "hunk_fix": "@@ -437,7 +437,7 @@ def _get_available_device_type():\n \n def _get_device_attr(get_member):\n     device_type = _get_available_device_type()\n-    if device_type.lower() == \"cuda\":\n+    if device_type and device_type.lower() == \"cuda\":\n         return get_member(torch.cuda)\n     # add more available device types here\n     return None"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 457,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/3611d26a25bd889627403a808ea667ac99c09904",
    "date": "2020-11-16T20:50:24-08:00",
    "message": "[JIT] Optimize FunctionSchema::checkArg for the Tensor case. (#48034)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/48034\n\nThe Tensor case is one of the most common and the existing check can be\nmade faster. This results in a ~21% improvement on DeepAndWide model and\nwould improve other models as well.\n\nBefore the change:\n```\n505[ms]\n491[ms]\n514[ms]\n538[ms]\n514[ms]\n554[ms]\n556[ms]\n512[ms]\n516[ms]\n527[ms]\n```\n\nAfter the change:\n```\n406[ms]\n394[ms]\n414[ms]\n423[ms]\n449[ms]\n397[ms]\n410[ms]\n389[ms]\n395[ms]\n414[ms]\n```\n\nDifferential Revision: D24999486\n\nTest Plan: Imported from OSS\n\nReviewed By: zdevito\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 7139a3a38f9c44e8ea793afe2fc662ff51cc0460",
    "changes": [
        {
            "name": "function_schema_inl.h",
            "path": "aten/src/ATen/core/function_schema_inl.h",
            "patches": [
                {
                    "old_start": 151,
                    "old_length": 6,
                    "new_start": 151,
                    "new_length": 10,
                    "hunk_buggy": "['     const IValue& value,\\n', '     const Argument& argument,\\n', '     optional<size_t> pos) const {\\n', '   if (!value.type()->isSubtypeOf(argument.type())) {\\n', '     TORCH_CHECK(\\n', '         false,']",
                    "hunk_fix": "@@ -151,6 +151,10 @@ inline void FunctionSchema::checkArg(\n     const IValue& value,\n     const Argument& argument,\n     optional<size_t> pos) const {\n+  if (value.isTensor() && argument.type() == TensorType::get()) {\n+    // Fast-path for the common case\n+    return;\n+  }\n   if (!value.type()->isSubtypeOf(argument.type())) {\n     TORCH_CHECK(\n         false,"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 458,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/611080a118fff166c85f3200d860f3b059abac6f",
    "date": "2020-11-06T10:34:25-08:00",
    "message": "[hot fix] cuda 11.0.x doesn't support sm86. (#47408)\n\nSummary:\nBump condition check from >11.0 to >11.0.3\n\nCMAKE 3.5 doesn't support VERSION_GREATER_EQUAL see [here](https://github.com/Dav1dde/glad/issues/134), so we might need to bump this again iv 11.0.4+ releases.\n\nshould fix https://github.com/pytorch/pytorch/issues/47352\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/47408\n\nReviewed By: glaringlee\n\nDifferential Revision: D24759949\n\nPulled By: walterddr\n\nfbshipit-source-id: de384c7b150babaf799cce53ed198e5e931899da",
    "changes": [
        {
            "name": "select_compute_arch.cmake",
            "path": "cmake/Modules_CUDA_fix/upstream/FindCUDA/select_compute_arch.cmake",
            "patches": [
                {
                    "old_start": 88,
                    "old_length": 14,
                    "new_start": 88,
                    "new_length": 15,
                    "hunk_buggy": "['   list(APPEND CUDA_ALL_GPU_ARCHITECTURES \"8.0\")\\n', ' \\n', '   if(CUDA_VERSION VERSION_LESS \"11.1\")\\n', '-    set(CUDA_LIMIT_GPU_ARCHITECTURE \"8.6\")\\n', '     list(APPEND CUDA_COMMON_GPU_ARCHITECTURES \"8.0+PTX\")\\n', '   endif()\\n', ' endif()\\n', ' \\n', '-if(CUDA_VERSION VERSION_GREATER \"11.0\")\\n', '   list(APPEND CUDA_COMMON_GPU_ARCHITECTURES \"8.6\" \"8.6+PTX\")\\n', '   list(APPEND CUDA_ALL_GPU_ARCHITECTURES \"8.6\")\\n', ' \\n', '   if(CUDA_VERSION VERSION_LESS \"12.0\")\\n', '     set(CUDA_LIMIT_GPU_ARCHITECTURE \"9.0\")']",
                    "hunk_fix": "@@ -88,14 +88,15 @@ if(CUDA_VERSION VERSION_GREATER \"10.5\")\n   list(APPEND CUDA_ALL_GPU_ARCHITECTURES \"8.0\")\n \n   if(CUDA_VERSION VERSION_LESS \"11.1\")\n-    set(CUDA_LIMIT_GPU_ARCHITECTURE \"8.6\")\n+    set(CUDA_LIMIT_GPU_ARCHITECTURE \"8.0\")\n     list(APPEND CUDA_COMMON_GPU_ARCHITECTURES \"8.0+PTX\")\n   endif()\n endif()\n \n-if(CUDA_VERSION VERSION_GREATER \"11.0\")\n+if(NOT CUDA_VERSION VERSION_LESS \"11.1\")\n   list(APPEND CUDA_COMMON_GPU_ARCHITECTURES \"8.6\" \"8.6+PTX\")\n   list(APPEND CUDA_ALL_GPU_ARCHITECTURES \"8.6\")\n+  set(CUDA_LIMIT_GPU_ARCHITECUTRE \"8.6\")\n \n   if(CUDA_VERSION VERSION_LESS \"12.0\")\n     set(CUDA_LIMIT_GPU_ARCHITECTURE \"9.0\")"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 459,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/dc0d68a1ee3800ed4024762d018f85256e80f5ad",
    "date": "2020-11-03T13:07:21-08:00",
    "message": "[JIT] Print out interface mismatch for prim::ModuleDictIndex (#47300)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/47300\n\n**Summary**\nThis commit augments the module interface subtyping check that is done\nbefore the emission of the `prim::ModuleDictIndex` operator so that the\nerror message that is printed if the subtyping check fails provides more\ninformation on which methods do not match.\n\n**Test Plan**\nExisting unit tests for `prim::ModuleDictIndex`. Compilation of `ModWithWrongAnnotation` now produces this error:\n```\nAttribute module is not of annotated type __torch__.jit.test_module_containers.ModuleInterface: Method on class '__torch__.jit.test_module_containers.DoesNotImplementInterface' (1) is not compatible with interface '__torch__.jit.test_module_containers.ModuleInterface' (2)\n  (1) forward(__torch__.jit.test_module_containers.DoesNotImplementInterface self, Tensor inp) -> ((Tensor, Tensor))\n  (2) forward(InterfaceType<ModuleInterface> self, Any inp) -> (Any)\n:\n```\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar\n\nDifferential Revision: D24709538\n\nPulled By: SplitInfinity\n\nfbshipit-source-id: 6b6cb75e4b2b12b08576a5530b4b90cbcad9b6e5",
    "changes": [
        {
            "name": "python_sugared_value.cpp",
            "path": "torch/csrc/jit/python/python_sugared_value.cpp",
            "patches": [
                {
                    "old_start": 260,
                    "old_length": 11,
                    "new_start": 260,
                    "new_length": 13,
                    "hunk_buggy": "['       for (size_t i = 0; i < self_type->numAttributes(); ++i) {\\n', '         const auto& attr_type = self_type->getAttribute(i);\\n', '         if (attr_type->is_module()) {\\n', '-          if (!attr_type->isSubtypeOf(type_hint)) {\\n', '             auto loc = self_->node()->sourceRange();\\n', '             throw ErrorReport(loc)\\n', '                 << \"Attribute \" << self_type->getAttributeName(i)\\n', '-                << \" is not of annotated type \" << type_hint->annotation_str();\\n', '           }\\n', '         }\\n', '       }']",
                    "hunk_fix": "@@ -260,11 +260,13 @@ SugaredValuePtr ModuleValue::getitem(\n       for (size_t i = 0; i < self_type->numAttributes(); ++i) {\n         const auto& attr_type = self_type->getAttribute(i);\n         if (attr_type->is_module()) {\n-          if (!attr_type->isSubtypeOf(type_hint)) {\n+          std::stringstream ss;\n+          if (!attr_type->isSubtypeOfExt(type_hint, &ss)) {\n             auto loc = self_->node()->sourceRange();\n             throw ErrorReport(loc)\n                 << \"Attribute \" << self_type->getAttributeName(i)\n-                << \" is not of annotated type \" << type_hint->annotation_str();\n+                << \" is not of annotated type \" << type_hint->annotation_str()\n+                << \": \" << ss.str();\n           }\n         }\n       }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 460,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/7f125bca1cd42ebd8e07c97f1bd1682dff5cf387",
    "date": "2020-11-02T14:00:12-08:00",
    "message": "[Metal] Add pin_memory check in empty_strided (#47228)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/47228\n\nAdd the false checking if pin_memory has been specified to `False`\nghstack-source-id: 115715087\n\nTest Plan:\n- CircleCI\n- Sandcastle\n\nReviewed By: IvanKobzarev\n\nDifferential Revision: D24690472\n\nfbshipit-source-id: c65fc494fcd7b0b409a80c86e108a029ca7fd71e",
    "changes": [
        {
            "name": "MetalAten.mm",
            "path": "aten/src/ATen/native/metal/MetalAten.mm",
            "patches": [
                {
                    "old_start": 91,
                    "old_length": 7,
                    "new_start": 91,
                    "new_length": 7,
                    "hunk_buggy": "['     optional<Device> device,\\n', '     optional<bool> pin_memory) {\\n', '   TORCH_CHECK(\\n', '-      !pin_memory.has_value(),\\n', '       \"\\'pin_memory\\' argument is incompatible with Metal tensor\");\\n', '   MetalTensor mt{size.vec(), stride.vec()};\\n', '   return MetalTensor::toTensor(']",
                    "hunk_fix": "@@ -91,7 +91,7 @@ at::Tensor empty_strided(\n     optional<Device> device,\n     optional<bool> pin_memory) {\n   TORCH_CHECK(\n-      !pin_memory.has_value(),\n+      !pin_memory.has_value() || !pin_memory.value(),\n       \"'pin_memory' argument is incompatible with Metal tensor\");\n   MetalTensor mt{size.vec(), stride.vec()};\n   return MetalTensor::toTensor("
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 461,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/6c98d904c09b69f1e7748cf3d80e2193df5fff63",
    "date": "2020-09-10T01:18:45-07:00",
    "message": "handle the case of -0.0 on tanh quantization (#44406)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/44406\n\nthis fix makes fakelowp identical to hw\n\n- mask out the floating point number with 0x7fff so we are always dealing\nwith positive numbers\n- dsp implementation is correct, ice-ref suffers from this same problem\n\nTest Plan: - tested with test_fusions.py, can't enable the test until the fix in ice-ref appears\n\nReviewed By: venkatacrc\n\nDifferential Revision: D23603878\n\nfbshipit-source-id: a72d93a4bc811f98d1b5e82ddb204be028addfeb",
    "changes": [
        {
            "name": "quant_lut_fp16_fake_op.h",
            "path": "caffe2/contrib/fakelowp/quant_lut_fp16_fake_op.h",
            "patches": [
                {
                    "old_start": 55,
                    "old_length": 10,
                    "new_start": 55,
                    "new_length": 10,
                    "hunk_buggy": "[' \\n', '     const float* X_data = X.template data<float>();\\n', '     for (int i = 0; i < X.numel(); i++) {\\n', '-        float val = X_data[i];\\n', '-        short shortAbsInput = _cvtss_sh(abs(val), 0);\\n', '-        // Clamp the input in the range of\\n', '-        //  (short)tanhLUTMinOffset to (short)(tanhLUTMaxOffset - 1)\\n', '         short clampShortAbsInput = shortAbsInput;\\n', '         if (shortAbsInput < (short)tanhLUTMinOffset) {\\n', '             clampShortAbsInput = (short)tanhLUTMinOffset;\\n']",
                    "hunk_fix": "@@ -55,10 +55,10 @@ class TanhInt8QuantizeNNPIOp final : public Operator<CPUContext> {\n \n     const float* X_data = X.template data<float>();\n     for (int i = 0; i < X.numel(); i++) {\n-        float val = X_data[i];\n-        short shortAbsInput = _cvtss_sh(abs(val), 0);\n-        // Clamp the input in the range of\n-        //  (short)tanhLUTMinOffset to (short)(tanhLUTMaxOffset - 1)\n+        short val = _cvtss_sh(X_data[i], 0);\n+        unsigned short max16BitPositive = 0x7FFF;\n+        unsigned short input16Bit = (*(unsigned short*)& val);\n+        short shortAbsInput = input16Bit & max16BitPositive; // mask out negative bit\n         short clampShortAbsInput = shortAbsInput;\n         if (shortAbsInput < (short)tanhLUTMinOffset) {\n             clampShortAbsInput = (short)tanhLUTMinOffset;\n"
                },
                {
                    "old_start": 70,
                    "old_length": 7,
                    "new_start": 70,
                    "new_length": 7,
                    "hunk_buggy": "['         short inputInLutRange = clampShortAbsInput - tanhLUTMinOffset;\\n', '         short temp =  tanhLUT[inputInLutRange];\\n', ' \\n', '-        if (val < 0.0) {\\n', '             temp = temp - Y_offset;\\n', '             temp = temp * (-1);\\n', '             temp = temp + Y_offset;']",
                    "hunk_fix": "@@ -70,7 +70,7 @@ class TanhInt8QuantizeNNPIOp final : public Operator<CPUContext> {\n         short inputInLutRange = clampShortAbsInput - tanhLUTMinOffset;\n         short temp =  tanhLUT[inputInLutRange];\n \n-        if (val < 0.0) {\n+        if (input16Bit > max16BitPositive) {  // negative value\n             temp = temp - Y_offset;\n             temp = temp * (-1);\n             temp = temp + Y_offset;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 462,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/57af1ec14594a73c8f2b73bf70c04ba7efeb6eab",
    "date": "2020-08-17T17:08:57-07:00",
    "message": "observers: use torch.all to check for valid min and max values (#43151)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/43151\n\nUsing `torch.all` instead of `torch.sum` and length check.\nIt's unclear whether the increase in perf (~5% for small inputs) is\nreal, but should be a net benefit, especially for larger channel inputs.\n\nTest Plan: Imported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D23170426\n\nfbshipit-source-id: ee5c25eb93cee1430661128ac9458a9c525df8e5",
    "changes": [
        {
            "name": "observer.py",
            "path": "torch/quantization/observer.py",
            "patches": [
                {
                    "old_start": 235,
                    "old_length": 7,
                    "new_start": 235,
                    "new_length": 7,
                    "hunk_buggy": "['                 min_val, max_val\\n', '             )\\n', '         else:\\n', '-            assert torch.sum(min_val <= max_val) == len(min_val), \"min {} should be less than max {}\".format(\\n', '                 min_val, max_val\\n', '             )\\n', ' ']",
                    "hunk_fix": "@@ -235,7 +235,7 @@ class _ObserverBase(ObserverBase):\n                 min_val, max_val\n             )\n         else:\n-            assert torch.sum(min_val <= max_val) == len(min_val), \"min {} should be less than max {}\".format(\n+            assert torch.all(min_val <= max_val), \"min {} should be less than max {}\".format(\n                 min_val, max_val\n             )\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 463,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/5023995292f5119c447de15c20a375b7e3aa2d0b",
    "date": "2020-08-05T15:55:46-07:00",
    "message": "fix output size adjustment for onnxifi_op\n\nSummary: this breaks if we cut the net at certain int8 ops boundary.\n\nTest Plan: with net_runner to lower a single Int8Quantize op. It used to break. Now it works.\n\nReviewed By: yinghai\n\nDifferential Revision: D22912178\n\nfbshipit-source-id: ca306068c9768df84c1cfa8b34226a1330e19912",
    "changes": [
        {
            "name": "onnxifi_op.cc",
            "path": "caffe2/opt/onnxifi_op.cc",
            "patches": [
                {
                    "old_start": 395,
                    "old_length": 7,
                    "new_start": 395,
                    "new_length": 7,
                    "hunk_buggy": "['           real_shape.dims(j),\\n', '           \")\");\\n', '       begin_ptr[j] = 0;\\n', '-      if (max_shape[j] > real_shape.dims(j)) {\\n', '         end_ptr[j] = real_shape.dims(j);\\n', '         mismatch += j;\\n', '       } else {']",
                    "hunk_fix": "@@ -395,7 +395,7 @@ int OnnxifiOp<CPUContext>::extractOutputBatchSizes() {\n           real_shape.dims(j),\n           \")\");\n       begin_ptr[j] = 0;\n-      if (max_shape[j] > real_shape.dims(j)) {\n+      if (max_shape[j] >= real_shape.dims(j)) {\n         end_ptr[j] = real_shape.dims(j);\n         mismatch += j;\n       } else {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 464,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/8a644f0c136cb12cf200050c2ae6875ec487d174",
    "date": "2020-07-28T16:43:19-07:00",
    "message": "[Shape Inference] Fix InferFC\n\nSummary: Sometimes first dim of X in FC is BATCH_OF_FEATURE_MAX instead of BATCH. This caused an issue in f207899183 (when first dim of X is 64 but is set to 1 in inferFC). Change the check from `!= BATCH` to `== UNKNOWN`\n\nTest Plan: unit test\n\nReviewed By: yinghai\n\nDifferential Revision: D22784691\n\nfbshipit-source-id: eb66ba361d6fe75672b13edbac2fbd269a7e7a00",
    "changes": [
        {
            "name": "bound_shape_inferencer.cc",
            "path": "caffe2/opt/bound_shape_inferencer.cc",
            "patches": [
                {
                    "old_start": 660,
                    "old_length": 7,
                    "new_start": 660,
                    "new_length": 7,
                    "hunk_buggy": "['         op.input(0), dimTypes, dims, w_data_type, int8_fc ? true : false);\\n', '   } else {\\n', '     ShapeInfo& x_shape_info = x_it->second;\\n', '-    if (x_shape_info.getDimType(0) != TensorBoundShape_DimType_BATCH) {\\n', '       CAFFE_ENFORCE_GE(x_shape_info.shape.dims_size(), 1);\\n', '       x_shape_info.shape.set_dims(0, spec_.max_batch_size);\\n', '       x_shape_info.setDimType(0, TensorBoundShape_DimType_BATCH);']",
                    "hunk_fix": "@@ -660,7 +660,7 @@ void BoundShapeInferencer::InferFC(const OperatorDef& op) {\n         op.input(0), dimTypes, dims, w_data_type, int8_fc ? true : false);\n   } else {\n     ShapeInfo& x_shape_info = x_it->second;\n-    if (x_shape_info.getDimType(0) != TensorBoundShape_DimType_BATCH) {\n+    if (x_shape_info.getDimType(0) == TensorBoundShape_DimType_UNKNOWN) {\n       CAFFE_ENFORCE_GE(x_shape_info.shape.dims_size(), 1);\n       x_shape_info.shape.set_dims(0, spec_.max_batch_size);\n       x_shape_info.setDimType(0, TensorBoundShape_DimType_BATCH);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 465,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/c5fdcd85c7570b654eec45b6cba7cc75b0cf8f6b",
    "date": "2020-07-23T15:56:48-07:00",
    "message": "check pruned attributes before deleting (#41913)\n\nSummary:\nI copyed a pruned model after deleteing the derived tensors. In order to be able to reparameter the model, we should check the existence of the tensors here.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/41913\n\nReviewed By: izdeby\n\nDifferential Revision: D22703248\n\nPulled By: mrshenli\n\nfbshipit-source-id: f5274d2c634a4c9a038100d8a6e837f132eabd34",
    "changes": [
        {
            "name": "prune.py",
            "path": "torch/nn/utils/prune.py",
            "patches": [
                {
                    "old_start": 233,
                    "old_length": 7,
                    "new_start": 233,
                    "new_length": 8,
                    "hunk_buggy": "['         weight = self.apply_mask(module)  # masked weights\\n', ' \\n', '         # delete and reset\\n', '-        delattr(module, self._tensor_name)\\n', '         orig = module._parameters[self._tensor_name + \"_orig\"]\\n', '         orig.data = weight.data\\n', '         del module._parameters[self._tensor_name + \"_orig\"]']",
                    "hunk_fix": "@@ -233,7 +233,8 @@ class BasePruningMethod(ABC):\n         weight = self.apply_mask(module)  # masked weights\n \n         # delete and reset\n-        delattr(module, self._tensor_name)\n+        if hasattr(module, self._tensor_name):\n+            delattr(module, self._tensor_name)\n         orig = module._parameters[self._tensor_name + \"_orig\"]\n         orig.data = weight.data\n         del module._parameters[self._tensor_name + \"_orig\"]"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 466,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/4ddf27ba48ba3313a20d3316a8929cd42436ddbc",
    "date": "2020-07-14T17:17:59-07:00",
    "message": "[op-bench] check device attribute in user inputs\n\nSummary: The device attribute in the op benchmark can only include 'cpu' or 'cuda'. So adding a check in this diff.\n\nTest Plan: buck run caffe2/benchmarks/operator_benchmark:benchmark_all_test -- --warmup_iterations 1 --iterations 1\n\nReviewed By: ngimel\n\nDifferential Revision: D22538252\n\nfbshipit-source-id: 3e5af72221fc056b8d867321ad22e35a2557b8c3",
    "changes": [
        {
            "name": "benchmark_utils.py",
            "path": "benchmarks/operator_benchmark/benchmark_utils.py",
            "patches": [
                {
                    "old_start": 17,
                    "old_length": 6,
                    "new_start": 17,
                    "new_length": 7,
                    "hunk_buggy": "[' \\n', ' # Here are the reserved keywords in the benchmark suite\\n', ' _reserved_keywords = {\"probs\", \"total_samples\", \"tags\"}\\n', ' \\n', ' def shape_to_string(shape):\\n', \"     return ', '.join([str(x) for x in shape])\\n\"]",
                    "hunk_fix": "@@ -17,6 +17,7 @@ This module contains utilities for writing microbenchmark tests.\n \n # Here are the reserved keywords in the benchmark suite\n _reserved_keywords = {\"probs\", \"total_samples\", \"tags\"}\n+_supported_devices = {\"cpu\", \"cuda\"}\n \n def shape_to_string(shape):\n     return ', '.join([str(x) for x in shape])\n"
                },
                {
                    "old_start": 108,
                    "old_length": 6,
                    "new_start": 109,
                    "new_length": 7,
                    "hunk_buggy": "[\"                       ({'M': 2}, {'N' : 4}),\\n\", \"                       ({'M': 2}, {'N' : 5}))\\n\", '     \"\"\"\\n', '     configs_attrs_list = []\\n', '     for key, values in configs.items():\\n', '         tmp_results = [{key : value} for value in values]\\n']",
                    "hunk_fix": "@@ -108,6 +109,7 @@ def cross_product_configs(**configs):\n                       ({'M': 2}, {'N' : 4}),\n                       ({'M': 2}, {'N' : 5}))\n     \"\"\"\n+    _validate(configs)\n     configs_attrs_list = []\n     for key, values in configs.items():\n         tmp_results = [{key : value} for value in values]\n"
                },
                {
                    "old_start": 120,
                    "old_length": 6,
                    "new_start": 122,
                    "new_length": 13,
                    "hunk_buggy": "['     return generated_configs\\n', ' \\n', ' \\n', ' def config_list(**configs):\\n', '     \"\"\" Generate configs based on the list of input shapes.\\n', '     This function will take input shapes specified in a list from user. Besides\\n']",
                    "hunk_fix": "@@ -120,6 +122,13 @@ def cross_product_configs(**configs):\n     return generated_configs\n \n \n+def _validate(configs):\n+    \"\"\" Validate inputs from users.\"\"\"\n+    if 'device' in configs:\n+        for v in configs['device']:\n+            assert(v in _supported_devices), \"Device needs to be a string.\"\n+\n+\n def config_list(**configs):\n     \"\"\" Generate configs based on the list of input shapes.\n     This function will take input shapes specified in a list from user. Besides\n"
                },
                {
                    "old_start": 153,
                    "old_length": 6,
                    "new_start": 162,
                    "new_length": 8,
                    "hunk_buggy": "['     if any(attr not in configs for attr in reserved_names):\\n', '         raise ValueError(\"Missing attrs in configs\")\\n', ' \\n', '     cross_configs = None\\n', \"     if 'cross_product_configs' in configs:\\n\", \"         cross_configs = cross_product_configs(**configs['cross_product_configs'])\"]",
                    "hunk_fix": "@@ -153,6 +162,8 @@ def config_list(**configs):\n     if any(attr not in configs for attr in reserved_names):\n         raise ValueError(\"Missing attrs in configs\")\n \n+    _validate(configs)\n+\n     cross_configs = None\n     if 'cross_product_configs' in configs:\n         cross_configs = cross_product_configs(**configs['cross_product_configs'])"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 467,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/402be850a8946e8967dedb3375fc6f33b379b397",
    "date": "2020-07-12T11:40:19-07:00",
    "message": "[quant] Adding zero point type check for per channel quantization (#40811)\n\nSummary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/40811\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D22319417\n\nPulled By: z-a-f\n\nfbshipit-source-id: 7be3a511ddd33b5fe749a83166bbc5874d1bd539",
    "changes": [
        {
            "name": "fake_quant_per_channel_affine.cpp",
            "path": "aten/src/ATen/native/quantized/fake_quant_per_channel_affine.cpp",
            "patches": [
                {
                    "old_start": 35,
                    "old_length": 6,
                    "new_start": 35,
                    "new_length": 10,
                    "hunk_buggy": "['     int64_t quant_min,\\n', '     int64_t quant_max) {\\n', '   TORCH_CHECK(self.scalar_type() == ScalarType::Float);\\n', '   TORCH_CHECK(scale.dim() == 1, \"scale should be a 1-D tensor\");\\n', '   TORCH_CHECK(zero_point.dim() == 1, \"zero point should be a 1-D tensor\");\\n', '   TORCH_CHECK(\\n']",
                    "hunk_fix": "@@ -35,6 +35,10 @@ Tensor fake_quantize_per_channel_affine(\n     int64_t quant_min,\n     int64_t quant_max) {\n   TORCH_CHECK(self.scalar_type() == ScalarType::Float);\n+  TORCH_CHECK(scale.scalar_type() == ScalarType::Float,\n+              \"Scale must be Float, found \", scale.scalar_type());\n+  TORCH_CHECK(zero_point.scalar_type() == ScalarType::Long,\n+              \"Zero-point must be Long, found \", zero_point.scalar_type());\n   TORCH_CHECK(scale.dim() == 1, \"scale should be a 1-D tensor\");\n   TORCH_CHECK(zero_point.dim() == 1, \"zero point should be a 1-D tensor\");\n   TORCH_CHECK(\n"
                },
                {
                    "old_start": 101,
                    "old_length": 6,
                    "new_start": 105,
                    "new_length": 10,
                    "hunk_buggy": "['     int64_t quant_max) {\\n', '   TORCH_CHECK(dY.scalar_type() == ScalarType::Float);\\n', '   TORCH_CHECK(X.scalar_type() == ScalarType::Float);\\n', ' \\n', '   TORCH_CHECK(X.sizes() == dY.sizes(), \"`X` and `dY` are not the same size\");\\n', '   TORCH_CHECK(']",
                    "hunk_fix": "@@ -101,6 +105,10 @@ Tensor fake_quantize_per_channel_affine_backward(\n     int64_t quant_max) {\n   TORCH_CHECK(dY.scalar_type() == ScalarType::Float);\n   TORCH_CHECK(X.scalar_type() == ScalarType::Float);\n+  TORCH_CHECK(scale.scalar_type() == ScalarType::Float,\n+              \"Scale must be Float, found \", scale.scalar_type());\n+  TORCH_CHECK(zero_point.scalar_type() == ScalarType::Long,\n+              \"Zero-point must be Long, found \", zero_point.scalar_type());\n \n   TORCH_CHECK(X.sizes() == dY.sizes(), \"`X` and `dY` are not the same size\");\n   TORCH_CHECK("
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 468,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/71af538e31547e5b1bc814c9e00323a21905baf3",
    "date": "2020-06-02T13:35:39-07:00",
    "message": "Updated assert to remove check on 3rd dim for MHA (#39402)\n\nSummary:\n## Description\n* Updated assert statement to remove check on 3rd dimension (features) for keys and values in MultiheadAttention / Transform\n* The feature dimension for keys and values can now be of different sizes\n* Refer to https://github.com/pytorch/pytorch/issues/27623\nPull Request resolved: https://github.com/pytorch/pytorch/pull/39402\n\nReviewed By: zhangguanheng66\n\nDifferential Revision: D21841678\n\nPulled By: Nayef211\n\nfbshipit-source-id: f0c9e5e0f33259ae2abb6bf9e7fb14e3aa9008eb",
    "changes": [
        {
            "name": "functional.py",
            "path": "torch/nn/functional.py",
            "patches": [
                {
                    "old_start": 3947,
                    "old_length": 7,
                    "new_start": 3947,
                    "new_length": 8,
                    "hunk_buggy": "['                 v_proj_weight=v_proj_weight, static_k=static_k, static_v=static_v)\\n', '     tgt_len, bsz, embed_dim = query.size()\\n', '     assert embed_dim == embed_dim_to_check\\n', '-    assert key.size() == value.size()\\n', ' \\n', '     head_dim = embed_dim // num_heads\\n', '     assert head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"']",
                    "hunk_fix": "@@ -3947,7 +3947,8 @@ def multi_head_attention_forward(query,                           # type: Tensor\n                 v_proj_weight=v_proj_weight, static_k=static_k, static_v=static_v)\n     tgt_len, bsz, embed_dim = query.size()\n     assert embed_dim == embed_dim_to_check\n-    assert key.size() == value.size()\n+    # allow MHA to have different sizes for the feature dimension\n+    assert key.size(0) == value.size(0) and key.size(1) == value.size(1)\n \n     head_dim = embed_dim // num_heads\n     assert head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\""
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 469,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/bbb5e106ad6228953df6c7f5c8916b26dc0cb457",
    "date": "2020-05-27T07:41:02-07:00",
    "message": "Improve error checking of CUDALoops. (#38810)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/38810\n\nSame change as was applied to CPU loops -- separate out checking of the inputs and outputs.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D21670339\n\nPulled By: gchanan\n\nfbshipit-source-id: 42f208538dce1a5598d14948d8d02a1c91ba152a",
    "changes": [
        {
            "name": "CUDALoops.cuh",
            "path": "aten/src/ATen/native/cuda/CUDALoops.cuh",
            "patches": [
                {
                    "old_start": 228,
                    "old_length": 7,
                    "new_start": 228,
                    "new_length": 8,
                    "hunk_buggy": "['   constexpr int ntensors = traits::arity + 1;\\n', ' \\n', '   TORCH_INTERNAL_ASSERT(iter.can_use_32bit_indexing());\\n', '-  TORCH_INTERNAL_ASSERT(iter.ntensors() == traits::arity + 1);\\n', ' \\n', '   at::detail::Array<char*, ntensors> data;\\n', '   for (int i = 0; i < ntensors; i++) {']",
                    "hunk_fix": "@@ -228,7 +228,8 @@ void gpu_kernel_impl(TensorIterator& iter, const func_t& f) {\n   constexpr int ntensors = traits::arity + 1;\n \n   TORCH_INTERNAL_ASSERT(iter.can_use_32bit_indexing());\n-  TORCH_INTERNAL_ASSERT(iter.ntensors() == traits::arity + 1);\n+  TORCH_INTERNAL_ASSERT(iter.ninputs() == traits::arity);\n+  TORCH_INTERNAL_ASSERT(iter.noutputs() == 1);\n \n   at::detail::Array<char*, ntensors> data;\n   for (int i = 0; i < ntensors; i++) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 470,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/b7882f9bd65de5a4c60f625d56186b583c1d6842",
    "date": "2020-05-27T07:38:58-07:00",
    "message": "Improve cpu/Loops.h arity asserts. (#38809)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/38809\n\nThis splits the asserts into separate input/output asserts and makes the numbers precise, instead of ranges.\n\nThis is an ongoing effort to improve the Loops assertion and to integrate dynamic cast checking into CPU loops.\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D21670263\n\nPulled By: gchanan\n\nfbshipit-source-id: b1868db5255a69158045b759dc9171690a2dcd01",
    "changes": [
        {
            "name": "Loops.h",
            "path": "aten/src/ATen/native/cpu/Loops.h",
            "patches": [
                {
                    "old_start": 186,
                    "old_length": 7,
                    "new_start": 186,
                    "new_length": 9,
                    "hunk_buggy": "[' template <typename func_t>\\n', ' void cpu_kernel(TensorIterator& iter, func_t&& op) {\\n', '   using traits = function_traits<func_t>;\\n', '-  TORCH_INTERNAL_ASSERT(iter.ntensors() >= traits::arity + 1);\\n', ' \\n', '   iter.for_each([&](char** data, const int64_t* strides, int64_t n) {\\n', '     if (is_contiguous<traits>(strides)) {\\n']",
                    "hunk_fix": "@@ -186,7 +186,9 @@ static inline void unroll_contiguous_scalar_checks(\n template <typename func_t>\n void cpu_kernel(TensorIterator& iter, func_t&& op) {\n   using traits = function_traits<func_t>;\n-  TORCH_INTERNAL_ASSERT(iter.ntensors() >= traits::arity + 1);\n+  // this could be extended to work with void return types\n+  TORCH_INTERNAL_ASSERT(iter.ninputs() == traits::arity);\n+  TORCH_INTERNAL_ASSERT(iter.noutputs() == 1);\n \n   iter.for_each([&](char** data, const int64_t* strides, int64_t n) {\n     if (is_contiguous<traits>(strides)) {\n"
                },
                {
                    "old_start": 204,
                    "old_length": 7,
                    "new_start": 206,
                    "new_length": 9,
                    "hunk_buggy": "[' template <typename func_t, typename vec_func_t>\\n', ' void cpu_kernel_vec(TensorIterator& iter, func_t&& op, vec_func_t&& vop) {\\n', '   using traits = function_traits<func_t>;\\n', '-  TORCH_INTERNAL_ASSERT(iter.ntensors() >= traits::arity + 1);\\n', ' \\n', '   iter.for_each([&](char** data, const int64_t* strides, int64_t n) {\\n', '     if (is_contiguous<traits>(strides)) {\\n']",
                    "hunk_fix": "@@ -204,7 +206,9 @@ void cpu_kernel(TensorIterator& iter, func_t&& op) {\n template <typename func_t, typename vec_func_t>\n void cpu_kernel_vec(TensorIterator& iter, func_t&& op, vec_func_t&& vop) {\n   using traits = function_traits<func_t>;\n-  TORCH_INTERNAL_ASSERT(iter.ntensors() >= traits::arity + 1);\n+  // this could be extended to work with void return types\n+  TORCH_INTERNAL_ASSERT(iter.ninputs() == traits::arity);\n+  TORCH_INTERNAL_ASSERT(iter.noutputs() == 1);\n \n   iter.for_each([&](char** data, const int64_t* strides, int64_t n) {\n     if (is_contiguous<traits>(strides)) {\n"
                },
                {
                    "old_start": 226,
                    "old_length": 8,
                    "new_start": 230,
                    "new_length": 9,
                    "hunk_buggy": "[' template <typename func_t>\\n', ' void cpu_serial_kernel(TensorIterator& iter, func_t&& op, const Range& range) {\\n', '   using traits = function_traits<func_t>;\\n', '-  TORCH_INTERNAL_ASSERT((std::is_void<typename traits::result_type>::value &&\\n', '-    iter.noutputs() == 0 && iter.ntensors() == traits::arity) || (iter.ntensors() >= traits::arity + 1));\\n', ' \\n', '   iter.serial_for_each([&](char** data, const int64_t* strides, int64_t n) {\\n', '     if (is_contiguous<traits>(strides)) {\\n']",
                    "hunk_fix": "@@ -226,8 +230,9 @@ void cpu_kernel_vec(TensorIterator& iter, func_t&& op, vec_func_t&& vop) {\n template <typename func_t>\n void cpu_serial_kernel(TensorIterator& iter, func_t&& op, const Range& range) {\n   using traits = function_traits<func_t>;\n-  TORCH_INTERNAL_ASSERT((std::is_void<typename traits::result_type>::value &&\n-    iter.noutputs() == 0 && iter.ntensors() == traits::arity) || (iter.ntensors() >= traits::arity + 1));\n+  constexpr bool result_void = std::is_void<typename traits::result_type>::value;\n+  TORCH_INTERNAL_ASSERT(iter.ninputs() == traits::arity &&\n+                        ((result_void && iter.noutputs() == 0) || (!result_void && iter.noutputs() == 1)));\n \n   iter.serial_for_each([&](char** data, const int64_t* strides, int64_t n) {\n     if (is_contiguous<traits>(strides)) {\n"
                },
                {
                    "old_start": 250,
                    "old_length": 7,
                    "new_start": 255,
                    "new_length": 9,
                    "hunk_buggy": "[' template <typename func_t, typename vec_func_t>\\n', ' void cpu_serial_kernel_vec(TensorIterator& iter, func_t&& op, vec_func_t&& vop, const Range& range) {\\n', '   using traits = function_traits<func_t>;\\n', '-  TORCH_INTERNAL_ASSERT(iter.ntensors() >= traits::arity + 1);\\n', ' \\n', '   iter.serial_for_each([&](char** data, const int64_t* strides, int64_t n) {\\n', '     if (is_contiguous<traits>(strides)) {']",
                    "hunk_fix": "@@ -250,7 +255,9 @@ void cpu_serial_kernel(TensorIterator& iter, func_t&& op) {\n template <typename func_t, typename vec_func_t>\n void cpu_serial_kernel_vec(TensorIterator& iter, func_t&& op, vec_func_t&& vop, const Range& range) {\n   using traits = function_traits<func_t>;\n-  TORCH_INTERNAL_ASSERT(iter.ntensors() >= traits::arity + 1);\n+  // this could be extended to work with void return types\n+  TORCH_INTERNAL_ASSERT(iter.ninputs() == traits::arity);\n+  TORCH_INTERNAL_ASSERT(iter.noutputs() == 1);\n \n   iter.serial_for_each([&](char** data, const int64_t* strides, int64_t n) {\n     if (is_contiguous<traits>(strides)) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 471,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/bae895cef0c12df5f64afa155ce5462e06f0e04a",
    "date": "2020-05-15T01:40:48-07:00",
    "message": "Issue 37819: Added check for kHIP in ATen/native/Copy.cpp (#38003)\n\nSummary:\nFixed https://github.com/pytorch/pytorch/issues/37819\nPull Request resolved: https://github.com/pytorch/pytorch/pull/38003\n\nDifferential Revision: D21533134\n\nPulled By: mruberry\n\nfbshipit-source-id: 97490a8729171b95b103e00780e36518b9865087",
    "changes": [
        {
            "name": "Copy.cpp",
            "path": "aten/src/ATen/native/Copy.cpp",
            "patches": [
                {
                    "old_start": 141,
                    "old_length": 6,
                    "new_start": 141,
                    "new_length": 8,
                    "hunk_buggy": "['   DeviceType device_type = iter.device_type(0);\\n', '   if (iter.device_type(1) == kCUDA) {\\n', '     device_type = kCUDA;\\n', '   }\\n', ' \\n', '   // TODO: if we need to, we can also enable this path for quantized tensor']",
                    "hunk_fix": "@@ -141,6 +141,8 @@ static Tensor & copy_impl(Tensor & self, const Tensor & src, bool non_blocking)\n   DeviceType device_type = iter.device_type(0);\n   if (iter.device_type(1) == kCUDA) {\n     device_type = kCUDA;\n+  } else if (iter.device_type(1) == kHIP) {\n+    device_type = kHIP;\n   }\n \n   // TODO: if we need to, we can also enable this path for quantized tensor"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 472,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/122587dcb41427f473b7833eaf254384919e06fc",
    "date": "2020-05-06T22:35:00-07:00",
    "message": "[ONNX] Improve error checking for large model export (#37798)\n\nSummary:\n* Add error message when onnx model file path is not a string.\n* Add error message when model size exceed 2GB when large model export is not turned on.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/37798\n\nReviewed By: hl475\n\nDifferential Revision: D21440571\n\nPulled By: houseroad\n\nfbshipit-source-id: 054aaa25ab0cffc229f9b487a2c160623c89b741",
    "changes": [
        {
            "name": "export.cpp",
            "path": "torch/csrc/jit/serialization/export.cpp",
            "patches": [
                {
                    "old_start": 616,
                    "old_length": 6,
                    "new_start": 616,
                    "new_length": 13,
                    "hunk_buggy": "['     validateGraph(graph, operator_export_type);\\n', '   }\\n', ' \\n', '   auto* imp = model_proto_.add_opset_import();\\n', '   // This is the version of ONNX operator set we are targeting\\n', '   imp->set_version(onnx_opset_version);\\n']",
                    "hunk_fix": "@@ -616,6 +616,13 @@ GraphEncoder::GraphEncoder(\n     validateGraph(graph, operator_export_type);\n   }\n \n+  if (use_external_data_format) {\n+    TORCH_CHECK(\n+        !onnx_file_path.empty(),\n+        \"For large model export, f in torch.onnx.export must be a non-empty string \"\n+        \"specifying the location of the model.\");\n+  }\n+\n   auto* imp = model_proto_.add_opset_import();\n   // This is the version of ONNX operator set we are targeting\n   imp->set_version(onnx_opset_version);\n"
                },
                {
                    "old_start": 957,
                    "old_length": 6,
                    "new_start": 964,
                    "new_length": 11,
                    "hunk_buggy": "['       add_node_names,\\n', '       use_external_data_format,\\n', '       onnx_file_path);\\n', '   return std::make_tuple(\\n', '       graph_encoder.get_model_proto().SerializeAsString(),\\n', '       graph_encoder.get_raw_data_export_map());']",
                    "hunk_fix": "@@ -957,6 +964,11 @@ std::tuple<std::string, RawDataExportMap> export_onnx(\n       add_node_names,\n       use_external_data_format,\n       onnx_file_path);\n+  const size_t proto_size = graph_encoder.get_model_proto().ByteSizeLong();\n+  TORCH_CHECK(\n+      proto_size <= INT_MAX,\n+      \"Exporting model exceed maximum protobuf size of 2GB. \"\n+      \"Please call torch.onnx.export with use_external_data_format=True.\");\n   return std::make_tuple(\n       graph_encoder.get_model_proto().SerializeAsString(),\n       graph_encoder.get_raw_data_export_map());"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 473,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/563bbeb8905f4cea0bc5353dc12518c61113128e",
    "date": "2020-05-05T16:31:24-07:00",
    "message": "fix undef CUDA_VERSION warning (#37866)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/37866\n\nmake sure not to check `CUDA_VERSION` if it is not defined\n\nTest Plan: CI gree\n\nReviewed By: anjali411\n\nDifferential Revision: D21408844\n\nfbshipit-source-id: 5a9afe372b3f1fbaf08a7c43fa3e0e654a569d5f",
    "changes": [
        {
            "name": "complex_math.h",
            "path": "c10/util/complex_math.h",
            "patches": [
                {
                    "old_start": 2,
                    "old_length": 7,
                    "new_start": 2,
                    "new_length": 7,
                    "hunk_buggy": "[' \\n', ' // Exponential functions\\n', ' \\n', '-#if CUDA_VERSION < 10000\\n', ' #define CUDA92_BUG(x) thrust::complex<T>(x.real(), x.imag())\\n', ' #else\\n', ' #define CUDA92_BUG(x) x']",
                    "hunk_fix": "@@ -2,7 +2,7 @@ namespace std {\n \n // Exponential functions\n \n-#if CUDA_VERSION < 10000\n+#if defined(CUDA_VERSION) && (CUDA_VERSION < 10000)\n #define CUDA92_BUG(x) thrust::complex<T>(x.real(), x.imag())\n #else\n #define CUDA92_BUG(x) x"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 474,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/bede7d999523d02e636a8981c0dff233b67f1a62",
    "date": "2020-04-16T08:58:03-07:00",
    "message": "Fixed check for the buffer overflow in assert (#36476)\n\nSummary:\nThis code looks like a mistake\n```C++\nAT_ASSERT(size_t(kind) < sizeof(names) / sizeof(AttributeKind));\n```\nIt does not check if `kind` variable fits in array of pointer called `names`\n\nEven if we write something like this: that assert won't fail\n```C++\nAttributeKind kind = AttributeKind::ival;\n*((unsigned int*)&kind2) += 1;\n```\nSo I fixed it\nPull Request resolved: https://github.com/pytorch/pytorch/pull/36476\n\nDifferential Revision: D21018748\n\nPulled By: colesbury\n\nfbshipit-source-id: f4d3b8faf64cf07232d595075f831805084f5d00",
    "changes": [
        {
            "name": "attributes.h",
            "path": "torch/csrc/jit/ir/attributes.h",
            "patches": [
                {
                    "old_start": 34,
                    "old_length": 7,
                    "new_start": 34,
                    "new_length": 7,
                    "hunk_buggy": "['                                 \"ty\",\\n', '                                 \"tys\",\\n', '                                 \"ival\"};\\n', '-  AT_ASSERT(size_t(kind) < sizeof(names) / sizeof(AttributeKind));\\n', '   return names[int(kind)];\\n', ' }\\n', ' ']",
                    "hunk_fix": "@@ -34,7 +34,7 @@ static inline const char* toString(AttributeKind kind) {\n                                 \"ty\",\n                                 \"tys\",\n                                 \"ival\"};\n-  AT_ASSERT(size_t(kind) < sizeof(names) / sizeof(AttributeKind));\n+  AT_ASSERT(size_t(kind) < sizeof(names) / sizeof(*names));\n   return names[int(kind)];\n }\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 475,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/93256617c8622760181dacf03c41cc0577ac0ea6",
    "date": "2020-04-08T11:22:50-07:00",
    "message": "C++ Adam optimizer - corrected messages for check of default options (#36161)\n\nSummary:\nModified messages in the check of default options for the Adam optimizer.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/36161\n\nDifferential Revision: D20920140\n\nPulled By: yf225\n\nfbshipit-source-id: e697ef1741d4dd86f7f18dc0be2c3b4bd3894d8f",
    "changes": [
        {
            "name": "adam.h",
            "path": "torch/csrc/api/include/torch/optim/adam.h",
            "patches": [
                {
                    "old_start": 53,
                    "old_length": 9,
                    "new_start": 53,
                    "new_length": 9,
                    "hunk_buggy": "['      TORCH_CHECK(defaults.lr() >= 0, \"Invalid learning rate: \", defaults.lr());\\n', '      TORCH_CHECK(defaults.eps() >= 0, \"Invalid epsilon value: \", defaults.eps());\\n', '      auto betas = defaults.betas();\\n', '-     TORCH_CHECK(std::get<0>(betas) >= 0, \"Invalid learning rate: \", std::get<0>(betas));\\n', '-     TORCH_CHECK(std::get<1>(betas) >= 0, \"Invalid learning rate: \", std::get<1>(betas));\\n', '-     TORCH_CHECK(defaults.weight_decay() >= 0, \"Invalid learning rate: \", defaults.weight_decay());\\n', '    }\\n', '    explicit Adam(\\n', '        std::vector<Tensor> params,']",
                    "hunk_fix": "@@ -53,9 +53,9 @@ class TORCH_API Adam : public Optimizer {\n      TORCH_CHECK(defaults.lr() >= 0, \"Invalid learning rate: \", defaults.lr());\n      TORCH_CHECK(defaults.eps() >= 0, \"Invalid epsilon value: \", defaults.eps());\n      auto betas = defaults.betas();\n-     TORCH_CHECK(std::get<0>(betas) >= 0, \"Invalid learning rate: \", std::get<0>(betas));\n-     TORCH_CHECK(std::get<1>(betas) >= 0, \"Invalid learning rate: \", std::get<1>(betas));\n-     TORCH_CHECK(defaults.weight_decay() >= 0, \"Invalid learning rate: \", defaults.weight_decay());\n+     TORCH_CHECK(0 <= std::get<0>(betas) && std::get<0>(betas) < 1.0, \"Invalid beta parameter at index 0: \", std::get<0>(betas));\n+     TORCH_CHECK(0 <= std::get<1>(betas) && std::get<1>(betas) < 1.0, \"Invalid beta parameter at index 1: \", std::get<1>(betas));\n+     TORCH_CHECK(defaults.weight_decay() >= 0, \"Invalid weight_decay value: \", defaults.weight_decay());\n    }\n    explicit Adam(\n        std::vector<Tensor> params,"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 476,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/b1f08e7426a56a323e6928365918093b65aa4fb6",
    "date": "2020-03-30T13:13:17-07:00",
    "message": "Call uncheckedSetDevice in ~InlineDeviceGuard only when device index are different (#35438)\n\nSummary:\nSetting device could be expensive, especially when a debugger is present. We should check the device are different before we set.\n\ncc: ptrblck\nPull Request resolved: https://github.com/pytorch/pytorch/pull/35438\n\nDifferential Revision: D20664084\n\nPulled By: ngimel\n\nfbshipit-source-id: 2440b4c9d96c41b4a19d5b1e8e1756fa40f090f0",
    "changes": [
        {
            "name": "CUDAGuardImpl.h",
            "path": "c10/cuda/impl/CUDAGuardImpl.h",
            "patches": [
                {
                    "old_start": 37,
                    "old_length": 12,
                    "new_start": 37,
                    "new_length": 27,
                    "hunk_buggy": "['     C10_CUDA_CHECK(cudaGetDevice(&device));\\n', '     return Device(DeviceType::CUDA, device);\\n', '   }\\n', '   void setDevice(Device d) const override {\\n', '     TORCH_INTERNAL_ASSERT(d.type() == DeviceType::CUDA);\\n', '-    C10_CUDA_CHECK(cudaSetDevice(d.index()));\\n', '   }\\n', '   void uncheckedSetDevice(Device d) const noexcept override {\\n', '-    C10_CUDA_CHECK_WARN(cudaSetDevice(d.index()));\\n', '   }\\n', '   Stream getStream(Device d) const noexcept override {\\n', '     return getCurrentCUDAStream(d.index()).unwrap();']",
                    "hunk_fix": "@@ -37,12 +37,27 @@ struct CUDAGuardImpl final : public c10::impl::DeviceGuardImplInterface {\n     C10_CUDA_CHECK(cudaGetDevice(&device));\n     return Device(DeviceType::CUDA, device);\n   }\n+  c10::optional<Device> uncheckedGetDevice() const noexcept {\n+    int device;\n+    auto err = cudaGetDevice(&device);\n+    C10_CUDA_CHECK_WARN(err);\n+    if (err != cudaSuccess) {\n+      return c10::nullopt;\n+    }\n+    return Device(DeviceType::CUDA, device);\n+  }\n   void setDevice(Device d) const override {\n     TORCH_INTERNAL_ASSERT(d.type() == DeviceType::CUDA);\n-    C10_CUDA_CHECK(cudaSetDevice(d.index()));\n+    Device current_device = getDevice();\n+    if (current_device != d) {\n+      C10_CUDA_CHECK(cudaSetDevice(d.index()));\n+    }\n   }\n   void uncheckedSetDevice(Device d) const noexcept override {\n-    C10_CUDA_CHECK_WARN(cudaSetDevice(d.index()));\n+    auto current_device = uncheckedGetDevice();\n+    if (!current_device.has_value() || current_device.value() != d) {\n+      C10_CUDA_CHECK_WARN(cudaSetDevice(d.index()));\n+    }\n   }\n   Stream getStream(Device d) const noexcept override {\n     return getCurrentCUDAStream(d.index()).unwrap();"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 477,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/8269c4f3d30ad950a873d900f7de0880cdd38878",
    "date": "2020-03-04T11:10:53-08:00",
    "message": "Added nullptr check for pthradpool_get_threads_count (#34087)\n\nSummary:\nWe get seg fault without this in using XNNPACK.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/34087\n\nDifferential Revision: D20199787\n\nPulled By: kimishpatel\n\nfbshipit-source-id: d3d274e7bb197461632b21688820cd4c10dcd819",
    "changes": [
        {
            "name": "pthreadpool_impl.cc",
            "path": "caffe2/utils/threadpool/pthreadpool_impl.cc",
            "patches": [
                {
                    "old_start": 28,
                    "old_length": 7,
                    "new_start": 28,
                    "new_length": 19,
                    "hunk_buggy": "[' }\\n', ' \\n', ' size_t pthreadpool_get_threads_count(pthreadpool_t threadpool) {\\n', '   return reinterpret_cast<caffe2::ThreadPool*>(threadpool)->getNumThreads();\\n', ' }\\n', ' \\n', ' pthreadpool_t pthreadpool_create(size_t threads_count) {']",
                    "hunk_fix": "@@ -28,7 +28,19 @@ void pthreadpool_compute_1d(\n }\n \n size_t pthreadpool_get_threads_count(pthreadpool_t threadpool) {\n+  // The current fix only useful when XNNPACK calls pthreadpool_get_threads_count with nullptr.\n+  if (threadpool == nullptr) {\n+    return 1;\n+  }\n   return reinterpret_cast<caffe2::ThreadPool*>(threadpool)->getNumThreads();\n+  // TODO: Future fix: If we keep maintaining two different threadpools.\n+  // Old C2 and new one for XNNPACK, then the we have two different pthreadpool pointer\n+  // types. One is caffe2::Thredpool*, the other is pthreadpool* (pthreadpool_new_if_impl.c)\n+  // XNNPACK calls pthreadpool_get_threads_count during op setup using pthreadpool*, and\n+  // uses _parallelize_ interface for for actual work.\n+  // While NNPACK uses caffe2::Threadpool*.\n+  // Thus if pthreadpool_get_threads_count is getting called from XNNPACK we cannot\n+  // reinterpret_cast it to ThreadPool. It will seg fault or worse will have unedfined behavior.\n }\n \n pthreadpool_t pthreadpool_create(size_t threads_count) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 478,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/4dad00b64b396ef81f16bdb896175688fc629f4d",
    "date": "2020-02-26T18:44:40-08:00",
    "message": "[rpc] special case tensor type check when getting RRef (#33582)\n\nSummary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/33582\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D20009837\n\nPulled By: wanchaol\n\nfbshipit-source-id: 7e9ab87d4dddb822c7575891a2b620eff83bfa00",
    "changes": [
        {
            "name": "rref_context.cpp",
            "path": "torch/csrc/distributed/rpc/rref_context.cpp",
            "patches": [
                {
                    "old_start": 175,
                    "old_length": 8,
                    "new_start": 175,
                    "new_length": 28,
                    "hunk_buggy": "['   auto& rrefId = rrefForkData.rrefId_;\\n', '   auto& forkId = rrefForkData.forkId_;\\n', '   if (ownerId == getWorkerId()) {\\n', '     auto ownerRRef = getOwnerRRef(rrefId);\\n', '-    TORCH_INTERNAL_ASSERT(ownerRRef->type() == type);\\n', '     return ownerRRef;\\n', '   } else {\\n', '     return createUserRRef(ownerId, rrefId, forkId, type);']",
                    "hunk_fix": "@@ -175,8 +175,28 @@ c10::intrusive_ptr<RRef> RRefContext::getOrCreateRRef(\n   auto& rrefId = rrefForkData.rrefId_;\n   auto& forkId = rrefForkData.forkId_;\n   if (ownerId == getWorkerId()) {\n+    // We have found the rref through the rrefId\n     auto ownerRRef = getOwnerRRef(rrefId);\n-    TORCH_INTERNAL_ASSERT(ownerRRef->type() == type);\n+    // Now double check if the two types are matched\n+    //\n+    // Why we are special casing the check for tensor type here?\n+    // this is because tensor types might get specialized on tensors when\n+    // we pass inputs to the function, i.e. TensorType can filled with\n+    // specific shape info, requires_grad info, etc. so the OwerRRef we\n+    // found might already have those infos, but the `type` we passed in\n+    // here is a plain TensorType, they are not equal relationship:\n+    // specialized TensorType <: plain TensorType\n+    //\n+    // In RPC we don't care the difference as we ser/de with just the\n+    // plain TensorType. This is not a issue for UserRRef creation either,\n+    // since Tensor can only get specialized with a previous run of local\n+    // JIT function, and we shouldn't preserve the specialized SubTensorType\n+    // information on other workers because it's only information only.\n+    if(type == TensorType::get()) {\n+      TORCH_INTERNAL_ASSERT(ownerRRef->type()->isSubtypeOf(TensorType::get()));\n+    } else {\n+      TORCH_INTERNAL_ASSERT(ownerRRef->type() == type);\n+    }\n     return ownerRRef;\n   } else {\n     return createUserRRef(ownerId, rrefId, forkId, type);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 479,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/ecd3c252b4da3056797f8a505c9ebe8d68db55c4",
    "date": "2020-02-13T22:53:11-08:00",
    "message": "Suport all length one SLS op lowering: C2 part (#33332)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/33332\n\nWe check the input shape of lengths and indices of SLS and add an attribute if they are the same.\n\nTest Plan:\n```\nbuck test glow/fb/test/numerics:test_operator_onnxifinnpi -- test_slws_fused_8bit_rowwise_length1_graph\n```\n\nReviewed By: ipiszy\n\nDifferential Revision: D19874903\n\nfbshipit-source-id: 06b643b5351d0ba19ba209b5a5b599fbb38b1dfc",
    "changes": [
        {
            "name": "onnxifi_transformer.cc",
            "path": "caffe2/opt/onnxifi_transformer.cc",
            "patches": [
                {
                    "old_start": 690,
                    "old_length": 6,
                    "new_start": 690,
                    "new_length": 30,
                    "hunk_buggy": "['     if ((op.type() == \"Concat\" || op.type() == \"Reshape\") &&\\n', '         op.output_size() == 2) {\\n', '       split_infos.emplace(op.output(1));\\n', '     }\\n', '   }\\n', '   onnxifi_net.clear_external_output();']",
                    "hunk_fix": "@@ -690,6 +690,30 @@ NetDef OnnxifiTransformer::SubnetToOnnxifiOpViaC2(\n     if ((op.type() == \"Concat\" || op.type() == \"Reshape\") &&\n         op.output_size() == 2) {\n       split_infos.emplace(op.output(1));\n+    } else if (\n+        op.type() == \"SparseLengthsSum\" ||\n+        op.type() == \"SparseLengthsSumFused8BitRowwise\" ||\n+        op.type() == \"SparseLengthsWeightedSum\" ||\n+        op.type() == \"SparseLengthsWeightedSumFused8BitRowwise\" ||\n+        op.type() == \"SparseLengthsSumFused4BitRowwise\" ||\n+        op.type() == \"SparseLengthsWeightedSumFused4BitRowwise\") {\n+      int weighted = (op.type() == \"SparseLengthsWeightedSum\" ||\n+                      op.type() == \"SparseLengthsWeightedSumFused8BitRowwise\" ||\n+                      op.type() == \"SparseLengthsWeightedSumFused4BitRowwise\")\n+          ? 1\n+          : 0;\n+      const auto& indices_hint = shape_hints.at(op.input(1 + weighted));\n+      const auto& lengths_hint = shape_hints.at(op.input(2 + weighted));\n+      const auto& indices_shape = indices_hint.shape;\n+      const auto& lengths_shape = lengths_hint.shape;\n+      if ((indices_hint.getDimType(0) ==\n+               TensorBoundShape_DimType_BATCH_OF_FEATURE_MAX ||\n+           indices_hint.getDimType(0) ==\n+               TensorBoundShape_DimType_BATCH_OF_FEATURE_MAX_DEFAULT) &&\n+          indices_shape.dims_size() == 1 && lengths_shape.dims_size() == 1 &&\n+          indices_shape.dims(0) == lengths_shape.dims(0)) {\n+        op.add_arg()->CopyFrom(MakeArgument<int>(\"length1\", 1));\n+      }\n     }\n   }\n   onnxifi_net.clear_external_output();"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 480,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/acd51e13f727af22e6c9e579518362898f1b12e6",
    "date": "2020-02-11T17:38:49-08:00",
    "message": "TorchScript add check if quantized\n\nSummary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32890\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19673463\n\nPulled By: z-a-f\n\nfbshipit-source-id: 453ff662810845fcaeb8e6d5919afa8e2d395768",
    "changes": [
        {
            "name": "__init__.py",
            "path": "torch/jit/__init__.py",
            "patches": [
                {
                    "old_start": 670,
                    "old_length": 6,
                    "new_start": 670,
                    "new_length": 10,
                    "hunk_buggy": "['             all_ok = True\\n', '             for i, (orig, ref) in enumerate(zip(original, reference)):\\n', '                 try:\\n', '                     torch.testing.assert_allclose(orig.double(), ref.double(), rtol=check_tolerance,\\n', '                                                   atol=torch.testing._get_default_tolerance(orig, ref)[1])\\n', '                 except AssertionError as e:']",
                    "hunk_fix": "@@ -670,6 +670,10 @@ def _check_trace(check_inputs, func, traced_func, check_tolerance,\n             all_ok = True\n             for i, (orig, ref) in enumerate(zip(original, reference)):\n                 try:\n+                    if orig.is_quantized:\n+                        orig = orig.dequantize()\n+                    if ref.is_quantized:\n+                        ref = ref.dequantize()\n                     torch.testing.assert_allclose(orig.double(), ref.double(), rtol=check_tolerance,\n                                                   atol=torch.testing._get_default_tolerance(orig, ref)[1])\n                 except AssertionError as e:"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 481,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/4cc6e6bbbe1fb114e7d7fb207ef2deb567950102",
    "date": "2020-02-01T13:15:50-08:00",
    "message": "Adding scalar to the c10 registration type check\n\nSummary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/32886\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D19673484\n\nPulled By: z-a-f\n\nfbshipit-source-id: ea8478a4fe6788dcb044ec1ab7d51dc50ab3fa60",
    "changes": [
        {
            "name": "register_c10_ops.cpp",
            "path": "torch/csrc/jit/register_c10_ops.cpp",
            "patches": [
                {
                    "old_start": 63,
                    "old_length": 6,
                    "new_start": 63,
                    "new_length": 8,
                    "hunk_buggy": "['             AT_ASSERT(iter->isString());\\n', '             tracer::addInputs(\\n', '                 node, args[i].name().c_str(), iter->toStringRef());\\n', '           } else if (type->kind() == TypeKind::ListType) {\\n', '             const auto& elem_type = type->expect<ListType>()->getElementType();\\n', '             if (elem_type->isSubtypeOf(TensorType::get())) {']",
                    "hunk_fix": "@@ -63,6 +63,8 @@ Operator createOperatorFromC10(const c10::OperatorHandle& op) {\n             AT_ASSERT(iter->isString());\n             tracer::addInputs(\n                 node, args[i].name().c_str(), iter->toStringRef());\n+          } else if (type->kind() == TypeKind::NumberType) {\n+            tracer::addInputs(node, args[i].name().c_str(), iter->toScalar());\n           } else if (type->kind() == TypeKind::ListType) {\n             const auto& elem_type = type->expect<ListType>()->getElementType();\n             if (elem_type->isSubtypeOf(TensorType::get())) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 482,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/22044c6f7cbdafdd340714bbe220b621e1927826",
    "date": "2020-01-07T10:04:24-08:00",
    "message": "Use TORCH_CHECK instead of AT_ASSERT in torch::cuda::gather() (#27456)\n\nSummary:\nThe error message produced by AT_ASSERT() in gather() encouraged users to file a bug report (\"please report a bug to PyTorch...\"). The assertion should be a regular argument check since it can be triggered by passing tensors with different dimensionality, e.g. `torch.cuda.comm.gather([torch.rand(1, device='cuda'), torch.rand(1, 1, device='cuda')])`.\n\nSee: https://github.com/pytorch/pytorch/issues/26400\nPull Request resolved: https://github.com/pytorch/pytorch/pull/27456\n\nDifferential Revision: D19300270\n\nPulled By: ezyang\n\nfbshipit-source-id: ec87d225e23445020b377521e0daccceb4748215",
    "changes": [
        {
            "name": "comm.cpp",
            "path": "torch/csrc/cuda/comm.cpp",
            "patches": [
                {
                    "old_start": 222,
                    "old_length": 7,
                    "new_start": 222,
                    "new_length": 10,
                    "hunk_buggy": "['   for (const auto& tensor : tensors) {\\n', '     TORCH_CHECK(\\n', '         tensor.is_cuda(), \"Gather expects all inputs to have CUDA type\");\\n', '-    AT_ASSERT(tensor.ndimension() == static_cast<int64_t>(expected_size.size()));\\n', '     expected_size[dim] = tensor.size(dim);\\n', '     for (size_t dimension = 0; dimension < expected_size.size(); ++dimension) {\\n', '       TORCH_CHECK(']",
                    "hunk_fix": "@@ -222,7 +222,10 @@ at::Tensor gather(\n   for (const auto& tensor : tensors) {\n     TORCH_CHECK(\n         tensor.is_cuda(), \"Gather expects all inputs to have CUDA type\");\n-    AT_ASSERT(tensor.ndimension() == static_cast<int64_t>(expected_size.size()));\n+    TORCH_CHECK(\n+        tensor.ndimension() == static_cast<int64_t>(expected_size.size()),\n+        \"Gather input tensors must have the same number of dimensions: got \",\n+        tensor.ndimension(), \", but expected \", expected_size.size());\n     expected_size[dim] = tensor.size(dim);\n     for (size_t dimension = 0; dimension < expected_size.size(); ++dimension) {\n       TORCH_CHECK("
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 483,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/3f5dc95b57496c4ea938be381efcdc2ea92bb4cc",
    "date": "2019-11-15T13:55:38-08:00",
    "message": "fix device check in op bench (#29918)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29918\n\nSome of the tests don't specify `device` in the input configs so filter by device won't work for them. This diff fixes that issue.\n\nTest Plan:\n```\nbuck run mode/opt //caffe2/benchmarks/operator_benchmark/pt:qpool_test -- --iterations 1 --device cpu\n# ----------------------------------------\n# PyTorch/Caffe2 Operator Micro-benchmarks\n# ----------------------------------------\n# Tag : short\n\n# Benchmarking PyTorch: QAdaptiveAvgPool2dBenchmark\n# Mode: Eager\n# Name: QAdaptiveAvgPool2dBenchmark_N4_C3_input_size(224,224)_output_size(112,112)_contigTrue_dtypetorch.qint32\n# Input: N: 4, C: 3, input_size: (224, 224), output_size: (112, 112), contig: True, dtype: torch.qint32\nForward Execution Time (us) : 2891.172\n\nReviewed By: hl475\n\nDifferential Revision: D18535766\n\nfbshipit-source-id: 09d89cf23b3caab6c0bc3b8a9ae55cc439b98e0f",
    "changes": [
        {
            "name": "benchmark_core.py",
            "path": "benchmarks/operator_benchmark/benchmark_core.py",
            "patches": [
                {
                    "old_start": 335,
                    "old_length": 7,
                    "new_start": 335,
                    "new_length": 8,
                    "hunk_buggy": "[\"                 (self.args.tag_filter == 'all' or\\n\", '                     self._check_keep(op_test_config.tag, self.args.tag_filter)) and\\n', '                 (not self.args.forward_only or op_test_config.run_backward != self.args.forward_only) and\\n', \"-                (self.args.device == 'None' or self.args.device in op_test_config.test_name)):\\n\", '             return True\\n', ' \\n', '         return False']",
                    "hunk_fix": "@@ -335,7 +335,8 @@ class BenchmarkRunner(object):\n                 (self.args.tag_filter == 'all' or\n                     self._check_keep(op_test_config.tag, self.args.tag_filter)) and\n                 (not self.args.forward_only or op_test_config.run_backward != self.args.forward_only) and\n-                (self.args.device == 'None' or self.args.device in op_test_config.test_name)):\n+                (self.args.device == 'None' or 'device' not in op_test_config.test_name or \n+                    self.args.device in op_test_config.test_name)):\n             return True\n \n         return False"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 484,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/f441bb1c2088e9ce6684765a75869a72817be39d",
    "date": "2019-11-08T09:43:51-08:00",
    "message": "check error status of CUDA launch after Magma kernels (#29003)\n\nSummary:\nas part of https://github.com/pytorch/hub/issues/62 I found that the stack-trace of a failed kernel launch was being recorded elsewhere, even with CUDA_LAUNCH_BLOCKING=1.\n\nSo, I started debugging, and found that magma launches don't do error checking.\n\nI eventually found the issue to be that I didn't compile-in sm37 SASS into the magma binary and the failure was on `x.inverse()`, and that's somehow a problem for magma 2.5.1 (but not 2.5.0).\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29003\n\nDifferential Revision: D18397358\n\nPulled By: soumith\n\nfbshipit-source-id: 04baca68eac209d7af773daddd0193697d4ab0d9",
    "changes": [
        {
            "name": "BatchLinearAlgebra.cu",
            "path": "aten/src/ATen/native/cuda/BatchLinearAlgebra.cu",
            "patches": [
                {
                    "old_start": 139,
                    "old_length": 6,
                    "new_start": 139,
                    "new_length": 7,
                    "hunk_buggy": "['     magma_int_t n, magma_int_t nrhs, double* dA, magma_int_t ldda,\\n', '     magma_int_t* ipiv, double* dB, magma_int_t lddb, magma_int_t* info) {\\n', '   magma_dgesv_gpu(n, nrhs, dA, ldda, ipiv, dB, lddb, info);\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -139,6 +139,7 @@ void magmaSolve<double>(\n     magma_int_t n, magma_int_t nrhs, double* dA, magma_int_t ldda,\n     magma_int_t* ipiv, double* dB, magma_int_t lddb, magma_int_t* info) {\n   magma_dgesv_gpu(n, nrhs, dA, ldda, ipiv, dB, lddb, info);\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 146,
                    "old_length": 6,
                    "new_start": 147,
                    "new_length": 7,
                    "hunk_buggy": "['     magma_int_t n, magma_int_t nrhs, float* dA, magma_int_t ldda,\\n', '     magma_int_t* ipiv, float* dB, magma_int_t lddb, magma_int_t* info) {\\n', '   magma_sgesv_gpu(n, nrhs, dA, ldda, ipiv, dB, lddb, info);\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -146,6 +147,7 @@ void magmaSolve<float>(\n     magma_int_t n, magma_int_t nrhs, float* dA, magma_int_t ldda,\n     magma_int_t* ipiv, float* dB, magma_int_t lddb, magma_int_t* info) {\n   magma_sgesv_gpu(n, nrhs, dA, ldda, ipiv, dB, lddb, info);\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 154,
                    "old_length": 6,
                    "new_start": 156,
                    "new_length": 7,
                    "hunk_buggy": "['     magma_int_t** dipiv_array, double** dB_array, magma_int_t lddb,\\n', '     magma_int_t* dinfo_array, magma_int_t batch_count, const MAGMAQueue& magma_queue) {\\n', '   magma_dgesv_batched(n, nrhs, dA_array, ldda, dipiv_array, dB_array, lddb, dinfo_array, batch_count, magma_queue.get_queue());\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -154,6 +156,7 @@ void magmaSolveBatched<double>(\n     magma_int_t** dipiv_array, double** dB_array, magma_int_t lddb,\n     magma_int_t* dinfo_array, magma_int_t batch_count, const MAGMAQueue& magma_queue) {\n   magma_dgesv_batched(n, nrhs, dA_array, ldda, dipiv_array, dB_array, lddb, dinfo_array, batch_count, magma_queue.get_queue());\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 162,
                    "old_length": 6,
                    "new_start": 165,
                    "new_length": 7,
                    "hunk_buggy": "['     magma_int_t** dipiv_array, float** dB_array, magma_int_t lddb,\\n', '     magma_int_t* dinfo_array, magma_int_t batch_count, const MAGMAQueue& magma_queue) {\\n', '   magma_sgesv_batched(n, nrhs, dA_array, ldda, dipiv_array, dB_array, lddb, dinfo_array, batch_count, magma_queue.get_queue());\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -162,6 +165,7 @@ void magmaSolveBatched<float>(\n     magma_int_t** dipiv_array, float** dB_array, magma_int_t lddb,\n     magma_int_t* dinfo_array, magma_int_t batch_count, const MAGMAQueue& magma_queue) {\n   magma_sgesv_batched(n, nrhs, dA_array, ldda, dipiv_array, dB_array, lddb, dinfo_array, batch_count, magma_queue.get_queue());\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 169,
                    "old_length": 6,
                    "new_start": 173,
                    "new_length": 7,
                    "hunk_buggy": "['     magma_int_t m, magma_int_t n, double* dA, magma_int_t ldda,\\n', '     magma_int_t* ipiv, magma_int_t* info) {\\n', '   magma_dgetrf_gpu(m, n, dA, ldda, ipiv, info);\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -169,6 +173,7 @@ void magmaLu<double>(\n     magma_int_t m, magma_int_t n, double* dA, magma_int_t ldda,\n     magma_int_t* ipiv, magma_int_t* info) {\n   magma_dgetrf_gpu(m, n, dA, ldda, ipiv, info);\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 176,
                    "old_length": 6,
                    "new_start": 181,
                    "new_length": 7,
                    "hunk_buggy": "['     magma_int_t m, magma_int_t n, float* dA, magma_int_t ldda,\\n', '     magma_int_t* ipiv, magma_int_t* info) {\\n', '   magma_sgetrf_gpu(m, n, dA, ldda, ipiv, info);\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -176,6 +181,7 @@ void magmaLu<float>(\n     magma_int_t m, magma_int_t n, float* dA, magma_int_t ldda,\n     magma_int_t* ipiv, magma_int_t* info) {\n   magma_sgetrf_gpu(m, n, dA, ldda, ipiv, info);\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 184,
                    "old_length": 6,
                    "new_start": 190,
                    "new_length": 7,
                    "hunk_buggy": "['     magma_int_t** ipiv_array, magma_int_t* info_array, magma_int_t batchsize,\\n', '     const MAGMAQueue& magma_queue) {\\n', '   magma_dgetrf_batched(m, n, dA_array, ldda, ipiv_array, info_array, batchsize, magma_queue.get_queue());\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -184,6 +190,7 @@ void magmaLuBatched<double>(\n     magma_int_t** ipiv_array, magma_int_t* info_array, magma_int_t batchsize,\n     const MAGMAQueue& magma_queue) {\n   magma_dgetrf_batched(m, n, dA_array, ldda, ipiv_array, info_array, batchsize, magma_queue.get_queue());\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 192,
                    "old_length": 6,
                    "new_start": 199,
                    "new_length": 7,
                    "hunk_buggy": "['     magma_int_t** ipiv_array, magma_int_t* info_array, magma_int_t batchsize,\\n', '     const MAGMAQueue& magma_queue) {\\n', '   magma_sgetrf_batched(m, n, dA_array, ldda, ipiv_array, info_array, batchsize, magma_queue.get_queue());\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -192,6 +199,7 @@ void magmaLuBatched<float>(\n     magma_int_t** ipiv_array, magma_int_t* info_array, magma_int_t batchsize,\n     const MAGMAQueue& magma_queue) {\n   magma_sgetrf_batched(m, n, dA_array, ldda, ipiv_array, info_array, batchsize, magma_queue.get_queue());\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 199,
                    "old_length": 6,
                    "new_start": 207,
                    "new_length": 7,
                    "hunk_buggy": "['     magma_int_t m, magma_int_t n, double* dA, magma_int_t ldda,\\n', '     magma_int_t* info) {\\n', '   magma_dgetrf_nopiv_gpu(m, n, dA, ldda, info);\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -199,6 +207,7 @@ void magmaLuNoPiv<double>(\n     magma_int_t m, magma_int_t n, double* dA, magma_int_t ldda,\n     magma_int_t* info) {\n   magma_dgetrf_nopiv_gpu(m, n, dA, ldda, info);\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 206,
                    "old_length": 6,
                    "new_start": 215,
                    "new_length": 7,
                    "hunk_buggy": "['     magma_int_t m, magma_int_t n, float* dA, magma_int_t ldda,\\n', '     magma_int_t* info) {\\n', '   magma_sgetrf_nopiv_gpu(m, n, dA, ldda, info);\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -206,6 +215,7 @@ void magmaLuNoPiv<float>(\n     magma_int_t m, magma_int_t n, float* dA, magma_int_t ldda,\n     magma_int_t* info) {\n   magma_sgetrf_nopiv_gpu(m, n, dA, ldda, info);\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 213,
                    "old_length": 6,
                    "new_start": 223,
                    "new_length": 7,
                    "hunk_buggy": "['     magma_int_t m, magma_int_t n, double** dA_array, magma_int_t ldda,\\n', '     magma_int_t* info_array, magma_int_t batchsize, const MAGMAQueue& magma_queue) {\\n', '   magma_dgetrf_nopiv_batched(m, n, dA_array, ldda, info_array, batchsize, magma_queue.get_queue());\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -213,6 +223,7 @@ void magmaLuNoPivBatched<double>(\n     magma_int_t m, magma_int_t n, double** dA_array, magma_int_t ldda,\n     magma_int_t* info_array, magma_int_t batchsize, const MAGMAQueue& magma_queue) {\n   magma_dgetrf_nopiv_batched(m, n, dA_array, ldda, info_array, batchsize, magma_queue.get_queue());\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 220,
                    "old_length": 6,
                    "new_start": 231,
                    "new_length": 7,
                    "hunk_buggy": "['     magma_int_t m, magma_int_t n, float** dA_array, magma_int_t ldda,\\n', '     magma_int_t* info_array, magma_int_t batchsize, const MAGMAQueue& magma_queue) {\\n', '   magma_sgetrf_nopiv_batched(m, n, dA_array, ldda, info_array, batchsize, magma_queue.get_queue());\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -220,6 +231,7 @@ void magmaLuNoPivBatched<float>(\n     magma_int_t m, magma_int_t n, float** dA_array, magma_int_t ldda,\n     magma_int_t* info_array, magma_int_t batchsize, const MAGMAQueue& magma_queue) {\n   magma_sgetrf_nopiv_batched(m, n, dA_array, ldda, info_array, batchsize, magma_queue.get_queue());\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 237,
                    "old_length": 6,
                    "new_start": 249,
                    "new_length": 7,
                    "hunk_buggy": "['     magma_int_t n, double* dA, magma_int_t ldda, magma_int_t* ipiv, double* dwork,\\n', '     magma_int_t lwork, magma_int_t* info) {\\n', '   magma_dgetri_gpu(n, dA, ldda, ipiv, dwork, lwork, info);\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -237,6 +249,7 @@ void magmaGetri<double>(\n     magma_int_t n, double* dA, magma_int_t ldda, magma_int_t* ipiv, double* dwork,\n     magma_int_t lwork, magma_int_t* info) {\n   magma_dgetri_gpu(n, dA, ldda, ipiv, dwork, lwork, info);\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 244,
                    "old_length": 6,
                    "new_start": 257,
                    "new_length": 7,
                    "hunk_buggy": "['     magma_int_t n, float* dA, magma_int_t ldda, magma_int_t* ipiv, float* dwork,\\n', '     magma_int_t lwork, magma_int_t* info) {\\n', '   magma_sgetri_gpu(n, dA, ldda, ipiv, dwork, lwork, info);\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -244,6 +257,7 @@ void magmaGetri<float>(\n     magma_int_t n, float* dA, magma_int_t ldda, magma_int_t* ipiv, float* dwork,\n     magma_int_t lwork, magma_int_t* info) {\n   magma_sgetri_gpu(n, dA, ldda, ipiv, dwork, lwork, info);\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 252,
                    "old_length": 6,
                    "new_start": 266,
                    "new_length": 7,
                    "hunk_buggy": "['     magma_int_t** ipiv_array, double** dinvA_array, magma_int_t lddia,\\n', '     magma_int_t* info_array, magma_int_t batchsize, const MAGMAQueue& magma_queue) {\\n', '   magma_dgetri_outofplace_batched(n, dA_array, ldda, ipiv_array, dinvA_array, lddia, info_array, batchsize, magma_queue.get_queue());\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -252,6 +266,7 @@ void magmaGetriBatched<double>(\n     magma_int_t** ipiv_array, double** dinvA_array, magma_int_t lddia,\n     magma_int_t* info_array, magma_int_t batchsize, const MAGMAQueue& magma_queue) {\n   magma_dgetri_outofplace_batched(n, dA_array, ldda, ipiv_array, dinvA_array, lddia, info_array, batchsize, magma_queue.get_queue());\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 260,
                    "old_length": 6,
                    "new_start": 275,
                    "new_length": 7,
                    "hunk_buggy": "['     magma_int_t** ipiv_array, float** dinvA_array, magma_int_t lddia,\\n', '     magma_int_t* info_array, magma_int_t batchsize, const MAGMAQueue& magma_queue) {\\n', '   magma_sgetri_outofplace_batched(n, dA_array, ldda, ipiv_array, dinvA_array, lddia, info_array, batchsize, magma_queue.get_queue());\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -260,6 +275,7 @@ void magmaGetriBatched<float>(\n     magma_int_t** ipiv_array, float** dinvA_array, magma_int_t lddia,\n     magma_int_t* info_array, magma_int_t batchsize, const MAGMAQueue& magma_queue) {\n   magma_sgetri_outofplace_batched(n, dA_array, ldda, ipiv_array, dinvA_array, lddia, info_array, batchsize, magma_queue.get_queue());\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 267,
                    "old_length": 6,
                    "new_start": 283,
                    "new_length": 7,
                    "hunk_buggy": "['     magma_uplo_t uplo, magma_int_t n, magma_int_t nrhs, double* dA, magma_int_t ldda,\\n', '     double* dB, magma_int_t lddb, magma_int_t* info) {\\n', '   magma_dpotrs_gpu(uplo, n, nrhs, dA, ldda, dB, lddb, info);\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -267,6 +283,7 @@ void magmaCholeskySolve<double>(\n     magma_uplo_t uplo, magma_int_t n, magma_int_t nrhs, double* dA, magma_int_t ldda,\n     double* dB, magma_int_t lddb, magma_int_t* info) {\n   magma_dpotrs_gpu(uplo, n, nrhs, dA, ldda, dB, lddb, info);\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 274,
                    "old_length": 6,
                    "new_start": 291,
                    "new_length": 7,
                    "hunk_buggy": "['     magma_uplo_t uplo, magma_int_t n, magma_int_t nrhs, float* dA, magma_int_t ldda,\\n', '     float* dB, magma_int_t lddb, magma_int_t* info) {\\n', '   magma_spotrs_gpu(uplo, n, nrhs, dA, ldda, dB, lddb, info);\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -274,6 +291,7 @@ void magmaCholeskySolve<float>(\n     magma_uplo_t uplo, magma_int_t n, magma_int_t nrhs, float* dA, magma_int_t ldda,\n     float* dB, magma_int_t lddb, magma_int_t* info) {\n   magma_spotrs_gpu(uplo, n, nrhs, dA, ldda, dB, lddb, info);\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 281,
                    "old_length": 6,
                    "new_start": 299,
                    "new_length": 7,
                    "hunk_buggy": "['     magma_uplo_t uplo, magma_int_t n, magma_int_t nrhs, double** dA_array, magma_int_t ldda,\\n', '     double** dB_array, magma_int_t lddb, magma_int_t& info, magma_int_t batchsize, const MAGMAQueue& magma_queue) {\\n', '   info = magma_dpotrs_batched(uplo, n, nrhs, dA_array, ldda, dB_array, lddb, batchsize, magma_queue.get_queue());\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -281,6 +299,7 @@ void magmaCholeskySolveBatched<double>(\n     magma_uplo_t uplo, magma_int_t n, magma_int_t nrhs, double** dA_array, magma_int_t ldda,\n     double** dB_array, magma_int_t lddb, magma_int_t& info, magma_int_t batchsize, const MAGMAQueue& magma_queue) {\n   info = magma_dpotrs_batched(uplo, n, nrhs, dA_array, ldda, dB_array, lddb, batchsize, magma_queue.get_queue());\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 288,
                    "old_length": 6,
                    "new_start": 307,
                    "new_length": 7,
                    "hunk_buggy": "['     magma_uplo_t uplo, magma_int_t n, magma_int_t nrhs, float** dA_array, magma_int_t ldda,\\n', '     float** dB_array, magma_int_t lddb, magma_int_t& info, magma_int_t batchsize, const MAGMAQueue& magma_queue) {\\n', '   info = magma_spotrs_batched(uplo, n, nrhs, dA_array, ldda, dB_array, lddb, batchsize, magma_queue.get_queue());\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -288,6 +307,7 @@ void magmaCholeskySolveBatched<float>(\n     magma_uplo_t uplo, magma_int_t n, magma_int_t nrhs, float** dA_array, magma_int_t ldda,\n     float** dB_array, magma_int_t lddb, magma_int_t& info, magma_int_t batchsize, const MAGMAQueue& magma_queue) {\n   info = magma_spotrs_batched(uplo, n, nrhs, dA_array, ldda, dB_array, lddb, batchsize, magma_queue.get_queue());\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 295,
                    "old_length": 6,
                    "new_start": 315,
                    "new_length": 7,
                    "hunk_buggy": "['     magma_uplo_t uplo, magma_int_t n, double* dA,\\n', '     magma_int_t ldda, magma_int_t* info) {\\n', '   magma_dpotrf_gpu(uplo, n, dA, ldda, info);\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -295,6 +315,7 @@ void magmaCholesky<double>(\n     magma_uplo_t uplo, magma_int_t n, double* dA,\n     magma_int_t ldda, magma_int_t* info) {\n   magma_dpotrf_gpu(uplo, n, dA, ldda, info);\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 302,
                    "old_length": 6,
                    "new_start": 323,
                    "new_length": 7,
                    "hunk_buggy": "['     magma_uplo_t uplo, magma_int_t n, float* dA,\\n', '     magma_int_t ldda, magma_int_t* info) {\\n', '   magma_spotrf_gpu(uplo, n, dA, ldda, info);\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -302,6 +323,7 @@ void magmaCholesky<float>(\n     magma_uplo_t uplo, magma_int_t n, float* dA,\n     magma_int_t ldda, magma_int_t* info) {\n   magma_spotrf_gpu(uplo, n, dA, ldda, info);\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 309,
                    "old_length": 6,
                    "new_start": 331,
                    "new_length": 7,
                    "hunk_buggy": "['     magma_uplo_t uplo, magma_int_t n, double** dA_array, magma_int_t ldda,\\n', '     magma_int_t* info_array, magma_int_t batchsize, const MAGMAQueue& magma_queue) {\\n', '   magma_dpotrf_batched(uplo, n, dA_array, ldda, info_array, batchsize, magma_queue.get_queue());\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -309,6 +331,7 @@ void magmaCholeskyBatched<double>(\n     magma_uplo_t uplo, magma_int_t n, double** dA_array, magma_int_t ldda,\n     magma_int_t* info_array, magma_int_t batchsize, const MAGMAQueue& magma_queue) {\n   magma_dpotrf_batched(uplo, n, dA_array, ldda, info_array, batchsize, magma_queue.get_queue());\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 316,
                    "old_length": 6,
                    "new_start": 339,
                    "new_length": 7,
                    "hunk_buggy": "['     magma_uplo_t uplo, magma_int_t n, float** dA_array, magma_int_t ldda,\\n', '     magma_int_t* info_array, magma_int_t batchsize, const MAGMAQueue& magma_queue) {\\n', '   magma_spotrf_batched(uplo, n, dA_array, ldda, info_array, batchsize, magma_queue.get_queue());\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -316,6 +339,7 @@ void magmaCholeskyBatched<float>(\n     magma_uplo_t uplo, magma_int_t n, float** dA_array, magma_int_t ldda,\n     magma_int_t* info_array, magma_int_t batchsize, const MAGMAQueue& magma_queue) {\n   magma_spotrf_batched(uplo, n, dA_array, ldda, info_array, batchsize, magma_queue.get_queue());\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 323,
                    "old_length": 6,
                    "new_start": 347,
                    "new_length": 7,
                    "hunk_buggy": "['     magma_uplo_t uplo, magma_trans_t trans, magma_diag_t diag, magma_int_t m, magma_int_t n,\\n', '     double* dA, magma_int_t ldda, double* dB, magma_int_t lddb) {\\n', '   magma_dtrsm(MagmaLeft, uplo, trans, diag, m, n, 1, dA, ldda, dB, lddb);\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -323,6 +347,7 @@ void magmaTriangularSolve<double>(\n     magma_uplo_t uplo, magma_trans_t trans, magma_diag_t diag, magma_int_t m, magma_int_t n,\n     double* dA, magma_int_t ldda, double* dB, magma_int_t lddb) {\n   magma_dtrsm(MagmaLeft, uplo, trans, diag, m, n, 1, dA, ldda, dB, lddb);\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 330,
                    "old_length": 6,
                    "new_start": 355,
                    "new_length": 7,
                    "hunk_buggy": "['     magma_uplo_t uplo, magma_trans_t trans, magma_diag_t diag, magma_int_t m, magma_int_t n,\\n', '     float* dA, magma_int_t ldda, float* dB, magma_int_t lddb) {\\n', '   magma_strsm(MagmaLeft, uplo, trans, diag, m, n, 1, dA, ldda, dB, lddb);\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -330,6 +355,7 @@ void magmaTriangularSolve<float>(\n     magma_uplo_t uplo, magma_trans_t trans, magma_diag_t diag, magma_int_t m, magma_int_t n,\n     float* dA, magma_int_t ldda, float* dB, magma_int_t lddb) {\n   magma_strsm(MagmaLeft, uplo, trans, diag, m, n, 1, dA, ldda, dB, lddb);\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 338,
                    "old_length": 6,
                    "new_start": 364,
                    "new_length": 7,
                    "hunk_buggy": "['     double** dA_array, magma_int_t ldda, double** dB_array, magma_int_t lddb, magma_int_t batchsize,\\n', '     const MAGMAQueue& magma_queue) {\\n', '   magmablas_dtrsm_batched(MagmaLeft, uplo, trans, diag, m, n, 1, dA_array, ldda, dB_array, lddb, batchsize, magma_queue.get_queue());\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -338,6 +364,7 @@ void magmaTriangularSolveBatched<double>(\n     double** dA_array, magma_int_t ldda, double** dB_array, magma_int_t lddb, magma_int_t batchsize,\n     const MAGMAQueue& magma_queue) {\n   magmablas_dtrsm_batched(MagmaLeft, uplo, trans, diag, m, n, 1, dA_array, ldda, dB_array, lddb, batchsize, magma_queue.get_queue());\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 346,
                    "old_length": 6,
                    "new_start": 373,
                    "new_length": 7,
                    "hunk_buggy": "['     float** dA_array, magma_int_t ldda, float** dB_array, magma_int_t lddb, magma_int_t batchsize,\\n', '     const MAGMAQueue& magma_queue) {\\n', '   magmablas_strsm_batched(MagmaLeft, uplo, trans, diag, m, n, 1, dA_array, ldda, dB_array, lddb, batchsize, magma_queue.get_queue());\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -346,6 +373,7 @@ void magmaTriangularSolveBatched<float>(\n     float** dA_array, magma_int_t ldda, float** dB_array, magma_int_t lddb, magma_int_t batchsize,\n     const MAGMAQueue& magma_queue) {\n   magmablas_strsm_batched(MagmaLeft, uplo, trans, diag, m, n, 1, dA_array, ldda, dB_array, lddb, batchsize, magma_queue.get_queue());\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 367,
                    "old_length": 6,
                    "new_start": 395,
                    "new_length": 7,
                    "hunk_buggy": "['   } else {\\n', '     magma_dgeqrf2_gpu(m, n, dA, ldda, tau, info);\\n', '   }\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -367,6 +395,7 @@ void magmaGeqrf<double>(\n   } else {\n     magma_dgeqrf2_gpu(m, n, dA, ldda, tau, info);\n   }\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 378,
                    "old_length": 6,
                    "new_start": 407,
                    "new_length": 7,
                    "hunk_buggy": "['   } else {\\n', '     magma_sgeqrf2_gpu(m, n, dA, ldda, tau, info);\\n', '   }\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -378,6 +407,7 @@ void magmaGeqrf<float>(\n   } else {\n     magma_sgeqrf2_gpu(m, n, dA, ldda, tau, info);\n   }\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 385,
                    "old_length": 6,
                    "new_start": 415,
                    "new_length": 7,
                    "hunk_buggy": "['     magma_int_t m, magma_int_t n, magma_int_t k, double* dA, magma_int_t ldda,\\n', '     double* tau, double* dT, magma_int_t nb, magma_int_t* info) {\\n', '   magma_dorgqr_gpu(m, n, k, dA, ldda, tau, dT, nb, info);\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -385,6 +415,7 @@ void magmaOrgqr<double>(\n     magma_int_t m, magma_int_t n, magma_int_t k, double* dA, magma_int_t ldda,\n     double* tau, double* dT, magma_int_t nb, magma_int_t* info) {\n   magma_dorgqr_gpu(m, n, k, dA, ldda, tau, dT, nb, info);\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 392,
                    "old_length": 6,
                    "new_start": 423,
                    "new_length": 7,
                    "hunk_buggy": "['     magma_int_t m, magma_int_t n, magma_int_t k, float* dA, magma_int_t ldda,\\n', '     float* tau, float* dT, magma_int_t nb, magma_int_t* info) {\\n', '   magma_sorgqr_gpu(m, n, k, dA, ldda, tau, dT, nb, info);\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -392,6 +423,7 @@ void magmaOrgqr<float>(\n     magma_int_t m, magma_int_t n, magma_int_t k, float* dA, magma_int_t ldda,\n     float* tau, float* dT, magma_int_t nb, magma_int_t* info) {\n   magma_sorgqr_gpu(m, n, k, dA, ldda, tau, dT, nb, info);\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 400,
                    "old_length": 6,
                    "new_start": 432,
                    "new_length": 7,
                    "hunk_buggy": "['     double* w, double* wA, magma_int_t ldwa, double* work, magma_int_t lwork,\\n', '     magma_int_t* iwork, magma_int_t liwork, magma_int_t* info) {\\n', '   magma_dsyevd_gpu(jobz, uplo, n, dA, ldda, w, wA, ldwa, work, lwork, iwork, liwork, info);\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -400,6 +432,7 @@ void magmaSymeig<double>(\n     double* w, double* wA, magma_int_t ldwa, double* work, magma_int_t lwork,\n     magma_int_t* iwork, magma_int_t liwork, magma_int_t* info) {\n   magma_dsyevd_gpu(jobz, uplo, n, dA, ldda, w, wA, ldwa, work, lwork, iwork, liwork, info);\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 408,
                    "old_length": 6,
                    "new_start": 441,
                    "new_length": 7,
                    "hunk_buggy": "['     float* w, float* wA, magma_int_t ldwa, float* work, magma_int_t lwork,\\n', '     magma_int_t* iwork, magma_int_t liwork, magma_int_t* info) {\\n', '   magma_ssyevd_gpu(jobz, uplo, n, dA, ldda, w, wA, ldwa, work, lwork, iwork, liwork, info);\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -408,6 +441,7 @@ void magmaSymeig<float>(\n     float* w, float* wA, magma_int_t ldwa, float* work, magma_int_t lwork,\n     magma_int_t* iwork, magma_int_t liwork, magma_int_t* info) {\n   magma_ssyevd_gpu(jobz, uplo, n, dA, ldda, w, wA, ldwa, work, lwork, iwork, liwork, info);\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 417,
                    "old_length": 6,
                    "new_start": 451,
                    "new_length": 7,
                    "hunk_buggy": "['     double* VT, magma_int_t ldvt, double* work, magma_int_t lwork,\\n', '     magma_int_t* iwork, magma_int_t* info) {\\n', '   magma_dgesdd(jobz, m, n, A, lda, s, U, ldu, VT, ldvt, work, lwork, iwork, info);\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -417,6 +451,7 @@ void magmaSvd<double>(\n     double* VT, magma_int_t ldvt, double* work, magma_int_t lwork,\n     magma_int_t* iwork, magma_int_t* info) {\n   magma_dgesdd(jobz, m, n, A, lda, s, U, ldu, VT, ldvt, work, lwork, iwork, info);\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 426,
                    "old_length": 6,
                    "new_start": 461,
                    "new_length": 7,
                    "hunk_buggy": "['     float* VT, magma_int_t ldvt, float* work, magma_int_t lwork,\\n', '     magma_int_t* iwork, magma_int_t* info) {\\n', '   magma_sgesdd(jobz, m, n, A, lda, s, U, ldu, VT, ldvt, work, lwork, iwork, info);\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -426,6 +461,7 @@ void magmaSvd<float>(\n     float* VT, magma_int_t ldvt, float* work, magma_int_t lwork,\n     magma_int_t* iwork, magma_int_t* info) {\n   magma_sgesdd(jobz, m, n, A, lda, s, U, ldu, VT, ldvt, work, lwork, iwork, info);\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 433,
                    "old_length": 6,
                    "new_start": 469,
                    "new_length": 7,
                    "hunk_buggy": "['     magma_int_t n, magma_int_t nrhs, double* dA, magma_int_t ldda, magma_int_t* ipiv,\\n', '     double* dB, magma_int_t lddb, magma_int_t* info) {\\n', '   magma_dgetrs_gpu(MagmaNoTrans, n, nrhs, dA, ldda, ipiv, dB, lddb, info);\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -433,6 +469,7 @@ void magmaLuSolve<double>(\n     magma_int_t n, magma_int_t nrhs, double* dA, magma_int_t ldda, magma_int_t* ipiv,\n     double* dB, magma_int_t lddb, magma_int_t* info) {\n   magma_dgetrs_gpu(MagmaNoTrans, n, nrhs, dA, ldda, ipiv, dB, lddb, info);\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 440,
                    "old_length": 6,
                    "new_start": 477,
                    "new_length": 7,
                    "hunk_buggy": "['     magma_int_t n, magma_int_t nrhs, float* dA, magma_int_t ldda, magma_int_t* ipiv,\\n', '     float* dB, magma_int_t lddb, magma_int_t* info) {\\n', '   magma_sgetrs_gpu(MagmaNoTrans, n, nrhs, dA, ldda, ipiv, dB, lddb, info);\\n', ' }\\n', ' \\n', ' \\n']",
                    "hunk_fix": "@@ -440,6 +477,7 @@ void magmaLuSolve<float>(\n     magma_int_t n, magma_int_t nrhs, float* dA, magma_int_t ldda, magma_int_t* ipiv,\n     float* dB, magma_int_t lddb, magma_int_t* info) {\n   magma_sgetrs_gpu(MagmaNoTrans, n, nrhs, dA, ldda, ipiv, dB, lddb, info);\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n \n"
                },
                {
                    "old_start": 449,
                    "old_length": 6,
                    "new_start": 487,
                    "new_length": 7,
                    "hunk_buggy": "['     double** dB_array, magma_int_t lddb, magma_int_t& info,\\n', '     magma_int_t batchsize, const MAGMAQueue& magma_queue) {\\n', '   info = magma_dgetrs_batched(MagmaNoTrans, n, nrhs, dA_array, ldda, dipiv_array, dB_array, lddb, batchsize, magma_queue.get_queue());\\n', ' }\\n', ' \\n', ' template<>\\n']",
                    "hunk_fix": "@@ -449,6 +487,7 @@ void magmaLuSolveBatched<double>(\n     double** dB_array, magma_int_t lddb, magma_int_t& info,\n     magma_int_t batchsize, const MAGMAQueue& magma_queue) {\n   info = magma_dgetrs_batched(MagmaNoTrans, n, nrhs, dA_array, ldda, dipiv_array, dB_array, lddb, batchsize, magma_queue.get_queue());\n+  AT_CUDA_CHECK(cudaGetLastError());\n }\n \n template<>\n"
                },
                {
                    "old_start": 457,
                    "old_length": 6,
                    "new_start": 496,
                    "new_length": 7,
                    "hunk_buggy": "['     float** dB_array, magma_int_t lddb, magma_int_t& info,\\n', '     magma_int_t batchsize, const MAGMAQueue& magma_queue) {\\n', '  info = magma_sgetrs_batched(MagmaNoTrans, n, nrhs, dA_array, ldda, dipiv_array, dB_array, lddb, batchsize, magma_queue.get_queue());\\n', ' }\\n', ' #endif\\n', ' ']",
                    "hunk_fix": "@@ -457,6 +496,7 @@ void magmaLuSolveBatched<float>(\n     float** dB_array, magma_int_t lddb, magma_int_t& info,\n     magma_int_t batchsize, const MAGMAQueue& magma_queue) {\n  info = magma_sgetrs_batched(MagmaNoTrans, n, nrhs, dA_array, ldda, dipiv_array, dB_array, lddb, batchsize, magma_queue.get_queue());\n+ AT_CUDA_CHECK(cudaGetLastError());\n }\n #endif\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 485,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/fa66a1498eb1fac5b36811d5c1d6ba1540ffc824",
    "date": "2019-11-07T15:53:05-08:00",
    "message": "Simplify _calculate_fan_in_and_fan_out (#29370)\n\nSummary:\nThe code checking `if dimensions == 2` is not needed\nbecause the case of a 2D tensor (Linear) is already handled\nby the statement:\n`receptive_field_size = 1`\nand this conditional:\n`if tensor.dim() > 2:`\nPull Request resolved: https://github.com/pytorch/pytorch/pull/29370\n\nDifferential Revision: D18372987\n\nPulled By: albanD\n\nfbshipit-source-id: fcb4dddbc76b9f4414c6d88c0aa2fb4435bf3385",
    "changes": [
        {
            "name": "init.py",
            "path": "torch/nn/init.py",
            "patches": [
                {
                    "old_start": 204,
                    "old_length": 17,
                    "new_start": 204,
                    "new_length": 13,
                    "hunk_buggy": "['     if dimensions < 2:\\n', '         raise ValueError(\"Fan in and fan out can not be computed for tensor with fewer than 2 dimensions\")\\n', ' \\n', '-    if dimensions == 2:  # Linear\\n', '-        fan_in = tensor.size(1)\\n', '-        fan_out = tensor.size(0)\\n', '-    else:\\n', '-        num_input_fmaps = tensor.size(1)\\n', '-        num_output_fmaps = tensor.size(0)\\n', '-        receptive_field_size = 1\\n', '-        if tensor.dim() > 2:\\n', '-            receptive_field_size = tensor[0][0].numel()\\n', '-        fan_in = num_input_fmaps * receptive_field_size\\n', '-        fan_out = num_output_fmaps * receptive_field_size\\n', ' \\n', '     return fan_in, fan_out\\n', ' ']",
                    "hunk_fix": "@@ -204,17 +204,13 @@ def _calculate_fan_in_and_fan_out(tensor):\n     if dimensions < 2:\n         raise ValueError(\"Fan in and fan out can not be computed for tensor with fewer than 2 dimensions\")\n \n-    if dimensions == 2:  # Linear\n-        fan_in = tensor.size(1)\n-        fan_out = tensor.size(0)\n-    else:\n-        num_input_fmaps = tensor.size(1)\n-        num_output_fmaps = tensor.size(0)\n-        receptive_field_size = 1\n-        if tensor.dim() > 2:\n-            receptive_field_size = tensor[0][0].numel()\n-        fan_in = num_input_fmaps * receptive_field_size\n-        fan_out = num_output_fmaps * receptive_field_size\n+    num_input_fmaps = tensor.size(1)\n+    num_output_fmaps = tensor.size(0)\n+    receptive_field_size = 1\n+    if tensor.dim() > 2:\n+        receptive_field_size = tensor[0][0].numel()\n+    fan_in = num_input_fmaps * receptive_field_size\n+    fan_out = num_output_fmaps * receptive_field_size\n \n     return fan_in, fan_out\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 486,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/2e224d62b6afecc78d885d0a4e160354950f6424",
    "date": "2019-08-27T18:43:11-07:00",
    "message": "Add USE_CUDNN check to AT_CUDNN_ENABLED definition (#25037)\n\nSummary:\nWe have environment variable USE_CUDNN with self-explanatory name. However cpp code is compiled based on cpp macro definition AT_CUDNN_ENABLED, which is defined as:\n\n```\n  IF (NOT AT_CUDA_ENABLED OR NOT CUDNN_FOUND)\n    MESSAGE(STATUS \"CuDNN not found. Compiling without CuDNN support\")\n    set(AT_CUDNN_ENABLED 0)\n  ELSE()\n    include_directories(SYSTEM ${CUDNN_INCLUDE_DIRS})\n    set(AT_CUDNN_ENABLED 1)\n  ENDIF()\n```\n\nSo, even if USE_CUDNN is set to 0, cpp is compiled with cuDNN if cmake finds cuDNN in the system. I actually tested it and was very surprised when I was debugging cuDNN code which I built with USE_CUDNN=0. I believe that cmake code above should look like this:\n\n`IF (NOT AT_CUDA_ENABLED OR NOT CUDNN_FOUND OR NOT USE_CUDNN) ...`\nPull Request resolved: https://github.com/pytorch/pytorch/pull/25037\n\nDifferential Revision: D17048683\n\nPulled By: pbelevich\n\nfbshipit-source-id: 48afa19eaae0bba2ffd49c1f68db0b4efd5cf85e",
    "changes": [
        {
            "name": "Dependencies.cmake",
            "path": "cmake/Dependencies.cmake",
            "patches": [
                {
                    "old_start": 1266,
                    "old_length": 8,
                    "new_start": 1266,
                    "new_length": 11,
                    "hunk_buggy": "['     SET(AT_CUDA_ENABLED 1)\\n', '   endif()\\n', ' \\n', '-  IF (NOT AT_CUDA_ENABLED OR NOT CUDNN_FOUND)\\n', '-    MESSAGE(STATUS \"CuDNN not found. Compiling without CuDNN support\")\\n', '     set(AT_CUDNN_ENABLED 0)\\n', '   ELSE()\\n', '     include_directories(SYSTEM ${CUDNN_INCLUDE_PATH})']",
                    "hunk_fix": "@@ -1266,8 +1266,11 @@ if (NOT INTERN_BUILD_MOBILE)\n     SET(AT_CUDA_ENABLED 1)\n   endif()\n \n-  IF (NOT AT_CUDA_ENABLED OR NOT CUDNN_FOUND)\n-    MESSAGE(STATUS \"CuDNN not found. Compiling without CuDNN support\")\n+  IF (NOT USE_CUDNN)\n+    MESSAGE(STATUS \"USE_CUDNN is set to 0. Compiling without cuDNN support\")\n+    set(AT_CUDNN_ENABLED 0)\n+  ELSEIF (NOT CUDNN_FOUND)\n+    MESSAGE(WARNING \"CuDNN not found. Compiling without CuDNN support\")\n     set(AT_CUDNN_ENABLED 0)\n   ELSE()\n     include_directories(SYSTEM ${CUDNN_INCLUDE_PATH})"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 487,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/24601daa1203a9ad1232e1d18a07ff4842d53d27",
    "date": "2019-07-23T21:11:06-07:00",
    "message": "Adding check for a single batch in adaptive_avg_pool\n\nSummary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/23137\n\nTest Plan: Imported from OSS\n\nDifferential Revision: D16403804\n\nPulled By: zafartahirov\n\nfbshipit-source-id: df79a8c768ffabeceb4c0044c967a623c5885484",
    "changes": [
        {
            "name": "AdaptiveAveragePooling.cpp",
            "path": "aten/src/ATen/native/AdaptiveAveragePooling.cpp",
            "patches": [
                {
                    "old_start": 129,
                    "old_length": 10,
                    "new_start": 129,
                    "new_length": 13,
                    "hunk_buggy": "['     auto osizeW = output_size[1];\\n', ' \\n', '     /* resize output */\\n', '-    if (input.ndimension() == 3)\\n', '     {\\n', '-      output.resize_({sizeD, osizeH, osizeW});\\n', '-\\n', '       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.scalar_type(), \"adaptive_avg_pool2d_cpu\", [&] {\\n', '           auto input_data = input.data<scalar_t>();\\n', '           auto output_data = output.data<scalar_t>();\\n']",
                    "hunk_fix": "@@ -129,10 +129,13 @@ namespace {\n     auto osizeW = output_size[1];\n \n     /* resize output */\n-    if (input.ndimension() == 3)\n+    if (input.ndimension() == 3 || input.size(-4) == 1)\n     {\n-      output.resize_({sizeD, osizeH, osizeW});\n-\n+      if (input.ndimension() == 3) {\n+        output.resize_({sizeD, osizeH, osizeW});\n+      } else {\n+        output.resize_({1, sizeD, osizeH, osizeW});\n+      }\n       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.scalar_type(), \"adaptive_avg_pool2d_cpu\", [&] {\n           auto input_data = input.data<scalar_t>();\n           auto output_data = output.data<scalar_t>();\n"
                },
                {
                    "old_start": 260,
                    "old_length": 7,
                    "new_start": 263,
                    "new_length": 7,
                    "hunk_buggy": "['     auto gradOutput = gradOutput_.contiguous();\\n', ' \\n', '     /* backprop */\\n', '-    if (input.ndimension() == 3)\\n', '     {\\n', '       AT_DISPATCH_FLOATING_TYPES_AND_HALF(\\n', '         input.scalar_type(), \"adaptive_avg_pool2d_backward_cpu\", [&] {']",
                    "hunk_fix": "@@ -260,7 +263,7 @@ namespace {\n     auto gradOutput = gradOutput_.contiguous();\n \n     /* backprop */\n-    if (input.ndimension() == 3)\n+    if (input.ndimension() == 3 || input.size(-4) == 1)\n     {\n       AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n         input.scalar_type(), \"adaptive_avg_pool2d_backward_cpu\", [&] {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 488,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/85cbe0d8258ab06897e2f049e61f74d8aa935240",
    "date": "2019-06-24T10:32:48-07:00",
    "message": "Fix Concat Dimension Bug (#22088)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/22088\n\nThis diff is similar to D14163001. We need to handle the edge case when add_axis=1.\n\nReviewed By: jspark1105\n\nDifferential Revision: D15949003\n\nfbshipit-source-id: 328d1e07b78b69bde81eee78c9ff5a8fb81f629b",
    "changes": [
        {
            "name": "concat_split_op.cc",
            "path": "caffe2/operators/concat_split_op.cc",
            "patches": [
                {
                    "old_start": 196,
                    "old_length": 7,
                    "new_start": 196,
                    "new_length": 9,
                    "hunk_buggy": "['       : GetDimFromOrderString(\\n', '             helper.GetSingleArgument<string>(\"order\", \"NCHW\"));\\n', '   bool add_axis = helper.GetSingleArgument<int>(\"add_axis\", 0) != 0;\\n', '-  const int canonical_axis = canonical_axis_index_(axis, in[0].dims_size());\\n', '   CAFFE_ENFORCE_GT(in.size(), 0);\\n', '   vector<int> out_shape(in[0].dims().begin(), in[0].dims().end());\\n', '   if (add_axis) {']",
                    "hunk_fix": "@@ -196,7 +196,9 @@ OpSchema::Cost CostInferenceForConcat(\n       : GetDimFromOrderString(\n             helper.GetSingleArgument<string>(\"order\", \"NCHW\"));\n   bool add_axis = helper.GetSingleArgument<int>(\"add_axis\", 0) != 0;\n-  const int canonical_axis = canonical_axis_index_(axis, in[0].dims_size());\n+  int adj_size = in[0].dims_size() + (add_axis ? 1 : 0);\n+  const int canonical_axis = canonical_axis_index_(axis, adj_size);\n+  CAFFE_ENFORCE_LT(canonical_axis, adj_size, \"Axis not in input ndim range.\");\n   CAFFE_ENFORCE_GT(in.size(), 0);\n   vector<int> out_shape(in[0].dims().begin(), in[0].dims().end());\n   if (add_axis) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 489,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/8dda19b79f2c4418f481a9f56932b3b5c5afdf39",
    "date": "2019-05-29T00:53:53-07:00",
    "message": "Remove extraneous TensorId checks in as_strided (#21045)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/21045\nghimport-source-id: e95fbf50bccf6ebc613bb13fb16915254912f22d\n\nDifferential Revision: D15528971\n\nPulled By: li-roy\n\nfbshipit-source-id: c721cc6280dff6e14c5533681d0b35aaa8f98f00",
    "changes": [
        {
            "name": "TensorShape.cpp",
            "path": "aten/src/ATen/native/TensorShape.cpp",
            "patches": [
                {
                    "old_start": 301,
                    "old_length": 9,
                    "new_start": 301,
                    "new_length": 6,
                    "hunk_buggy": "[' Tensor as_strided_tensorimpl(const Tensor& self, IntArrayRef size, IntArrayRef stride, optional<int64_t> storage_offset_) {\\n', '   auto storage_offset = storage_offset_.value_or(self.storage_offset());\\n', '   auto tid = self.type_id();\\n', '-  TORCH_CHECK(\\n', '-      tid == CPUTensorId() || tid == CUDATensorId(),\\n', '-      \"as_strided is only implemented for strided CPU, CUDA and QuantizedCPU tensors.\");\\n', '   auto result = detail::make_tensor<TensorImpl>(Storage(self.storage()), tid);\\n', '   setStrided(result, size, stride, storage_offset);\\n', '   return result;\\n']",
                    "hunk_fix": "@@ -301,9 +301,6 @@ Tensor sum_to_size(const Tensor& self, IntArrayRef size) {\n Tensor as_strided_tensorimpl(const Tensor& self, IntArrayRef size, IntArrayRef stride, optional<int64_t> storage_offset_) {\n   auto storage_offset = storage_offset_.value_or(self.storage_offset());\n   auto tid = self.type_id();\n-  TORCH_CHECK(\n-      tid == CPUTensorId() || tid == CUDATensorId(),\n-      \"as_strided is only implemented for strided CPU, CUDA and QuantizedCPU tensors.\");\n   auto result = detail::make_tensor<TensorImpl>(Storage(self.storage()), tid);\n   setStrided(result, size, stride, storage_offset);\n   return result;\n"
                },
                {
                    "old_start": 312,
                    "old_length": 9,
                    "new_start": 309,
                    "new_length": 6,
                    "hunk_buggy": "[' Tensor as_strided_qtensorimpl(const Tensor& self, IntArrayRef size, IntArrayRef stride, optional<int64_t> storage_offset_) {\\n', '   auto storage_offset = storage_offset_.value_or(self.storage_offset());\\n', '   auto tid = self.type_id();\\n', '-  TORCH_CHECK(\\n', '-      tid == QuantizedCPUTensorId(),\\n', '-      \"as_strided is only implemented for strided CPU, CUDA and QuantizedCPU tensors.\");\\n', '   auto result = detail::make_tensor<QTensorImpl>(Storage(self.storage()), tid, get_qtensorimpl(self)->quantizer());\\n', '   setStrided(result, size, stride, storage_offset);\\n', '   return result;']",
                    "hunk_fix": "@@ -312,9 +309,6 @@ Tensor as_strided_tensorimpl(const Tensor& self, IntArrayRef size, IntArrayRef s\n Tensor as_strided_qtensorimpl(const Tensor& self, IntArrayRef size, IntArrayRef stride, optional<int64_t> storage_offset_) {\n   auto storage_offset = storage_offset_.value_or(self.storage_offset());\n   auto tid = self.type_id();\n-  TORCH_CHECK(\n-      tid == QuantizedCPUTensorId(),\n-      \"as_strided is only implemented for strided CPU, CUDA and QuantizedCPU tensors.\");\n   auto result = detail::make_tensor<QTensorImpl>(Storage(self.storage()), tid, get_qtensorimpl(self)->quantizer());\n   setStrided(result, size, stride, storage_offset);\n   return result;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 490,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/65496e4e67b42e52b3428b0cf2d994e0aa1a9902",
    "date": "2019-04-25T14:50:19-07:00",
    "message": "Bug fix in bound shape inferencer (#19729)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/19729\n\nAccessing dims() without boundary check is not good.\n\nReviewed By: zrphercule\n\nDifferential Revision: D15078912\n\nfbshipit-source-id: 3746d0c18261abeec0c4880c30430125928c3309",
    "changes": [
        {
            "name": "bound_shape_inferencer.cc",
            "path": "caffe2/opt/bound_shape_inferencer.cc",
            "patches": [
                {
                    "old_start": 283,
                    "old_length": 7,
                    "new_start": 283,
                    "new_length": 13,
                    "hunk_buggy": "['     const auto it = shape_info_.find(i);\\n', '     if (it != shape_info_.end()) {\\n', '       const auto& current_input_shape = it->second;\\n', '-      channel_acc += current_input_shape.shape.dims(axis);\\n', '     } else if (missing_shape_infos) {\\n', '       LOG(INFO) << \"More than one missing shapes, previous one: \"\\n', '                 << input_to_infer;']",
                    "hunk_fix": "@@ -283,7 +283,13 @@ void BoundShapeInferencer::InferConcatInputs(const OperatorDef& op) {\n     const auto it = shape_info_.find(i);\n     if (it != shape_info_.end()) {\n       const auto& current_input_shape = it->second;\n-      channel_acc += current_input_shape.shape.dims(axis);\n+      if (axis < current_input_shape.shape.dims_size()) {\n+        channel_acc += current_input_shape.shape.dims(axis);\n+      } else {\n+        LOG(INFO) << \"Mismatched input dim along axis \" << axis\n+                  << \". We cannot infer missing input shape for Concat\";\n+        return;\n+      }\n     } else if (missing_shape_infos) {\n       LOG(INFO) << \"More than one missing shapes, previous one: \"\n                 << input_to_infer;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 491,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/e7fc7c732cbde822f9490840704b1f57fe86c50a",
    "date": "2019-04-22T20:55:17-07:00",
    "message": "Bugfix for fusion device check (#19594)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/19594\n\nI missed a callsite\n\nReviewed By: wanchaol\n\nDifferential Revision: D15041457\n\nfbshipit-source-id: eef76ad51bee06a56d31b4ab64f19250fe2ad8f0",
    "changes": [
        {
            "name": "graph_fuser.cpp",
            "path": "torch/csrc/jit/passes/graph_fuser.cpp",
            "patches": [
                {
                    "old_start": 1153,
                    "old_length": 7,
                    "new_start": 1153,
                    "new_length": 7,
                    "hunk_buggy": "['   }\\n', ' \\n', '   bool canFuseWithConcat(Value* producer, Node* before_check) {\\n', '-    if (!isFusable(producer->node())) {\\n', '       return false;\\n', '     }\\n', '     // NB: it is important that this check happens after isFusable, which checks']",
                    "hunk_fix": "@@ -1153,7 +1153,7 @@ struct GraphFuser {\n   }\n \n   bool canFuseWithConcat(Value* producer, Node* before_check) {\n-    if (!isFusable(producer->node())) {\n+    if (!isFusableDevice(producer) || !isFusable(producer->node())) {\n       return false;\n     }\n     // NB: it is important that this check happens after isFusable, which checks"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 492,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/65dfe1203ffab064d4e32fa8f76833042369d2f5",
    "date": "2019-04-03T12:47:23-07:00",
    "message": "add an assertion to check the param num (#18145)\n\nSummary:\nIntroduce this check to see whether it will break any existing workflow\nPull Request resolved: https://github.com/pytorch/pytorch/pull/18145\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D14511711\n\nPulled By: houseroad\n\nfbshipit-source-id: a7bb6ac84c9133fe94d3fe2f1a8566faed14a136",
    "changes": [
        {
            "name": "utils.py",
            "path": "torch/onnx/utils.py",
            "patches": [
                {
                    "old_start": 253,
                    "old_length": 6,
                    "new_start": 253,
                    "new_length": 10,
                    "hunk_buggy": "[' \\n', '     _set_input_and_output_names(graph, input_names, output_names)\\n', ' \\n', '     input_and_param_names = [val.uniqueName() for val in graph.inputs()]\\n', '     param_names = input_and_param_names[len(input_and_param_names) - len(params):]\\n', '     params_dict = dict(zip(param_names, params))']",
                    "hunk_fix": "@@ -253,6 +253,10 @@ def _model_to_graph(model, args, f, verbose=False, training=False,\n \n     _set_input_and_output_names(graph, input_names, output_names)\n \n+    # make sure that the param dict and the graph match each other\n+    flatten_args, _ = torch._C._jit_flatten(args)\n+    assert len(params) + len(flatten_args) == sum(1 for _ in graph.inputs())\n+\n     input_and_param_names = [val.uniqueName() for val in graph.inputs()]\n     param_names = input_and_param_names[len(input_and_param_names) - len(params):]\n     params_dict = dict(zip(param_names, params))"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 493,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/828a6a3b3943a0a0701ecacacd2bcc34fc03fe03",
    "date": "2019-03-31T02:08:11-07:00",
    "message": "Use proper isnan check\n\nSummary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/18663\n\nDifferential Revision: D14699385\n\nPulled By: bddppq\n\nfbshipit-source-id: 596ad3371e7704802591e49f7e1c55dc6cd2896f",
    "changes": [
        {
            "name": "THTensorMoreMath.cpp",
            "path": "aten/src/TH/generic/THTensorMoreMath.cpp",
            "patches": [
                {
                    "old_start": 544,
                    "old_length": 7,
                    "new_start": 544,
                    "new_length": 7,
                    "hunk_buggy": "[' /* Emulate NumPy behavior of putting NaNs\\n', '  * at the end of an ascending list. */\\n', ' #define GT_OR_NAN(x, y) \\\\\\n', '-  ((x != x && y == y) || (x > y))\\n', ' \\n', ' static void THTensor_(quicksortascend)(scalar_t *arr, int64_t *idx, int64_t elements, int64_t stride)\\n', ' {']",
                    "hunk_fix": "@@ -544,7 +544,7 @@ void THTensor_(diag)(THTensor *r_, THTensor *t, int k)\n /* Emulate NumPy behavior of putting NaNs\n  * at the end of an ascending list. */\n #define GT_OR_NAN(x, y) \\\n-  ((x != x && y == y) || (x > y))\n+  ((th_isnan(x) && !(th_isnan(y))) || (x > y))\n \n static void THTensor_(quicksortascend)(scalar_t *arr, int64_t *idx, int64_t elements, int64_t stride)\n {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 494,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/647154f82ac2c57769f080c41452b3e5960ab94f",
    "date": "2019-03-25T08:44:17-07:00",
    "message": "Assert tensor isn't sparse in enforce_invariants. (#18338)\n\nSummary:\nThere's no reason we can't check this, but I'm punting on implementing it for now.  But it currently segfaults, so this is an improvements.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/18338\n\nDifferential Revision: D14580308\n\nPulled By: gchanan\n\nfbshipit-source-id: 44d4cafeab12e1beeb3453a2d4068d221c2e9c4f",
    "changes": [
        {
            "name": "Tensor.cpp",
            "path": "aten/src/ATen/core/Tensor.cpp",
            "patches": [
                {
                    "old_start": 19,
                    "old_length": 6,
                    "new_start": 19,
                    "new_length": 9,
                    "hunk_buggy": "['       AT_ASSERTM(\\n', '           impl_->dtype_initialized(),\\n', '           \"Partially-initialized tensor not supported by at::Tensor\");\\n', '       AT_ASSERTM(\\n', '           impl_->storage_initialized(),\\n', '           \"Partially-initialized tensor not supported by at::Tensor\");']",
                    "hunk_fix": "@@ -19,6 +19,9 @@ void Tensor::enforce_invariants() {\n       AT_ASSERTM(\n           impl_->dtype_initialized(),\n           \"Partially-initialized tensor not supported by at::Tensor\");\n+      AT_ASSERTM(\n+          !impl_->is_sparse(),\n+          \"Sparse Tensors are supported by at::Tensor, but invariant checking isn't implemented.  Please file a bug.\");\n       AT_ASSERTM(\n           impl_->storage_initialized(),\n           \"Partially-initialized tensor not supported by at::Tensor\");"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 495,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/260f66c3165ce0c48dd1514a916da6971d981578",
    "date": "2019-02-21T19:34:30-08:00",
    "message": "Fix concat dimension check bug (#17343)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/17343\n\nSee [post](https://fb.workplace.com/groups/1405155842844877/permalink/2630764056950710/)\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D14163001\n\nfbshipit-source-id: 038f15d6a58b3bc31910e7bfa47c335e25739f12",
    "changes": [
        {
            "name": "concat_split_op.cc",
            "path": "caffe2/operators/concat_split_op.cc",
            "patches": [
                {
                    "old_start": 179,
                    "old_length": 7,
                    "new_start": 179,
                    "new_length": 10,
                    "hunk_buggy": "['           : GetDimFromOrderString(\\n', '                 helper.GetSingleArgument<string>(\"order\", \"NCHW\"));\\n', '       bool add_axis = helper.GetSingleArgument<int>(\"add_axis\", 0) != 0;\\n', '-      const int canonical_axis = canonical_axis_index_(axis, in[0].dims_size());\\n', '       CAFFE_ENFORCE_GT(in.size(), 0);\\n', '       vector<int> split_shape(1, in.size());\\n', '       vector<int> out_shape(in[0].dims().begin(), in[0].dims().end());']",
                    "hunk_fix": "@@ -179,7 +179,10 @@ OPERATOR_SCHEMA(Concat)\n           : GetDimFromOrderString(\n                 helper.GetSingleArgument<string>(\"order\", \"NCHW\"));\n       bool add_axis = helper.GetSingleArgument<int>(\"add_axis\", 0) != 0;\n-      const int canonical_axis = canonical_axis_index_(axis, in[0].dims_size());\n+      int adj_size = in[0].dims_size() + (add_axis ? 1 : 0);\n+      const int canonical_axis = canonical_axis_index_(axis, adj_size);\n+      CAFFE_ENFORCE_LT(\n+          canonical_axis, adj_size, \"Axis not in input ndim range.\");\n       CAFFE_ENFORCE_GT(in.size(), 0);\n       vector<int> split_shape(1, in.size());\n       vector<int> out_shape(in[0].dims().begin(), in[0].dims().end());"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 496,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/68ad9ae5bebd9efab127fa99e2bafd6852bbd8ed",
    "date": "2018-12-12T09:58:03-08:00",
    "message": "Ensure there aren't variables in checked_tensor_unwrap, checked_tenso\u2026 (#15105)\n\nSummary:\n\u2026r_list_unwrap.\n\nThese functions use unsafeGetTensorImpl(), which doesn't work with Variables (in a silent way that may blow up later).\nSo let's do early checking.\nPull Request resolved: https://github.com/pytorch/pytorch/pull/15105\n\nReviewed By: ezyang\n\nDifferential Revision: D13429149\n\nPulled By: gchanan\n\nfbshipit-source-id: b85f6f5b7cdb9a6dd0c40205b924c840a3920ba0",
    "changes": [
        {
            "name": "Utils.h",
            "path": "aten/src/ATen/Utils.h",
            "patches": [
                {
                    "old_start": 73,
                    "old_length": 6,
                    "new_start": 73,
                    "new_length": 9,
                    "hunk_buggy": "['     AT_ERROR(\"Expected object of scalar type \", scalar_type, \" but got scalar type \", expr.scalar_type(),\\n', '              \" for argument #\", pos, \" \\'\", name, \"\\'\");\\n', '   }\\n', '   return expr.unsafeGetTensorImpl();\\n', ' }\\n', ' \\n']",
                    "hunk_fix": "@@ -73,6 +73,9 @@ static inline TensorImpl* checked_tensor_unwrap(const Tensor& expr, const char *\n     AT_ERROR(\"Expected object of scalar type \", scalar_type, \" but got scalar type \", expr.scalar_type(),\n              \" for argument #\", pos, \" '\", name, \"'\");\n   }\n+  if (expr.is_variable()) {\n+    AT_ERROR(\"Expected Tensor (not Variable) for argument #\", pos, \" '\", name, \"'\");\n+  }\n   return expr.unsafeGetTensorImpl();\n }\n \n"
                },
                {
                    "old_start": 88,
                    "old_length": 7,
                    "new_start": 91,
                    "new_length": 11,
                    "hunk_buggy": "['     }\\n', '     if (expr.scalar_type() != scalar_type) {\\n', '       AT_ERROR(\"Expected object of scalar type \", scalar_type, \" but got scalar type \", expr.scalar_type(),\\n', '-               \" for sequence elment \", i , \" in sequence argument at position #\", pos, \" \\'\", name, \"\\'\");\\n', '     }\\n', '     unwrapped.emplace_back(expr.unsafeGetTensorImpl());\\n', '   }']",
                    "hunk_fix": "@@ -88,7 +91,11 @@ static inline std::vector<TensorImpl*> checked_tensor_list_unwrap(ArrayRef<Tenso\n     }\n     if (expr.scalar_type() != scalar_type) {\n       AT_ERROR(\"Expected object of scalar type \", scalar_type, \" but got scalar type \", expr.scalar_type(),\n-               \" for sequence elment \", i , \" in sequence argument at position #\", pos, \" '\", name, \"'\");\n+               \" for sequence element \", i , \" in sequence argument at position #\", pos, \" '\", name, \"'\");\n+    }\n+    if (expr.is_variable()) {\n+      AT_ERROR(\"Expected Tensor (not Variable) for sequence element \",\n+               i , \" in sequence argument at position #\", pos, \" '\", name, \"'\");\n     }\n     unwrapped.emplace_back(expr.unsafeGetTensorImpl());\n   }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 497,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/55092b1cc604fad3d70d31e71bbdd3a43a279423",
    "date": "2018-12-05T12:00:23-08:00",
    "message": "Validate matching input shapes in Int8Add operator (#14520)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/14520\n\nDefault engine doesn't support broadcast semantics in Int8Add operator. This patch adds a check that shapes are equivalent.\n\nReviewed By: bertmaher\n\nDifferential Revision: D13250922\n\nfbshipit-source-id: 8526d07723bd9a34d54dee04d121c57f8b33c481",
    "changes": [
        {
            "name": "int8_add_op.h",
            "path": "caffe2/operators/quantized/int8_add_op.h",
            "patches": [
                {
                    "old_start": 32,
                    "old_length": 6,
                    "new_start": 32,
                    "new_length": 11,
                    "hunk_buggy": "['     const auto& B = Inputs()[1]->template Get<Int8TensorCPU>();\\n', '     auto* Y = Outputs()[0]->template GetMutable<Int8TensorCPU>();\\n', ' \\n', '     /*\\n', '      * Record quantization parameters for A and B inputs, because if the op is\\n', '      * in-place, we may overwrite these parameters later, when we set']",
                    "hunk_fix": "@@ -32,6 +32,11 @@ class Int8AddOp final : public Operator<CPUContext> {\n     const auto& B = Inputs()[1]->template Get<Int8TensorCPU>();\n     auto* Y = Outputs()[0]->template GetMutable<Int8TensorCPU>();\n \n+    CAFFE_ENFORCE_EQ(\n+        A.t.sizes(),\n+        B.t.sizes(),\n+        \"inputs must have the same shape (broadcast semantics is not supported)\");\n+\n     /*\n      * Record quantization parameters for A and B inputs, because if the op is\n      * in-place, we may overwrite these parameters later, when we set"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 498,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/dc07102b17915f21170fae9a9d52c6f2d59726ca",
    "date": "2018-10-16T17:27:44-07:00",
    "message": "Check dim size preventively when doing shape inference for BatchMatMul (#12691)\n\nSummary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/12691\n\nWe check input(0) but not input(1) in BatchMatMul. This may result in a protobuf exception which won't be caught by upstream and causing termination of the program. Check that with `CAFFE_ENFORCE` will be caught by upstream inference function. Plus, it will print out clean stack tracing showing where went wrong.\n\nReviewed By: bddppq, houseroad, BIT-silence\n\nDifferential Revision: D10391130\n\nfbshipit-source-id: daf8dcd8fcf9629a0626edad660dff54dd9aeae3",
    "changes": [
        {
            "name": "batch_matmul_op.cc",
            "path": "caffe2/operators/batch_matmul_op.cc",
            "patches": [
                {
                    "old_start": 13,
                    "old_length": 6,
                    "new_start": 13,
                    "new_length": 7,
                    "hunk_buggy": "['   if (!broadcast) {\\n', '     const auto ndim = in[0].dims_size();\\n', '     CAFFE_ENFORCE_GE(ndim, 2);\\n', '     int a_dim0;\\n', '     int b_dim1;\\n', '     if (helper.GetSingleArgument<int>(\"trans_a\", 0)) {']",
                    "hunk_fix": "@@ -13,6 +13,7 @@ vector<TensorShape> TensorInferenceForBatchMatMul(\n   if (!broadcast) {\n     const auto ndim = in[0].dims_size();\n     CAFFE_ENFORCE_GE(ndim, 2);\n+    CAFFE_ENFORCE_GE(in[1].dims_size(), 2);\n     int a_dim0;\n     int b_dim1;\n     if (helper.GetSingleArgument<int>(\"trans_a\", 0)) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 499,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/f14887a63f83b931df9fc5d841c7d3829141ff58",
    "date": "2018-06-18T20:16:34-07:00",
    "message": "check for exact shape match before loading (#8619)\n\n* check for exact shape match before loading\r\n\r\n* Use RuntimeError instead of ValueError to keep it consistent with other errors\r\n\r\n* fix lint",
    "changes": [
        {
            "name": "module.py",
            "path": "torch/nn/modules/module.py",
            "patches": [
                {
                    "old_start": 629,
                    "old_length": 6,
                    "new_start": 629,
                    "new_length": 13,
                    "hunk_buggy": "['             key = prefix + name\\n', '             if key in state_dict:\\n', '                 input_param = state_dict[key]\\n', '                 if isinstance(input_param, Parameter):\\n', '                     # backwards compatibility for serialized parameters\\n', '                     input_param = input_param.data']",
                    "hunk_fix": "@@ -629,6 +629,13 @@ class Module(object):\n             key = prefix + name\n             if key in state_dict:\n                 input_param = state_dict[key]\n+\n+                if input_param.shape != param.shape:\n+                    # local shape should match the one in checkpoint\n+                    error_msgs.append('Size mismatch: copying a param of {} from checkpoint, '\n+                                      'where the shape is {} in current model.'\n+                                      .format(param.shape, input_param.shape))\n+\n                 if isinstance(input_param, Parameter):\n                     # backwards compatibility for serialized parameters\n                     input_param = input_param.data"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 500,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/8af88f3525e1908deb9ac181ebfb9f8eb49bcb46",
    "date": "2018-06-06T20:20:33-07:00",
    "message": "[Caffe2] Add ADD operator for IDEEP (#8220)\n\n* Add ADD operator for IDEEP\r\n\r\n* Add boradcast check\r\n\r\n* Comments",
    "changes": [
        {
            "name": "elementwise_sum_op.cc",
            "path": "caffe2/ideep/operators/elementwise_sum_op.cc",
            "patches": [
                {
                    "old_start": 12,
                    "old_length": 7,
                    "new_start": 12,
                    "new_length": 7,
                    "hunk_buggy": "['   virtual ~IDEEPSumOp() {}\\n', ' \\n', '   bool RunOnDevice() override {\\n', '-    const auto &X = Input(INPUT0);\\n', '     auto* Y = Output(OUTPUT);\\n', ' \\n', '     if (InputSize() == 1) {\\n']",
                    "hunk_fix": "@@ -12,7 +12,7 @@ class IDEEPSumOp final : public IDEEPOperator {\n   virtual ~IDEEPSumOp() {}\n \n   bool RunOnDevice() override {\n-    const auto &X = Input(INPUT0);\n+    const auto& X = Input(INPUT0);\n     auto* Y = Output(OUTPUT);\n \n     if (InputSize() == 1) {\n"
                },
                {
                    "old_start": 20,
                    "old_length": 8,
                    "new_start": 20,
                    "new_length": 15,
                    "hunk_buggy": "[' \\n', '     } else {\\n', '       vector<itensor> inputs;\\n', '-      vector<float> scales (InputSize(), 1.0);\\n', '       for (int i = 0; i < InputSize(); ++i) {\\n', '         inputs.emplace_back(Input(i));\\n', '       }\\n', ' \\n']",
                    "hunk_fix": "@@ -20,8 +20,15 @@ class IDEEPSumOp final : public IDEEPOperator {\n \n     } else {\n       vector<itensor> inputs;\n-      vector<float> scales (InputSize(), 1.0);\n+      const vector<float> scales(InputSize(), 1.0);\n+      const auto dims = X.get_dims();\n       for (int i = 0; i < InputSize(); ++i) {\n+        if (Input(i).get_dims() != dims) {\n+          CAFFE_ENFORCE_EQ(\n+              dims,\n+              Input(i).get_dims(),\n+              \"Broadcast is not yet supported with IDEEP.\");\n+        }\n         inputs.emplace_back(Input(i));\n       }\n \n"
                },
                {
                    "old_start": 38,
                    "old_length": 5,
                    "new_start": 44,
                    "new_length": 6,
                    "hunk_buggy": "[' };\\n', ' \\n', ' REGISTER_IDEEP_OPERATOR(Sum, IDEEPSumOp);\\n', ' \\n', ' } // namespace caffe2']",
                    "hunk_fix": "@@ -38,5 +44,6 @@ class IDEEPSumOp final : public IDEEPOperator {\n };\n \n REGISTER_IDEEP_OPERATOR(Sum, IDEEPSumOp);\n+REGISTER_IDEEP_OPERATOR(Add, IDEEPSumOp);\n \n } // namespace caffe2"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 501,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/5e50993be72bec4ad939993328dd02691ef7777d",
    "date": "2018-05-26T11:16:41-04:00",
    "message": "Better type checking for pack_padded_sequence symbolic (#7874)",
    "changes": [
        {
            "name": "rnn.py",
            "path": "torch/nn/utils/rnn.py",
            "patches": [
                {
                    "old_start": 151,
                    "old_length": 6,
                    "new_start": 151,
                    "new_length": 12,
                    "hunk_buggy": "['     def _onnx_symbolic_pack_padded_sequence(g, input, lengths):\\n', '         if batch_first:\\n', \"             input = g.op('Transpose', input, perm_i=[1, 0, 2])\\n\", '         return g.op(\"prim::PackPadded\", input, lengths, outputs=2)\\n', ' \\n', '     def pack_padded_sequence_trace_wrapper(input, lengths):']",
                    "hunk_fix": "@@ -151,6 +151,12 @@ def _symbolic_pack_padded_sequence(g, input, lengths, batch_first=False, padding\n     def _onnx_symbolic_pack_padded_sequence(g, input, lengths):\n         if batch_first:\n             input = g.op('Transpose', input, perm_i=[1, 0, 2])\n+        if lengths.type().kind() != 'TensorType':\n+            raise RuntimeError(\"Lengths must be a Tensor for ONNX export\")\n+        # We know it's a TensorType so this check is now safe.\n+        if lengths.type().scalarType() != 'Int':\n+            raise RuntimeError(\"ONNX export requires that the lengths passed \"\n+                               \"to pack_padded_sequence must be of type Int\")\n         return g.op(\"prim::PackPadded\", input, lengths, outputs=2)\n \n     def pack_padded_sequence_trace_wrapper(input, lengths):"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 502,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/23631eee5ae484d8397769492b3ea36f9eca282d",
    "date": "2018-03-19T16:38:55-07:00",
    "message": "[C2] Fix the check of current scope in optimizer (#2316)\n\nscope.CurrentDeviceScope() can return a None type, which was not considered.",
    "changes": [
        {
            "name": "optimizer.py",
            "path": "caffe2/python/optimizer.py",
            "patches": [
                {
                    "old_start": 130,
                    "old_length": 7,
                    "new_start": 130,
                    "new_length": 8,
                    "hunk_buggy": "[' \\n', '         if self._lr_multiplier is not None:\\n', '             current_scope = scope.CurrentDeviceScope()\\n', '-            if (current_scope.device_type == caffe2_pb2.CUDA\\n', '                     and not self._lr_multiplier_on_gpu):\\n', '                 lr_multiplier = net.CopyFromCPUInput(\\n', '                     self._lr_multiplier,\\n']",
                    "hunk_fix": "@@ -130,7 +130,8 @@ class Optimizer(object):\n \n         if self._lr_multiplier is not None:\n             current_scope = scope.CurrentDeviceScope()\n-            if (current_scope.device_type == caffe2_pb2.CUDA\n+            if (current_scope is not None\n+                    and current_scope.device_type == caffe2_pb2.CUDA\n                     and not self._lr_multiplier_on_gpu):\n                 lr_multiplier = net.CopyFromCPUInput(\n                     self._lr_multiplier,\n"
                },
                {
                    "old_start": 224,
                    "old_length": 7,
                    "new_start": 225,
                    "new_length": 8,
                    "hunk_buggy": "['             current_scope = scope.CurrentDeviceScope()\\n', '             self.add_lr_multiplier(\\n', '                 lr_lars_multiplier,\\n', '-                is_gpu_blob=(current_scope.device_type == caffe2_pb2.CUDA),\\n', '             )\\n', ' \\n', '         # We need negative sign for LR when used directly with WeightedSum\\n']",
                    "hunk_fix": "@@ -224,7 +225,8 @@ class SgdOptimizer(Optimizer):\n             current_scope = scope.CurrentDeviceScope()\n             self.add_lr_multiplier(\n                 lr_lars_multiplier,\n-                is_gpu_blob=(current_scope.device_type == caffe2_pb2.CUDA),\n+                is_gpu_blob=(current_scope is not None\n+                    and current_scope.device_type == caffe2_pb2.CUDA),\n             )\n \n         # We need negative sign for LR when used directly with WeightedSum\n"
                },
                {
                    "old_start": 499,
                    "old_length": 7,
                    "new_start": 501,
                    "new_length": 8,
                    "hunk_buggy": "['             current_scope = scope.CurrentDeviceScope()\\n', '             self.add_lr_multiplier(\\n', '                 lr_lars_multiplier,\\n', '-                is_gpu_blob=(current_scope.device_type == caffe2_pb2.CUDA),\\n', '             )\\n', ' \\n', '         lr, _ = self.build_lr(']",
                    "hunk_fix": "@@ -499,7 +501,8 @@ class AdagradOptimizer(Optimizer):\n             current_scope = scope.CurrentDeviceScope()\n             self.add_lr_multiplier(\n                 lr_lars_multiplier,\n-                is_gpu_blob=(current_scope.device_type == caffe2_pb2.CUDA),\n+                is_gpu_blob=(current_scope is not None\n+                    and current_scope.device_type == caffe2_pb2.CUDA),\n             )\n \n         lr, _ = self.build_lr("
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 503,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/14c47fb211bc929ae4e505e7e13411faa2339f00",
    "date": "2018-03-06T00:33:11-08:00",
    "message": "fix invalid-null-argument UBSAN error in math_cpu.cc\n\nAdd an if statement to check if the destination buffer is not nullptr.",
    "changes": [
        {
            "name": "math_cpu.cc",
            "path": "caffe2/utils/math_cpu.cc",
            "patches": [
                {
                    "old_start": 1477,
                    "old_length": 6,
                    "new_start": 1477,
                    "new_length": 9,
                    "hunk_buggy": "['     const int ldb,\\n', '     CPUContext* /*context*/,\\n', '     TypeMeta::TypedCopy copy) {\\n', '   if (lda == N && ldb == N) {\\n', '     // can coalese to a single memcpy of size M * N\\n', '     if (copy) {']",
                    "hunk_fix": "@@ -1477,6 +1477,9 @@ void CopyMatrix<CPUContext>(\n     const int ldb,\n     CPUContext* /*context*/,\n     TypeMeta::TypedCopy copy) {\n+  if (A == nullptr) {\n+    return;\n+  }\n   if (lda == N && ldb == N) {\n     // can coalese to a single memcpy of size M * N\n     if (copy) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 504,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/4f63f348aef3da8b4d53f61098f4e32bd916c221",
    "date": "2018-01-29T14:53:03-05:00",
    "message": "Fix condition in inferUnsqueezeGeometry (#4909)\n\nThe bounds check was too conservative by an extra one.",
    "changes": [
        {
            "name": "TensorShape.cpp",
            "path": "aten/src/ATen/native/TensorShape.cpp",
            "patches": [
                {
                    "old_start": 240,
                    "old_length": 7,
                    "new_start": 240,
                    "new_length": 7,
                    "hunk_buggy": "[' \\n', '   std::vector<int64_t> sizes(tensor.sizes());\\n', '   std::vector<int64_t> strides(tensor.strides());\\n', '-  int64_t new_stride = dim >= tensor.dim() - 1 ? 1 : sizes[dim] * strides[dim];\\n', '   sizes.insert(sizes.begin() + dim, 1);\\n', '   strides.insert(strides.begin() + dim, new_stride);\\n', ' ']",
                    "hunk_fix": "@@ -240,7 +240,7 @@ inferUnsqueezeGeometry(const Tensor& tensor, int64_t dim) {\n \n   std::vector<int64_t> sizes(tensor.sizes());\n   std::vector<int64_t> strides(tensor.strides());\n-  int64_t new_stride = dim >= tensor.dim() - 1 ? 1 : sizes[dim] * strides[dim];\n+  int64_t new_stride = dim >= tensor.dim() ? 1 : sizes[dim] * strides[dim];\n   sizes.insert(sizes.begin() + dim, 1);\n   strides.insert(strides.begin() + dim, new_stride);\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 505,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/91066559a8c8e5978ed4de722317576b222267c5",
    "date": "2018-01-19T21:34:09-08:00",
    "message": "truthy check for empty string in NameScope()\n\nSummary:\nAs in name. LATTE translation team moving some code from Python 2 to 3 uncovered a case where comparison between unicode and str types leads NameScope('') to prepend a separator to the beginning of blob names. This fixes it.\n\nThank you so much to dzhulgakov for tracking down the cause of this so quickly!\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D6766866\n\nfbshipit-source-id: fbe46cff581f425ba10e8668400915ea40baab94",
    "changes": [
        {
            "name": "scope.py",
            "path": "caffe2/python/scope.py",
            "patches": [
                {
                    "old_start": 53,
                    "old_length": 7,
                    "new_start": 53,
                    "new_length": 7,
                    "hunk_buggy": "['     assert isinstance(prefix, basestring), \\\\\\n', '         \"NameScope takes in a string as its argument.\"\\n', '     old_scope = CurrentNameScope()\\n', \"-    prefix = prefix + _NAMESCOPE_SEPARATOR if prefix is not '' else ''\\n\", '     if reset:\\n', '         _threadlocal_scope.namescope = prefix\\n', '     else:']",
                    "hunk_fix": "@@ -53,7 +53,7 @@ def NameScope(prefix, reset=False):\n     assert isinstance(prefix, basestring), \\\n         \"NameScope takes in a string as its argument.\"\n     old_scope = CurrentNameScope()\n-    prefix = prefix + _NAMESCOPE_SEPARATOR if prefix is not '' else ''\n+    prefix = prefix + _NAMESCOPE_SEPARATOR if prefix else ''\n     if reset:\n         _threadlocal_scope.namescope = prefix\n     else:"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 506,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/77523df413ff7f8a336b6481cfa47967c234a149",
    "date": "2018-01-11T15:14:33-05:00",
    "message": "Add more check on softmax ONNX exporting logic (#4592)\n\n* Add more check on softmax exporting logic\r\n\r\n* Add more comments about axis and dim",
    "changes": [
        {
            "name": "symbolic.py",
            "path": "torch/onnx/symbolic.py",
            "patches": [
                {
                    "old_start": 282,
                    "old_length": 6,
                    "new_start": 282,
                    "new_length": 24,
                    "hunk_buggy": "[' \\n', ' \\n', ' def softmax(g, input, dim=None):\\n', \"     return g.op('Softmax', input, axis_i=dim)\\n\", ' \\n', ' ']",
                    "hunk_fix": "@@ -282,6 +282,24 @@ def glu(g, input, dim):\n \n \n def softmax(g, input, dim=None):\n+    # Softmax does normalization at vector level.\n+    # PyTorch and ONNX use different strategies to split the input tensor into vectors.\n+    # Thus dim and axis have different meanings.\n+    # PyTorch slices the input tensor into vectors along the `dim`-th dimension.\n+    # ONNX reshapes the input into a 2-D tensor, and `axis` indicates where the input is coerced.\n+    # If input is a 2 x 3 tensor:\n+    # input = [[1.0, 1.0, 1.0],\n+    #          [1.0, 1,0, 1,0]]\n+    # with dim = 0, the result is:\n+    # result = [[0.5, 0.5, 0.5],\n+    #           [0.5, 0.5, 0.5]]\n+    # with axis = 0, the result is:\n+    # result = [[0.167, 0.167, 0.167],\n+    #           [0.167, 0.167, 0.167]]\n+    # So only when dim and axis both equal to ndim - 1 (the last dimension),\n+    # their semantics are equivalent.\n+    if len(input.type().sizes()) != dim + 1:\n+        return _unimplemented(\"dim\", \"ONNX and PyTorch use different strategies to split the input.\")\n     return g.op('Softmax', input, axis_i=dim)\n \n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 507,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/a7cc6531399300f999a404718827e2a94c115aaf",
    "date": "2018-01-03T17:42:55-08:00",
    "message": "cmake: handle CUDA 9.1 in GCC version check\n\nSummary:\nGCC version check is currently being skipped when using the\nnewly released CUDA 9.1.\n\nThis will also handle other CUDA 9.x minor releases if any,\nreducing our work if there are such releases like 9.2. This\nassumes that the next major CUDA version will be 10.0,\nneeding adjustment only after such major version is\nreleased.\nCloses https://github.com/caffe2/caffe2/pull/1658\n\nDifferential Revision: D6659000\n\nPulled By: pietern\n\nfbshipit-source-id: 79291b5da9d4e8b4f2c7ac82fe2b1e7939438bc9",
    "changes": [
        {
            "name": "Dependencies.cmake",
            "path": "cmake/Dependencies.cmake",
            "patches": [
                {
                    "old_start": 326,
                    "old_length": 13,
                    "new_start": 326,
                    "new_length": 14,
                    "hunk_buggy": "[' if(USE_CUDA)\\n', '   include(cmake/Cuda.cmake)\\n', '   if(HAVE_CUDA)\\n', '-    # CUDA 9.0 requires GCC version <= 6\\n', '-    if (CUDA_VERSION VERSION_EQUAL 9.0)\\n', '       if (CMAKE_C_COMPILER_ID STREQUAL \"GNU\" AND\\n', '           NOT CMAKE_C_COMPILER_VERSION VERSION_LESS 7.0 AND\\n', '           CUDA_HOST_COMPILER STREQUAL CMAKE_C_COMPILER)\\n', '         message(FATAL_ERROR\\n', '-          \"CUDA 9.0 is not compatible with GCC version >= 7. \"\\n', '           \"Use the following option to use another version (for example): \\\\n\"\\n', '           \"  -DCUDA_HOST_COMPILER=/usr/bin/gcc-6\\\\n\")\\n', '       endif()']",
                    "hunk_fix": "@@ -326,13 +326,14 @@ endif()\n if(USE_CUDA)\n   include(cmake/Cuda.cmake)\n   if(HAVE_CUDA)\n-    # CUDA 9.0 requires GCC version <= 6\n-    if (CUDA_VERSION VERSION_EQUAL 9.0)\n+    # CUDA 9.x requires GCC version <= 6\n+    if ((CUDA_VERSION VERSION_EQUAL   9.0) OR\n+        (CUDA_VERSION VERSION_GREATER 9.0  AND CUDA_VERSION VERSION_LESS 10.0))\n       if (CMAKE_C_COMPILER_ID STREQUAL \"GNU\" AND\n           NOT CMAKE_C_COMPILER_VERSION VERSION_LESS 7.0 AND\n           CUDA_HOST_COMPILER STREQUAL CMAKE_C_COMPILER)\n         message(FATAL_ERROR\n-          \"CUDA 9.0 is not compatible with GCC version >= 7. \"\n+          \"CUDA ${CUDA_VERSION} is not compatible with GCC version >= 7. \"\n           \"Use the following option to use another version (for example): \\n\"\n           \"  -DCUDA_HOST_COMPILER=/usr/bin/gcc-6\\n\")\n       endif()"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 508,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/a24c11329a1bdfb00848b4af6dced5368622d637",
    "date": "2017-12-08T15:03:49-08:00",
    "message": "Fix out-of-place allocations\n\nSummary:\nAlso add int as a datatype and correctly check error codes on group\nstart, end\nCloses https://github.com/caffe2/caffe2/pull/1590\n\nDifferential Revision: D6524086\n\nPulled By: pietern\n\nfbshipit-source-id: 385aab6fe1bbf6b5c06fa905066bc576a733c856",
    "changes": [
        {
            "name": "cuda_nccl_gpu.cc",
            "path": "caffe2/contrib/nccl/cuda_nccl_gpu.cc",
            "patches": [
                {
                    "old_start": 122,
                    "old_length": 6,
                    "new_start": 122,
                    "new_length": 12,
                    "hunk_buggy": "['   static const ncclDataType_t type = ncclFloat;\\n', ' };\\n', ' \\n', ' #ifdef CAFFE_HAS_CUDA_FP16\\n', ' template <>\\n', ' class ncclTypeWrapper<float16> {\\n']",
                    "hunk_fix": "@@ -122,6 +122,12 @@ class ncclTypeWrapper<float> {\n   static const ncclDataType_t type = ncclFloat;\n };\n \n+template <>\n+class ncclTypeWrapper<int> {\n+ public:\n+  static const ncclDataType_t type = ncclInt;\n+};\n+\n #ifdef CAFFE_HAS_CUDA_FP16\n template <>\n class ncclTypeWrapper<float16> {\n"
                },
                {
                    "old_start": 134,
                    "old_length": 6,
                    "new_start": 140,
                    "new_length": 8,
                    "hunk_buggy": "[' void runNCCL(const NCCLExecution& ex, InitF&& init_f, F&& f) {\\n', '   // do initialization\\n', '   for (auto i = 0; i < ex.elements.size(); ++i) {\\n', '     init_f(ex.elements[i]);\\n', '   }\\n', ' \\n']",
                    "hunk_fix": "@@ -134,6 +140,8 @@ template <typename T, typename InitF, typename F>\n void runNCCL(const NCCLExecution& ex, InitF&& init_f, F&& f) {\n   // do initialization\n   for (auto i = 0; i < ex.elements.size(); ++i) {\n+    auto& ctx = ex.elements[i];\n+    DeviceGuard g(ctx.device);\n     init_f(ex.elements[i]);\n   }\n \n"
                },
                {
                    "old_start": 155,
                    "old_length": 7,
                    "new_start": 163,
                    "new_length": 7,
                    "hunk_buggy": "['     std::lock_guard<std::mutex> lock(CUDAContext::mutex());\\n', ' \\n', ' #if NCCL_VERSION_MIN(2, 0, 0)\\n', '-    ncclGroupStart();\\n', ' #endif\\n', ' \\n', '     for (auto i = 0; i < ex.elements.size(); ++i) {\\n']",
                    "hunk_fix": "@@ -155,7 +163,7 @@ void runNCCL(const NCCLExecution& ex, InitF&& init_f, F&& f) {\n     std::lock_guard<std::mutex> lock(CUDAContext::mutex());\n \n #if NCCL_VERSION_MIN(2, 0, 0)\n-    ncclGroupStart();\n+    CAFFE_NCCL_CHECK(ncclGroupStart());\n #endif\n \n     for (auto i = 0; i < ex.elements.size(); ++i) {\n"
                },
                {
                    "old_start": 171,
                    "old_length": 7,
                    "new_start": 179,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', ' \\n', ' #if NCCL_VERSION_MIN(2, 0, 0)\\n', '-    ncclGroupEnd();\\n', ' #endif\\n', ' \\n', '     for (auto i = 0; i < ex.elements.size(); ++i) {\\n']",
                    "hunk_fix": "@@ -171,7 +179,7 @@ void runNCCL(const NCCLExecution& ex, InitF&& init_f, F&& f) {\n     }\n \n #if NCCL_VERSION_MIN(2, 0, 0)\n-    ncclGroupEnd();\n+    CAFFE_NCCL_CHECK(ncclGroupEnd());\n #endif\n \n     for (auto i = 0; i < ex.elements.size(); ++i) {\n"
                },
                {
                    "old_start": 321,
                    "old_length": 6,
                    "new_start": 329,
                    "new_length": 7,
                    "hunk_buggy": "[' \\n', ' // Explicit instantiation\\n', ' template class NCCL<float>;\\n', ' #ifdef CAFFE_HAS_CUDA_FP16\\n', ' template class NCCL<float16>;\\n', ' #endif']",
                    "hunk_fix": "@@ -321,6 +329,7 @@ void NCCL<T>::ReduceScatter(const NCCLExecution& ex) {\n \n // Explicit instantiation\n template class NCCL<float>;\n+template class NCCL<int>;\n #ifdef CAFFE_HAS_CUDA_FP16\n template class NCCL<float16>;\n #endif"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 509,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/ce3413549f5cc312f48c4e2a2f60c41674e26257",
    "date": "2017-11-16T13:30:53-08:00",
    "message": "check cloned observer in RNN Executor\n\nSummary: Ensure the clone() function didn't return a nullptr before attaching to an RNN operator\n\nReviewed By: salexspb\n\nDifferential Revision: D6341735\n\nfbshipit-source-id: acf89c32f8dae2fd9bc8cb1029bc00df5dbe9dbd",
    "changes": [
        {
            "name": "recurrent_network_executor.h",
            "path": "caffe2/operators/recurrent_network_executor.h",
            "patches": [
                {
                    "old_start": 128,
                    "old_length": 7,
                    "new_start": 128,
                    "new_length": 12,
                    "hunk_buggy": "[' \\n', '           rnn_op.op = CreateOperator(op_copy, ws);\\n', '           for (const auto& observer : observers) {\\n', '-            rnn_op.op->AttachObserver(observer.second->clone());\\n', '           }\\n', '         } else {\\n', '           if (t > max_parallel_timesteps_ && max_parallel_timesteps_ > 0 &&\\n']",
                    "hunk_fix": "@@ -128,7 +128,12 @@ class RecurrentNetworkExecutorBase {\n \n           rnn_op.op = CreateOperator(op_copy, ws);\n           for (const auto& observer : observers) {\n-            rnn_op.op->AttachObserver(observer.second->clone());\n+            std::unique_ptr<ObserverBase<OperatorBase>> observer_copy =\n+                observer.second->clone();\n+            CAFFE_ENFORCE(\n+                observer_copy,\n+                \"Observers without clone() implemented cannot be attached to RNN using RNNExecutor.\");\n+            rnn_op.op->AttachObserver(std::move(observer_copy));\n           }\n         } else {\n           if (t > max_parallel_timesteps_ && max_parallel_timesteps_ > 0 &&\n"
                },
                {
                    "old_start": 138,
                    "old_length": 7,
                    "new_start": 143,
                    "new_length": 12,
                    "hunk_buggy": "['           } else {\\n', '             rnn_op.op = CreateOperator(step_net_def_.op(rnn_op.order), ws);\\n', '             for (const auto& observer : observers) {\\n', '-              rnn_op.op->AttachObserver(observer.second->clone());\\n', '             }\\n', '           }\\n', '         }']",
                    "hunk_fix": "@@ -138,7 +143,12 @@ class RecurrentNetworkExecutorBase {\n           } else {\n             rnn_op.op = CreateOperator(step_net_def_.op(rnn_op.order), ws);\n             for (const auto& observer : observers) {\n-              rnn_op.op->AttachObserver(observer.second->clone());\n+              std::unique_ptr<ObserverBase<OperatorBase>> observer_copy =\n+                  observer.second->clone();\n+              CAFFE_ENFORCE(\n+                  observer_copy,\n+                  \"Observers without clone() implemented cannot be attached to RNN using RNNExecutor.\");\n+              rnn_op.op->AttachObserver(std::move(observer_copy));\n             }\n           }\n         }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 510,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/e8e29690ef0306da25b5f191623476001d29a18b",
    "date": "2017-11-10T03:24:49-08:00",
    "message": "Add has_debug_def() check to net's debug_def()\n\nSummary: same as title\n\nReviewed By: salexspb\n\nDifferential Revision: D6264232\n\nfbshipit-source-id: e9f499e0c8758bcb52f079521fa95973fcba441f",
    "changes": [
        {
            "name": "net.h",
            "path": "caffe2/core/net.h",
            "patches": [
                {
                    "old_start": 120,
                    "old_length": 8,
                    "new_start": 120,
                    "new_length": 13,
                    "hunk_buggy": "['     return name_;\\n', '   }\\n', ' \\n', '-  inline const std::shared_ptr<const NetDef> debug_def() const {\\n', '-    return net_def_;\\n', '   }\\n', ' \\n', '  protected:']",
                    "hunk_fix": "@@ -120,8 +120,13 @@ class NetBase : public Observable<NetBase> {\n     return name_;\n   }\n \n-  inline const std::shared_ptr<const NetDef> debug_def() const {\n-    return net_def_;\n+  inline const NetDef& debug_def() const {\n+    CAFFE_ENFORCE(has_debug_def(), \"net_def was null!\");\n+    return *net_def_;\n+  }\n+\n+  inline bool has_debug_def() const {\n+    return net_def_ != nullptr;\n   }\n \n  protected:"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 511,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/cf348bcdeecfe0b47a2245d95eaa8ef37fb7b53e",
    "date": "2017-11-02T19:53:36-04:00",
    "message": "tighten hasCUDA check",
    "changes": [
        {
            "name": "Context.cpp",
            "path": "aten/src/ATen/Context.cpp",
            "patches": [
                {
                    "old_start": 55,
                    "old_length": 6,
                    "new_start": 55,
                    "new_length": 11,
                    "hunk_buggy": "[' \\n', ' bool Context::hasCUDA() const {\\n', ' #ifdef AT_CUDA_ENABLED\\n', '   return true;\\n', ' #else\\n', '   return false;']",
                    "hunk_fix": "@@ -55,6 +55,11 @@ Context & globalContext() {\n \n bool Context::hasCUDA() const {\n #ifdef AT_CUDA_ENABLED\n+  int count;\n+  cudaError_t err = cudaGetDeviceCount(&count);\n+  if (err == cudaErrorInsufficientDriver) {\n+    return false;\n+  }\n   return true;\n #else\n   return false;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 512,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/8d377617e73a399e5b8838cf96dfd2896576ac52",
    "date": "2017-11-02T11:25:49-07:00",
    "message": "Fix MKLMemory::CopyTo for case where shapes don't match'\n\nSummary:\nThere were cases where the direct copy succeeded, but the\ndimensions didn't match. Now, we check dimensions and reset if they\ndon't match before issuing the copy.\n\nReviewed By: salexspb\n\nDifferential Revision: D6103325\n\nfbshipit-source-id: 602605d8b119cae74e006c792bc42f355a5a9b4e",
    "changes": [
        {
            "name": "mkl_memory.h",
            "path": "caffe2/mkl/utils/mkl_memory.h",
            "patches": [
                {
                    "old_start": 344,
                    "old_length": 6,
                    "new_start": 344,
                    "new_length": 12,
                    "hunk_buggy": "['       const dnnPrimitive_t primitive = nullptr,\\n', '       const dnnResourceType_t type = dnnResourceNumber) {\\n', '     if (buffer_.get() == other->buffer_.get()) {\\n', '       VLOG(2) << \"CopyTo does not need actual copying, as we are sharing \"\\n', '                  \"memory with the output.\";\\n', '       // This is already mapping to the same memory region. Skip copy.\\n']",
                    "hunk_fix": "@@ -344,6 +344,12 @@ class MKLMemory {\n       const dnnPrimitive_t primitive = nullptr,\n       const dnnResourceType_t type = dnnResourceNumber) {\n     if (buffer_.get() == other->buffer_.get()) {\n+      CAFFE_ENFORCE(\n+          dnnLayoutCompare<T>(other->layout_, layout_),\n+          \"MKLMemory layout does not match, despite in-place buffers\");\n+      CAFFE_ENFORCE(\n+          other->dims() == dims(),\n+          \"MKLMemory dimensions do not match, despite in-place buffers\");\n       VLOG(2) << \"CopyTo does not need actual copying, as we are sharing \"\n                  \"memory with the output.\";\n       // This is already mapping to the same memory region. Skip copy.\n"
                },
                {
                    "old_start": 355,
                    "old_length": 21,
                    "new_start": 361,
                    "new_length": 13,
                    "hunk_buggy": "['     // consistently copying stuff with fixed src and dst layouts, consider\\n', '     // making a cache for the primitive below.\\n', '     VLOG(2) << \"CopyTo requires copying. Performing direct copy.\";\\n', '     PrimitiveWrapper<T> convert(\\n', '         dnnConversionCreate<T>, layout_, other->layout_);\\n', '-    if (dnnPrimitive_t(convert) == nullptr ||\\n', '-        dnnConversionExecute<T>(convert, buffer_.get(), other->buffer()) !=\\n', '-            E_SUCCESS) {\\n', '-      VLOG(2) << \"Direct copy failed, will need to allocate output.\";\\n', '-      // If CopyTo directly did not succeed, it could be because the target\\n', '-      // MKLMemory is not having the right layout. In this case we will reset\\n', '-      // the target and then do another copy.\\n', '-      other->Reset(dims_, primitive, type);\\n', '-      PrimitiveWrapper<T> convert2(\\n', '-          dnnConversionCreate<T>, layout_, other->layout_);\\n', '-      MKLDNN_SAFE_CALL(\\n', '-          dnnConversionExecute<T>(convert2, buffer_.get(), other->buffer()));\\n', '-    }\\n', '   }\\n', ' \\n', '   inline void* buffer() {']",
                    "hunk_fix": "@@ -355,21 +361,13 @@ class MKLMemory {\n     // consistently copying stuff with fixed src and dst layouts, consider\n     // making a cache for the primitive below.\n     VLOG(2) << \"CopyTo requires copying. Performing direct copy.\";\n+    if (dims() != other->dims()) {\n+      other->Reset(dims(), primitive, type);\n+    }\n     PrimitiveWrapper<T> convert(\n         dnnConversionCreate<T>, layout_, other->layout_);\n-    if (dnnPrimitive_t(convert) == nullptr ||\n-        dnnConversionExecute<T>(convert, buffer_.get(), other->buffer()) !=\n-            E_SUCCESS) {\n-      VLOG(2) << \"Direct copy failed, will need to allocate output.\";\n-      // If CopyTo directly did not succeed, it could be because the target\n-      // MKLMemory is not having the right layout. In this case we will reset\n-      // the target and then do another copy.\n-      other->Reset(dims_, primitive, type);\n-      PrimitiveWrapper<T> convert2(\n-          dnnConversionCreate<T>, layout_, other->layout_);\n-      MKLDNN_SAFE_CALL(\n-          dnnConversionExecute<T>(convert2, buffer_.get(), other->buffer()));\n-    }\n+    MKLDNN_SAFE_CALL(\n+        dnnConversionExecute<T>(convert, buffer_.get(), other->buffer()));\n   }\n \n   inline void* buffer() {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 513,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/cf256ee268c30d4ca965b38b45467cf7f738542f",
    "date": "2017-11-01T05:51:23-04:00",
    "message": "Added tensor op check for cudnn rnns (#3409)",
    "changes": [
        {
            "name": "__init__.py",
            "path": "torch/backends/cudnn/__init__.py",
            "patches": [
                {
                    "old_start": 2,
                    "old_length": 6,
                    "new_start": 2,
                    "new_length": 7,
                    "hunk_buggy": "[' import sys\\n', ' import torch\\n', ' import warnings\\n', ' from contextlib import contextmanager\\n', ' \\n', ' enabled = True  # set to False to globally disable cuDNN\\n']",
                    "hunk_fix": "@@ -2,6 +2,7 @@ import ctypes\n import sys\n import torch\n import warnings\n+from torch.version import cuda\n from contextlib import contextmanager\n \n enabled = True  # set to False to globally disable cuDNN\n"
                },
                {
                    "old_start": 82,
                    "old_length": 6,
                    "new_start": 83,
                    "new_length": 9,
                    "hunk_buggy": "[' CUDNN_RNN_ALGO_PERSIST_STATIC = 1\\n', ' CUDNN_RNN_ALGO_PERSIST_DYNAMIC = 2\\n', ' \\n', ' \\n', ' def set_flags(_enabled, _benchmark, _deterministic, _verbose):\\n', '     global enabled, benchmark, deterministic, verbose\\n']",
                    "hunk_fix": "@@ -82,6 +83,9 @@ CUDNN_RNN_ALGO_STANDARD = 0\n CUDNN_RNN_ALGO_PERSIST_STATIC = 1\n CUDNN_RNN_ALGO_PERSIST_DYNAMIC = 2\n \n+CUDNN_DEFAULT_MATH = 0\n+CUDNN_TENSOR_OP_MATH = 1\n+\n \n def set_flags(_enabled, _benchmark, _deterministic, _verbose):\n     global enabled, benchmark, deterministic, verbose\n"
                },
                {
                    "old_start": 253,
                    "old_length": 6,
                    "new_start": 257,
                    "new_length": 10,
                    "hunk_buggy": "['                 CUDNN_RNN_ALGO_STANDARD,\\n', '                 datatype\\n', '             ))\\n', '         else:\\n', '             check_error(lib.cudnnSetRNNDescriptor(\\n', '                 self,']",
                    "hunk_fix": "@@ -253,6 +257,10 @@ class RNNDescriptor(object):\n                 CUDNN_RNN_ALGO_STANDARD,\n                 datatype\n             ))\n+        if version() >= 7000 and int(cuda[0]) >= 9:\n+            lib.cudnnSetRNNMatrixMathType(self, CUDNN_DEFAULT_MATH)\n+            if datatype == CUDNN_DATA_HALF:\n+                lib.cudnnSetRNNMatrixMathType(self, CUDNN_TENSOR_OP_MATH)\n         else:\n             check_error(lib.cudnnSetRNNDescriptor(\n                 self,"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 514,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/0c0c9e743e82b398435ed07719e998aa15ac1ce1",
    "date": "2017-10-20T10:28:01-07:00",
    "message": "Fix dimensions check\n\nSummary:\nTo match CPU implementation [here](https://github.com/caffe2/caffe2/blob/master/caffe2/operators/segment_reduction_op.h#L323)\nCloses https://github.com/caffe2/caffe2/pull/1360\n\nDifferential Revision: D6111071\n\nPulled By: Maratyszcza\n\nfbshipit-source-id: ba0019ff483ff28f4aa452103c3bad5d9294af96",
    "changes": [
        {
            "name": "segment_reduction_op_gpu.cu",
            "path": "caffe2/operators/segment_reduction_op_gpu.cu",
            "patches": [
                {
                    "old_start": 89,
                    "old_length": 7,
                    "new_start": 89,
                    "new_length": 7,
                    "hunk_buggy": "['     const auto* input_data = input.template data<T>();\\n', '     auto* Y = Output(0);\\n', ' \\n', '-    CHECK_LT(num_reduce_dims_, input.dims().size());\\n', '     const int M = FIRSTDIMS\\n', '         ? input.size_to_dim(num_reduce_dims_)\\n', '         : input.size_to_dim(input.ndim() - num_reduce_dims_);']",
                    "hunk_fix": "@@ -89,7 +89,7 @@ class ReduceDimsOp : public Operator<CUDAContext> {\n     const auto* input_data = input.template data<T>();\n     auto* Y = Output(0);\n \n-    CHECK_LT(num_reduce_dims_, input.dims().size());\n+    CHECK_LE(num_reduce_dims_, input.dims().size());\n     const int M = FIRSTDIMS\n         ? input.size_to_dim(num_reduce_dims_)\n         : input.size_to_dim(input.ndim() - num_reduce_dims_);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 515,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/ba766ef39a4fff2d8856e17747393d469e409775",
    "date": "2017-10-04T16:03:20-04:00",
    "message": "Fix BN size check in eval mode (#2977)",
    "changes": [
        {
            "name": "functional.py",
            "path": "torch/nn/functional.py",
            "patches": [
                {
                    "old_start": 702,
                    "old_length": 9,
                    "new_start": 702,
                    "new_length": 10,
                    "hunk_buggy": "[' \\n', ' def batch_norm(input, running_mean, running_var, weight=None, bias=None,\\n', '                training=False, momentum=0.1, eps=1e-5):\\n', '-    size = list(input.size())\\n', '-    if reduce(mul, size[2:], size[0]) == 1:\\n', \"-        raise ValueError('Expected more than 1 value per channel, got input size {}'.format(size))\\n\", '     f = torch._C._functions.BatchNorm(running_mean, running_var, training, momentum, eps, torch.backends.cudnn.enabled)\\n', '     return f(input, weight, bias)\\n', ' ']",
                    "hunk_fix": "@@ -702,9 +702,10 @@ def embedding(input, embedding_matrix,\n \n def batch_norm(input, running_mean, running_var, weight=None, bias=None,\n                training=False, momentum=0.1, eps=1e-5):\n-    size = list(input.size())\n-    if reduce(mul, size[2:], size[0]) == 1:\n-        raise ValueError('Expected more than 1 value per channel, got input size {}'.format(size))\n+    if training:\n+        size = list(input.size())\n+        if reduce(mul, size[2:], size[0]) == 1:\n+            raise ValueError('Expected more than 1 value per channel when training, got input size {}'.format(size))\n     f = torch._C._functions.BatchNorm(running_mean, running_var, training, momentum, eps, torch.backends.cudnn.enabled)\n     return f(input, weight, bias)\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 516,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/181b2481d338a24efc553378c837dcc48b656e3f",
    "date": "2017-09-29T15:18:31-04:00",
    "message": "add error checking to grid sampler (#2902)",
    "changes": [
        {
            "name": "SpatialGridSamplerBilinear.cu",
            "path": "torch/lib/THCUNN/generic/SpatialGridSamplerBilinear.cu",
            "patches": [
                {
                    "old_start": 56,
                    "old_length": 6,
                    "new_start": 56,
                    "new_length": 7,
                    "hunk_buggy": "['   SpatialGridSamplerBilinear_updateOutput_kernel\\n', '     <<<GET_BLOCKS(count), CUDA_NUM_THREADS, 0, THCState_getCurrentStream(state)>>>(\\n', '       count, devInput, devGrid, devOutput);\\n', ' }\\n', ' \\n', ' TH_API void THNN_(SpatialGridSamplerBilinear_updateGradInput)(\\n']",
                    "hunk_fix": "@@ -56,6 +56,7 @@ TH_API void THNN_(SpatialGridSamplerBilinear_updateOutput)(\n   SpatialGridSamplerBilinear_updateOutput_kernel\n     <<<GET_BLOCKS(count), CUDA_NUM_THREADS, 0, THCState_getCurrentStream(state)>>>(\n       count, devInput, devGrid, devOutput);\n+  THCudaCheck(cudaGetLastError());\n }\n \n TH_API void THNN_(SpatialGridSamplerBilinear_updateGradInput)(\n"
                },
                {
                    "old_start": 88,
                    "old_length": 6,
                    "new_start": 89,
                    "new_length": 7,
                    "hunk_buggy": "['   SpatialGridSamplerBilinear_updateGradInput_kernel\\n', '     <<<GET_BLOCKS(count), CUDA_NUM_THREADS, 0, THCState_getCurrentStream(state)>>>(\\n', '       count, devInput, devGradInput, devGrid, devGradGrid, devGradOutput);\\n', ' }\\n', ' \\n', ' #endif']",
                    "hunk_fix": "@@ -88,6 +89,7 @@ TH_API void THNN_(SpatialGridSamplerBilinear_updateGradInput)(\n   SpatialGridSamplerBilinear_updateGradInput_kernel\n     <<<GET_BLOCKS(count), CUDA_NUM_THREADS, 0, THCState_getCurrentStream(state)>>>(\n       count, devInput, devGradInput, devGrid, devGradGrid, devGradOutput);\n+  THCudaCheck(cudaGetLastError());\n }\n \n #endif"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 517,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/7caceea6e8c352a934f8fecf21f55454007d63b2",
    "date": "2017-09-25T23:53:59-04:00",
    "message": "better error messages for Conv*d input shape checking",
    "changes": [
        {
            "name": "convolution.cpp",
            "path": "torch/csrc/autograd/functions/convolution.cpp",
            "patches": [
                {
                    "old_start": 127,
                    "old_length": 6,
                    "new_start": 127,
                    "new_length": 30,
                    "hunk_buggy": "['   return tensor.squeeze(2);\\n', ' }\\n', ' \\n', ' static at::Tensor subtensor(at::Tensor& tensor, int dim, int groups, int g) {\\n', '   if (!tensor.defined()) {\\n', '     return at::Tensor();\\n']",
                    "hunk_fix": "@@ -127,6 +127,30 @@ static auto view3d(const at::Tensor& tensor) -> at::Tensor {\n   return tensor.squeeze(2);\n }\n \n+static void check_input_shape_forward(const at::Tensor& input,\n+\t\t\t\t      const at::Tensor& weight,\n+\t\t\t\t      int64_t groups, bool transposed) {\n+  if (!transposed) {\n+    if (input.size(1) != (weight.size(1) * groups)) {\n+      std::stringstream ss;\n+      ss << \"Given groups=\" << groups << \", weight\" << weight.sizes()\n+\t << \", so expected input\" << input.sizes() << \"  to have \"\n+\t << (weight.size(1) * groups) << \" channels, but got \" << input.size(1)\n+\t << \" channels instead\";\n+      throw std::runtime_error(ss.str());\n+    }\n+  } else { // transposed\n+    if (input.size(1) != weight.size(0)) {\n+      std::stringstream ss;\n+      ss << \"Given transposed=\" << transposed << \", weight\" << weight.sizes()\n+\t << \", so expected input\" << input.sizes() << \"  to have \"\n+\t << weight.size(0) << \" channels, but got \" << input.size(1)\n+\t << \" channels instead\";\n+      throw std::runtime_error(ss.str());\n+    }\n+  }\n+}\n+\n static at::Tensor subtensor(at::Tensor& tensor, int dim, int groups, int g) {\n   if (!tensor.defined()) {\n     return at::Tensor();\n"
                },
                {
                    "old_start": 166,
                    "old_length": 6,
                    "new_start": 190,
                    "new_length": 8,
                    "hunk_buggy": "['   auto weight = inputs[1].data();\\n', '   auto bias = inputs[2].opt_data();\\n', ' \\n', '   int k = input.ndimension();\\n', '   if (k == 3) {\\n', '     view1d_as_2d();']",
                    "hunk_fix": "@@ -166,6 +190,8 @@ auto ConvForward::apply(const variable_list& inputs) -> variable_list {\n   auto weight = inputs[1].data();\n   auto bias = inputs[2].opt_data();\n \n+  check_input_shape_forward(input, weight, groups, transposed);\n+\n   int k = input.ndimension();\n   if (k == 3) {\n     view1d_as_2d();"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 518,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/b8ab3080b1043a610ba2825a2be406a1833b1d70",
    "date": "2017-09-21T14:50:43-07:00",
    "message": "Fix InferShapesAndTypes() for convolutions\n\nSummary:\nIf kernel sizes were specified via \"kernel_w\" and \"kernel_h\", tensor size\ninference was incorrect in InferShapesAndTypes(): it was checking for\n\"helper_w\" instead of \"kernel_w\".\n\nReviewed By: akyrola\n\nDifferential Revision: D5884280\n\nfbshipit-source-id: 430cbedcedadbe3570384e706198a4ddc499504e",
    "changes": [
        {
            "name": "conv_pool_op_base.h",
            "path": "caffe2/operators/conv_pool_op_base.h",
            "patches": [
                {
                    "old_start": 423,
                    "old_length": 7,
                    "new_start": 423,
                    "new_length": 7,
                    "hunk_buggy": "['     if (helper.HasArgument(\"kernel\")) {\\n', '       kernel.resize(2, helper.GetSingleArgument<int>(\"kernel\", 1));\\n', '     } else if (\\n', '-        helper.HasArgument(\"kernel_h\") && helper.HasArgument(\"helper_w\")) {\\n', '       kernel.push_back(helper.GetSingleArgument<int>(\"kernel_h\", 1));\\n', '       kernel.push_back(helper.GetSingleArgument<int>(\"kernel_w\", 1));\\n', '     }']",
                    "hunk_fix": "@@ -423,7 +423,7 @@ class ConvPoolOpBase : public Operator<Context> {\n     if (helper.HasArgument(\"kernel\")) {\n       kernel.resize(2, helper.GetSingleArgument<int>(\"kernel\", 1));\n     } else if (\n-        helper.HasArgument(\"kernel_h\") && helper.HasArgument(\"helper_w\")) {\n+        helper.HasArgument(\"kernel_h\") && helper.HasArgument(\"kernel_w\")) {\n       kernel.push_back(helper.GetSingleArgument<int>(\"kernel_h\", 1));\n       kernel.push_back(helper.GetSingleArgument<int>(\"kernel_w\", 1));\n     }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 519,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/0f0829d88e839be1e150e917aca5b1edb64752ee",
    "date": "2017-09-14T01:30:58-07:00",
    "message": "Strict bound check for SequenceFunctor\n\nSummary:\nThis exhibits the problem in NMT training where some out of bound data seems to\nhave silently written over bound, and causing random segfaults elsewhere in the\ncode. This itself does not solve the problem, but will trigger us to then fix the out\nof bound issues.\n\nDifferential Revision: D5832646\n\nfbshipit-source-id: 5eb259e4584e5341ef3f19362f98f0a9554e9aec",
    "changes": [
        {
            "name": "boolean_mask_ops.cc",
            "path": "caffe2/operators/boolean_mask_ops.cc",
            "patches": [
                {
                    "old_start": 165,
                    "old_length": 13,
                    "new_start": 165,
                    "new_length": 15,
                    "hunk_buggy": "[' \\n', ' class SequenceFunctor {\\n', '  public:\\n', '-  explicit SequenceFunctor(const int* sl) : sl(sl) {}\\n', '   bool operator()(int i, int j, float /* val*/) {\\n', '-    return j >= sl[i];\\n', '   }\\n', ' \\n', '  private:\\n', '-  const int* sl;\\n', ' };\\n', ' \\n', ' class WindowFunctor {\\n']",
                    "hunk_fix": "@@ -165,13 +165,15 @@ namespace {\n \n class SequenceFunctor {\n  public:\n-  explicit SequenceFunctor(const int* sl) : sl(sl) {}\n+  explicit SequenceFunctor(const int* sl, const size_t len) : sl_(sl), len_(len) {}\n   bool operator()(int i, int j, float /* val*/) {\n-    return j >= sl[i];\n+    CAFFE_ENFORCE(i < len_, \"Out of bound.\");\n+    return j >= sl_[i];\n   }\n \n  private:\n-  const int* sl;\n+  const int* sl_;\n+  const size_t len_;\n };\n \n class WindowFunctor {\n"
                },
                {
                    "old_start": 250,
                    "old_length": 7,
                    "new_start": 252,
                    "new_length": 7,
                    "hunk_buggy": "['         left,\\n', '         right,\\n', '         input->data<T>(),\\n', '-        SequenceFunctor(sequence_lengths->data<int>()),\\n', '         fill_val,\\n', '         output->mutable_data<T>());\\n', '   } else if (mode_ == \"window\") {']",
                    "hunk_fix": "@@ -250,7 +252,7 @@ bool SequenceMaskOp<CPUContext>::DoRunWithType() {\n         left,\n         right,\n         input->data<T>(),\n-        SequenceFunctor(sequence_lengths->data<int>()),\n+        SequenceFunctor(sequence_lengths->data<int>(), sequence_lengths->size()),\n         fill_val,\n         output->mutable_data<T>());\n   } else if (mode_ == \"window\") {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 520,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/b31cf0ebd4ffc0e25801b4e40762266ad54721d6",
    "date": "2017-09-10T13:47:34-04:00",
    "message": "Added support for nInputDim parameter in legacy Padding class (#2645)\n\n* Added support for nInputDim parameter in Padding class\r\n\r\n* moved nInputDim to the end so as to not break backwards compatibilty\r\n\r\n* hasattr to check if nInputDim is actually set\r\n\r\n* check if nInputDim is positive before checking against input dim",
    "changes": [
        {
            "name": "Padding.py",
            "path": "torch/legacy/nn/Padding.py",
            "patches": [
                {
                    "old_start": 7,
                    "old_length": 12,
                    "new_start": 7,
                    "new_length": 15,
                    "hunk_buggy": "['     # index [index] in that dimension. If pad<0, index counts from the left.\\n', '     # If pad>0 index counts from the right index = 1 pads before index 1.\\n', '     # index = 2 pads starting before index 2 and after index 1 in dimension [dim]\\n', ' \\n', '-    def __init__(self, dim, pad, value=0, index=0):\\n', '         self.value = value\\n', '         self.index = index\\n', '         self.dim = dim\\n', '         self.pad = pad\\n', '         self.outputSize = torch.Size()\\n', '         super(Padding, self).__init__()\\n', ' \\n']",
                    "hunk_fix": "@@ -7,12 +7,15 @@ class Padding(Module):\n     # index [index] in that dimension. If pad<0, index counts from the left.\n     # If pad>0 index counts from the right index = 1 pads before index 1.\n     # index = 2 pads starting before index 2 and after index 1 in dimension [dim]\n+    # When nInputDim is provided, inputs larger than that value will be considered batches\n+    # where the actual dim to be padded will be dimension dim + 1.\n \n-    def __init__(self, dim, pad, value=0, index=0):\n+    def __init__(self, dim, pad, value=0, index=0, nInputDim=0):\n         self.value = value\n         self.index = index\n         self.dim = dim\n         self.pad = pad\n+        self.nInputDim = nInputDim\n         self.outputSize = torch.Size()\n         super(Padding, self).__init__()\n \n"
                },
                {
                    "old_start": 22,
                    "old_length": 6,
                    "new_start": 25,
                    "new_length": 9,
                    "hunk_buggy": "['         self.outputSize = torch.Size(outputSize)\\n', '         dim = self.dim\\n', ' \\n', '         self.output.resize_(self.outputSize)\\n', '         self.output.fill_(self.value)\\n', '         index = self.index\\n']",
                    "hunk_fix": "@@ -22,6 +25,9 @@ class Padding(Module):\n         self.outputSize = torch.Size(outputSize)\n         dim = self.dim\n \n+        if hasattr(self, \"nInputDim\") and self.nInputDim > 0 and input.dim() != self.nInputDim:\n+            dim = dim + 1\n+\n         self.output.resize_(self.outputSize)\n         self.output.fill_(self.value)\n         index = self.index\n"
                },
                {
                    "old_start": 46,
                    "old_length": 6,
                    "new_start": 52,
                    "new_length": 9,
                    "hunk_buggy": "['         self.gradInput.resize_as_(input)\\n', '         dim = self.dim\\n', ' \\n', '         index = self.index\\n', '         pad = self.pad\\n', '         if pad > 0:']",
                    "hunk_fix": "@@ -46,6 +52,9 @@ class Padding(Module):\n         self.gradInput.resize_as_(input)\n         dim = self.dim\n \n+        if hasattr(self, \"nInputDim\") and self.nInputDim > 0 and input.dim() != self.nInputDim:\n+            dim = dim + 1\n+\n         index = self.index\n         pad = self.pad\n         if pad > 0:"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 521,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/eed22921237eb4c1f4399af177ce912147a885c3",
    "date": "2017-08-31T10:48:57-07:00",
    "message": "check for null commonworld in DestroyCommonWorld\n\nSummary: Check for nullptr before closing a common world.\n\nReviewed By: pietern\n\nDifferential Revision: D5746256\n\nfbshipit-source-id: d395bf60d3b7f2c2629761d2b6fd46085683390c",
    "changes": [
        {
            "name": "common_world_ops.h",
            "path": "caffe2/contrib/gloo/common_world_ops.h",
            "patches": [
                {
                    "old_start": 137,
                    "old_length": 6,
                    "new_start": 137,
                    "new_length": 9,
                    "hunk_buggy": "['   }\\n', ' \\n', '   bool RunOnDevice() override {\\n', '     const auto& context =\\n', '         OperatorBase::Input<std::shared_ptr<::gloo::Context>>(0);\\n', ' ']",
                    "hunk_fix": "@@ -137,6 +137,9 @@ class DestroyCommonWorld final : public Operator<CPUContext> {\n   }\n \n   bool RunOnDevice() override {\n+    if (OperatorBase::InputBlob(0).GetRaw() == nullptr) {\n+      return true;\n+    }\n     const auto& context =\n         OperatorBase::Input<std::shared_ptr<::gloo::Context>>(0);\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 522,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/4d0fbb0e6f578bea14f3f52b0a927bcc20f8b109",
    "date": "2017-08-17T13:03:18-07:00",
    "message": "ConcatOp: fix axis check with add_axis.\n\nSummary: when adding a new axis to concatenate along, allow it to be the last axis. For example, concated 1D columns into a 2D matrix with axis=1, add_axis=1.\n\nReviewed By: hoangmit\n\nDifferential Revision: D5622495\n\nfbshipit-source-id: 8d7c8650c198450ccd4f9e1c98e4ea9f40162be0",
    "changes": [
        {
            "name": "concat_split_op.h",
            "path": "caffe2/operators/concat_split_op.h",
            "patches": [
                {
                    "old_start": 168,
                    "old_length": 7,
                    "new_start": 168,
                    "new_length": 10,
                    "hunk_buggy": "['   split->Resize(vector<TIndex>(1, InputSize()));\\n', '   int* axis_data = split->template mutable_data<int>();\\n', '   auto& input_zero = Input(0);\\n', '-  CAFFE_ENFORCE_LT(axis_, input_zero.ndim(), \"Axis not in input ndim range.\");\\n', '   for (int i = 1; i < InputSize(); ++i) {\\n', '     CAFFE_ENFORCE(\\n', '         Input(i).meta() == input_zero.meta(),']",
                    "hunk_fix": "@@ -168,7 +168,10 @@ bool ConcatOp<Context>::RunOnDevice() {\n   split->Resize(vector<TIndex>(1, InputSize()));\n   int* axis_data = split->template mutable_data<int>();\n   auto& input_zero = Input(0);\n-  CAFFE_ENFORCE_LT(axis_, input_zero.ndim(), \"Axis not in input ndim range.\");\n+  CAFFE_ENFORCE_LT(\n+      axis_,\n+      input_zero.ndim() + (add_axis_ ? 1 : 0),\n+      \"Axis not in input ndim range.\");\n   for (int i = 1; i < InputSize(); ++i) {\n     CAFFE_ENFORCE(\n         Input(i).meta() == input_zero.meta(),"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 523,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/218f4506fdcde69e3f8f2f2b2b51fefd996c577b",
    "date": "2017-08-08T22:35:04-07:00",
    "message": "Fix CUDA check for gcc > 5.\n\nSummary:\nIn response to https://github.com/caffe2/caffe2/pull/504 , this PR modifies the gcc compiler check for CUDA slightly. All ABI since [gcc-3](https://gcc.gnu.org/onlinedocs/libstdc++/manual/abi.html) are compatible with eachother. The check from https://github.com/caffe2/caffe2/pull/504 forced the 'regular' CXX / CC compiler to be set to gcc < 6 but this is not required.\n\nAccording to the documentation for [FindCUDA](https://cmake.org/cmake/help/v3.0/module/FindCUDA.html), `CUDA_HOST_COMPILER` is set to `CMAKE_C_COMPILER` by default. This PR checks if `CMAKE_C_COMPILER` is too new for CUDA 8 and whether `CUDA_HOST_COMPILER` is set to `CMAKE_C_COMPILER`. It also modifies the message slightly.\nCloses https://github.com/caffe2/caffe2/pull/525\n\nDifferential Revision: D5590749\n\nPulled By: Yangqing\n\nfbshipit-source-id: 89f9ea7aecc787d6b74bf794da8aea82fc547ec1",
    "changes": [
        {
            "name": "Dependencies.cmake",
            "path": "cmake/Dependencies.cmake",
            "patches": [
                {
                    "old_start": 285,
                    "old_length": 14,
                    "new_start": 285,
                    "new_length": 13,
                    "hunk_buggy": "['   include(cmake/Cuda.cmake)\\n', '   # CUDA 8.0 requires GCC 5\\n', '   if(HAVE_CUDA)\\n', '-    if (CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\" AND\\n', '-        NOT CMAKE_CXX_COMPILER_VERSION VERSION_LESS 6.0)\\n', '       message(FATAL_ERROR\\n', '         \"CUDA 8.0 is not compatible with GCC version >= 6. \"\\n', '-        \"Use the following options to use another version (for example): \\\\n\"\\n', '-        \"  -DCMAKE_CXX_COMPILER=/usr/bin/g++-5\\\\n\"\\n', '-        \"  -DCMAKE_C_COMPILER=/usr/bin/gcc-5\\\\n\"\\n', '-        \"  -DCUDA_HOST_COMPILER:FILEPATH=/usr/bin/gcc-5\\\\n\")\\n', '     endif()\\n', '   endif()\\n', '   # ---[ CUDNN']",
                    "hunk_fix": "@@ -285,14 +285,13 @@ if(USE_CUDA)\n   include(cmake/Cuda.cmake)\n   # CUDA 8.0 requires GCC 5\n   if(HAVE_CUDA)\n-    if (CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\" AND\n-        NOT CMAKE_CXX_COMPILER_VERSION VERSION_LESS 6.0)\n+    if (CMAKE_C_COMPILER_ID STREQUAL \"GNU\" AND\n+        NOT CMAKE_C_COMPILER_VERSION VERSION_LESS 6.0 AND\n+        CUDA_HOST_COMPILER STREQUAL CMAKE_C_COMPILER)\n       message(FATAL_ERROR\n         \"CUDA 8.0 is not compatible with GCC version >= 6. \"\n-        \"Use the following options to use another version (for example): \\n\"\n-        \"  -DCMAKE_CXX_COMPILER=/usr/bin/g++-5\\n\"\n-        \"  -DCMAKE_C_COMPILER=/usr/bin/gcc-5\\n\"\n-        \"  -DCUDA_HOST_COMPILER:FILEPATH=/usr/bin/gcc-5\\n\")\n+        \"Use the following option to use another version (for example): \\n\"\n+        \"  -DCUDA_HOST_COMPILER=/usr/bin/gcc-5\\n\")\n     endif()\n   endif()\n   # ---[ CUDNN"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 524,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/a53f4b0f9bbc007c0a92e4fd28dd22af027e24a8",
    "date": "2017-07-27T09:54:44-07:00",
    "message": "add dimension check to NHWC2NCHW shape inference\n\nSummary: To prevent assertion from protobuffer when accessing the dims.\n\nReviewed By: asaadaldien\n\nDifferential Revision: D5504362\n\nfbshipit-source-id: d9b55fab3126e2760a3e790615ed30a1af2ddc32",
    "changes": [
        {
            "name": "order_switch_ops.cc",
            "path": "caffe2/operators/order_switch_ops.cc",
            "patches": [
                {
                    "old_start": 53,
                    "old_length": 6,
                    "new_start": 53,
                    "new_length": 8,
                    "hunk_buggy": "['     .NumOutputs(1)\\n', '     .TensorInferenceFunction([](const OperatorDef& /*unused*/ /*def*/,\\n', '                                 const vector<TensorShape>& in) {\\n', '       vector<TensorShape> out(1);\\n', '       out[0].add_dims(in[0].dims(0));\\n', '       out[0].add_dims(in[0].dims(3));']",
                    "hunk_fix": "@@ -53,6 +53,8 @@ OPERATOR_SCHEMA(NHWC2NCHW)\n     .NumOutputs(1)\n     .TensorInferenceFunction([](const OperatorDef& /*unused*/ /*def*/,\n                                 const vector<TensorShape>& in) {\n+      CAFFE_ENFORCE_EQ(\n+          in[0].dims_size(), 4, \"Input for NHWC2NCHW must be 4 dimensional\");\n       vector<TensorShape> out(1);\n       out[0].add_dims(in[0].dims(0));\n       out[0].add_dims(in[0].dims(3));"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 525,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/babb28d2a3a755424f72518bc360d9f511a24463",
    "date": "2017-07-24T21:52:30-07:00",
    "message": "Change DHCECK to CAFFE_ENFORCE in softmax_with_loss_op.cc\n\nSummary:\nBased on discussion on the post in Caffe2 users. Changing DCHECK that works only in debug mode to CAFFE_ENFORCE that throws exception and is a better option.\n\nUpdate: Also correct the check for label_data >= 0, did not check for all elements previously. Moved it to inner loop.\n\nReviewed By: akyrola\n\nDifferential Revision: D5483788\n\nfbshipit-source-id: ccbff09e19e05e7036db772498f71795063c1fed",
    "changes": [
        {
            "name": "softmax_with_loss_op.cc",
            "path": "caffe2/operators/softmax_with_loss_op.cc",
            "patches": [
                {
                    "old_start": 140,
                    "old_length": 20,
                    "new_start": 140,
                    "new_length": 25,
                    "hunk_buggy": "['     const float* label_data = T.data<float>();\\n', ' \\n', '     for (int i = 0; i < N; ++i) {\\n', '-      CAFFE_ENFORCE(\\n', '-          label_data[i] >= 0,\\n', '-          \"Label prob seems incorrect: label prob value must be nonnegative: \",\\n', '-          label_data[i]);\\n', '       float l = 0.0;\\n', '       float total_prob = 0.0;\\n', '       float weight = weights ? weights[i] : 1.0;\\n', '       for (int j = 0; j < D; ++j) {\\n', '         l += -log(std::max(Pdata[i * D + j], 1e-20f)) * label_data[i * D + j] *\\n', '             weight;\\n', '         total_prob += label_data[i * D + j];\\n', '       }\\n', '       loss_sum += l;\\n', '-      DCHECK(std::abs(total_prob - 1.) < 1e-5f);\\n', '       weight_sum += weight;\\n', '     }\\n', '   }']",
                    "hunk_fix": "@@ -140,20 +140,25 @@ bool SoftmaxWithLossOp<float, CPUContext>::RunOnDevice() {\n     const float* label_data = T.data<float>();\n \n     for (int i = 0; i < N; ++i) {\n-      CAFFE_ENFORCE(\n-          label_data[i] >= 0,\n-          \"Label prob seems incorrect: label prob value must be nonnegative: \",\n-          label_data[i]);\n       float l = 0.0;\n       float total_prob = 0.0;\n       float weight = weights ? weights[i] : 1.0;\n       for (int j = 0; j < D; ++j) {\n+        CAFFE_ENFORCE(\n+            label_data[i * D + j] >= 0,\n+            \"Label prob seems incorrect: label prob value must be nonnegative:\",\n+            \" \",\n+            label_data[i * D + j]);\n         l += -log(std::max(Pdata[i * D + j], 1e-20f)) * label_data[i * D + j] *\n             weight;\n         total_prob += label_data[i * D + j];\n       }\n       loss_sum += l;\n-      DCHECK(std::abs(total_prob - 1.) < 1e-5f);\n+      CAFFE_ENFORCE(\n+          std::abs(total_prob - 1.) < 1e-5f,\n+          \"Label prob seems incorrect: label prob values do not sum to 1.0: \",\n+          total_prob,\n+          \" vs 1.0 (+/- 1e-5)\");\n       weight_sum += weight;\n     }\n   }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 526,
    "LibraryName": "pytorch",
    "commit_link": "https://github.com/pytorch/pytorch/commit/4b1ebd2f65e49d251ac2cfdb635794c7c6eb362f",
    "date": "2017-07-10T17:52:22-07:00",
    "message": "Fast path for serializing large floating-point tensors to protobuf\n\nSummary: Our existing serialization routines take a significant amount of time for large numpy arrays in order to verify the type of each element in the array as well as converting each element to a canonical type.  For large floating-point tensors, such as model parameters, this checking and converting takes a significant amount of time.  Adding a fast track path for just float32 arrays as this is the most common use case to worry about.\n\nReviewed By: akyrola\n\nDifferential Revision: D5389953\n\nfbshipit-source-id: 26f44cb2426ea3efb849e7707b27d5485f69956c",
    "changes": [
        {
            "name": "utils.py",
            "path": "caffe2/python/utils.py",
            "patches": [
                {
                    "old_start": 69,
                    "old_length": 6,
                    "new_start": 69,
                    "new_length": 14,
                    "hunk_buggy": "['     argument.name = key\\n', '     iterable = isinstance(value, collections.Iterable)\\n', ' \\n', '     if isinstance(value, np.ndarray):\\n', '         value = value.flatten().tolist()\\n', '     elif isinstance(value, np.generic):']",
                    "hunk_fix": "@@ -69,6 +69,14 @@ def MakeArgument(key, value):\n     argument.name = key\n     iterable = isinstance(value, collections.Iterable)\n \n+    # Fast tracking common use case where a float32 array of tensor parameters\n+    # needs to be serialized.  The entire array is guaranteed to have the same\n+    # dtype, so no per-element checking necessary and no need to convert each\n+    # element separately.\n+    if isinstance(value, np.ndarray) and value.dtype.type is np.float32:\n+        argument.floats.extend(value.flatten().tolist())\n+        return argument\n+\n     if isinstance(value, np.ndarray):\n         value = value.flatten().tolist()\n     elif isinstance(value, np.generic):"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
}]