[{
    "Id": 0,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/88609e2e22fa5c296de2e27e04d1cc4743b2dfcd",
    "date": "2022-02-18T00:01:10+00:00",
    "message": "Add appropriate dtype check for tf.boolean_mask's mask\n\nThis PR tries to address the issue raised in 54412 where\nmask's dtype was checked in tf.boolean_mask and an invalid\nresult has been returned instead.\n\nThis PR fixes 54412.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "changes": [
        {
            "name": "array_ops.py",
            "path": "tensorflow/python/ops/array_ops.py",
            "patches": [
                {
                    "old_start": 1881,
                    "old_length": 6,
                    "new_start": 1881,
                    "new_length": 8,
                    "hunk_buggy": "['   with ops.name_scope(name, values=[tensor, mask]):\\n', '     tensor = ops.convert_to_tensor(tensor, name=\"tensor\")\\n', '     mask = ops.convert_to_tensor(mask, name=\"mask\")\\n', ' \\n', '     shape_mask = mask.get_shape()\\n', '     ndims_mask = shape_mask.ndims']",
                    "hunk_fix": "@@ -1881,6 +1881,8 @@ def boolean_mask(tensor, mask, name=\"boolean_mask\", axis=None):\n   with ops.name_scope(name, values=[tensor, mask]):\n     tensor = ops.convert_to_tensor(tensor, name=\"tensor\")\n     mask = ops.convert_to_tensor(mask, name=\"mask\")\n+    if mask.dtype != dtypes.bool:\n+      raise TypeError(\"Invalid `mask`: expected bool but got %s.\" % mask.dtype)\n \n     shape_mask = mask.get_shape()\n     ndims_mask = shape_mask.ndims"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 1,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/66e0cb1d9afd251931f4f920c5d7bd638bc882b4",
    "date": "2023-07-11T15:13:29+05:30",
    "message": "validate clip_norm argument in clip_by_norm API\n\nThe API clip_by_norm have argument clip_norm which accepts  0-D (scalar) `Tensor` > 0 . But if we pass -ve value for this argument then its not raising intended error and converting the input tensor into Negative which IMO is wrong. Hence I am adding validation code for -ve values to raise value error.",
    "changes": [
        {
            "name": "clip_ops.py",
            "path": "tensorflow/python/ops/clip_ops.py",
            "patches": [
                {
                    "old_start": 212,
                    "old_length": 6,
                    "new_start": 212,
                    "new_length": 8,
                    "hunk_buggy": "['     values = ops.convert_to_tensor(\\n', '         t.values if isinstance(t, indexed_slices.IndexedSlices) else t,\\n', '         name=\"t\")\\n', ' \\n', '     # Calculate L2-norm, clip elements by ratio of clip_norm to L2-norm\\n', '     l2sum = math_ops.reduce_sum(values * values, axes, keepdims=True)']",
                    "hunk_fix": "@@ -212,6 +212,8 @@ def clip_by_norm(t, clip_norm, axes=None, name=None):\n     values = ops.convert_to_tensor(\n         t.values if isinstance(t, indexed_slices.IndexedSlices) else t,\n         name=\"t\")\n+    if clip_norm < 0:\n+      raise ValueError('clip_norm should be a 0-D (scalar) Tensor > 0')\n \n     # Calculate L2-norm, clip elements by ratio of clip_norm to L2-norm\n     l2sum = math_ops.reduce_sum(values * values, axes, keepdims=True)"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 2,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/86abddb56350bccd95d1b7140b003fb03525b890",
    "date": "2022-02-17T23:11:34+00:00",
    "message": "Add appropriate error check for nbins in tf.histogram_fixed_width_bins\n\nThis PR tries to address the issue raised in 54415 where\nnbins was not checked for tf.histogram_fixed_width_bins\nand an incorrect result was returned when nbins < 0.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "changes": [
        {
            "name": "histogram_ops.py",
            "path": "tensorflow/python/ops/histogram_ops.py",
            "patches": [
                {
                    "old_start": 20,
                    "old_length": 6,
                    "new_start": 20,
                    "new_length": 7,
                    "hunk_buggy": "[' from tensorflow.python.framework import ops\\n', ' from tensorflow.python.ops import array_ops\\n', ' from tensorflow.python.ops import clip_ops\\n', ' from tensorflow.python.ops import gen_math_ops\\n', ' from tensorflow.python.ops import math_ops\\n', ' from tensorflow.python.util import dispatch\\n']",
                    "hunk_fix": "@@ -20,6 +20,7 @@ from tensorflow.python.framework import dtypes\n from tensorflow.python.framework import ops\n from tensorflow.python.ops import array_ops\n from tensorflow.python.ops import clip_ops\n+from tensorflow.python.ops import control_flow_ops\n from tensorflow.python.ops import gen_math_ops\n from tensorflow.python.ops import math_ops\n from tensorflow.python.util import dispatch\n"
                },
                {
                    "old_start": 77,
                    "old_length": 6,
                    "new_start": 78,
                    "new_length": 9,
                    "hunk_buggy": "['     values = array_ops.reshape(values, [-1])\\n', \"     value_range = ops.convert_to_tensor(value_range, name='value_range')\\n\", \"     nbins = ops.convert_to_tensor(nbins, dtype=dtypes.int32, name='nbins')\\n\", '     nbins_float = math_ops.cast(nbins, values.dtype)\\n', ' \\n', '     # Map tensor values that fall within value_range to [0, 1].']",
                    "hunk_fix": "@@ -77,6 +78,9 @@ def histogram_fixed_width_bins(values,\n     values = array_ops.reshape(values, [-1])\n     value_range = ops.convert_to_tensor(value_range, name='value_range')\n     nbins = ops.convert_to_tensor(nbins, dtype=dtypes.int32, name='nbins')\n+    check = control_flow_ops.Assert(\n+        math_ops.greater(nbins, 0), [\"nbins %s must > 0\" % nbins])\n+    nbins = control_flow_ops.with_dependencies([check], nbins)\n     nbins_float = math_ops.cast(nbins, values.dtype)\n \n     # Map tensor values that fall within value_range to [0, 1]."
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 3,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/8c3822edbb31cf71cedaf49f2167e45c1e2d0b83",
    "date": "2023-03-13T12:53:59-07:00",
    "message": "Update the is_dtensor check to only run in eager mode.\n\nPiperOrigin-RevId: 516294602",
    "changes": [
        {
            "name": "dtensor_device.py",
            "path": "tensorflow/dtensor/python/dtensor_device.py",
            "patches": [
                {
                    "old_start": 307,
                    "old_length": 7,
                    "new_start": 307,
                    "new_length": 12,
                    "hunk_buggy": "[' \\n', '     Returns:\\n', '       bool, True if the given tensor is a DTensor.\\n', '     \"\"\"\\n', '     if not tensor_util.is_tensor(tensor):\\n', '       return False\\n', '     if isinstance(tensor, variables.Variable):']",
                    "hunk_fix": "@@ -307,7 +307,12 @@ class DTensorDevice(object):\n \n     Returns:\n       bool, True if the given tensor is a DTensor.\n+\n+    Raises:\n+      RuntimeError: When not called eagerly.\n     \"\"\"\n+    if not context.executing_eagerly():\n+      raise RuntimeError(\"is_dtensor must be called eagerly.\")\n     if not tensor_util.is_tensor(tensor):\n       return False\n     if isinstance(tensor, variables.Variable):"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 4,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/480641e3599775a8895254ffbc0fc45621334f68",
    "date": "2021-04-24T16:51:15-07:00",
    "message": "Validate (and ensure validation sticks) inputs for `MatrixTriangularSolve`.\n\nPiperOrigin-RevId: 370282444\nChange-Id: Iaed61a0b0727cc42c830658b72eb69f785f48dc5",
    "changes": [
        {
            "name": "matrix_triangular_solve_op_impl.h",
            "path": "tensorflow/core/kernels/linalg/matrix_triangular_solve_op_impl.h",
            "patches": [
                {
                    "old_start": 162,
                    "old_length": 6,
                    "new_start": 162,
                    "new_length": 9,
                    "hunk_buggy": "['     const Tensor& in1 = ctx->input(1);\\n', ' \\n', '     ValidateInputTensors(ctx, in0, in1);\\n', ' \\n', '     MatMulBCast bcast(in0.shape().dim_sizes(), in1.shape().dim_sizes());\\n', '     OP_REQUIRES(\\n']",
                    "hunk_fix": "@@ -162,6 +162,9 @@ class BaseMatrixTriangularSolveOp : public OpKernel {\n     const Tensor& in1 = ctx->input(1);\n \n     ValidateInputTensors(ctx, in0, in1);\n+    if (!ctx->status().ok()) {\n+      return;\n+    }\n \n     MatMulBCast bcast(in0.shape().dim_sizes(), in1.shape().dim_sizes());\n     OP_REQUIRES(\n"
                },
                {
                    "old_start": 230,
                    "old_length": 13,
                    "new_start": 233,
                    "new_length": 22,
                    "hunk_buggy": "['  private:\\n', '   void ValidateInputTensors(OpKernelContext* ctx, const Tensor& in0,\\n', '                             const Tensor& in1) override {\\n', '     OP_REQUIRES(\\n', '-        ctx, in0.dims() >= 2,\\n', '-        errors::InvalidArgument(\"In[0] ndims must be >= 2: \", in0.dims()));\\n', ' \\n', '     OP_REQUIRES(\\n', '-        ctx, in1.dims() >= 2,\\n', '-        errors::InvalidArgument(\"In[0] ndims must be >= 2: \", in1.dims()));\\n', '   }\\n', ' };\\n', ' ']",
                    "hunk_fix": "@@ -230,13 +233,22 @@ class MatrixTriangularSolveOp\n  private:\n   void ValidateInputTensors(OpKernelContext* ctx, const Tensor& in0,\n                             const Tensor& in1) override {\n+    const auto in0_num_dims = in0.dims();\n     OP_REQUIRES(\n-        ctx, in0.dims() >= 2,\n-        errors::InvalidArgument(\"In[0] ndims must be >= 2: \", in0.dims()));\n+        ctx, in0_num_dims >= 2,\n+        errors::InvalidArgument(\"In[0] ndims must be >= 2: \", in0_num_dims));\n \n+    const auto in1_num_dims = in1.dims();\n     OP_REQUIRES(\n-        ctx, in1.dims() >= 2,\n-        errors::InvalidArgument(\"In[0] ndims must be >= 2: \", in1.dims()));\n+        ctx, in1_num_dims >= 2,\n+        errors::InvalidArgument(\"In[1] ndims must be >= 2: \", in1_num_dims));\n+\n+    const auto in0_last_dim = in0.dim_size(in0_num_dims - 1);\n+    const auto in0_prev_dim = in0.dim_size(in0_num_dims - 2);\n+    OP_REQUIRES(ctx, in0_last_dim == in0_prev_dim,\n+                errors::InvalidArgument(\n+                    \"In[0] matrices in the last dimensions must be square (\",\n+                    in0_last_dim, \" =/= \", in0_prev_dim, \")\"));\n   }\n };\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 5,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/4f4a0276a2cf9186c0541072964676159368286e",
    "date": "2022-02-18T07:04:46+00:00",
    "message": "Add appropriate PyObject type check for bool\n\nThis PR fixes an issue where PyObject type in tf's C bindings\ndoes not check if an input is a boolean and will always cast\nto bool. As an result, an invalid type like int can be passed\nto an arg expecting bool:\n```\nimport tensorflow as tf\nx = [0.5, 1.0, 2.0, 4.0]\naxis = 0\nexclusive = -1\nreverse = -1\nres_1 = tf.math.cumsum(x, axis=axis, exclusive=exclusive, reverse=reverse)\nprint(res_1) # tf.Tensor([7. 6. 4. 0.], shape=(4,), dtype=float32)\n```\n\nNote other PyObejct types (e.g., in ParseIntValue) does perform check in\ntensorflow/python/eager/pywrap_tfe_src.cc\n\nThis PR adds appropriate check in the input type when PyObject path\nis invoked.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "changes": [
        {
            "name": "pywrap_tfe_src.cc",
            "path": "tensorflow/python/eager/pywrap_tfe_src.cc",
            "patches": [
                {
                    "old_start": 347,
                    "old_length": 8,
                    "new_start": 347,
                    "new_length": 16,
                    "hunk_buggy": "[' \\n', ' bool ParseBoolValue(const string& key, PyObject* py_value, TF_Status* status,\\n', '                     unsigned char* value) {\\n', '-  *value = PyObject_IsTrue(py_value);\\n', '-  return true;\\n', ' }\\n', ' \\n', ' // The passed in py_value is expected to be an object of the python type']",
                    "hunk_fix": "@@ -347,8 +347,16 @@ bool ParseStringValue(const string& key, PyObject* py_value, TF_Status* status,\n \n bool ParseBoolValue(const string& key, PyObject* py_value, TF_Status* status,\n                     unsigned char* value) {\n-  *value = PyObject_IsTrue(py_value);\n-  return true;\n+  if (PyBool_Check(py_value)) {\n+    *value = PyObject_IsTrue(py_value);\n+    return true;\n+  }\n+  TF_SetStatus(\n+      status, TF_INVALID_ARGUMENT,\n+      tensorflow::strings::StrCat(\"Expecting bool value for attr \", key,\n+                                  \", got \", py_value->ob_type->tp_name)\n+          .c_str());\n+  return false;\n }\n \n // The passed in py_value is expected to be an object of the python type"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 6,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/9a4b6b6bcc7a813162bf0378727950e321aca19c",
    "date": "2023-05-09T14:49:51-04:00",
    "message": "Add stricter type checking for tf.math.real (using is_numeric)",
    "changes": [
        {
            "name": "math_ops.py",
            "path": "tensorflow/python/ops/math_ops.py",
            "patches": [
                {
                    "old_start": 822,
                    "old_length": 7,
                    "new_start": 822,
                    "new_length": 7,
                    "hunk_buggy": "['     if input.dtype.is_complex:\\n', '       real_dtype = input.dtype.real_dtype\\n', '       return gen_math_ops.real(input, Tout=real_dtype, name=name)\\n', '-    elif tf.debugging.is_numeric_tensor(input):\\n', '       return input\\n', '     else:\\n', '       raise TypeError(\"input must be a numeric tensor, but got tensor with dtype {}\".format(input.dtype))']",
                    "hunk_fix": "@@ -822,7 +822,7 @@ def real(input, name=None):\n     if input.dtype.is_complex:\n       real_dtype = input.dtype.real_dtype\n       return gen_math_ops.real(input, Tout=real_dtype, name=name)\n-    elif tf.debugging.is_numeric_tensor(input):\n+    elif input.dtype.is_numeric:\n       return input\n     else:\n       raise TypeError(\"input must be a numeric tensor, but got tensor with dtype {}\".format(input.dtype))"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 7,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/2e4d3951eb618a7c34d5e629fc2506ea2a62b4a7",
    "date": "2019-07-01T15:53:56+02:00",
    "message": "Correct Tensor order for dilation2D\n\n`gen_nn_ops.dilation2d` seems to be in `NHWC` while the parent function was asking for `NCHW`.\r\nI corrected the doc and the check.",
    "changes": [
        {
            "name": "nn_ops.py",
            "path": "tensorflow/python/ops/nn_ops.py",
            "patches": [
                {
                    "old_start": 280,
                    "old_length": 7,
                    "new_start": 280,
                    "new_length": 7,
                    "hunk_buggy": "['       tensor. Must be: `[1, stride_height, stride_width, 1]`.\\n', '     padding: A `string` from: `\"SAME\", \"VALID\"`.\\n', '       The type of padding algorithm to use.\\n', '-    data_format: A `string`, only `\"NCHW\"` is currently supported.\\n', '     dilations: A list of `ints` that has length `>= 4`.\\n', '       The input stride for atrous morphological dilation. Must be:\\n', '       `[1, rate_height, rate_width, 1]`.\\n']",
                    "hunk_fix": "@@ -280,7 +280,7 @@ def dilation2d_v2(\n       tensor. Must be: `[1, stride_height, stride_width, 1]`.\n     padding: A `string` from: `\"SAME\", \"VALID\"`.\n       The type of padding algorithm to use.\n-    data_format: A `string`, only `\"NCHW\"` is currently supported.\n+    data_format: A `string`, only `\"NHWC\"` is currently supported.\n     dilations: A list of `ints` that has length `>= 4`.\n       The input stride for atrous morphological dilation. Must be:\n       `[1, rate_height, rate_width, 1]`.\n"
                },
                {
                    "old_start": 289,
                    "old_length": 8,
                    "new_start": 289,
                    "new_length": 8,
                    "hunk_buggy": "['   Returns:\\n', '     A `Tensor`. Has the same type as `input`.\\n', '   \"\"\"\\n', '-  if data_format != \"NCHW\":\\n', '-    raise ValueError(\"Data formats other than NCHW are not yet supported\")\\n', ' \\n', '   return gen_nn_ops.dilation2d(input=input,\\n', '                                filter=filters,']",
                    "hunk_fix": "@@ -289,8 +289,8 @@ def dilation2d_v2(\n   Returns:\n     A `Tensor`. Has the same type as `input`.\n   \"\"\"\n-  if data_format != \"NCHW\":\n-    raise ValueError(\"Data formats other than NCHW are not yet supported\")\n+  if data_format != \"NHWC\":\n+    raise ValueError(\"Data formats other than NHWC are not yet supported\")\n \n   return gen_nn_ops.dilation2d(input=input,\n                                filter=filters,"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 8,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/80bb2f5511e7d2d386c79da52ff517691e19ac54",
    "date": "2022-05-24T08:40:36-07:00",
    "message": "Add check condition for large values of range_max, which is causing session abort.\nFixes: https://github.com/tensorflow/tensorflow/issues/46938\n\nPiperOrigin-RevId: 450688945",
    "changes": [
        {
            "name": "candidate_sampling_ops.py",
            "path": "tensorflow/python/ops/candidate_sampling_ops.py",
            "patches": [
                {
                    "old_start": 206,
                    "old_length": 6,
                    "new_start": 206,
                    "new_length": 9,
                    "hunk_buggy": "[' \\n', '   \"\"\"\\n', '   seed1, seed2 = random_seed.get_seed(seed)\\n', '   return gen_candidate_sampling_ops.learned_unigram_candidate_sampler(\\n', '       true_classes, num_true, num_sampled, unique, range_max, seed=seed1,\\n', '       seed2=seed2, name=name)']",
                    "hunk_fix": "@@ -206,6 +206,9 @@ def learned_unigram_candidate_sampler(true_classes, num_true, num_sampled,\n \n   \"\"\"\n   seed1, seed2 = random_seed.get_seed(seed)\n+  # Limiting to Max int32 value\n+  if range_max > 2147483647:\n+    raise ValueError(f'Value of range_max:{range_max} is too large to handle')\n   return gen_candidate_sampling_ops.learned_unigram_candidate_sampler(\n       true_classes, num_true, num_sampled, unique, range_max, seed=seed1,\n       seed2=seed2, name=name)"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 9,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/1610da3f992487bd9a8181d1e83cae99fe1e34d9",
    "date": "2023-04-10T13:56:07-07:00",
    "message": "add more sanity check on AvgPoolGrad op",
    "changes": [
        {
            "name": "mkl_avgpooling_op.cc",
            "path": "tensorflow/core/kernels/mkl/mkl_avgpooling_op.cc",
            "patches": [
                {
                    "old_start": 289,
                    "old_length": 6,
                    "new_start": 289,
                    "new_length": 14,
                    "hunk_buggy": "['                                                       this->data_format_tf_)\\n', '                           : TFShapeToMklDnnDimsInNCDHW(grad_tensor.shape(),\\n', '                                                        this->data_format_tf_);\\n', '       memory::dims output_dims_mkl_order;\\n', '       this->GetOutputDims(pool_params, &output_dims_mkl_order);\\n', ' ']",
                    "hunk_fix": "@@ -289,6 +289,14 @@ class MklAvgPoolingGradOp : public MklPoolingBackwardOpBase<T> {\n                                                       this->data_format_tf_)\n                           : TFShapeToMklDnnDimsInNCDHW(grad_tensor.shape(),\n                                                        this->data_format_tf_);\n+\n+      OP_REQUIRES(\n+          context, orig_input_dims_mkl_order[0] == diff_dst_dims[0],\n+          errors::InvalidArgument(\n+              \"Expected first dimension of orig_input and diff_dst to match, \"\n+              \"got \",\n+              orig_input_dims_mkl_order[0], \" and \", diff_dst_dims[0]));\n+\n       memory::dims output_dims_mkl_order;\n       this->GetOutputDims(pool_params, &output_dims_mkl_order);\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 10,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/e99e31597c1b5cc9f0cbc8a3dea71674d81c20b1",
    "date": "2023-02-15T12:44:47-08:00",
    "message": "Fix GRUCellBlockOp message for invalid rank of x\n\nThe validation checks that x is a matrix, so rank must be 2. ff45913\nfixed the crash in #58261 but left this typo in an exception message.\n\nFixes #58261\n\nSigned-off-by: Reid Wahl <nrwahl@protonmail.com>",
    "changes": [
        {
            "name": "gru_ops.cc",
            "path": "tensorflow/core/kernels/rnn/gru_ops.cc",
            "patches": [
                {
                    "old_start": 53,
                    "old_length": 8,
                    "new_start": 53,
                    "new_length": 8,
                    "hunk_buggy": "[' \\n', \"     // Shape of 'x' must be [batch_size, input_size]\\n\", '     OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(x_tensor->shape()),\\n', '-                errors::InvalidArgument(\"Rank of x must be 2\", x_tensor->dims(),\\n', '-                                        \" vs. 2\"));\\n', '     const int64_t batch_size = x_tensor->dim_size(0);\\n', '     const int64_t input_size = x_tensor->dim_size(1);\\n', ' ']",
                    "hunk_fix": "@@ -53,8 +53,8 @@ class GRUCellBlockOp : public OpKernel {\n \n     // Shape of 'x' must be [batch_size, input_size]\n     OP_REQUIRES(ctx, TensorShapeUtils::IsMatrix(x_tensor->shape()),\n-                errors::InvalidArgument(\"Rank of x must be 2\", x_tensor->dims(),\n-                                        \" vs. 2\"));\n+                errors::InvalidArgument(\"Rank of x must be 2, got \",\n+                                        x_tensor->dims()));\n     const int64_t batch_size = x_tensor->dim_size(0);\n     const int64_t input_size = x_tensor->dim_size(1);\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 11,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/8b742f8559e88474735d0a2c03e00da65e40b412",
    "date": "2023-05-03T10:28:05-07:00",
    "message": "Fix check error on shape overflow.\n\nFixes #60198, #60275\n\nPiperOrigin-RevId: 529127714",
    "changes": [
        {
            "name": "linalg_ops_common.cc",
            "path": "tensorflow/core/kernels/linalg/linalg_ops_common.cc",
            "patches": [
                {
                    "old_start": 153,
                    "old_length": 8,
                    "new_start": 153,
                    "new_length": 10,
                    "hunk_buggy": "['     const int col_dimension = input_rank - 1;\\n', '     const int64_t num_rows = in.dim_size(row_dimension);\\n', '     const int64_t num_cols = in.dim_size(col_dimension);\\n', '-    input_matrix_shapes->emplace_back(\\n', '-        std::initializer_list<int64_t>({num_rows, num_cols}));\\n', '     inputs->emplace_back(&in);\\n', '     OP_REQUIRES(\\n', '         context, in.dtype() == DataTypeToEnum<InputScalar>::v(),']",
                    "hunk_fix": "@@ -153,8 +153,10 @@ void LinearAlgebraOp<InputScalar, OutputScalar>::AnalyzeInputs(\n     const int col_dimension = input_rank - 1;\n     const int64_t num_rows = in.dim_size(row_dimension);\n     const int64_t num_cols = in.dim_size(col_dimension);\n-    input_matrix_shapes->emplace_back(\n-        std::initializer_list<int64_t>({num_rows, num_cols}));\n+    TensorShape input_shape;\n+    OP_REQUIRES_OK(context, TensorShape::BuildTensorShape({num_rows, num_cols},\n+                                                          &input_shape));\n+    input_matrix_shapes->push_back(std::move(input_shape));\n     inputs->emplace_back(&in);\n     OP_REQUIRES(\n         context, in.dtype() == DataTypeToEnum<InputScalar>::v(),"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 12,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/199f1ff12a28d571100b323ec54a5eee47078d8b",
    "date": "2022-03-17T15:04:54+00:00",
    "message": "Add necessary check in fft ops to fix crash\n\nThis PR tries to address the issue raised in 55263 where\ntf.single.rfft2d will crash when length contains negative value.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "changes": [
        {
            "name": "fft_ops.cc",
            "path": "tensorflow/core/kernels/fft_ops.cc",
            "patches": [
                {
                    "old_start": 66,
                    "old_length": 6,
                    "new_start": 66,
                    "new_length": 12,
                    "hunk_buggy": "[' \\n', '       auto fft_length_as_vec = fft_length.vec<int32>();\\n', '       for (int i = 0; i < fft_rank; ++i) {\\n', '         fft_shape[i] = fft_length_as_vec(i);\\n', '         // Each input dimension must have length of at least fft_shape[i]. For\\n', '         // IRFFTs, the inner-most input dimension must have length of at least']",
                    "hunk_fix": "@@ -66,6 +66,12 @@ class FFTBase : public OpKernel {\n \n       auto fft_length_as_vec = fft_length.vec<int32>();\n       for (int i = 0; i < fft_rank; ++i) {\n+        OP_REQUIRES(\n+            ctx,\n+            fft_length_as_vec(i) >= 0,\n+            errors::InvalidArgument(\n+                \"fft_length[\" , i,\n+                \"] must >= 0, but got: \", fft_length_as_vec(i)));\n         fft_shape[i] = fft_length_as_vec(i);\n         // Each input dimension must have length of at least fft_shape[i]. For\n         // IRFFTs, the inner-most input dimension must have length of at least"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 13,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/0666d8bb711b41c9f03dec238d7d165bc946fc70",
    "date": "2020-12-14T23:11:16+00:00",
    "message": "Prevent crash of tensorflow if shape is too large for tf.sparse.reorder\n\nThis PR tries to address the issue raised in 45392 where\ntensorflow crashes if shape of sparse tensor is too large for\ntf.sparse.reorder\n\nThis PR adds additional checks and exit gracefully if the shape\nis too large.\n\nThis PR fixes 45392.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "changes": [
        {
            "name": "sparse_reorder_op.cc",
            "path": "tensorflow/core/kernels/sparse_reorder_op.cc",
            "patches": [
                {
                    "old_start": 27,
                    "old_length": 6,
                    "new_start": 27,
                    "new_length": 7,
                    "hunk_buggy": "[' #include \"tensorflow/core/framework/types.h\"\\n', ' #include \"tensorflow/core/lib/gtl/inlined_vector.h\"\\n', ' #include \"tensorflow/core/util/sparse/sparse_tensor.h\"\\n', ' \\n', ' namespace tensorflow {\\n', ' \\n']",
                    "hunk_fix": "@@ -27,6 +27,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/types.h\"\n #include \"tensorflow/core/lib/gtl/inlined_vector.h\"\n #include \"tensorflow/core/util/sparse/sparse_tensor.h\"\n+#include \"tensorflow/core/util/overflow.h\"\n \n namespace tensorflow {\n \n"
                },
                {
                    "old_start": 54,
                    "old_length": 6,
                    "new_start": 55,
                    "new_length": 18,
                    "hunk_buggy": "['                     \"Input shape should be a vector but received shape \",\\n', '                     input_shape_in.shape().DebugString()));\\n', ' \\n', '     const TensorShape input_shape(input_shape_in.vec<int64>());\\n', ' \\n', '     gtl::InlinedVector<int64, 8> std_order(input_shape.dims());']",
                    "hunk_fix": "@@ -54,6 +55,18 @@ class SparseReorderOp : public OpKernel {\n                     \"Input shape should be a vector but received shape \",\n                     input_shape_in.shape().DebugString()));\n \n+    // Check if the sparse tensor input shape is valid\n+    int64 total = 1;\n+    for (int64 i = 0; i < input_shape_in.NumElements(); ++i) {\n+      int dim = input_shape_in.vec<int64>()(i);\n+      OP_REQUIRES(context, (dim >= 0),\n+                  errors::InvalidArgument(\"Dimension \", dim, \" must be >= 0\"));\n+      total = MultiplyWithoutOverflow(total, dim);\n+      OP_REQUIRES(context, (total > 0),\n+                  errors::InvalidArgument(\n+                      \"Shape would have more than 2**63 - 1 elements\"));\n+    }\n+\n     const TensorShape input_shape(input_shape_in.vec<int64>());\n \n     gtl::InlinedVector<int64, 8> std_order(input_shape.dims());"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 14,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/e560136d757867482a93be74e108ef516920bcfc",
    "date": "2021-12-09T08:00:55+00:00",
    "message": "Fix wrong output of tf.stack with 0-dimension tensor\n\nThis PR tries to address the issue raised in 53300 where\ntf.stack will silently output wrong result with 0-dimension tensor.\nThe issue was that the shape check was skipped when num of output elements\nwas zero. This PR fixed the issue.\n\nThis PR fixes 53300.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "changes": [
        {
            "name": "pack_op.cc",
            "path": "tensorflow/core/kernels/pack_op.cc",
            "patches": [
                {
                    "old_start": 87,
                    "old_length": 25,
                    "new_start": 87,
                    "new_length": 25,
                    "hunk_buggy": "['     const int64_t axis_dim = output_shape.dim_size(axis);\\n', ' \\n', '     const int64_t output_size = output->NumElements();\\n', '     if (output_size > 0) {\\n', '-      auto output_flat =\\n', '-          output->shaped<T, 2>({before_dim, after_dim * axis_dim});\\n', '-\\n', '-      // Except for shapes, pack is a special case of concat, so we reuse the\\n', '-      // same computational kernels.\\n', '-      ConstMatrixVector inputs_flat;\\n', '-      inputs_flat.reserve(num);\\n', '-      for (int i = 0; i < num; ++i) {\\n', '-        const Tensor& input = c->input(i);\\n', '-        OP_REQUIRES(c, first_input.shape().IsSameSize(input.shape()),\\n', '-                    errors::InvalidArgument(\\n', '-                        \"Shapes of all inputs must match: values[0].shape = \",\\n', '-                        first_input.shape().DebugString(), \" != values[\", i,\\n', '-                        \"].shape = \", input.shape().DebugString()));\\n', '-\\n', '-        inputs_flat.emplace_back(new typename TTypes<T, 2>::ConstMatrix(\\n', '-            input.shaped<T, 2>({before_dim, after_dim})));\\n', '-      }\\n', ' #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\\n', '       if (std::is_same<Device, GPUDevice>::value) {\\n', '         ConcatGPU<T>(c, inputs_flat, output, &output_flat);']",
                    "hunk_fix": "@@ -87,25 +87,25 @@ class PackOp : public OpKernel {\n     const int64_t axis_dim = output_shape.dim_size(axis);\n \n     const int64_t output_size = output->NumElements();\n+    auto output_flat =\n+        output->shaped<T, 2>({before_dim, after_dim * axis_dim});\n+\n+    // Except for shapes, pack is a special case of concat, so we reuse the\n+    // same computational kernels.\n+    ConstMatrixVector inputs_flat;\n+    inputs_flat.reserve(num);\n+    for (int i = 0; i < num; ++i) {\n+      const Tensor& input = c->input(i);\n+      OP_REQUIRES(c, first_input.shape().IsSameSize(input.shape()),\n+                  errors::InvalidArgument(\n+                      \"Shapes of all inputs must match: values[0].shape = \",\n+                      first_input.shape().DebugString(), \" != values[\", i,\n+                      \"].shape = \", input.shape().DebugString()));\n+\n+      inputs_flat.emplace_back(new typename TTypes<T, 2>::ConstMatrix(\n+          input.shaped<T, 2>({before_dim, after_dim})));\n+    }\n     if (output_size > 0) {\n-      auto output_flat =\n-          output->shaped<T, 2>({before_dim, after_dim * axis_dim});\n-\n-      // Except for shapes, pack is a special case of concat, so we reuse the\n-      // same computational kernels.\n-      ConstMatrixVector inputs_flat;\n-      inputs_flat.reserve(num);\n-      for (int i = 0; i < num; ++i) {\n-        const Tensor& input = c->input(i);\n-        OP_REQUIRES(c, first_input.shape().IsSameSize(input.shape()),\n-                    errors::InvalidArgument(\n-                        \"Shapes of all inputs must match: values[0].shape = \",\n-                        first_input.shape().DebugString(), \" != values[\", i,\n-                        \"].shape = \", input.shape().DebugString()));\n-\n-        inputs_flat.emplace_back(new typename TTypes<T, 2>::ConstMatrix(\n-            input.shaped<T, 2>({before_dim, after_dim})));\n-      }\n #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\n       if (std::is_same<Device, GPUDevice>::value) {\n         ConcatGPU<T>(c, inputs_flat, output, &output_flat);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 15,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/9ce847ed140702d1dd4cb204a8afe0ffedb70b15",
    "date": "2020-10-02T16:04:32-07:00",
    "message": "Remove a few check ops that no longer need to run in tf.Variable's constructor\n\nVarHandleOp ensures there is no sharing. These aren't a huge part of startup time for replicated models, but there's still no reason to run them.\n\nPiperOrigin-RevId: 335117818\nChange-Id: I38a0f944a565907630f1da0ef6d896a633b296c0",
    "changes": [
        {
            "name": "resource_variable_ops.py",
            "path": "tensorflow/python/ops/resource_variable_ops.py",
            "patches": [
                {
                    "old_start": 35,
                    "old_length": 12,
                    "new_start": 35,
                    "new_length": 12,
                    "hunk_buggy": "[' from tensorflow.python.framework import constant_op\\n', ' from tensorflow.python.framework import cpp_shape_inference_pb2\\n', ' from tensorflow.python.framework import dtypes\\n', ' from tensorflow.python.framework import ops\\n', ' from tensorflow.python.framework import tensor_shape\\n', ' from tensorflow.python.framework import tensor_spec\\n', ' from tensorflow.python.ops import array_ops\\n', ' from tensorflow.python.ops import gen_array_ops\\n', '-from tensorflow.python.ops import gen_logging_ops\\n', ' from tensorflow.python.ops import gen_resource_variable_ops\\n', ' from tensorflow.python.ops import gen_state_ops\\n', ' from tensorflow.python.ops import math_ops\\n']",
                    "hunk_fix": "@@ -35,12 +35,12 @@ from tensorflow.python.framework import auto_control_deps_utils as acd\n from tensorflow.python.framework import constant_op\n from tensorflow.python.framework import cpp_shape_inference_pb2\n from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import errors\n from tensorflow.python.framework import ops\n from tensorflow.python.framework import tensor_shape\n from tensorflow.python.framework import tensor_spec\n from tensorflow.python.ops import array_ops\n from tensorflow.python.ops import gen_array_ops\n-from tensorflow.python.ops import gen_logging_ops\n from tensorflow.python.ops import gen_resource_variable_ops\n from tensorflow.python.ops import gen_state_ops\n from tensorflow.python.ops import math_ops\n"
                },
                {
                    "old_start": 156,
                    "old_length": 6,
                    "new_start": 156,
                    "new_length": 12,
                    "hunk_buggy": "['     container = \"\"\\n', '   shape = tensor_shape.as_shape(shape)\\n', '   dtype = dtypes.as_dtype(dtype)\\n', '   handle = gen_resource_variable_ops.var_handle_op(\\n', '       shape=shape,\\n', '       dtype=dtype,\\n']",
                    "hunk_fix": "@@ -156,6 +156,12 @@ def _variable_handle_from_shape_and_dtype(shape,\n     container = \"\"\n   shape = tensor_shape.as_shape(shape)\n   dtype = dtypes.as_dtype(dtype)\n+  if not graph_mode:\n+    if shared_name is not None:\n+      raise errors.InternalError(\n+          \"Using an explicit shared_name is not supported executing eagerly.\")\n+    shared_name = context.shared_name()\n+\n   handle = gen_resource_variable_ops.var_handle_op(\n       shape=shape,\n       dtype=dtype,\n"
                },
                {
                    "old_start": 169,
                    "old_length": 19,
                    "new_start": 175,
                    "new_length": 6,
                    "hunk_buggy": "['     _set_handle_shapes_and_types(handle, full_handle_data, graph_mode)\\n', '     return handle\\n', '   else:\\n', '-    # We do not want two distinct ResourceVariable objects for the same\\n', '-    # underlying resource in the runtime.\\n', \"-    # When in eager mode, explicitly ensure so here. When in graph mode, it's\\n\", '-    # ensured by always generating different variable names.\\n', '-    exists = gen_resource_variable_ops.var_is_initialized_op(handle)\\n', '-\\n', '-    # We create an assert Op instead of checking right away in order to be\\n', '-    # compatible with ASYNC execution mode. Further, since not all devices\\n', '-    # support string tensors, we encode the assertion string in the Op name\\n', '-    gen_logging_ops._assert(  # pylint: disable=protected-access\\n', '-        math_ops.logical_not(exists), [exists],\\n', '-        name=\"EagerVariableNameReuse\")\\n', '-\\n', '     handle_data = cpp_shape_inference_pb2.CppShapeInferenceResult.HandleData()\\n', '     handle_data.is_set = True\\n', '     handle_data.shape_and_type.append(\\n']",
                    "hunk_fix": "@@ -169,19 +175,6 @@ def _variable_handle_from_shape_and_dtype(shape,\n     _set_handle_shapes_and_types(handle, full_handle_data, graph_mode)\n     return handle\n   else:\n-    # We do not want two distinct ResourceVariable objects for the same\n-    # underlying resource in the runtime.\n-    # When in eager mode, explicitly ensure so here. When in graph mode, it's\n-    # ensured by always generating different variable names.\n-    exists = gen_resource_variable_ops.var_is_initialized_op(handle)\n-\n-    # We create an assert Op instead of checking right away in order to be\n-    # compatible with ASYNC execution mode. Further, since not all devices\n-    # support string tensors, we encode the assertion string in the Op name\n-    gen_logging_ops._assert(  # pylint: disable=protected-access\n-        math_ops.logical_not(exists), [exists],\n-        name=\"EagerVariableNameReuse\")\n-\n     handle_data = cpp_shape_inference_pb2.CppShapeInferenceResult.HandleData()\n     handle_data.is_set = True\n     handle_data.shape_and_type.append(\n"
                },
                {
                    "old_start": 1709,
                    "old_length": 7,
                    "new_start": 1702,
                    "new_length": 7,
                    "hunk_buggy": "['           # When in eager mode use a uid for the shared_name, to prevent\\n', '           # accidental sharing.\\n', '           unique_id = \"%s_%d\" % (handle_name, ops.uid())\\n', '-          shared_name = context.shared_name()\\n', '         # Use attr_scope and device(None) to simulate the behavior of\\n', \"         # colocate_with when the variable we want to colocate with doesn't\\n\", '         # yet exist.\\n']",
                    "hunk_fix": "@@ -1709,7 +1702,7 @@ class ResourceVariable(BaseResourceVariable):\n           # When in eager mode use a uid for the shared_name, to prevent\n           # accidental sharing.\n           unique_id = \"%s_%d\" % (handle_name, ops.uid())\n-          shared_name = context.shared_name()\n+          shared_name = None  # Never shared\n         # Use attr_scope and device(None) to simulate the behavior of\n         # colocate_with when the variable we want to colocate with doesn't\n         # yet exist.\n"
                },
                {
                    "old_start": 1960,
                    "old_length": 7,
                    "new_start": 1953,
                    "new_length": 7,
                    "hunk_buggy": "['           unique_id = shared_name\\n', '         else:\\n', '           unique_id = \"%s_%d\" % (handle_name, ops.uid())\\n', '-          shared_name = context.shared_name()\\n', '         handle = _variable_handle_from_shape_and_dtype(\\n', '             shape=shape,\\n', '             dtype=dtype,']",
                    "hunk_fix": "@@ -1960,7 +1953,7 @@ class UninitializedVariable(BaseResourceVariable):\n           unique_id = shared_name\n         else:\n           unique_id = \"%s_%d\" % (handle_name, ops.uid())\n-          shared_name = context.shared_name()\n+          shared_name = None  # Never shared\n         handle = _variable_handle_from_shape_and_dtype(\n             shape=shape,\n             dtype=dtype,"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 16,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/b4aadb17b7aa5ea926b5220008e41f33e582baed",
    "date": "2023-08-07T11:49:18-07:00",
    "message": "Return error on invalid input in `tfl.where`\n\nPiperOrigin-RevId: 554544503",
    "changes": [
        {
            "name": "where.cc",
            "path": "tensorflow/lite/kernels/where.cc",
            "patches": [
                {
                    "old_start": 101,
                    "old_length": 6,
                    "new_start": 101,
                    "new_length": 7,
                    "hunk_buggy": "['       TF_LITE_KERNEL_LOG(context,\\n', '                          \"Condition tensor has unsupported type: \\'%s\\'.\",\\n', '                          TfLiteTypeGetName(cond_tensor->type));\\n', '   }\\n', '   return kTfLiteOk;\\n', ' }\\n']",
                    "hunk_fix": "@@ -101,6 +101,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n       TF_LITE_KERNEL_LOG(context,\n                          \"Condition tensor has unsupported type: '%s'.\",\n                          TfLiteTypeGetName(cond_tensor->type));\n+      return kTfLiteError;\n   }\n   return kTfLiteOk;\n }\n"
                },
                {
                    "old_start": 147,
                    "old_length": 6,
                    "new_start": 148,
                    "new_length": 7,
                    "hunk_buggy": "['         TF_LITE_KERNEL_LOG(context,\\n', '                            \"Condition tensor has unsupported type: \\'%s\\'.\",\\n', '                            TfLiteTypeGetName(cond_tensor->type));\\n', '     }\\n', '   }\\n', ' \\n']",
                    "hunk_fix": "@@ -147,6 +148,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n         TF_LITE_KERNEL_LOG(context,\n                            \"Condition tensor has unsupported type: '%s'.\",\n                            TfLiteTypeGetName(cond_tensor->type));\n+        return kTfLiteError;\n     }\n   }\n \n"
                },
                {
                    "old_start": 197,
                    "old_length": 6,
                    "new_start": 199,
                    "new_length": 7,
                    "hunk_buggy": "['       TF_LITE_KERNEL_LOG(context,\\n', '                          \"Condition tensor has unsupported type: \\'%s\\'.\",\\n', '                          TfLiteTypeGetName(cond_tensor->type));\\n', '   }\\n', '   return kTfLiteOk;\\n', ' }']",
                    "hunk_fix": "@@ -197,6 +199,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n       TF_LITE_KERNEL_LOG(context,\n                          \"Condition tensor has unsupported type: '%s'.\",\n                          TfLiteTypeGetName(cond_tensor->type));\n+      return kTfLiteError;\n   }\n   return kTfLiteOk;\n }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 44,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/c510c1b8b1ef5be1d65971f5b9e21e61becd0bb3",
    "date": "2024-02-08T04:11:31-08:00",
    "message": "[XLA] Remove IsCalledComputation from HloComputation.\n\nThe function doesn't do what it seems. There are more types of called instruction that are not accounted in this check.\n\nPiperOrigin-RevId: 605270151",
    "changes": [
        {
            "name": "hlo_computation.h",
            "path": "third_party/xla/xla/hlo/ir/hlo_computation.h",
            "patches": [
                {
                    "old_start": 807,
                    "old_length": 18,
                    "new_start": 807,
                    "new_length": 14,
                    "hunk_buggy": "['   HloInstruction* AsyncStart() const { return async_start_; }\\n', ' \\n', '   void AddAsyncStart(HloInstruction* async_instruction) {\\n', '-    CHECK(!IsCalledComputation());\\n', '     CHECK(async_instruction->opcode() == HloOpcode::kAsyncStart);\\n', '     async_start_ = async_instruction;\\n', '   }\\n', ' \\n', '   void RemoveAsyncStart() { async_start_ = nullptr; }\\n', ' \\n', '-  // Returns if this computation is invoked by an Hlo instruction.\\n', '-  bool IsCalledComputation() const {\\n', '-    return IsFusionComputation() || IsCustomCallComputation();\\n', '-  }\\n', '-\\n', '   // Clear the unique ID of the computation so that it can be re-assigned, such\\n', '   // as for the purpose of compacting the unique IDs.\\n', '   void ClearUniqueIdInternal() { unique_id_ = -1; }']",
                    "hunk_fix": "@@ -807,18 +807,14 @@ class HloComputation {\n   HloInstruction* AsyncStart() const { return async_start_; }\n \n   void AddAsyncStart(HloInstruction* async_instruction) {\n-    CHECK(!IsCalledComputation());\n+    // TODO: Add instruction type for async instructions.\n+    CHECK(instruction_type() == InstructionType::kUnset);\n     CHECK(async_instruction->opcode() == HloOpcode::kAsyncStart);\n     async_start_ = async_instruction;\n   }\n \n   void RemoveAsyncStart() { async_start_ = nullptr; }\n \n-  // Returns if this computation is invoked by an Hlo instruction.\n-  bool IsCalledComputation() const {\n-    return IsFusionComputation() || IsCustomCallComputation();\n-  }\n-\n   // Clear the unique ID of the computation so that it can be re-assigned, such\n   // as for the purpose of compacting the unique IDs.\n   void ClearUniqueIdInternal() { unique_id_ = -1; }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 45,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/06b89ed1bdf606adb21d66664ca7ab5eaffdd58f",
    "date": "2024-02-05T06:44:09-08:00",
    "message": "BundleReader was not waiting for concurrents reads to complete before\nchecking their result value.\n\nAlso changed the large value reading test to actually exercise the\nmulti-threaded reading path. Previously, the whole multi-threaded path\nwas being skipped because the reads were smaller than kBufferSize.\n\nPiperOrigin-RevId: 604300943",
    "changes": [
        {
            "name": "tensor_bundle.cc",
            "path": "tensorflow/core/util/tensor_bundle/tensor_bundle.cc",
            "patches": [
                {
                    "old_start": 916,
                    "old_length": 7,
                    "new_start": 916,
                    "new_length": 7,
                    "hunk_buggy": "['   if (DataTypeCanUseMemcpy(entry.dtype())) {\\n', '     char* backing_buffer = const_cast<char*>((ret->tensor_data().data()));\\n', '     size_t unused_bytes_read;\\n', '-    if (entry.size() > kBufferSize) {\\n', '       StringPiece sp;\\n', '       if (!enable_multi_threading_for_testing_ &&\\n', '           entry.size() < kLargeTensorThreshold) {\\n']",
                    "hunk_fix": "@@ -916,7 +916,7 @@ Status BundleReader::GetValue(const BundleEntryProto& entry, Tensor* val) {\n   if (DataTypeCanUseMemcpy(entry.dtype())) {\n     char* backing_buffer = const_cast<char*>((ret->tensor_data().data()));\n     size_t unused_bytes_read;\n-    if (entry.size() > kBufferSize) {\n+    if (entry.size() > kBufferSize || enable_multi_threading_for_testing_) {\n       StringPiece sp;\n       if (!enable_multi_threading_for_testing_ &&\n           entry.size() < kLargeTensorThreshold) {\n"
                },
                {
                    "old_start": 964,
                    "old_length": 6,
                    "new_start": 964,
                    "new_length": 8,
                    "hunk_buggy": "['             statuses[i] = std::move(status);\\n', '           });\\n', '         }\\n', '         for (const auto& status : statuses) {\\n', '           TF_RETURN_IF_ERROR(status);\\n', '         }']",
                    "hunk_fix": "@@ -964,6 +964,8 @@ Status BundleReader::GetValue(const BundleEntryProto& entry, Tensor* val) {\n             statuses[i] = std::move(status);\n           });\n         }\n+        reader_pool = nullptr;  // Wait for reads to finish\n+\n         for (const auto& status : statuses) {\n           TF_RETURN_IF_ERROR(status);\n         }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 46,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/d5862d423742ec26c46737d4526eca3b8b8a0d9b",
    "date": "2024-01-26T14:54:39+00:00",
    "message": "[TFLite] Add check in Softmax reference function to ensure exponent is within valid range\n\n* Add check to ensure the exponent does not cause an overflow in gemmlowp::RoundingDivideByPOT",
    "changes": [
        {
            "name": "softmax.h",
            "path": "tensorflow/lite/kernels/internal/reference/softmax.h",
            "patches": [
                {
                    "old_start": 115,
                    "old_length": 6,
                    "new_start": 115,
                    "new_length": 9,
                    "hunk_buggy": "['     FixedPoint0 shifted_scale = FixedPoint0::FromRaw(GetReciprocal(\\n', '         sum_of_exps.raw(), kAccumulationIntegerBits, &num_bits_over_unit));\\n', ' \\n', '     for (int c = 0; c < depth; ++c) {\\n', '       int32_t input_diff =\\n', '           static_cast<int32_t>(input_data[i * depth + c]) - max_in_row;\\n']",
                    "hunk_fix": "@@ -115,6 +115,9 @@ inline void Softmax(const SoftmaxParams& params,\n     FixedPoint0 shifted_scale = FixedPoint0::FromRaw(GetReciprocal(\n         sum_of_exps.raw(), kAccumulationIntegerBits, &num_bits_over_unit));\n \n+    const int exponent = num_bits_over_unit + 31 - (sizeof(OutputT) * 8);\n+    TFLITE_CHECK(0 <= exponent && exponent <= 31);\n+\n     for (int c = 0; c < depth; ++c) {\n       int32_t input_diff =\n           static_cast<int32_t>(input_data[i * depth + c]) - max_in_row;\n"
                },
                {
                    "old_start": 127,
                    "old_length": 8,
                    "new_start": 130,
                    "new_length": 7,
                    "hunk_buggy": "[' \\n', '         FixedPoint0 exp_in_0 = exp_on_negative_values(scaled_diff_f8);\\n', '         int32_t unsat_output = gemmlowp::RoundingDivideByPOT(\\n', '-            (shifted_scale * exp_in_0).raw(),\\n', '-            num_bits_over_unit + 31 - (sizeof(OutputT) * 8));\\n', ' \\n', '         const int32_t shifted_output =\\n', '             unsat_output +']",
                    "hunk_fix": "@@ -127,8 +130,7 @@ inline void Softmax(const SoftmaxParams& params,\n \n         FixedPoint0 exp_in_0 = exp_on_negative_values(scaled_diff_f8);\n         int32_t unsat_output = gemmlowp::RoundingDivideByPOT(\n-            (shifted_scale * exp_in_0).raw(),\n-            num_bits_over_unit + 31 - (sizeof(OutputT) * 8));\n+            (shifted_scale * exp_in_0).raw(), exponent);\n \n         const int32_t shifted_output =\n             unsat_output +"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 47,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/1a1a381b5be7701843c3f1e34aa1846ae2a1d0ce",
    "date": "2024-01-25T10:27:28-08:00",
    "message": "Fix a SIGSEGV bug in `InferShapeForXlaGatherOp`\n\nSince `ComputeOutputComponent` may return nullptr, we need to check for null attributes explicitly to be safe.\n\nPiperOrigin-RevId: 601487562",
    "changes": [
        {
            "name": "shape_inference.cc",
            "path": "tensorflow/compiler/mlir/tensorflow/transforms/shape_inference.cc",
            "patches": [
                {
                    "old_start": 1977,
                    "old_length": 7,
                    "new_start": 1977,
                    "new_length": 7,
                    "hunk_buggy": "['     slice_sizes_attr = attr;\\n', '   } else if (const auto it = results_.find(ValuePort(op.getSliceSizes()));\\n', '              it != results_.end() &&\\n', '-             llvm::isa<DenseIntElementsAttr>(it->second)) {\\n', '     slice_sizes_attr = llvm::cast<DenseIntElementsAttr>(it->second);\\n', '   } else {\\n', '     return false;']",
                    "hunk_fix": "@@ -1977,7 +1977,7 @@ bool ShapeInference::InferShapeForXlaGatherOp(XlaGatherOp op) {\n     slice_sizes_attr = attr;\n   } else if (const auto it = results_.find(ValuePort(op.getSliceSizes()));\n              it != results_.end() &&\n-             llvm::isa<DenseIntElementsAttr>(it->second)) {\n+             llvm::isa_and_nonnull<DenseIntElementsAttr>(it->second)) {\n     slice_sizes_attr = llvm::cast<DenseIntElementsAttr>(it->second);\n   } else {\n     return false;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 48,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/9718fed7b9aba244359b3d38c2a1dc20e50428bd",
    "date": "2024-01-24T17:18:37-08:00",
    "message": "Added size check to avoid memory corruption in GraphDefImporter::ConvertNodeDef.\n\nPiperOrigin-RevId: 601277773",
    "changes": [
        {
            "name": "graphdef_import.cc",
            "path": "tensorflow/core/ir/importexport/graphdef_import.cc",
            "patches": [
                {
                    "old_start": 823,
                    "old_length": 6,
                    "new_start": 823,
                    "new_length": 9,
                    "hunk_buggy": "['   // Get the result types. Ops can have multiple named results. Track the\\n', '   // segment sizes.\\n', '   SmallVector<std::pair<unsigned, unsigned>> result_segments;\\n', '   result_segments.reserve(op_def->output_arg_size());\\n', '   state.types.reserve(op_def->output_arg_size() + 1);\\n', '   for (const OpDef::ArgDef &def : op_def->output_arg()) {']",
                    "hunk_fix": "@@ -823,6 +823,9 @@ Status GraphDefImporter::ConvertNodeDef(OpBuilder &builder, ConversionState &s,\n   // Get the result types. Ops can have multiple named results. Track the\n   // segment sizes.\n   SmallVector<std::pair<unsigned, unsigned>> result_segments;\n+\n+  if (op_def->output_arg_size() < 0)\n+    return InvalidArgument(\"Node \", node.name(), \" output arg size < 0\");\n   result_segments.reserve(op_def->output_arg_size());\n   state.types.reserve(op_def->output_arg_size() + 1);\n   for (const OpDef::ArgDef &def : op_def->output_arg()) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 49,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/798b2ebda0cc6f12f1ca6460611f760149771a11",
    "date": "2024-01-12T16:40:23-08:00",
    "message": "Ensure the allocation type is kTfLiteCustom when doing shallow copies in DeepOrShallowCopyTensorsShapeTypeData.\n\nThis code is correct only under the assumption that the caller has correctly prepared\nthe tensors that get passed in for shallow copying, by setting their allocation\ntypes to kTfLiteCustom. This ensures that those tensors won't be double `free`'d\nlater on. This check simply ensures that that assumption always holds, to ensure\nwe fail early if ever a bug is introduced that breaks that assumption.\n\nPiperOrigin-RevId: 597990125",
    "changes": [
        {
            "name": "control_flow_common.h",
            "path": "tensorflow/lite/kernels/control_flow_common.h",
            "patches": [
                {
                    "old_start": 150,
                    "old_length": 6,
                    "new_start": 150,
                    "new_length": 10,
                    "hunk_buggy": "['       TfLiteTensorRealloc(src_tensor->bytes, dst_tensor);\\n', '       TF_LITE_ENSURE_OK(context, TfLiteTensorCopy(src_tensor, dst_tensor));\\n', '     } else {\\n', '       dst_tensor->bytes = src_tensor->bytes;\\n', '       dst_tensor->data.raw = src_tensor->data.raw;\\n', '     }']",
                    "hunk_fix": "@@ -150,6 +150,10 @@ TfLiteStatus DeepOrShallowCopyTensorsShapeTypeData(\n       TfLiteTensorRealloc(src_tensor->bytes, dst_tensor);\n       TF_LITE_ENSURE_OK(context, TfLiteTensorCopy(src_tensor, dst_tensor));\n     } else {\n+      // Make a shallow copy of the data. This is only safe because the caller\n+      // is expected to have previously set dst_tensor->allocation_type to\n+      // kTfLiteCustom, to ensure the buffer is never double-freed later on.\n+      TF_LITE_ENSURE_EQ(context, dst_tensor->allocation_type, kTfLiteCustom);\n       dst_tensor->bytes = src_tensor->bytes;\n       dst_tensor->data.raw = src_tensor->data.raw;\n     }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 50,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/89334fb15c688e7dbd81878745755db01579ea70",
    "date": "2024-01-05T02:07:14-08:00",
    "message": "PR #7789: [NVIDIA] Update VersionCheck APIs for CuDNN\n\nImported from GitHub PR https://github.com/openxla/xla/pull/7789\n\nThis PR updates the cudnnXXXVersionCheck to the latest for the next CUDNN release.\n\ncc. @reedwm @nluehr\nCopybara import of the project:\n\n--\n4c564d9bf8fd4c033af942da6689c219c49f4052 by Kaixi Hou <kaixih@nvidia.com>:\n\nUse new version check APIs\n\nMerging this change closes #7789\n\nPiperOrigin-RevId: 595932222",
    "changes": [
        {
            "name": "cuda_dnn.cc",
            "path": "third_party/xla/xla/stream_executor/cuda/cuda_dnn.cc",
            "patches": [
                {
                    "old_start": 363,
                    "old_length": 28,
                    "new_start": 363,
                    "new_length": 41,
                    "hunk_buggy": "[\" // Preload sub libs for cudnn 8.0.4+ to make sure that the loading time isn't\\n\", ' // measured in the autotuning.\\n', ' void PreloadCudnnSubLibs(PreloadCudnnType type) {\\n', '-#if CUDNN_VERSION >= 8004\\n', '   switch (type) {\\n', '     case PreloadCudnnType::ConvBwdFilter:\\n', '     case PreloadCudnnType::ConvBwdData: {\\n', '       cudnnOpsTrainVersionCheck();\\n', '       cudnnCnnTrainVersionCheck();\\n', '       [[clang::fallthrough]];\\n', '     }\\n', '     case PreloadCudnnType::ConvFwd: {\\n', '       cudnnOpsInferVersionCheck();\\n', '       cudnnCnnInferVersionCheck();\\n', '       break;\\n', '     }\\n', '     case PreloadCudnnType::Rnn: {\\n', '       cudnnOpsInferVersionCheck();\\n', '       cudnnAdvInferVersionCheck();\\n', '       cudnnOpsTrainVersionCheck();\\n', '       cudnnAdvTrainVersionCheck();\\n', '       break;\\n', '     }\\n', '   }\\n', '-#endif  // CUDNN_VERSION >= 8004\\n', ' }\\n', ' \\n', ' void PreloadCudnnSubLibsHelper(dnn::ConvolutionKind kind) {']",
                    "hunk_fix": "@@ -363,28 +363,41 @@ enum class PreloadCudnnType { ConvFwd, ConvBwdFilter, ConvBwdData, Rnn };\n // Preload sub libs for cudnn 8.0.4+ to make sure that the loading time isn't\n // measured in the autotuning.\n void PreloadCudnnSubLibs(PreloadCudnnType type) {\n-#if CUDNN_VERSION >= 8004\n   switch (type) {\n     case PreloadCudnnType::ConvBwdFilter:\n     case PreloadCudnnType::ConvBwdData: {\n+#if CUDNN_VERSION >= 8004 && CUDNN_VERSION < 9000\n       cudnnOpsTrainVersionCheck();\n       cudnnCnnTrainVersionCheck();\n+#endif  // CUDNN_VERSION >= 8004 && CUDNN_VERSION < 9000\n       [[clang::fallthrough]];\n     }\n     case PreloadCudnnType::ConvFwd: {\n+#if CUDNN >= 9000 && TF_ENABLE_CUDNN_FRONTEND\n+      cudnnGraphVersionCheck();\n+      cudnnOpsVersionCheck();\n+#elif CUDNN_VERSION >= 9000\n+      cudnnCnnVersionCheck();\n+      cudnnOpsVersionCheck();\n+#elif CUDNN_VERSION >= 8004\n       cudnnOpsInferVersionCheck();\n       cudnnCnnInferVersionCheck();\n+#endif  // CUDNN >= 9000 && TF_ENABLE_CUDNN_FRONTEND\n       break;\n     }\n     case PreloadCudnnType::Rnn: {\n+#if CUDNN_VERSION >= 9000\n+      cudnnOpsVersionCheck();\n+      cudnnAdvVersionCheck();\n+#elif CUDNN_VERSION >= 8004\n       cudnnOpsInferVersionCheck();\n       cudnnAdvInferVersionCheck();\n       cudnnOpsTrainVersionCheck();\n       cudnnAdvTrainVersionCheck();\n+#endif  // CUDNN_VERSION >= 9000\n       break;\n     }\n   }\n-#endif  // CUDNN_VERSION >= 8004\n }\n \n void PreloadCudnnSubLibsHelper(dnn::ConvolutionKind kind) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 51,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/6c472f6632c4864da749e7a4aee8c001a905287f",
    "date": "2023-12-08T13:36:13-08:00",
    "message": "only found `CU_MEM_LOCATION_TYPE_HOST`, `CU_MEM_LOCATION_TYPE_HOST_NUMA` and `CU_MEM_LOCATION_TYPE_HOST_NUMA_CURRENT` in [CUDA version 12.3.1 doc](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TYPES.html), and didn't find the evidence of their existence in formal versions' doc, so suggest to use check `CUDA_VERSION` at `12030` here, for `maxSize`, resolved directly in the same way\n\nPiperOrigin-RevId: 589226039",
    "changes": [
        {
            "name": "cuda_driver.cc",
            "path": "third_party/xla/xla/stream_executor/cuda/cuda_driver.cc",
            "patches": [
                {
                    "old_start": 908,
                    "old_length": 7,
                    "new_start": 908,
                    "new_length": 7,
                    "hunk_buggy": "['       return CU_MEM_LOCATION_TYPE_INVALID;\\n', '     case GpuDriver::MemLocationType::kDevice:\\n', '       return CU_MEM_LOCATION_TYPE_DEVICE;\\n', '-#if CUDA_VERSION >= 12000\\n', '     case GpuDriver::MemLocationType::kHost:\\n', '       return CU_MEM_LOCATION_TYPE_HOST;\\n', '     case GpuDriver::MemLocationType::kHostNuma:\\n']",
                    "hunk_fix": "@@ -908,7 +908,7 @@ static CUmemLocationType ToCudaLocationType(\n       return CU_MEM_LOCATION_TYPE_INVALID;\n     case GpuDriver::MemLocationType::kDevice:\n       return CU_MEM_LOCATION_TYPE_DEVICE;\n-#if CUDA_VERSION >= 12000\n+#if CUDA_VERSION >= 12030\n     case GpuDriver::MemLocationType::kHost:\n       return CU_MEM_LOCATION_TYPE_HOST;\n     case GpuDriver::MemLocationType::kHostNuma:\n"
                },
                {
                    "old_start": 920,
                    "old_length": 7,
                    "new_start": 920,
                    "new_length": 7,
                    "hunk_buggy": "['     case GpuDriver::MemLocationType::kHostNuma:\\n', '     case GpuDriver::MemLocationType::kHostNumaCurrent:\\n', '       return CU_MEM_LOCATION_TYPE_INVALID;\\n', '-#endif  // CUDA_VERSION >= 12000\\n', '   }\\n', ' }\\n', ' \\n']",
                    "hunk_fix": "@@ -920,7 +920,7 @@ static CUmemLocationType ToCudaLocationType(\n     case GpuDriver::MemLocationType::kHostNuma:\n     case GpuDriver::MemLocationType::kHostNumaCurrent:\n       return CU_MEM_LOCATION_TYPE_INVALID;\n-#endif  // CUDA_VERSION >= 12000\n+#endif  // CUDA_VERSION >= 12030\n   }\n }\n \n"
                },
                {
                    "old_start": 955,
                    "old_length": 9,
                    "new_start": 955,
                    "new_length": 9,
                    "hunk_buggy": "['   mem_pool_props.allocType = ToCudaAllocationType(allocation_type);\\n', '   mem_pool_props.handleTypes = CU_MEM_HANDLE_TYPE_NONE;\\n', '   mem_pool_props.location = mem_location;\\n', '-#if CUDA_VERSION >= 12000\\n', '   mem_pool_props.maxSize = max_pool_size;\\n', '-#endif  // CUDA_VERSION >= 12000\\n', '   // cuda graph requires reserved space initialized to 0\\n', '   memset(mem_pool_props.reserved, 0, sizeof(mem_pool_props.reserved));\\n', ' ']",
                    "hunk_fix": "@@ -955,9 +955,9 @@ static CUmemAllocationType ToCudaAllocationType(\n   mem_pool_props.allocType = ToCudaAllocationType(allocation_type);\n   mem_pool_props.handleTypes = CU_MEM_HANDLE_TYPE_NONE;\n   mem_pool_props.location = mem_location;\n-#if CUDA_VERSION >= 12000\n+#if CUDA_VERSION >= 12030\n   mem_pool_props.maxSize = max_pool_size;\n-#endif  // CUDA_VERSION >= 12000\n+#endif  // CUDA_VERSION >= 12030\n   // cuda graph requires reserved space initialized to 0\n   memset(mem_pool_props.reserved, 0, sizeof(mem_pool_props.reserved));\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 52,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/e1dbfeba8acb1df8f42dfa6f76262f5cb23e1fa1",
    "date": "2023-11-28T20:24:49-08:00",
    "message": "[stream_executor] NFC: Guard new features with CUDA_VERSION check\n\nPiperOrigin-RevId: 586180704",
    "changes": [
        {
            "name": "cuda_driver.cc",
            "path": "third_party/xla/xla/stream_executor/cuda/cuda_driver.cc",
            "patches": [
                {
                    "old_start": 902,
                    "old_length": 12,
                    "new_start": 902,
                    "new_length": 19,
                    "hunk_buggy": "['       return CU_MEM_LOCATION_TYPE_INVALID;\\n', '     case GpuDriver::MemLocationType::kDevice:\\n', '       return CU_MEM_LOCATION_TYPE_DEVICE;\\n', '     case GpuDriver::MemLocationType::kHost:\\n', '       return CU_MEM_LOCATION_TYPE_HOST;\\n', '     case GpuDriver::MemLocationType::kHostNuma:\\n', '       return CU_MEM_LOCATION_TYPE_HOST_NUMA;\\n', '     case GpuDriver::MemLocationType::kHostNumaCurrent:\\n', '       return CU_MEM_LOCATION_TYPE_HOST_NUMA_CURRENT;\\n', '   }\\n', ' }\\n', ' \\n']",
                    "hunk_fix": "@@ -902,12 +902,19 @@ static CUmemLocationType ToCudaLocationType(\n       return CU_MEM_LOCATION_TYPE_INVALID;\n     case GpuDriver::MemLocationType::kDevice:\n       return CU_MEM_LOCATION_TYPE_DEVICE;\n+#if CUDA_VERSION >= 12000\n     case GpuDriver::MemLocationType::kHost:\n       return CU_MEM_LOCATION_TYPE_HOST;\n     case GpuDriver::MemLocationType::kHostNuma:\n       return CU_MEM_LOCATION_TYPE_HOST_NUMA;\n     case GpuDriver::MemLocationType::kHostNumaCurrent:\n       return CU_MEM_LOCATION_TYPE_HOST_NUMA_CURRENT;\n+#else\n+    case GpuDriver::MemLocationType::kHost:\n+    case GpuDriver::MemLocationType::kHostNuma:\n+    case GpuDriver::MemLocationType::kHostNumaCurrent:\n+      return CU_MEM_LOCATION_TYPE_INVALID;\n+#endif  // CUDA_VERSION >= 12000\n   }\n }\n \n"
                },
                {
                    "old_start": 942,
                    "old_length": 7,
                    "new_start": 949,
                    "new_length": 9,
                    "hunk_buggy": "['   mem_pool_props.allocType = ToCudaAllocationType(allocation_type);\\n', '   mem_pool_props.handleTypes = CU_MEM_HANDLE_TYPE_NONE;\\n', '   mem_pool_props.location = mem_location;\\n', '   mem_pool_props.maxSize = max_pool_size;\\n', ' \\n', '   params.accessDescCount = 1;\\n', '   params.bytesize = size;']",
                    "hunk_fix": "@@ -942,7 +949,9 @@ static CUmemAllocationType ToCudaAllocationType(\n   mem_pool_props.allocType = ToCudaAllocationType(allocation_type);\n   mem_pool_props.handleTypes = CU_MEM_HANDLE_TYPE_NONE;\n   mem_pool_props.location = mem_location;\n+#if CUDA_VERSION >= 12000\n   mem_pool_props.maxSize = max_pool_size;\n+#endif  // CUDA_VERSION >= 12000\n \n   params.accessDescCount = 1;\n   params.bytesize = size;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 53,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/ac012e26d4331919335d4bceb8abe22b68ed5434",
    "date": "2023-11-17T02:07:13-08:00",
    "message": "[xla:python] Don't assume GPU clients are PJRT-compatible.\n\nInstead explicitly check for compatibility.\n\nPiperOrigin-RevId: 583311176",
    "changes": [
        {
            "name": "py_client.cc",
            "path": "third_party/xla/xla/python/py_client.cc",
            "patches": [
                {
                    "old_start": 372,
                    "old_length": 15,
                    "new_start": 372,
                    "new_length": 19,
                    "hunk_buggy": "['     std::vector<pybind11::capsule> host_callbacks) {\\n', '   // Pass allocated device memory size to compile options for pjrt compatible\\n', '   // backends.\\n', '-  if ((ifrt_client_->platform_id() == xla::CudaId() ||\\n', '-       ifrt_client_->platform_id() == xla::RocmId()) &&\\n', '-      !pjrt_client()->devices().empty()) {\\n', '-    auto maybe_stats = pjrt_client()->devices()[0]->GetAllocatorStats();\\n', '-    if (maybe_stats.ok() && maybe_stats->bytes_limit) {\\n', '-      options.executable_build_options.set_device_memory_size(\\n', '-          *maybe_stats->bytes_limit);\\n', '     }\\n', '   }\\n', '   std::unique_ptr<ifrt::LoadedExecutable> ifrt_loaded_executable;\\n', '   std::optional<std::string> fingerprint;\\n', '   auto ifrt_compile_options =']",
                    "hunk_fix": "@@ -372,15 +372,19 @@ StatusOr<std::shared_ptr<PyLoadedExecutable>> PyClient::Compile(\n     std::vector<pybind11::capsule> host_callbacks) {\n   // Pass allocated device memory size to compile options for pjrt compatible\n   // backends.\n-  if ((ifrt_client_->platform_id() == xla::CudaId() ||\n-       ifrt_client_->platform_id() == xla::RocmId()) &&\n-      !pjrt_client()->devices().empty()) {\n-    auto maybe_stats = pjrt_client()->devices()[0]->GetAllocatorStats();\n-    if (maybe_stats.ok() && maybe_stats->bytes_limit) {\n-      options.executable_build_options.set_device_memory_size(\n-          *maybe_stats->bytes_limit);\n+  auto* pjrt_compatible_client =\n+      llvm::dyn_cast_or_null<ifrt::PjRtCompatibleClient>(ifrt_client_.get());\n+  if (pjrt_compatible_client != nullptr) {\n+    auto devices = pjrt_compatible_client->pjrt_client()->devices();\n+    if (!devices.empty()) {\n+      auto stats = devices[0]->GetAllocatorStats();\n+      if (stats.ok() && stats->bytes_limit) {\n+        options.executable_build_options.set_device_memory_size(\n+            *stats->bytes_limit);\n+      }\n     }\n   }\n+\n   std::unique_ptr<ifrt::LoadedExecutable> ifrt_loaded_executable;\n   std::optional<std::string> fingerprint;\n   auto ifrt_compile_options ="
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 54,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/20d54796563631c23c27548b321487e8b0c982a9",
    "date": "2023-11-03T10:34:02-07:00",
    "message": "Add a nil check before init the device_name string, and also assign an empty string as a placeholder.\n\nPiperOrigin-RevId: 579225867",
    "changes": [
        {
            "name": "metal_delegate.mm",
            "path": "tensorflow/lite/delegates/gpu/metal_delegate.mm",
            "patches": [
                {
                    "old_start": 337,
                    "old_length": 7,
                    "new_start": 337,
                    "new_length": 8,
                    "hunk_buggy": "['       tensor->delegate = &delegate_;\\n', '     }\\n', ' \\n', '-    std::string device_name = std::string([[metal_device_ name] UTF8String]);\\n', '     GpuInfo gpu_info;\\n', '     GetGpuInfoFromDeviceDescription(device_name, GpuApi::kMetal, &gpu_info);\\n', '     size_t storage_type_size;']",
                    "hunk_fix": "@@ -337,7 +337,8 @@ class Delegate {\n       tensor->delegate = &delegate_;\n     }\n \n-    std::string device_name = std::string([[metal_device_ name] UTF8String]);\n+    auto utf8_name = [[metal_device_ name] UTF8String];\n+    const std::string device_name = utf8_name != nil ? utf8_name : \"\";\n     GpuInfo gpu_info;\n     GetGpuInfoFromDeviceDescription(device_name, GpuApi::kMetal, &gpu_info);\n     size_t storage_type_size;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 55,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/81ff894c113a5912ba52078ac27e36d06831112e",
    "date": "2023-11-01T04:43:01-07:00",
    "message": "[XLA] Add bounds checks to xla::Array::Slice\n\nTo guard against specifying limits that are out of bounds, which ends up\ntouching OOB data.\n\nPiperOrigin-RevId: 578475805",
    "changes": [
        {
            "name": "array.h",
            "path": "third_party/xla/xla/array.h",
            "patches": [
                {
                    "old_start": 438,
                    "old_length": 6,
                    "new_start": 438,
                    "new_length": 8,
                    "hunk_buggy": "[' \\n', '     OwnedBuffer<int64_t> sizes(starts.size());\\n', '     for (int64_t i = 0; i < starts.size(); ++i) {\\n', '       sizes[i] = limits[i] - starts[i];\\n', '     }\\n', '     Array<T> result(sizes.span());']",
                    "hunk_fix": "@@ -438,6 +438,8 @@ class Array {\n \n     OwnedBuffer<int64_t> sizes(starts.size());\n     for (int64_t i = 0; i < starts.size(); ++i) {\n+      CHECK_GE(starts[i], 0);\n+      CHECK_LE(limits[i], dim(i));\n       sizes[i] = limits[i] - starts[i];\n     }\n     Array<T> result(sizes.span());"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 56,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/e44f8a08051baa58bde9130a844a1b82a8179526",
    "date": "2023-10-31T12:18:59-07:00",
    "message": "check hasattr on the type, not the instance.\n\nhasattr on the instance triggers __getattr__ which carries very undesirable\neffects, such as running Ops on a donated buffer.\n\nLong term, we may want to audit all uses of hasattr on TensorFlow instances\nthat overrides __getattr__ in nontrival (e.g. running tf Ops) ways. They will\nalmost always cause trouble here and there because TensorFlow is quite far\nfrom being able guarantee if an Op returns or consumes is actually valid in all cases. Things will improve give it time, but if we can avoid such strong assumptions the system tend to get more robust.\n\nPiperOrigin-RevId: 578261984",
    "changes": [
        {
            "name": "async_checkpoint_helper.py",
            "path": "tensorflow/python/checkpoint/async_checkpoint_helper.py",
            "patches": [
                {
                    "old_start": 263,
                    "old_length": 10,
                    "new_start": 263,
                    "new_length": 10,
                    "hunk_buggy": "['     # custom __getattr__ code, see b/152031870 for context.\\n', '     for t in all_trackables:\\n', '       # Special case 1: TPU Embedding, populate object_map here\\n', '-    # Special case 1: Handle TPU Embedding by addnig a dummy instance to the\\n', '-    # object map. Also add TPUEmbedding to separate list for special handling\\n', '-    # with values copy.\\n', '-      if hasattr(t, _TPU_EMBEDDING_ATTR):\\n', '         self._handle_tpu_embedding(t)\\n', '       # Special case 2: handle slot variables. The object_map is populated later\\n', '       # when the variable values are being copied to host CPU for the first\\n']",
                    "hunk_fix": "@@ -263,10 +263,10 @@ class AsyncCheckpointHelper:\n     # custom __getattr__ code, see b/152031870 for context.\n     for t in all_trackables:\n       # Special case 1: TPU Embedding, populate object_map here\n-    # Special case 1: Handle TPU Embedding by addnig a dummy instance to the\n-    # object map. Also add TPUEmbedding to separate list for special handling\n-    # with values copy.\n-      if hasattr(t, _TPU_EMBEDDING_ATTR):\n+      # Special case 1: Handle TPU Embedding by addnig a dummy instance to the\n+      # object map. Also add TPUEmbedding to separate list for special handling\n+      # with values copy.\n+      if hasattr(type(t), _TPU_EMBEDDING_ATTR):\n         self._handle_tpu_embedding(t)\n       # Special case 2: handle slot variables. The object_map is populated later\n       # when the variable values are being copied to host CPU for the first\n"
                },
                {
                    "old_start": 414,
                    "old_length": 9,
                    "new_start": 414,
                    "new_length": 9,
                    "hunk_buggy": "['     Raises:\\n', '       AttributeError: if the input trackable is not TPUEmbedding type.\\n', '     \"\"\"\\n', '-    if not hasattr(\\n', '-        tpu_embedding, _TPU_EMBEDDING_ATTR\\n', '-    ) or not callable(tpu_embedding._create_copy_for_async_checkpoint):  # pylint: disable=protected-access\\n', '       raise AttributeError(\\n', '           \"Expecting TPUEmbedding type; got %s\" % type(tpu_embedding)\\n', '       )']",
                    "hunk_fix": "@@ -414,9 +414,9 @@ class AsyncCheckpointHelper:\n     Raises:\n       AttributeError: if the input trackable is not TPUEmbedding type.\n     \"\"\"\n-    if not hasattr(\n-        tpu_embedding, _TPU_EMBEDDING_ATTR\n-    ) or not callable(tpu_embedding._create_copy_for_async_checkpoint):  # pylint: disable=protected-access\n+    if not hasattr(type(tpu_embedding), _TPU_EMBEDDING_ATTR) or not callable(\n+        tpu_embedding._create_copy_for_async_checkpoint  # pylint: disable=protected-access\n+    ):\n       raise AttributeError(\n           \"Expecting TPUEmbedding type; got %s\" % type(tpu_embedding)\n       )"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 57,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/1707ed9b9b0cc5cb02df22a06718c9c738825d39",
    "date": "2023-10-26T19:00:50-07:00",
    "message": "Add a check to make sure that the allocation before an Evict() is not a prefetch.\n\nPiperOrigin-RevId: 577049438",
    "changes": [
        {
            "name": "memory_space_assignment.cc",
            "path": "third_party/xla/xla/service/memory_space_assignment/memory_space_assignment.cc",
            "patches": [
                {
                    "old_start": 6277,
                    "old_length": 9,
                    "new_start": 6277,
                    "new_length": 15,
                    "hunk_buggy": "['   CHECK_GT(request.allocation_value->allocation_sequence()->size(), 0);\\n', '   MemorySpaceAssignment::Allocation* prev_allocation =\\n', '       request.allocation_value->allocation_sequence()->back().get();\\n', '-  // TODO(b/306478911): prev_allocation can never be a prefetch, or we would be\\n', '-  // using an incorrect start time (we would need to wait until the copies\\n', '-  // finish)\\n', ' \\n', \"   // The previous allocation's inclusive start time is the eviction's exclusive\\n\", '   // start time to ensure that the value is created before we start copying']",
                    "hunk_fix": "@@ -6277,9 +6277,15 @@ AlternateMemoryBestFitHeap::Result AlternateMemoryBestFitHeap::Evict(\n   CHECK_GT(request.allocation_value->allocation_sequence()->size(), 0);\n   MemorySpaceAssignment::Allocation* prev_allocation =\n       request.allocation_value->allocation_sequence()->back().get();\n-  // TODO(b/306478911): prev_allocation can never be a prefetch, or we would be\n-  // using an incorrect start time (we would need to wait until the copies\n-  // finish)\n+  // We do not ever expect an Evict() to be immediately proceeded by a prefetch.\n+  // If that case ever occurs, the eviction_exclusive_start_time below will be\n+  // calculated incorrectly, as it will need to come after the prefetch finishes\n+  // coping data.\n+  CHECK(!prev_allocation->is_copy_like_allocation())\n+      << \"Evict has been given copy-like previous allocation.\\nEvict \"\n+         \"candidate:\\n\"\n+      << request.allocation_value->ToString() << \"\\nPrevious allocation:\\n\"\n+      << prev_allocation->ToString();\n \n   // The previous allocation's inclusive start time is the eviction's exclusive\n   // start time to ensure that the value is created before we start copying"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 58,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/893aa7518fe3175739ac1ba70d7355a0b091115c",
    "date": "2023-10-13T13:12:05-07:00",
    "message": "Added a null check in `string_util.cc`\n\nPiperOrigin-RevId: 573300006",
    "changes": [
        {
            "name": "string_util.cc",
            "path": "tensorflow/lite/string_util.cc",
            "patches": [
                {
                    "old_start": 17,
                    "old_length": 9,
                    "new_start": 17,
                    "new_length": 9,
                    "hunk_buggy": "[' \\n', ' #include <stddef.h>\\n', ' \\n', ' #include <cstdlib>\\n', ' #include <cstring>\\n', '-#include <limits>\\n', ' #include <vector>\\n', ' \\n', ' #include \"tensorflow/lite/core/c/c_api_types.h\"\\n']",
                    "hunk_fix": "@@ -17,9 +17,9 @@ limitations under the License.\n \n #include <stddef.h>\n \n+#include <cstddef>\n #include <cstdlib>\n #include <cstring>\n-#include <limits>\n #include <vector>\n \n #include \"tensorflow/lite/core/c/c_api_types.h\"\n"
                },
                {
                    "old_start": 89,
                    "old_length": 6,
                    "new_start": 88,
                    "new_length": 10,
                    "hunk_buggy": "['   // Caller will take ownership of buffer.\\n', '   *buffer = reinterpret_cast<char*>(malloc(bytes));\\n', ' \\n', '   // Set num of string\\n', '   //\\n', \"   // NOTE: The string buffer is accessed here as if it's native endian (instead\"]",
                    "hunk_fix": "@@ -89,6 +88,10 @@ int DynamicBuffer::WriteToBuffer(char** buffer) {\n   // Caller will take ownership of buffer.\n   *buffer = reinterpret_cast<char*>(malloc(bytes));\n \n+  if (*buffer == nullptr) {\n+    return -1;\n+  }\n+\n   // Set num of string\n   //\n   // NOTE: The string buffer is accessed here as if it's native endian (instead"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 59,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/63feaf321165e1e2795f43e3834c007364921df6",
    "date": "2023-09-29T14:53:21+03:00",
    "message": "Add check for raster bits.",
    "changes": [
        {
            "name": "gif_io.cc",
            "path": "tensorflow/core/lib/gif/gif_io.cc",
            "patches": [
                {
                    "old_start": 78,
                    "old_length": 6,
                    "new_start": 78,
                    "new_length": 12,
                    "hunk_buggy": "['   if (DGifSlurp(gif_file) != GIF_OK) {\\n', '     *error_string = absl::StrCat(\"failed to slurp gif file: \",\\n', '                                  GifErrorStringNonNull(gif_file->Error));\\n', '     LOG(ERROR) << *error_string;\\n', '     return nullptr;\\n', '   }']",
                    "hunk_fix": "@@ -78,6 +78,12 @@ uint8* Decode(const void* srcdata, int datasize,\n   if (DGifSlurp(gif_file) != GIF_OK) {\n     *error_string = absl::StrCat(\"failed to slurp gif file: \",\n                                  GifErrorStringNonNull(gif_file->Error));\n+    // Stop load if no images are detected or the allocation of the last image\n+    // buffer was failed.\n+    if (gif_file->ImageCount <= 0 ||\n+        gif_file->SavedImages[gif_file->ImageCount - 1].RasterBits == NULL) {\n+    }\n+\n     LOG(ERROR) << *error_string;\n     return nullptr;\n   }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 60,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/450dec35448a73b3fcb5d4f82108d5fdcb3f59b4",
    "date": "2023-09-25T16:10:37-07:00",
    "message": "Internal change, add some checks on the sparseTensor format checking.\n\nPiperOrigin-RevId: 568349775",
    "changes": [
        {
            "name": "sparse_core_preprocess_ops.cc",
            "path": "tensorflow/core/tpu/kernels/sparse_core_preprocess_ops.cc",
            "patches": [
                {
                    "old_start": 122,
                    "old_length": 8,
                    "new_start": 122,
                    "new_length": 15,
                    "hunk_buggy": "['     // The row ids are just the sample ids which is the first dim of the\\n', '     // indices.\\n', '     auto indices_matrix = indices_or_row_splits.matrix<int32>();\\n', '     for (int32 i = 0; i < total_id_count; ++i) {\\n', '-      *(row_ids_before_padding + i) = indices_matrix(i, 0);\\n', '     }\\n', '   } else if (indices_or_row_splits.dims() == 1 &&\\n', '              indices_or_row_splits.NumElements() > 0) {']",
                    "hunk_fix": "@@ -122,8 +122,15 @@ Status ComputeRowIdsBeforePadding(const Tensor& indices_or_row_splits,\n     // The row ids are just the sample ids which is the first dim of the\n     // indices.\n     auto indices_matrix = indices_or_row_splits.matrix<int32>();\n+    int32 previous_row_id = -1;\n     for (int32 i = 0; i < total_id_count; ++i) {\n-      *(row_ids_before_padding + i) = indices_matrix(i, 0);\n+      int32 current_row_id = indices_matrix(i, 0);\n+      if (current_row_id < previous_row_id) {\n+        return absl::InvalidArgumentError(\n+            \"Invalid indices_or_row_splits input, indices of SparseTensor need \"\n+            \"to be sorted in ascending order.\");\n+      }\n+      *(row_ids_before_padding + i) = current_row_id;\n     }\n   } else if (indices_or_row_splits.dims() == 1 &&\n              indices_or_row_splits.NumElements() > 0) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 61,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/11030308c5d25df5b36f8a583f1b4607e4ea2b7f",
    "date": "2023-08-25T10:18:19-07:00",
    "message": "Add a check to check if all sharding strategies are dropped due to infinity costs\n\nPiperOrigin-RevId: 560125787",
    "changes": [
        {
            "name": "auto_sharding_util.cc",
            "path": "tensorflow/compiler/xla/hlo/experimental/auto_sharding/auto_sharding_util.cc",
            "patches": [
                {
                    "old_start": 941,
                    "old_length": 8,
                    "new_start": 941,
                    "new_length": 10,
                    "hunk_buggy": "['     std::vector<ShardingStrategy> new_vector;\\n', '     std::vector<ShardingStrategy> deduped_replicated_strategies;\\n', '     absl::flat_hash_set<std::string> added;\\n', '     for (size_t i = 0; i < strategies->leaf_vector.size(); ++i) {\\n', '       if (AllInfinityCosts(strategies->leaf_vector[i].resharding_costs)) {\\n', '         continue;\\n', '       }\\n', '       std::string key = strategies->leaf_vector[i].output_sharding.ToString();\\n']",
                    "hunk_fix": "@@ -941,8 +941,10 @@ void RemoveDuplicatedStrategy(std::unique_ptr<StrategyVector>& strategies) {\n     std::vector<ShardingStrategy> new_vector;\n     std::vector<ShardingStrategy> deduped_replicated_strategies;\n     absl::flat_hash_set<std::string> added;\n+    size_t num_skipped_due_to_infinity_costs = 0;\n     for (size_t i = 0; i < strategies->leaf_vector.size(); ++i) {\n       if (AllInfinityCosts(strategies->leaf_vector[i].resharding_costs)) {\n+        num_skipped_due_to_infinity_costs++;\n         continue;\n       }\n       std::string key = strategies->leaf_vector[i].output_sharding.ToString();\n"
                },
                {
                    "old_start": 962,
                    "old_length": 6,
                    "new_start": 964,
                    "new_length": 8,
                    "hunk_buggy": "['         }\\n', '       }\\n', '     }\\n', '     // Keeps replicated strategies as the last ones.\\n', '     if (!deduped_replicated_strategies.empty()) {\\n', '       for (size_t i = 0; i < deduped_replicated_strategies.size(); ++i) {']",
                    "hunk_fix": "@@ -962,6 +964,8 @@ void RemoveDuplicatedStrategy(std::unique_ptr<StrategyVector>& strategies) {\n         }\n       }\n     }\n+    CHECK_LT(num_skipped_due_to_infinity_costs, strategies->leaf_vector.size())\n+        << \"All strategies removed due to infinite resharding costs\";\n     // Keeps replicated strategies as the last ones.\n     if (!deduped_replicated_strategies.empty()) {\n       for (size_t i = 0; i < deduped_replicated_strategies.size(); ++i) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 62,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/5322fd40cd58cfa8c551e602fede7a3be19fff95",
    "date": "2023-08-24T16:12:36-07:00",
    "message": "[PJRT] Fix checking for output sharding\n\nOutput sharding for empty tuple needs to have one \"replicated\" element.\n\nPiperOrigin-RevId: 559899447",
    "changes": [
        {
            "name": "pjrt_executable.cc",
            "path": "tensorflow/compiler/xla/python/pjrt_ifrt/pjrt_executable.cc",
            "patches": [
                {
                    "old_start": 286,
                    "old_length": 6,
                    "new_start": 286,
                    "new_length": 18,
                    "hunk_buggy": "['     output_shapes.push_back(Shape({}));\\n', '     output_shardings.push_back(OpaqueSharding::Create(devices, MemoryKind()));\\n', '   };\\n', ' \\n', '   if (result_shape.IsArray()) {\\n', '     output_dtypes.reserve(1);\\n']",
                    "hunk_fix": "@@ -286,6 +286,18 @@ PjRtLoadedExecutable::CreateInternal(\n     output_shapes.push_back(Shape({}));\n     output_shardings.push_back(OpaqueSharding::Create(devices, MemoryKind()));\n   };\n+  auto check_tuple_output_sharding_condition =\n+      [](const xla::Shape& shape, const xla::HloSharding& sharding) {\n+        // Check that the HLO sharding of the result is a tuple and that it has\n+        // the same number of elements as the output tuple shape. If the output\n+        // is an empty tuple then the output sharding will have a single element\n+        // for the tuple as a special case, so we will have to allow that by\n+        // checking this condition specifically.\n+        return sharding.IsTuple() && (shape.tuple_shapes().size() ==\n+                                          sharding.tuple_elements().size() ||\n+                                      (shape.tuple_shapes().empty() &&\n+                                       sharding.tuple_elements().size() == 1));\n+      };\n \n   if (result_shape.IsArray()) {\n     output_dtypes.reserve(1);\n"
                },
                {
                    "old_start": 313,
                    "old_length": 9,
                    "new_start": 325,
                    "new_length": 8,
                    "hunk_buggy": "['     output_shapes.reserve(result_shape.tuple_shapes().size());\\n', '     output_shardings.reserve(result_shape.tuple_shapes().size());\\n', '     if (result_hlo_sharding.has_value() &&\\n', '-        (!result_hlo_sharding->IsTuple() ||\\n', '-         result_hlo_sharding->tuple_elements().size() !=\\n', '-             result_shape.tuple_shapes().size())) {\\n', '       return FailedPrecondition(\\n', '           \"Output sharding is inconsistent with the tuple result\");\\n', '     }']",
                    "hunk_fix": "@@ -313,9 +325,8 @@ PjRtLoadedExecutable::CreateInternal(\n     output_shapes.reserve(result_shape.tuple_shapes().size());\n     output_shardings.reserve(result_shape.tuple_shapes().size());\n     if (result_hlo_sharding.has_value() &&\n-        (!result_hlo_sharding->IsTuple() ||\n-         result_hlo_sharding->tuple_elements().size() !=\n-             result_shape.tuple_shapes().size())) {\n+        !check_tuple_output_sharding_condition(result_shape,\n+                                               *result_hlo_sharding)) {\n       return FailedPrecondition(\n           \"Output sharding is inconsistent with the tuple result\");\n     }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 63,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/e56206755ff0c98269eb3e50c98fccbaadb6884d",
    "date": "2023-08-21T18:50:57-07:00",
    "message": "Support quantized i64 during flatbuffer import\n\nSometimes tflite flatbuffer represent `i32` quantized values as `i64`s. In these cases we should truncate down to a lower bit width to avoid creating illegal types.\n\nWe check the bitwidth of the type at load time and pick a power-of-2 bit\nwidth where the value can be safely truncated.\n\nPiperOrigin-RevId: 558958449",
    "changes": [
        {
            "name": "flatbuffer_import.cc",
            "path": "tensorflow/compiler/mlir/lite/flatbuffer_import.cc",
            "patches": [
                {
                    "old_start": 162,
                    "old_length": 18,
                    "new_start": 162,
                    "new_length": 20,
                    "hunk_buggy": "[' // Returns the correct type for a quantized tensor\\n', ' // We have a special case for constants since they have a higher minimum value.\\n', ' StatusOr<QuantizedType> GetQuantizedType(const TensorT& tensor, Builder builder,\\n', '-                                         bool is_constant = false) {\\n', '   tflite::QuantizationParametersT& quant_params = *tensor.quantization;\\n', '   if (quant_params.details.AsCustomQuantization()) {\\n', '     return errors::Unimplemented(\"Cannot handle experimental quantization\");\\n', '   }\\n', ' \\n', '   bool is_signed = true;\\n', '-  mlir::IntegerType storage_type;\\n', '   if (tensor.type == tflite::TensorType_UINT8) {\\n', '     is_signed = false;\\n', '-    storage_type = builder.getIntegerType(8);\\n', '-  } else {\\n', '     auto raw_elem_type = ConvertElementType(tensor.type, builder);\\n', '     if (!raw_elem_type.isa<mlir::IntegerType>()) {\\n', '       return errors::InvalidArgument(\\n']",
                    "hunk_fix": "@@ -162,18 +162,20 @@ Location OpLoc(const OperatorT& op,\n // Returns the correct type for a quantized tensor\n // We have a special case for constants since they have a higher minimum value.\n StatusOr<QuantizedType> GetQuantizedType(const TensorT& tensor, Builder builder,\n-                                         bool is_constant = false) {\n+                                         bool is_constant = false,\n+                                         mlir::Type storage_type = {}) {\n   tflite::QuantizationParametersT& quant_params = *tensor.quantization;\n   if (quant_params.details.AsCustomQuantization()) {\n     return errors::Unimplemented(\"Cannot handle experimental quantization\");\n   }\n \n   bool is_signed = true;\n-  mlir::IntegerType storage_type;\n   if (tensor.type == tflite::TensorType_UINT8) {\n     is_signed = false;\n-    storage_type = builder.getIntegerType(8);\n-  } else {\n+    storage_type = mlir::IntegerType::get(builder.getContext(), 8);\n+  }\n+\n+  if (!storage_type) {\n     auto raw_elem_type = ConvertElementType(tensor.type, builder);\n     if (!raw_elem_type.isa<mlir::IntegerType>()) {\n       return errors::InvalidArgument(\n"
                },
                {
                    "old_start": 186,
                    "old_length": 13,
                    "new_start": 188,
                    "new_length": 14,
                    "hunk_buggy": "[\"   // Since we don't know which ones are weights, we represent this optimization\\n\", '   // as a change in the storage bounds for the type for all constants of this\\n', '   // type.\\n', '-  bool is_weight_buffer = is_constant && (storage_type.getWidth() == 8);\\n', '-\\n', '-  int64_t storage_min = QuantizedType::getDefaultMinimumForInteger(\\n', '-                            is_signed, storage_type.getWidth()) +\\n', '-                        static_cast<int>(is_weight_buffer);\\n', '-  int64_t storage_max = QuantizedType::getDefaultMaximumForInteger(\\n', '-      is_signed, storage_type.getWidth());\\n', '   uint32_t flags =\\n', '       is_signed ? mlir::quant::QuantizationFlags::FlagValue::Signed : 0;\\n', ' \\n']",
                    "hunk_fix": "@@ -186,13 +188,14 @@ StatusOr<QuantizedType> GetQuantizedType(const TensorT& tensor, Builder builder,\n   // Since we don't know which ones are weights, we represent this optimization\n   // as a change in the storage bounds for the type for all constants of this\n   // type.\n-  bool is_weight_buffer = is_constant && (storage_type.getWidth() == 8);\n-\n-  int64_t storage_min = QuantizedType::getDefaultMinimumForInteger(\n-                            is_signed, storage_type.getWidth()) +\n-                        static_cast<int>(is_weight_buffer);\n-  int64_t storage_max = QuantizedType::getDefaultMaximumForInteger(\n-      is_signed, storage_type.getWidth());\n+  int bitwidth = storage_type.getIntOrFloatBitWidth();\n+  bool is_weight_buffer = is_constant && (bitwidth == 8);\n+\n+  int64_t storage_min =\n+      QuantizedType::getDefaultMinimumForInteger(is_signed, bitwidth) +\n+      static_cast<int>(is_weight_buffer);\n+  int64_t storage_max =\n+      QuantizedType::getDefaultMaximumForInteger(is_signed, bitwidth);\n   uint32_t flags =\n       is_signed ? mlir::quant::QuantizationFlags::FlagValue::Signed : 0;\n \n"
                },
                {
                    "old_start": 232,
                    "old_length": 7,
                    "new_start": 235,
                    "new_length": 8,
                    "hunk_buggy": "[' \\n', ' StatusOr<mlir::TensorType> GetTensorType(const TensorT& tensor, Builder builder,\\n', '                                          bool is_constant = false,\\n', '-                                         bool is_intermediate = false) {\\n', '   mlir::Type elem_type = ConvertElementType(tensor.type, builder);\\n', '   if (tensor.type == tflite::TensorType_VARIANT) {\\n', '     llvm::SmallVector<mlir::TensorType> tensor_types;\\n']",
                    "hunk_fix": "@@ -232,7 +235,8 @@ StatusOr<QuantizedType> GetCalibratedQuantizedType(const TensorT& tensor,\n \n StatusOr<mlir::TensorType> GetTensorType(const TensorT& tensor, Builder builder,\n                                          bool is_constant = false,\n-                                         bool is_intermediate = false) {\n+                                         bool is_intermediate = false,\n+                                         bool get_storage = false) {\n   mlir::Type elem_type = ConvertElementType(tensor.type, builder);\n   if (tensor.type == tflite::TensorType_VARIANT) {\n     llvm::SmallVector<mlir::TensorType> tensor_types;\n"
                },
                {
                    "old_start": 254,
                    "old_length": 9,
                    "new_start": 258,
                    "new_length": 13,
                    "hunk_buggy": "['     }\\n', '     elem_type = mlir::TF::VariantType::get(tensor_types, builder.getContext());\\n', '   }\\n', '-  if (IsQuantized(tensor)) {\\n', '     TF_ASSIGN_OR_RETURN(elem_type,\\n', '                         GetQuantizedType(tensor, builder, is_constant));\\n', '   }\\n', ' \\n', '   // Intermediate tensors with calibration value (but not scale and zero points)\\n']",
                    "hunk_fix": "@@ -254,9 +258,13 @@ StatusOr<mlir::TensorType> GetTensorType(const TensorT& tensor, Builder builder,\n     }\n     elem_type = mlir::TF::VariantType::get(tensor_types, builder.getContext());\n   }\n-  if (IsQuantized(tensor)) {\n+  if (IsQuantized(tensor) && !get_storage) {\n     TF_ASSIGN_OR_RETURN(elem_type,\n                         GetQuantizedType(tensor, builder, is_constant));\n+  } else if (IsQuantized(tensor) && get_storage) {\n+    // If the type is quantized we strip the signedness from the storage type.\n+    elem_type = mlir::IntegerType::get(elem_type.getContext(),\n+                                       elem_type.getIntOrFloatBitWidth());\n   }\n \n   // Intermediate tensors with calibration value (but not scale and zero points)\n"
                },
                {
                    "old_start": 355,
                    "old_length": 8,
                    "new_start": 363,
                    "new_length": 8,
                    "hunk_buggy": "[' // The read_size parameter is present to allow reading both float16 and float32s\\n', ' // without a case split.\\n', ' template <typename T>\\n', '-std::vector<T> ReadAsHostEndian(ArrayRef<uint8_t> bytes) {\\n', '-  std::vector<T> ret;\\n', '   size_t read_size = sizeof(T);\\n', '   int bytes_len = bytes.size();\\n', '   assert(bytes_len % read_size == 0);\\n']",
                    "hunk_fix": "@@ -355,8 +363,8 @@ std::string GetMlirOpName(const tflite::OperatorT& op,\n // The read_size parameter is present to allow reading both float16 and float32s\n // without a case split.\n template <typename T>\n-std::vector<T> ReadAsHostEndian(ArrayRef<uint8_t> bytes) {\n-  std::vector<T> ret;\n+llvm::SmallVector<mlir::APInt> ReadAsHostEndian(ArrayRef<uint8_t> bytes) {\n+  llvm::SmallVector<mlir::APInt> ret;\n   size_t read_size = sizeof(T);\n   int bytes_len = bytes.size();\n   assert(bytes_len % read_size == 0);\n"
                },
                {
                    "old_start": 366,
                    "old_length": 9,
                    "new_start": 374,
                    "new_length": 10,
                    "hunk_buggy": "[' \\n', '   const char* data_ptr = reinterpret_cast<const char*>(bytes.data());\\n', '   for (int i = 0; i < elem_count; i++) {\\n', '-    ret.push_back(llvm::support::endian::readNext<\\n', '-                  T, llvm::support::endian::system_endianness(),\\n', '-                  llvm::support::unaligned>(data_ptr));\\n', '   }\\n', '   return ret;\\n', ' }\\n']",
                    "hunk_fix": "@@ -366,9 +374,10 @@ std::vector<T> ReadAsHostEndian(ArrayRef<uint8_t> bytes) {\n \n   const char* data_ptr = reinterpret_cast<const char*>(bytes.data());\n   for (int i = 0; i < elem_count; i++) {\n-    ret.push_back(llvm::support::endian::readNext<\n-                  T, llvm::support::endian::system_endianness(),\n-                  llvm::support::unaligned>(data_ptr));\n+    T val = llvm::support::endian::readNext<\n+        T, llvm::support::endian::system_endianness(),\n+        llvm::support::unaligned>(data_ptr);\n+    ret.push_back(mlir::APInt(sizeof(T) * 8, val));\n   }\n   return ret;\n }\n"
                },
                {
                    "old_start": 398,
                    "old_length": 12,
                    "new_start": 407,
                    "new_length": 12,
                    "hunk_buggy": "[' }\\n', ' \\n', ' StatusOr<mlir::ElementsAttr> ConvertFloatBuffer(\\n', '-    mlir::RankedTensorType shaped_type, mlir::FloatType elem_type,\\n', '-    const std::vector<uint8_t>& buffer) {\\n', '   size_t bytes_len = buffer.size();\\n', ' \\n', '   // The bytes of floats are stored little-endian.\\n', '-  switch (elem_type.getWidth()) {\\n', '     case 16: {\\n', '       assert(bytes_len % 2 == 0);\\n', '       assert(elem_type.isF16());\\n']",
                    "hunk_fix": "@@ -398,12 +407,12 @@ tensorflow::TensorProto ConvertTfliteConstTensor(\n }\n \n StatusOr<mlir::ElementsAttr> ConvertFloatBuffer(\n-    mlir::RankedTensorType shaped_type, mlir::FloatType elem_type,\n-    const std::vector<uint8_t>& buffer) {\n+    mlir::RankedTensorType shaped_type, const std::vector<uint8_t>& buffer) {\n   size_t bytes_len = buffer.size();\n+  mlir::Type elem_type = shaped_type.getElementType();\n \n   // The bytes of floats are stored little-endian.\n-  switch (elem_type.getWidth()) {\n+  switch (elem_type.getIntOrFloatBitWidth()) {\n     case 16: {\n       assert(bytes_len % 2 == 0);\n       assert(elem_type.isF16());\n"
                },
                {
                    "old_start": 458,
                    "old_length": 16,
                    "new_start": 467,
                    "new_length": 43,
                    "hunk_buggy": "['           DenseElementsAttr::get(shaped_type, ArrayRef<double>(values)));\\n', '     }\\n', '   }\\n', '-  return errors::InvalidArgument(\"unsupported bit width\", elem_type.getWidth());\\n', ' }\\n', ' \\n', ' StatusOr<mlir::ElementsAttr> ConvertIntBuffer(\\n', '-    mlir::RankedTensorType shaped_type, mlir::Type elem_type,\\n', '-    const std::vector<uint8_t>& buffer) {\\n', '   unsigned bit_width;\\n', '   if (auto itype = elem_type.dyn_cast<mlir::IntegerType>()) {\\n', '     bit_width = itype.getWidth();\\n', '-  } else if (auto qtype = elem_type.dyn_cast<QuantizedType>()) {\\n', '     bit_width = qtype.getStorageTypeIntegralWidth();\\n', '     shaped_type = tensorflow::GetTypeFromTFTensorShape(shaped_type.getShape(),\\n', '                                                        qtype.getStorageType());\\n']",
                    "hunk_fix": "@@ -458,16 +467,43 @@ StatusOr<mlir::ElementsAttr> ConvertFloatBuffer(\n           DenseElementsAttr::get(shaped_type, ArrayRef<double>(values)));\n     }\n   }\n-  return errors::InvalidArgument(\"unsupported bit width\", elem_type.getWidth());\n+  return errors::InvalidArgument(\"unsupported bit width\",\n+                                 elem_type.getIntOrFloatBitWidth());\n+}\n+\n+// If the values in the buffer can be clamped to a bitwidth, truncate\n+// and return the new clamped integer width.\n+void truncateLimitedIntegerAPInt(llvm::SmallVector<mlir::APInt>& values) {\n+  mlir::APInt min = values[0];\n+  mlir::APInt max = values[0];\n+  for (auto& val : values) {\n+    min = llvm::APIntOps::smin(val, min);\n+    max = llvm::APIntOps::smax(val, max);\n+  }\n+\n+  for (int64_t bw = 8; bw < min.getBitWidth(); bw += bw) {\n+    auto limitMin = mlir::APInt::getSignedMinValue(bw).sext(min.getBitWidth());\n+    auto limitMax = mlir::APInt::getSignedMaxValue(bw).sext(min.getBitWidth());\n+    if (min.sle(limitMin) || max.sle(limitMin) || min.sge(limitMax) ||\n+        max.sge(limitMax)) {\n+      continue;\n+    }\n+\n+    for (int i = 0; i < values.size(); i++) {\n+      values[i] = values[i].trunc(bw);\n+    }\n+    break;\n+  }\n }\n \n StatusOr<mlir::ElementsAttr> ConvertIntBuffer(\n-    mlir::RankedTensorType shaped_type, mlir::Type elem_type,\n-    const std::vector<uint8_t>& buffer) {\n+    mlir::RankedTensorType shaped_type, const std::vector<uint8_t>& buffer,\n+    bool truncate = false) {\n+  mlir::Type elem_type = shaped_type.getElementType();\n   unsigned bit_width;\n   if (auto itype = elem_type.dyn_cast<mlir::IntegerType>()) {\n     bit_width = itype.getWidth();\n-  } else if (auto qtype = elem_type.dyn_cast<QuantizedType>()) {\n+  } else if (auto qtype = elem_type.dyn_cast<mlir::quant::QuantizedType>()) {\n     bit_width = qtype.getStorageTypeIntegralWidth();\n     shaped_type = tensorflow::GetTypeFromTFTensorShape(shaped_type.getShape(),\n                                                        qtype.getStorageType());\n"
                },
                {
                    "old_start": 475,
                    "old_length": 47,
                    "new_start": 511,
                    "new_length": 57,
                    "hunk_buggy": "['     return errors::InvalidArgument(\"unsupported integer constant type\");\\n', '   }\\n', ' \\n', '   switch (bit_width) {\\n', '     case 1: {\\n', \"       // vector<bool> doesn't convert to an ArrayRef\\n\", '-      llvm::SmallVector<bool, 8> values;\\n', '-      values.reserve(buffer.size());\\n', '       for (auto b : buffer) {\\n', '-        values.emplace_back(b != 0);\\n', '       }\\n', '       return mlir::ElementsAttr(\\n', '-          DenseElementsAttr::get(shaped_type, ArrayRef<bool>(values)));\\n', '     }\\n', '     case 4: {\\n', '-      auto values =\\n', '           tflite::UnpackDenseInt4IntoInt8(buffer, shaped_type.getNumElements());\\n', '       // Use `getFromRawBuffer()` instead of `get()` to bypass a templated size\\n', \"       // check which doesn't work with int4 because int4_t doesn't exist.\\n\", '       return mlir::ElementsAttr(DenseElementsAttr::getFromRawBuffer(\\n', '-          shaped_type, ArrayRef<char>(values)));\\n', '     }\\n', '     case 8: {\\n', '       return mlir::ElementsAttr(\\n', '           DenseElementsAttr::get(shaped_type, ArrayRef<uint8_t>(buffer)));\\n', '     }\\n', '     case 16: {\\n', '-      auto values = ReadAsHostEndian<uint16_t>(buffer);\\n', '-      return mlir::ElementsAttr(\\n', '-          DenseElementsAttr::get(shaped_type, ArrayRef<uint16_t>(values)));\\n', '     }\\n', '     case 32: {\\n', '-      auto values = ReadAsHostEndian<uint32_t>(buffer);\\n', '-      return mlir::ElementsAttr(\\n', '-          DenseElementsAttr::get(shaped_type, ArrayRef<uint32_t>(values)));\\n', '     }\\n', '     case 64: {\\n', '-      auto values = ReadAsHostEndian<uint64_t>(buffer);\\n', '-      return mlir::ElementsAttr(\\n', '-          DenseElementsAttr::get(shaped_type, ArrayRef<uint64_t>(values)));\\n', '     }\\n', '     default:\\n', '       return errors::Unimplemented(\"Cannot handle bit width \", bit_width);\\n', '   }\\n', ' }\\n', ' \\n', ' StatusOr<Operation*> BuildExternalConstOp(const tflite::TensorT& tensor,\\n']",
                    "hunk_fix": "@@ -475,47 +511,57 @@ StatusOr<mlir::ElementsAttr> ConvertIntBuffer(\n     return errors::InvalidArgument(\"unsupported integer constant type\");\n   }\n \n+  llvm::SmallVector<mlir::APInt> values;\n   switch (bit_width) {\n     case 1: {\n       // vector<bool> doesn't convert to an ArrayRef\n-      llvm::SmallVector<bool, 8> values;\n-      values.reserve(buffer.size());\n+      llvm::SmallVector<bool, 8> boolValues;\n+      boolValues.reserve(buffer.size());\n       for (auto b : buffer) {\n-        values.emplace_back(b != 0);\n+        boolValues.emplace_back(b != 0);\n       }\n       return mlir::ElementsAttr(\n-          DenseElementsAttr::get(shaped_type, ArrayRef<bool>(values)));\n+          DenseElementsAttr::get(shaped_type, ArrayRef<bool>(boolValues)));\n     }\n     case 4: {\n-      auto values =\n+      auto i4Values =\n           tflite::UnpackDenseInt4IntoInt8(buffer, shaped_type.getNumElements());\n       // Use `getFromRawBuffer()` instead of `get()` to bypass a templated size\n       // check which doesn't work with int4 because int4_t doesn't exist.\n       return mlir::ElementsAttr(DenseElementsAttr::getFromRawBuffer(\n-          shaped_type, ArrayRef<char>(values)));\n+          shaped_type, ArrayRef<char>(i4Values)));\n     }\n     case 8: {\n       return mlir::ElementsAttr(\n           DenseElementsAttr::get(shaped_type, ArrayRef<uint8_t>(buffer)));\n     }\n     case 16: {\n-      auto values = ReadAsHostEndian<uint16_t>(buffer);\n-      return mlir::ElementsAttr(\n-          DenseElementsAttr::get(shaped_type, ArrayRef<uint16_t>(values)));\n+      values = ReadAsHostEndian<uint16_t>(buffer);\n+      break;\n     }\n     case 32: {\n-      auto values = ReadAsHostEndian<uint32_t>(buffer);\n-      return mlir::ElementsAttr(\n-          DenseElementsAttr::get(shaped_type, ArrayRef<uint32_t>(values)));\n+      values = ReadAsHostEndian<uint32_t>(buffer);\n+      break;\n     }\n     case 64: {\n-      auto values = ReadAsHostEndian<uint64_t>(buffer);\n-      return mlir::ElementsAttr(\n-          DenseElementsAttr::get(shaped_type, ArrayRef<uint64_t>(values)));\n+      values = ReadAsHostEndian<uint64_t>(buffer);\n+      break;\n     }\n     default:\n       return errors::Unimplemented(\"Cannot handle bit width \", bit_width);\n   }\n+\n+  if (truncate) {\n+    truncateLimitedIntegerAPInt(values);\n+    auto sign = mlir::cast<mlir::IntegerType>(shaped_type.getElementType())\n+                    .getSignedness();\n+    auto ety = mlir::IntegerType::get(shaped_type.getContext(),\n+                                      values[0].getBitWidth(), sign);\n+    shaped_type =\n+        tensorflow::GetTypeFromTFTensorShape(shaped_type.getShape(), ety);\n+  }\n+\n+  return mlir::ElementsAttr(DenseElementsAttr::get(shaped_type, values));\n }\n \n StatusOr<Operation*> BuildExternalConstOp(const tflite::TensorT& tensor,\n"
                },
                {
                    "old_start": 561,
                    "old_length": 9,
                    "new_start": 607,
                    "new_length": 15,
                    "hunk_buggy": "[' // variable `stateful_variable_idx` is used as a unique value for each constant\\n', ' // to avoid CSEed. `tensor` is the data structure of flatbuffer. `shaped_type`\\n', ' // is the ShapedType for the const op.\\n', '-Operation* BuildVariableOp(const tflite::TensorT& tensor,\\n', '-                           mlir::RankedTensorType shaped_type,\\n', '-                           OpBuilder builder, Location loc) {\\n', '   static int stateful_variable_idx = 0;\\n', '   mlir::ElementsAttr value =\\n', '       GetSplat(shaped_type, stateful_variable_idx++, builder);\\n']",
                    "hunk_fix": "@@ -561,9 +607,15 @@ static mlir::ElementsAttr GetSplat(RankedTensorType type, int unique_index,\n // variable `stateful_variable_idx` is used as a unique value for each constant\n // to avoid CSEed. `tensor` is the data structure of flatbuffer. `shaped_type`\n // is the ShapedType for the const op.\n-Operation* BuildVariableOp(const tflite::TensorT& tensor,\n-                           mlir::RankedTensorType shaped_type,\n-                           OpBuilder builder, Location loc) {\n+StatusOr<Operation*> BuildVariableOp(const tflite::TensorT& tensor,\n+                                     OpBuilder builder, Location loc) {\n+  TF_ASSIGN_OR_RETURN(auto type, GetTensorType(tensor, builder,\n+                                               /*is_constant=*/true));\n+  auto shaped_type = type.dyn_cast<mlir::RankedTensorType>();\n+  if (!shaped_type) {\n+    return errors::Internal(\"Constant doesn't have a shape\");\n+  }\n+\n   static int stateful_variable_idx = 0;\n   mlir::ElementsAttr value =\n       GetSplat(shaped_type, stateful_variable_idx++, builder);\n"
                },
                {
                    "old_start": 607,
                    "old_length": 8,
                    "new_start": 659,
                    "new_length": 20,
                    "hunk_buggy": "[' \\n', ' static StatusOr<Operation*> BuildSparseConstOp(\\n', '     const tflite::TensorT& tensor, const std::vector<uint8_t>& buffer,\\n', '-    const mlir::RankedTensorType shaped_type, OpBuilder& builder,\\n', '-    Location loc) {\\n', '   tensorflow::TensorProto repr = ConvertTfliteConstTensor(tensor, buffer);\\n', '   repr.clear_tensor_shape();\\n', '   if (IsQuantized(tensor)) {\\n']",
                    "hunk_fix": "@@ -607,8 +659,20 @@ static StatusOr<std::vector<int32_t>> ConvertSparseIndexVector(\n \n static StatusOr<Operation*> BuildSparseConstOp(\n     const tflite::TensorT& tensor, const std::vector<uint8_t>& buffer,\n-    const mlir::RankedTensorType shaped_type, OpBuilder& builder,\n-    Location loc) {\n+    OpBuilder& builder, Location loc) {\n+  TF_ASSIGN_OR_RETURN(auto type, GetTensorType(tensor, builder,\n+                                               /*is_constant=*/true));\n+  auto shaped_type = type.dyn_cast<mlir::RankedTensorType>();\n+  if (!shaped_type) {\n+    return errors::Internal(\"Constant doesn't have a shape\");\n+  }\n+\n+  TF_ASSIGN_OR_RETURN(type, GetTensorType(tensor, builder,\n+                                          /*is_constant=*/true,\n+                                          /*is_intermediate=*/false,\n+                                          /*get_storage=*/true));\n+  auto value_type = mlir::dyn_cast<mlir::RankedTensorType>(type);\n+\n   tensorflow::TensorProto repr = ConvertTfliteConstTensor(tensor, buffer);\n   repr.clear_tensor_shape();\n   if (IsQuantized(tensor)) {\n"
                },
                {
                    "old_start": 652,
                    "old_length": 13,
                    "new_start": 716,
                    "new_length": 6,
                    "hunk_buggy": "['       builder.getContext(), tensor.sparsity->traversal_order,\\n', '       tensor.sparsity->block_map, dim_metadata);\\n', ' \\n', '-  auto value_type = shaped_type;\\n', '-  if (IsQuantized(tensor)) {\\n', '-    value_type = tensorflow::GetTypeFromTFTensorShape(\\n', '-        shaped_type.getShape(), shaped_type.getElementType()\\n', '-                                    .dyn_cast<mlir::quant::QuantizedType>()\\n', '-                                    .getStorageType());\\n', '-  }\\n', '   std::vector<char> dense_buffer(\\n', '       value_type.getElementType().getIntOrFloatBitWidth() / CHAR_BIT);\\n', '   mlir::TypedAttr dummy_value =\\n']",
                    "hunk_fix": "@@ -652,13 +716,6 @@ static StatusOr<Operation*> BuildSparseConstOp(\n       builder.getContext(), tensor.sparsity->traversal_order,\n       tensor.sparsity->block_map, dim_metadata);\n \n-  auto value_type = shaped_type;\n-  if (IsQuantized(tensor)) {\n-    value_type = tensorflow::GetTypeFromTFTensorShape(\n-        shaped_type.getShape(), shaped_type.getElementType()\n-                                    .dyn_cast<mlir::quant::QuantizedType>()\n-                                    .getStorageType());\n-  }\n   std::vector<char> dense_buffer(\n       value_type.getElementType().getIntOrFloatBitWidth() / CHAR_BIT);\n   mlir::TypedAttr dummy_value =\n"
                },
                {
                    "old_start": 680,
                    "old_length": 28,
                    "new_start": 737,
                    "new_length": 41,
                    "hunk_buggy": "['                                   const std::vector<uint8_t>& buffer,\\n', '                                   bool is_variable, OpBuilder builder,\\n', '                                   Location loc, bool use_stablehlo_constant) {\\n', '   TF_ASSIGN_OR_RETURN(auto type, GetTensorType(tensor, builder,\\n', '-                                               /*is_constant=*/true));\\n', '   auto shaped_type = type.dyn_cast<mlir::RankedTensorType>();\\n', '   if (!shaped_type) {\\n', '     return errors::Internal(\"Constant doesn\\'t have a shape\");\\n', '   }\\n', ' \\n', '-  if (tensor.sparsity != nullptr) {\\n', '-    return BuildSparseConstOp(tensor, buffer, shaped_type, builder, loc);\\n', '   }\\n', ' \\n', '   auto elem_type = shaped_type.getElementType();\\n', '-\\n', '-  mlir::ElementsAttr value;\\n', '-  if (is_variable) {\\n', '-    return BuildVariableOp(tensor, shaped_type, builder, loc);\\n', '-  } else if (auto float_type = elem_type.dyn_cast<mlir::FloatType>()) {\\n', '-    TF_ASSIGN_OR_RETURN(value,\\n', '-                        ConvertFloatBuffer(shaped_type, float_type, buffer));\\n', '-  } else if (elem_type.isa<mlir::IntegerType, QuantizedType>()) {\\n', '-    TF_ASSIGN_OR_RETURN(value,\\n', '-                        ConvertIntBuffer(shaped_type, elem_type, buffer));\\n', '   } else if (elem_type.isa<mlir::TF::StringType>()) {\\n', '     tensorflow::TensorProto repr = ConvertTfliteConstTensor(tensor, buffer);\\n', '     std::vector<llvm::StringRef> refs;\\n']",
                    "hunk_fix": "@@ -680,28 +737,41 @@ StatusOr<Operation*> BuildConstOp(const tflite::TensorT& tensor,\n                                   const std::vector<uint8_t>& buffer,\n                                   bool is_variable, OpBuilder builder,\n                                   Location loc, bool use_stablehlo_constant) {\n+  if (tensor.sparsity != nullptr) {\n+    return BuildSparseConstOp(tensor, buffer, builder, loc);\n+  }\n+\n+  if (is_variable) {\n+    return BuildVariableOp(tensor, builder, loc);\n+  }\n+\n   TF_ASSIGN_OR_RETURN(auto type, GetTensorType(tensor, builder,\n-                                               /*is_constant=*/true));\n+                                               /*is_constant=*/true,\n+                                               /*is_intermediate=*/false,\n+                                               /*get_storage=*/true));\n   auto shaped_type = type.dyn_cast<mlir::RankedTensorType>();\n   if (!shaped_type) {\n     return errors::Internal(\"Constant doesn't have a shape\");\n   }\n \n-  if (tensor.sparsity != nullptr) {\n-    return BuildSparseConstOp(tensor, buffer, shaped_type, builder, loc);\n+  mlir::ElementsAttr value;\n+  if (IsQuantized(tensor)) {\n+    bool truncate = shaped_type.getElementType().getIntOrFloatBitWidth() == 64;\n+    TF_ASSIGN_OR_RETURN(value, ConvertIntBuffer(shaped_type, buffer, truncate));\n+    TF_ASSIGN_OR_RETURN(\n+        auto type, GetQuantizedType(tensor, builder, /*is_constant=*/true,\n+                                    /*storage_type=*/value.getElementType()));\n+    shaped_type = shaped_type.clone(type);\n+    auto op = builder.create<tfl::QConstOp>(\n+        loc, mlir::TypeAttr::get(shaped_type), value);\n+    return op.getOperation();\n   }\n \n   auto elem_type = shaped_type.getElementType();\n-\n-  mlir::ElementsAttr value;\n-  if (is_variable) {\n-    return BuildVariableOp(tensor, shaped_type, builder, loc);\n-  } else if (auto float_type = elem_type.dyn_cast<mlir::FloatType>()) {\n-    TF_ASSIGN_OR_RETURN(value,\n-                        ConvertFloatBuffer(shaped_type, float_type, buffer));\n-  } else if (elem_type.isa<mlir::IntegerType, QuantizedType>()) {\n-    TF_ASSIGN_OR_RETURN(value,\n-                        ConvertIntBuffer(shaped_type, elem_type, buffer));\n+  if (auto float_type = elem_type.dyn_cast<mlir::FloatType>()) {\n+    TF_ASSIGN_OR_RETURN(value, ConvertFloatBuffer(shaped_type, buffer));\n+  } else if (elem_type.isa<mlir::IntegerType>()) {\n+    TF_ASSIGN_OR_RETURN(value, ConvertIntBuffer(shaped_type, buffer));\n   } else if (elem_type.isa<mlir::TF::StringType>()) {\n     tensorflow::TensorProto repr = ConvertTfliteConstTensor(tensor, buffer);\n     std::vector<llvm::StringRef> refs;\n"
                },
                {
                    "old_start": 720,
                    "old_length": 11,
                    "new_start": 790,
                    "new_length": 6,
                    "hunk_buggy": "['     return errors::Unimplemented(\"Constant of unsupported type\");\\n', '   }\\n', ' \\n', '-  if (IsQuantized(tensor)) {\\n', '-    auto op = builder.create<tfl::QConstOp>(\\n', '-        loc, mlir::TypeAttr::get(shaped_type), value);\\n', '-    return op.getOperation();\\n', '-  }\\n', '   if (use_stablehlo_constant) {\\n', '     auto op = builder.create<mlir::stablehlo::ConstantOp>(loc, value);\\n', '     return op.getOperation();']",
                    "hunk_fix": "@@ -720,11 +790,6 @@ StatusOr<Operation*> BuildConstOp(const tflite::TensorT& tensor,\n     return errors::Unimplemented(\"Constant of unsupported type\");\n   }\n \n-  if (IsQuantized(tensor)) {\n-    auto op = builder.create<tfl::QConstOp>(\n-        loc, mlir::TypeAttr::get(shaped_type), value);\n-    return op.getOperation();\n-  }\n   if (use_stablehlo_constant) {\n     auto op = builder.create<mlir::stablehlo::ConstantOp>(loc, value);\n     return op.getOperation();"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 64,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/f636be3bb1f556c15dba3028e61a8969d90dadd9",
    "date": "2023-08-16T11:23:04-07:00",
    "message": "Return error on invalid input in `tfl.sign_custom`\n\nPiperOrigin-RevId: 557545596",
    "changes": [
        {
            "name": "sign_custom.cc",
            "path": "tensorflow/lite/kernels/sign_custom.cc",
            "patches": [
                {
                    "old_start": 73,
                    "old_length": 11,
                    "new_start": 73,
                    "new_length": 11,
                    "hunk_buggy": "['           context,\\n', '           (PointwiseUnaryOpDoEval<Op, double>(context, input, output)));\\n', '       break;\\n', '-    default:\\n', '-      TF_LITE_KERNEL_LOG(\\n', '-          context,\\n', '-          \"Unsupported datatype for atan2 output: %s\",\\n', '-          TfLiteTypeGetName(output->type));\\n', '   }\\n', ' \\n', '   return TfLiteStatus::kTfLiteOk;']",
                    "hunk_fix": "@@ -73,11 +73,11 @@ TfLiteStatus PointwiseUnaryOpEval(TfLiteContext* context, TfLiteNode* node) {\n           context,\n           (PointwiseUnaryOpDoEval<Op, double>(context, input, output)));\n       break;\n-    default:\n-      TF_LITE_KERNEL_LOG(\n-          context,\n-          \"Unsupported datatype for atan2 output: %s\",\n-          TfLiteTypeGetName(output->type));\n+    default: {\n+      TF_LITE_KERNEL_LOG(context, \"Unsupported datatype for sign output: %s\",\n+                         TfLiteTypeGetName(output->type));\n+      return TfLiteStatus::kTfLiteError;\n+    }\n   }\n \n   return TfLiteStatus::kTfLiteOk;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 65,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/43fd10302bcc8447e7a7205bae848a3a88624775",
    "date": "2023-08-14T08:27:11-07:00",
    "message": "Return error on invalid input in `tfl.atan2_custom`\n\nPiperOrigin-RevId: 556797683",
    "changes": [
        {
            "name": "atan2_custom.cc",
            "path": "tensorflow/lite/kernels/atan2_custom.cc",
            "patches": [
                {
                    "old_start": 79,
                    "old_length": 9,
                    "new_start": 79,
                    "new_length": 11,
                    "hunk_buggy": "['     case kTfLiteFloat64:\\n', '       TF_LITE_ENSURE_OK(context, Atan2<double>(input_y, input_x, output));\\n', '       break;\\n', '-    default:\\n', '       TF_LITE_KERNEL_LOG(context, \"Unsupported datatype for atan2 output: %s\",\\n', '                          TfLiteTypeGetName(output->type));\\n', '   }\\n', ' \\n', '   return TfLiteStatus::kTfLiteOk;']",
                    "hunk_fix": "@@ -79,9 +79,11 @@ TfLiteStatus Atan2Eval(TfLiteContext* context, TfLiteNode* node) {\n     case kTfLiteFloat64:\n       TF_LITE_ENSURE_OK(context, Atan2<double>(input_y, input_x, output));\n       break;\n-    default:\n+    default: {\n       TF_LITE_KERNEL_LOG(context, \"Unsupported datatype for atan2 output: %s\",\n                          TfLiteTypeGetName(output->type));\n+      return TfLiteStatus::kTfLiteError;\n+    }\n   }\n \n   return TfLiteStatus::kTfLiteOk;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 66,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/00517642a356c5e04f009ea61c74638d89746392",
    "date": "2023-08-09T06:07:32-07:00",
    "message": "Return error on invalid input in `tfl.splitv`\n\nPiperOrigin-RevId: 555138718",
    "changes": [
        {
            "name": "split_v.cc",
            "path": "tensorflow/lite/kernels/split_v.cc",
            "patches": [
                {
                    "old_start": 106,
                    "old_length": 6,
                    "new_start": 106,
                    "new_length": 7,
                    "hunk_buggy": "['       TF_LITE_KERNEL_LOG(\\n', '           context,\\n', '           \"The sum of size_splits must be less than the dimension of value.\");\\n', '     } else {\\n', '       size_splits_vector[minus_one_index] = input_size - size_splits_sum;\\n', '     }\\n']",
                    "hunk_fix": "@@ -106,6 +106,7 @@ TfLiteStatus ResizeOutputTensors(TfLiteContext* context, TfLiteNode* node,\n       TF_LITE_KERNEL_LOG(\n           context,\n           \"The sum of size_splits must be less than the dimension of value.\");\n+      return kTfLiteError;\n     } else {\n       size_splits_vector[minus_one_index] = input_size - size_splits_sum;\n     }\n"
                },
                {
                    "old_start": 113,
                    "old_length": 6,
                    "new_start": 114,
                    "new_length": 7,
                    "hunk_buggy": "['     TF_LITE_KERNEL_LOG(\\n', '         context,\\n', '         \"The size_splits must sum to the dimension of value along axis.\");\\n', '   }\\n', ' \\n', '   for (int i = 0; i < NumOutputs(node); ++i) {']",
                    "hunk_fix": "@@ -113,6 +114,7 @@ TfLiteStatus ResizeOutputTensors(TfLiteContext* context, TfLiteNode* node,\n     TF_LITE_KERNEL_LOG(\n         context,\n         \"The size_splits must sum to the dimension of value along axis.\");\n+    return kTfLiteError;\n   }\n \n   for (int i = 0; i < NumOutputs(node); ++i) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 67,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/40c7fe94824100338ef0c495143b26501b1c367e",
    "date": "2023-08-08T08:22:26-07:00",
    "message": "Return error on invalid input in `tfl.topkv2`\n\nPiperOrigin-RevId: 554830225",
    "changes": [
        {
            "name": "topk_v2.cc",
            "path": "tensorflow/lite/kernels/topk_v2.cc",
            "patches": [
                {
                    "old_start": 328,
                    "old_length": 6,
                    "new_start": 328,
                    "new_length": 7,
                    "hunk_buggy": "['       TF_LITE_KERNEL_LOG(\\n', '           context, \"Output index type %s is currently not supported by TopK.\",\\n', '           TfLiteTypeGetName(output_values->type));\\n', '   }\\n', ' \\n', '   return kTfLiteOk;']",
                    "hunk_fix": "@@ -328,6 +328,7 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n       TF_LITE_KERNEL_LOG(\n           context, \"Output index type %s is currently not supported by TopK.\",\n           TfLiteTypeGetName(output_values->type));\n+      return kTfLiteError;\n   }\n \n   return kTfLiteOk;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 68,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/9720b405905dee209a3f7d003de21d388e1aaef4",
    "date": "2023-07-26T10:32:37-05:00",
    "message": "Avoid nullptr as row offsets to cusparseCreateCsr\n\nAs of CUDA 12.2 additional input validation allows NULL for the row offsets\nonly when rows=0.",
    "changes": [
        {
            "name": "sparse_mat_mul_op.cc",
            "path": "tensorflow/core/kernels/sparse/sparse_mat_mul_op.cc",
            "patches": [
                {
                    "old_start": 493,
                    "old_length": 7,
                    "new_start": 493,
                    "new_length": 7,
                    "hunk_buggy": "['                      matC.InitializeCsr<int, T>(\\n', '                          a_input_dense_shape(a_input_dense_shape.size() - 2),\\n', '                          b_input_dense_shape(b_input_dense_shape.size() - 1), 0,\\n', '-                         nullptr, nullptr, nullptr));\\n', ' \\n', '       // Check required size for buffer1 and possibly re-allocate\\n', '       size_t bufferSize1;']",
                    "hunk_fix": "@@ -493,7 +493,7 @@ class CSRSparseMatMulGPUOp : public OpKernel {\n                      matC.InitializeCsr<int, T>(\n                          a_input_dense_shape(a_input_dense_shape.size() - 2),\n                          b_input_dense_shape(b_input_dense_shape.size() - 1), 0,\n-                         nullptr, nullptr, nullptr));\n+                         c_row_ptr.data(), nullptr, nullptr));\n \n       // Check required size for buffer1 and possibly re-allocate\n       size_t bufferSize1;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 69,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/e1ad3b74ad44b883c7b3fdc3a19adcea1d28bfbc",
    "date": "2023-07-17T04:14:46-07:00",
    "message": "[XLA:GPU] Handle edge case in Triton Softmax rewriter where bitcast is an\neffective scalar. This short-circuit avoids crashing within last_dimension when\nattempting to match and either the operand or the result of the bitcast has a\nshape with rank 0.\n\nPiperOrigin-RevId: 548645429",
    "changes": [
        {
            "name": "softmax_rewriter_triton.cc",
            "path": "tensorflow/compiler/xla/service/gpu/softmax_rewriter_triton.cc",
            "patches": [
                {
                    "old_start": 78,
                    "old_length": 7,
                    "new_start": 78,
                    "new_length": 7,
                    "hunk_buggy": "['                          const GpuVersion& gpu_version) {\\n', '   CHECK_EQ(bitcast->opcode(), HloOpcode::kBitcast);\\n', ' \\n', '-  if (bitcast->shape().rank() == 0) {\\n', '     return true;\\n', '   }\\n', ' ']",
                    "hunk_fix": "@@ -78,7 +78,7 @@ bool BitcastIsTilingNoop(HloInstruction* bitcast,\n                          const GpuVersion& gpu_version) {\n   CHECK_EQ(bitcast->opcode(), HloOpcode::kBitcast);\n \n-  if (bitcast->shape().rank() == 0) {\n+  if (ShapeUtil::IsEffectiveScalar(bitcast->shape())) {\n     return true;\n   }\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 70,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/6e153325b66330dafea4e4e8b67b5d56b1a37852",
    "date": "2023-07-14T04:46:03-07:00",
    "message": "[XLA:GPU] Handle edge case in Triton Softmax rewriter where bitcast produces a\nscalar. This avoids crashing within last_dimension when attempting to match.\n\nPiperOrigin-RevId: 548090995",
    "changes": [
        {
            "name": "softmax_rewriter_triton.cc",
            "path": "tensorflow/compiler/xla/service/gpu/softmax_rewriter_triton.cc",
            "patches": [
                {
                    "old_start": 69,
                    "old_length": 6,
                    "new_start": 69,
                    "new_length": 10,
                    "hunk_buggy": "[' bool BitcastIsTilingNoop(HloInstruction* bitcast) {\\n', '   CHECK_EQ(bitcast->opcode(), HloOpcode::kBitcast);\\n', ' \\n', '   // In the Softmax rewriter for now, tiling is derived from a hero reduction\\n', '   // operation, which should be reducing its input on the last axis. Therefore,\\n', '   // a bitcast is always a no-op with regards to a tile if']",
                    "hunk_fix": "@@ -69,6 +69,10 @@ bool TrivialEdge(HloInstruction** producer, HloInstruction* consumer,\n bool BitcastIsTilingNoop(HloInstruction* bitcast) {\n   CHECK_EQ(bitcast->opcode(), HloOpcode::kBitcast);\n \n+  if (bitcast->shape().rank() == 0) {\n+    return true;\n+  }\n+\n   // In the Softmax rewriter for now, tiling is derived from a hero reduction\n   // operation, which should be reducing its input on the last axis. Therefore,\n   // a bitcast is always a no-op with regards to a tile if"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 71,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/84a1cf61dd7239aa5d682083d34e0f7c99039734",
    "date": "2023-06-27T02:23:38-07:00",
    "message": "[XLA] Do not suggest trying to use TF_XLA_FLAGS when failing to parse XLA_FLAGS\n\nThe error can be very misleading, as we never check whether the new flag is actually supported by TF_XLA_FLAGS.\n\nPiperOrigin-RevId: 543680270",
    "changes": [
        {
            "name": "parse_flags_from_env.cc",
            "path": "tensorflow/compiler/xla/parse_flags_from_env.cc",
            "patches": [
                {
                    "old_start": 207,
                    "old_length": 25,
                    "new_start": 207,
                    "new_length": 9,
                    "hunk_buggy": "['     // Skip the first argv, which is the fake argv[0].\\n', '     auto unknown_flags = absl::MakeSpan(env_argv->argv);\\n', '     unknown_flags.remove_prefix(1);\\n', '-\\n', '-    // Some flags are set on XLA_FLAGS, others on TF_XLA_FLAGS.  If we find an\\n', '-    // unrecognized flag, suggest the alternative.\\n', '-    std::string alternate_envvar;\\n', '-    if (envvar == \"TF_XLA_FLAGS\") {\\n', '-      alternate_envvar = \"XLA_FLAGS\";\\n', '-    } else if (envvar == \"XLA_FLAGS\") {\\n', '-      alternate_envvar = \"TF_XLA_FLAGS\";\\n', '-    }\\n', '-    std::string did_you_mean;\\n', '-    if (!alternate_envvar.empty()) {\\n', '-      did_you_mean = absl::StrFormat(\\n', '-          \"\\\\nPerhaps you meant to specify these on the %s envvar?\",\\n', '-          alternate_envvar);\\n', '-    }\\n', '-\\n', '     LOG(QFATAL) << \"Unknown flag\" << (unknown_flags.size() > 1 ? \"s\" : \"\")\\n', '-                << \" in \" << envvar << \": \" << absl::StrJoin(unknown_flags, \" \")\\n', '-                << did_you_mean;\\n', '     return false;\\n', '   }\\n', '   return result;']",
                    "hunk_fix": "@@ -207,25 +207,9 @@ bool ParseFlagsFromEnvAndDieIfUnknown(absl::string_view envvar,\n     // Skip the first argv, which is the fake argv[0].\n     auto unknown_flags = absl::MakeSpan(env_argv->argv);\n     unknown_flags.remove_prefix(1);\n-\n-    // Some flags are set on XLA_FLAGS, others on TF_XLA_FLAGS.  If we find an\n-    // unrecognized flag, suggest the alternative.\n-    std::string alternate_envvar;\n-    if (envvar == \"TF_XLA_FLAGS\") {\n-      alternate_envvar = \"XLA_FLAGS\";\n-    } else if (envvar == \"XLA_FLAGS\") {\n-      alternate_envvar = \"TF_XLA_FLAGS\";\n-    }\n-    std::string did_you_mean;\n-    if (!alternate_envvar.empty()) {\n-      did_you_mean = absl::StrFormat(\n-          \"\\nPerhaps you meant to specify these on the %s envvar?\",\n-          alternate_envvar);\n-    }\n-\n     LOG(QFATAL) << \"Unknown flag\" << (unknown_flags.size() > 1 ? \"s\" : \"\")\n-                << \" in \" << envvar << \": \" << absl::StrJoin(unknown_flags, \" \")\n-                << did_you_mean;\n+                << \" in \" << envvar << \": \"\n+                << absl::StrJoin(unknown_flags, \" \");\n     return false;\n   }\n   return result;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 72,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/62cb54f2caf48480dc6b3c1ce9629eaac4688f83",
    "date": "2023-06-13T17:16:58+10:00",
    "message": "Set 2nd output shape for SparseSegmentReduceGradV2\n\n- Fixes a debug check failure.",
    "changes": [
        {
            "name": "math_ops.cc",
            "path": "tensorflow/core/ops/math_ops.cc",
            "patches": [
                {
                    "old_start": 1231,
                    "old_length": 6,
                    "new_start": 1231,
                    "new_length": 9,
                    "hunk_buggy": "['   ShapeHandle out;\\n', '   TF_RETURN_IF_ERROR(c->Concatenate(dim0_shape, subshape, &out));\\n', '   c->set_output(0, out);\\n', '   return OkStatus();\\n', ' }\\n', ' ']",
                    "hunk_fix": "@@ -1231,6 +1231,9 @@ Status SparseSegmentReductionGradShapeFnImpl(InferenceContext* c,\n   ShapeHandle out;\n   TF_RETURN_IF_ERROR(c->Concatenate(dim0_shape, subshape, &out));\n   c->set_output(0, out);\n+  if (outputs_unique_indices) {\n+    c->set_output(1, c->Vector(InferenceContext::kUnknownDim));\n+  }\n   return OkStatus();\n }\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 73,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/4377a561b7757ed83757f07532e6564c42c286ba",
    "date": "2023-06-06T12:17:00-07:00",
    "message": "Add a check for group size when sorting grouped AllReduces within a block.\n\nPiperOrigin-RevId: 538255219",
    "changes": [
        {
            "name": "dtensor_allreduce_combine_optimization.cc",
            "path": "tensorflow/dtensor/mlir/dtensor_allreduce_combine_optimization.cc",
            "patches": [
                {
                    "old_start": 583,
                    "old_length": 9,
                    "new_start": 583,
                    "new_length": 14,
                    "hunk_buggy": "['         all_reduce_groups = createSubgroupsByReductionAttr(all_reduce_groups);\\n', '         all_reduce_groups = createSubgroupsByGroupAssignment(all_reduce_groups);\\n', ' \\n', '         std::sort(all_reduce_groups.begin(), all_reduce_groups.end(),\\n', '                   [](std::vector<mlir::TF::DTensorAllReduceOp> lhs,\\n', '                      std::vector<mlir::TF::DTensorAllReduceOp> rhs) {\\n', '                     if (lhs[0]->getBlock() == rhs[0]->getBlock())\\n', '                       return lhs[0]->isBeforeInBlock(rhs[0]);\\n', '                     return true;']",
                    "hunk_fix": "@@ -583,9 +583,14 @@ struct DTensorAllReduceCombineOptimization\n         all_reduce_groups = createSubgroupsByReductionAttr(all_reduce_groups);\n         all_reduce_groups = createSubgroupsByGroupAssignment(all_reduce_groups);\n \n+        // Maintain relative order of ALLReduces within the block.\n         std::sort(all_reduce_groups.begin(), all_reduce_groups.end(),\n                   [](std::vector<mlir::TF::DTensorAllReduceOp> lhs,\n                      std::vector<mlir::TF::DTensorAllReduceOp> rhs) {\n+                    if (lhs.empty() || rhs.empty()) {\n+                      // Skip order check if either group is empty.\n+                      return false;\n+                    }\n                     if (lhs[0]->getBlock() == rhs[0]->getBlock())\n                       return lhs[0]->isBeforeInBlock(rhs[0]);\n                     return true;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 74,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/178d62a63ea043a4b9969b4cd6f8983eb8eae523",
    "date": "2023-05-17T13:28:42-07:00",
    "message": "Update check failure to logging a warning for repeated computation placer registration.\n\nThis is to bypass a duplicated registration issue seen in open-source build during TF/PJRT integration.\n\nPiperOrigin-RevId: 532886771",
    "changes": [
        {
            "name": "computation_placer.cc",
            "path": "tensorflow/compiler/xla/service/computation_placer.cc",
            "patches": [
                {
                    "old_start": 163,
                    "old_length": 7,
                    "new_start": 163,
                    "new_length": 13,
                    "hunk_buggy": "['     ComputationPlacerCreationFunction creation_function) {\\n', '   absl::MutexLock lock(&ComputationPlacer::platform_computation_placer_mutex_);\\n', '   auto* computation_placers = GetPlatformComputationPlacers();\\n', '-  CHECK(computation_placers->find(platform_id) == computation_placers->end());\\n', '   (*computation_placers)[platform_id].creation_function = creation_function;\\n', ' }\\n', ' ']",
                    "hunk_fix": "@@ -163,7 +163,13 @@ StatusOr<DeviceAssignment> ComputationPlacer::AssignDevices(\n     ComputationPlacerCreationFunction creation_function) {\n   absl::MutexLock lock(&ComputationPlacer::platform_computation_placer_mutex_);\n   auto* computation_placers = GetPlatformComputationPlacers();\n-  CHECK(computation_placers->find(platform_id) == computation_placers->end());\n+  if (computation_placers->find(platform_id) != computation_placers->end()) {\n+    // TODO(b/282059652): Consider logging the platform name using\n+    // MultiPlatformManager::PlatformWithId(). No doing that for now to avoid\n+    // introducing unwanted dependency.\n+    LOG(WARNING) << \"computation placer already registered. Please check \"\n+                    \"linkage and avoid linking the same target more than once.\";\n+  }\n   (*computation_placers)[platform_id].creation_function = creation_function;\n }\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 75,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/84d7bf6f64fd9c8677f7f26511ce3031fe8d35a6",
    "date": "2023-05-09T14:44:28-04:00",
    "message": "Add is_numeric to dtypes.cc to check whether a data type is numeric",
    "changes": [
        {
            "name": "dtypes.cc",
            "path": "tensorflow/python/framework/dtypes.cc",
            "patches": [
                {
                    "old_start": 120,
                    "old_length": 6,
                    "new_start": 120,
                    "new_length": 12,
                    "hunk_buggy": "['             return tensorflow::BaseType(self) == tensorflow::DT_BOOL;\\n', '           },\\n', '           \"Returns whether this is a boolean data type.\")\\n', '       .def_property_readonly(\\n', '           \"is_complex\",\\n', '           [](tensorflow::DataType self) {']",
                    "hunk_fix": "@@ -120,6 +120,12 @@ PYBIND11_MODULE(_dtypes, m) {\n             return tensorflow::BaseType(self) == tensorflow::DT_BOOL;\n           },\n           \"Returns whether this is a boolean data type.\")\n+      .def_property_readonly(\n+          \"is_numeric\",\n+          [](tensorflow::DataType self) {\n+            return tensorflow::DataTypeIsNumeric(tensorflow::BaseType(self));\n+          },\n+          \"Returns whether this is a numeric data type.\")\n       .def_property_readonly(\n           \"is_complex\",\n           [](tensorflow::DataType self) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 76,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/cd34289b744040974ebe81e1b1e88f1c752d68e0",
    "date": "2023-05-09T14:42:57-04:00",
    "message": "Update types.h to check if a data type is numeric",
    "changes": [
        {
            "name": "types.h",
            "path": "tensorflow/core/framework/types.h",
            "patches": [
                {
                    "old_start": 469,
                    "old_length": 6,
                    "new_start": 469,
                    "new_length": 11,
                    "hunk_buggy": "['   return kDataTypeIsFloating.Contains(dt);\\n', ' }\\n', ' \\n', \" // Returns true iff 'dt' is a complex type.\\n\", ' constexpr DataTypeSet kDataTypeIsComplex =\\n', '     ToSet(DT_COMPLEX64) | ToSet(DT_COMPLEX128);']",
                    "hunk_fix": "@@ -469,6 +469,11 @@ inline bool DataTypeIsFloating(DataType dt) {\n   return kDataTypeIsFloating.Contains(dt);\n }\n \n+// Returns true iff 'dt' is a numeric type.\n+inline bool DataTypeIsNumeric(DataType dt) {\n+  return kNumberTypes.Contains(dt);\n+}\n+\n // Returns true iff 'dt' is a complex type.\n constexpr DataTypeSet kDataTypeIsComplex =\n     ToSet(DT_COMPLEX64) | ToSet(DT_COMPLEX128);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 77,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/924f80a4fdb34230965a7a8a4476901847463645",
    "date": "2023-05-08T16:20:25-04:00",
    "message": "Add stricter type checking for tf.math.real\n\nFix for tf.math.real so that it only accepts tensors with numeric entries as input. This makes it consistent with its documentation at https://www.tensorflow.org/api_docs/python/tf/math/real and raises a TypeError saying input must have numeric entries when called incorrectly.",
    "changes": [
        {
            "name": "math_ops.py",
            "path": "tensorflow/python/ops/math_ops.py",
            "patches": [
                {
                    "old_start": 822,
                    "old_length": 8,
                    "new_start": 822,
                    "new_length": 10,
                    "hunk_buggy": "['     if input.dtype.is_complex:\\n', '       real_dtype = input.dtype.real_dtype\\n', '       return gen_math_ops.real(input, Tout=real_dtype, name=name)\\n', '-    else:\\n', '       return input\\n', ' \\n', ' \\n', ' @tf_export(\"math.imag\", v1=[\"math.imag\", \"imag\"])']",
                    "hunk_fix": "@@ -822,8 +822,10 @@ def real(input, name=None):\n     if input.dtype.is_complex:\n       real_dtype = input.dtype.real_dtype\n       return gen_math_ops.real(input, Tout=real_dtype, name=name)\n-    else:\n+    elif tf.debugging.is_numeric_tensor(input):\n       return input\n+    else:\n+      raise TypeError(\"input must be a numeric tensor, but got tensor with dtype {}\".format(input.dtype))\n \n \n @tf_export(\"math.imag\", v1=[\"math.imag\", \"imag\"])"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 78,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/47eaa828a1dd4bf50ec4203ef4bbb348b3ef0dd0",
    "date": "2023-05-04T09:01:40+00:00",
    "message": "Add nullptr check",
    "changes": [
        {
            "name": "kernels.cc",
            "path": "tensorflow/c/kernels.cc",
            "patches": [
                {
                    "old_start": 357,
                    "old_length": 6,
                    "new_start": 357,
                    "new_length": 10,
                    "hunk_buggy": "['     return;\\n', '   }\\n', '   const ::tensorflow::Tensor& cc_tensor(cc_ctx->input(i));\\n', '   TF_Tensor* result =\\n', '       ::tensorflow::TF_TensorFromTensor(cc_tensor, &status->status);\\n', '   if (TF_GetCode(status) == TF_OK) {']",
                    "hunk_fix": "@@ -357,6 +357,10 @@ void TF_GetInput(TF_OpKernelContext* ctx, int i, TF_Tensor** tensor,\n     return;\n   }\n   const ::tensorflow::Tensor& cc_tensor(cc_ctx->input(i));\n+  if ((&cc_tensor) == nullptr) {\n+    *tensor = nullptr;\n+    return;\n+  }\n   TF_Tensor* result =\n       ::tensorflow::TF_TensorFromTensor(cc_tensor, &status->status);\n   if (TF_GetCode(status) == TF_OK) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 79,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/8a2e7deb21f02e4072d6b62cf7f447b9264afe01",
    "date": "2023-04-20T11:41:01-07:00",
    "message": "Adjust checks for `type(Tensor)` to isinstance or is_eager/is_symbolic_tensor.\n\nPiperOrigin-RevId: 525801792",
    "changes": [
        {
            "name": "subscribe.py",
            "path": "tensorflow/python/framework/subscribe.py",
            "patches": [
                {
                    "old_start": 42,
                    "old_length": 7,
                    "new_start": 42,
                    "new_length": 7,
                    "hunk_buggy": "['     `TypeError` if undefined type in the tensors structure.\\n', '   \"\"\"\\n', '   tensors_type = type(tensors)\\n', '-  if tensors_type is ops.Tensor:\\n', '     return apply_fn(tensors)\\n', '   elif isinstance(tensors, variables.Variable):\\n', '     return apply_fn(tensors.value())']",
                    "hunk_fix": "@@ -42,7 +42,7 @@ def _recursive_apply(tensors, apply_fn):\n     `TypeError` if undefined type in the tensors structure.\n   \"\"\"\n   tensors_type = type(tensors)\n-  if tensors_type is ops.Tensor:\n+  if isinstance(tensors, ops.Tensor):\n     return apply_fn(tensors)\n   elif isinstance(tensors, variables.Variable):\n     return apply_fn(tensors.value())"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 80,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/ef049bdfc4f307c8b3a9dc480a90a5ff287f3d55",
    "date": "2023-04-17T16:51:00-07:00",
    "message": "Add check for `ResizeOutput` return value in `range.cc`\n\nPiperOrigin-RevId: 524984384",
    "changes": [
        {
            "name": "range.cc",
            "path": "tensorflow/lite/kernels/range.cc",
            "patches": [
                {
                    "old_start": 155,
                    "old_length": 7,
                    "new_start": 155,
                    "new_length": 8,
                    "hunk_buggy": "['       IsConstantOrPersistentTensor(limit) &&\\n', '       IsConstantOrPersistentTensor(delta)) {\\n', '     SetTensorToPersistentRo(output);\\n', '-    ResizeOutput(context, start, limit, delta, output);\\n', ' \\n', '     op_data->noop = true;\\n', '     return EvalImpl(context, start, delta, output);']",
                    "hunk_fix": "@@ -155,7 +155,8 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n       IsConstantOrPersistentTensor(limit) &&\n       IsConstantOrPersistentTensor(delta)) {\n     SetTensorToPersistentRo(output);\n-    ResizeOutput(context, start, limit, delta, output);\n+    TF_LITE_ENSURE_OK(context,\n+                      ResizeOutput(context, start, limit, delta, output));\n \n     op_data->noop = true;\n     return EvalImpl(context, start, delta, output);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 81,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/c2ff14318050e26302785a49a1719d29ddcc91b4",
    "date": "2023-04-12T09:01:41-07:00",
    "message": "[XNNPACK] Fix incorrect check in slice node\n\nbegin+size == input dimension is valid, e.g. input size is 3, begin is 2, size is 1.\n\nPiperOrigin-RevId: 523713369",
    "changes": [
        {
            "name": "xnnpack_delegate.cc",
            "path": "tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc",
            "patches": [
                {
                    "old_start": 4813,
                    "old_length": 10,
                    "new_start": 4813,
                    "new_length": 10,
                    "hunk_buggy": "['                                  size[i], output_shape->data[i], i, node_index);\\n', '         return kTfLiteError;\\n', '       }\\n', '-      if (begin[i] + size[i] >= input_shape->data[i]) {\\n', '         TF_LITE_MAYBE_KERNEL_LOG(logging_context,\\n', '                                  \"begin + size (%\" PRId64 \" + %\" PRId64\\n', '-                                 \") must be less input \"\\n', '                                  \"dimension %d in SLICE node #%d\",\\n', '                                  begin[i], size[i], input_shape->data[i],\\n', '                                  node_index);']",
                    "hunk_fix": "@@ -4813,10 +4813,10 @@ class Subgraph {\n                                  size[i], output_shape->data[i], i, node_index);\n         return kTfLiteError;\n       }\n-      if (begin[i] + size[i] >= input_shape->data[i]) {\n+      if (begin[i] + size[i] > input_shape->data[i]) {\n         TF_LITE_MAYBE_KERNEL_LOG(logging_context,\n                                  \"begin + size (%\" PRId64 \" + %\" PRId64\n-                                 \") must be less input \"\n+                                 \") must not be greater than input \"\n                                  \"dimension %d in SLICE node #%d\",\n                                  begin[i], size[i], input_shape->data[i],\n                                  node_index);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 82,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/edd9fb416e04b8ca4398c4eea65f14dc6704a44a",
    "date": "2023-04-07T11:45:48-07:00",
    "message": "TfLiteTensorCopy returns an error status when src and dest bytes are not equal. So we don't need to check them specifically if we ensure the status of the call to copy (which we should do anyways).\n\nPiperOrigin-RevId: 522647125",
    "changes": [
        {
            "name": "while.cc",
            "path": "tensorflow/lite/kernels/while.cc",
            "patches": [
                {
                    "old_start": 100,
                    "old_length": 8,
                    "new_start": 100,
                    "new_length": 7,
                    "hunk_buggy": "['     if (IsDynamicTensor(dst_tensor)) {\\n', '       TfLiteTensorRealloc(src_tensor->bytes, dst_tensor);\\n', '     }\\n', '-    TF_LITE_ENSURE_EQ(context, src_tensor->bytes, dst_tensor->bytes);\\n', '-    TfLiteTensorCopy(src_tensor, dst_tensor);\\n', '   }\\n', '   return kTfLiteOk;\\n', ' }']",
                    "hunk_fix": "@@ -100,8 +100,7 @@ TfLiteStatus CopyTensorsData(TfLiteContext* context, Subgraph* src_subgraph,\n     if (IsDynamicTensor(dst_tensor)) {\n       TfLiteTensorRealloc(src_tensor->bytes, dst_tensor);\n     }\n-    TF_LITE_ENSURE_EQ(context, src_tensor->bytes, dst_tensor->bytes);\n-    TfLiteTensorCopy(src_tensor, dst_tensor);\n+    TF_LITE_ENSURE_OK(context, TfLiteTensorCopy(src_tensor, dst_tensor));\n   }\n   return kTfLiteOk;\n }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 83,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/65c5dd69676db159ddd3a1fd7b2f6836dfe37f49",
    "date": "2023-03-30T16:40:56-07:00",
    "message": "Add nullptr check for external registration init / prepare / invoke / free.\n\nPiperOrigin-RevId: 520773677",
    "changes": [
        {
            "name": "subgraph.cc",
            "path": "tensorflow/lite/core/subgraph.cc",
            "patches": [
                {
                    "old_start": 1173,
                    "old_length": 6,
                    "new_start": 1173,
                    "new_length": 7,
                    "hunk_buggy": "['     TfLiteRegistration* referenced_registration =\\n', '         &nodes_and_registration_[op_reg.registration_external->node_index]\\n', '              .second;\\n', '     return referenced_registration->init(&context_, buffer, length);\\n', '   }\\n', ' \\n']",
                    "hunk_fix": "@@ -1173,6 +1173,7 @@ void* Subgraph::OpInit(const TfLiteRegistration& op_reg, const char* buffer,\n     TfLiteRegistration* referenced_registration =\n         &nodes_and_registration_[op_reg.registration_external->node_index]\n              .second;\n+    if (referenced_registration->init == nullptr) return nullptr;\n     return referenced_registration->init(&context_, buffer, length);\n   }\n \n"
                },
                {
                    "old_start": 1203,
                    "old_length": 6,
                    "new_start": 1204,
                    "new_length": 18,
                    "hunk_buggy": "['     TfLiteRegistration* referenced_registration =\\n', '         &nodes_and_registration_[op_reg.registration_external->node_index]\\n', '              .second;\\n', '     return referenced_registration->prepare(&context_, node);\\n', '   }\\n', '   if (op_reg.registration_external && op_reg.registration_external->prepare) {\\n']",
                    "hunk_fix": "@@ -1203,6 +1204,18 @@ TfLiteStatus Subgraph::OpPrepare(const TfLiteRegistration& op_reg,\n     TfLiteRegistration* referenced_registration =\n         &nodes_and_registration_[op_reg.registration_external->node_index]\n              .second;\n+    if (referenced_registration->prepare == nullptr) {\n+      if (IsUnresolvedCustomOp(op_reg)) {\n+        ReportError(\n+            \"Encountered unresolved custom op: %s.\\nSee instructions: \"\n+            \"https://www.tensorflow.org/lite/guide/ops_custom \",\n+            op_reg.custom_name ? op_reg.custom_name : \"UnknownOp\");\n+        return kTfLiteUnresolvedOps;\n+      } else {\n+        // Resolved ops can have a null Prepare function.\n+        return kTfLiteOk;\n+      }\n+    }\n     return referenced_registration->prepare(&context_, node);\n   }\n   if (op_reg.registration_external && op_reg.registration_external->prepare) {\n"
                },
                {
                    "old_start": 1257,
                    "old_length": 6,
                    "new_start": 1270,
                    "new_length": 7,
                    "hunk_buggy": "['     TfLiteRegistration* referenced_registration =\\n', '         &nodes_and_registration_[op_reg.registration_external->node_index]\\n', '              .second;\\n', '     return referenced_registration->invoke(&context_, node);\\n', '   }\\n', ' \\n']",
                    "hunk_fix": "@@ -1257,6 +1270,7 @@ TfLiteStatus Subgraph::OpInvoke(const TfLiteRegistration& op_reg,\n     TfLiteRegistration* referenced_registration =\n         &nodes_and_registration_[op_reg.registration_external->node_index]\n              .second;\n+    if (referenced_registration->invoke == nullptr) return kTfLiteError;\n     return referenced_registration->invoke(&context_, node);\n   }\n \n"
                },
                {
                    "old_start": 1289,
                    "old_length": 6,
                    "new_start": 1303,
                    "new_length": 7,
                    "hunk_buggy": "['     TfLiteRegistration* referenced_registration =\\n', '         &nodes_and_registration_[op_reg.registration_external->node_index]\\n', '              .second;\\n', '     return referenced_registration->free(&context_, buffer);\\n', '   }\\n', '   if (op_reg.registration_external && op_reg.registration_external->free &&']",
                    "hunk_fix": "@@ -1289,6 +1303,7 @@ void Subgraph::OpFree(const TfLiteRegistration& op_reg, void* buffer) {\n     TfLiteRegistration* referenced_registration =\n         &nodes_and_registration_[op_reg.registration_external->node_index]\n              .second;\n+    if (referenced_registration->free == nullptr) return;\n     return referenced_registration->free(&context_, buffer);\n   }\n   if (op_reg.registration_external && op_reg.registration_external->free &&"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 84,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/685418cd85e09bc2117fa15bc1b6a75d21248348",
    "date": "2023-03-29T22:25:19-07:00",
    "message": "maxpooling op should check that ksize must be positive.\n\nPiperOrigin-RevId: 520539022",
    "changes": [
        {
            "name": "pooling_ops_common.h",
            "path": "tensorflow/core/kernels/pooling_ops_common.h",
            "patches": [
                {
                    "old_start": 355,
                    "old_length": 6,
                    "new_start": 355,
                    "new_length": 10,
                    "hunk_buggy": "['       OP_REQUIRES(context, ksize_.size() == 4,\\n', '                   errors::InvalidArgument(\"Sliding window ksize field must \"\\n', '                                           \"specify 4 dimensions\"));\\n', '       OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &stride_));\\n', '       OP_REQUIRES(context, stride_.size() == 4,\\n', '                   errors::InvalidArgument(\"Sliding window stride field must \"\\n']",
                    "hunk_fix": "@@ -355,6 +355,10 @@ class MaxPoolingV2Op : public OpKernel {\n       OP_REQUIRES(context, ksize_.size() == 4,\n                   errors::InvalidArgument(\"Sliding window ksize field must \"\n                                           \"specify 4 dimensions\"));\n+      OP_REQUIRES(\n+          context,\n+          ksize_[0] > 0 && ksize_[1] > 0 && ksize_[2] > 0 && ksize_[3] > 0,\n+          errors::InvalidArgument(\"Sliding window ksize must be positive.\"));\n       OP_REQUIRES_OK(context, context->GetAttr(\"strides\", &stride_));\n       OP_REQUIRES(context, stride_.size() == 4,\n                   errors::InvalidArgument(\"Sliding window stride field must \"\n"
                },
                {
                    "old_start": 387,
                    "old_length": 6,
                    "new_start": 391,
                    "new_length": 9,
                    "hunk_buggy": "['     OP_REQUIRES(context, ksize.size() == 4,\\n', '                 errors::InvalidArgument(\"Sliding window ksize field must \"\\n', '                                         \"specify 4 dimensions\"));\\n', '     OP_REQUIRES(context, stride.size() == 4,\\n', '                 errors::InvalidArgument(\"Sliding window stride field must \"\\n', '                                         \"specify 4 dimensions\"));']",
                    "hunk_fix": "@@ -387,6 +391,9 @@ class MaxPoolingV2Op : public OpKernel {\n     OP_REQUIRES(context, ksize.size() == 4,\n                 errors::InvalidArgument(\"Sliding window ksize field must \"\n                                         \"specify 4 dimensions\"));\n+    OP_REQUIRES(\n+        context, ksize[0] > 0 && ksize[1] > 0 && ksize[2] > 0 && ksize[3] > 0,\n+        errors::InvalidArgument(\"Sliding window ksize must be positive.\"));\n     OP_REQUIRES(context, stride.size() == 4,\n                 errors::InvalidArgument(\"Sliding window stride field must \"\n                                         \"specify 4 dimensions\"));"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 85,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/a0dc73569fc193c1ce26a7bd2d4a8776e7b813ac",
    "date": "2023-03-22T05:07:32+00:00",
    "message": "add check for empty cs_prev_tensor",
    "changes": [
        {
            "name": "lstm_ops.cc",
            "path": "tensorflow/core/kernels/rnn/lstm_ops.cc",
            "patches": [
                {
                    "old_start": 424,
                    "old_length": 6,
                    "new_start": 424,
                    "new_length": 10,
                    "hunk_buggy": "['         ctx, cs_prev_tensor->dims() == 2,\\n', '         errors::InvalidArgument(\"cs_prev_tensor must be rank 2 but is rank \",\\n', '                                 cs_prev_tensor->dims(), \".\"));\\n', '     OP_REQUIRES(\\n', '         ctx, h_prev_tensor->dims() == 2,\\n', '         errors::InvalidArgument(\"h_prev_tensor must be rank 2 but is rank \",']",
                    "hunk_fix": "@@ -424,6 +424,10 @@ class LSTMBlockCellOp : public OpKernel {\n         ctx, cs_prev_tensor->dims() == 2,\n         errors::InvalidArgument(\"cs_prev_tensor must be rank 2 but is rank \",\n                                 cs_prev_tensor->dims(), \".\"));\n+    OP_REQUIRES(ctx, \n+        cs_prev_tensor->dim_size(0) > 0 && cs_prev_tensor->dim_size(1) > 0,\n+                errors::InvalidArgument(\"cs_prev_tensor is empty, has shape: (\",\n+                            cs_prev_tensor->dim_size(0), \",\", cs_prev_tensor->dim_size(1), \").\"));\n     OP_REQUIRES(\n         ctx, h_prev_tensor->dims() == 2,\n         errors::InvalidArgument(\"h_prev_tensor must be rank 2 but is rank \","
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 86,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/2465d4e77654f0d4f7799bc46d5fd5812590acc6",
    "date": "2023-03-21T18:59:56-07:00",
    "message": "Add a check in auto-sharding setup and die if the input mesh shape contains more than two shardable dimensions, which is currently not supported.\n\nPiperOrigin-RevId: 518440655",
    "changes": [
        {
            "name": "auto_sharding.h",
            "path": "tensorflow/compiler/xla/hlo/experimental/auto_sharding/auto_sharding.h",
            "patches": [
                {
                    "old_start": 251,
                    "old_length": 6,
                    "new_start": 251,
                    "new_length": 12,
                    "hunk_buggy": "['                        \"device_mesh_shape=\",\\n', '                        absl::StrJoin(device_mesh_shape, \",\")));\\n', '     }\\n', '     if (device_mesh_alpha.empty()) {\\n', '       // Generates simple device_mesh_alpha based on the size of\\n', '       // device_mesh_shape.']",
                    "hunk_fix": "@@ -251,6 +251,12 @@ struct AutoShardingOption {\n                        \"device_mesh_shape=\",\n                        absl::StrJoin(device_mesh_shape, \",\")));\n     }\n+    if (spmd::VectorGreaterThanOneElementCount(device_mesh_shape) > 2) {\n+      return tsl::errors::OutOfRange(\n+          absl::StrCat(\"the auto-sharding pass currently does not support \",\n+                       \"more than two shardable dims: device_mesh_shape=\",\n+                       absl::StrJoin(device_mesh_shape, \",\")));\n+    }\n     if (device_mesh_alpha.empty()) {\n       // Generates simple device_mesh_alpha based on the size of\n       // device_mesh_shape."
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 87,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/c2fc1f2b5a8b8152c43b81cf31394f3e0a2cb837",
    "date": "2023-03-18T01:55:45+00:00",
    "message": "Add null pointer check",
    "changes": [
        {
            "name": "cuda_blas.cc",
            "path": "tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc",
            "patches": [
                {
                    "old_start": 679,
                    "old_length": 6,
                    "new_start": 679,
                    "new_length": 8,
                    "hunk_buggy": "['       static_cast<int>(transa), static_cast<int>(transb), m, n, k, alpha,\\n', '       a.opaque(), lda, b.opaque(), ldb, beta, c->opaque(), ldc);\\n', ' \\n', '   switch (dtype) {\\n', '     case blas::DataType::kHalf: {\\n', ' #if CUDA_VERSION < 7050']",
                    "hunk_fix": "@@ -679,6 +679,8 @@ tsl::Status CUDABlas::DoBlasGemm(Stream *stream, blas::Transpose transa,\n       static_cast<int>(transa), static_cast<int>(transb), m, n, k, alpha,\n       a.opaque(), lda, b.opaque(), ldb, beta, c->opaque(), ldc);\n \n+  CHECK(a.opaque() != nullptr);\n+\n   switch (dtype) {\n     case blas::DataType::kHalf: {\n #if CUDA_VERSION < 7050"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 88,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/80b65ab79bf8dd6ec03c570b59a1208bb27fec24",
    "date": "2023-03-13T15:16:22-07:00",
    "message": "Small fix to axis check for tfl.pack to tosa\n\nThere was an off-by-one error when checking the axis value\nbased on the input rank.\n\nPiperOrigin-RevId: 516334935",
    "changes": [
        {
            "name": "legalize_common.cc",
            "path": "tensorflow/compiler/mlir/tosa/transforms/legalize_common.cc",
            "patches": [
                {
                    "old_start": 208,
                    "old_length": 7,
                    "new_start": 208,
                    "new_length": 7,
                    "hunk_buggy": "['   // Negative values are also allowed up to -(rank(input)+1)\\n', '   // where the axis \"wraps around\".\\n', '   if (axis < 0) axis += input_tensor_rank;\\n', '-  if ((axis < 0) || (axis > (input_tensor_rank + 1))) {\\n', '     (void)rewriter.notifyMatchFailure(op, \"axis out of valid range\");\\n', '     return std::nullopt;\\n', '   }']",
                    "hunk_fix": "@@ -208,7 +208,7 @@ std::optional<Value> convertPackOp(PatternRewriter& rewriter, Operation* op,\n   // Negative values are also allowed up to -(rank(input)+1)\n   // where the axis \"wraps around\".\n   if (axis < 0) axis += input_tensor_rank;\n-  if ((axis < 0) || (axis > (input_tensor_rank + 1))) {\n+  if ((axis < 0) || (axis > input_tensor_rank)) {\n     (void)rewriter.notifyMatchFailure(op, \"axis out of valid range\");\n     return std::nullopt;\n   }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 89,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/076f909b70b251daea6c443c9b1929b9745aed20",
    "date": "2023-03-06T07:36:43-08:00",
    "message": "fix boolean expression in length check\n\nPiperOrigin-RevId: 513891216",
    "changes": [
        {
            "name": "tensor_list_ops.cc",
            "path": "tensorflow/compiler/tf2xla/kernels/tensor_list_ops.cc",
            "patches": [
                {
                    "old_start": 553,
                    "old_length": 7,
                    "new_start": 553,
                    "new_length": 7,
                    "hunk_buggy": "['       OP_REQUIRES(ctx, len == length,\\n', '                   errors::Unimplemented(\"All lengths have to be the same\"));\\n', '     }\\n', '-    OP_REQUIRES(ctx, length,\\n', '                 errors::Unimplemented(\"All lengths must be positive\"));\\n', '     OP_REQUIRES(\\n', '         ctx, element_dims[0] % length == 0,']",
                    "hunk_fix": "@@ -553,7 +553,7 @@ class TensorListSplitOp : public XlaOpKernel {\n       OP_REQUIRES(ctx, len == length,\n                   errors::Unimplemented(\"All lengths have to be the same\"));\n     }\n-    OP_REQUIRES(ctx, length,\n+    OP_REQUIRES(ctx, length > 0,\n                 errors::Unimplemented(\"All lengths must be positive\"));\n     OP_REQUIRES(\n         ctx, element_dims[0] % length == 0,"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 90,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/9a0de0ca6a39f3037e1be6ec740829863bcda3e8",
    "date": "2023-03-02T14:28:58-08:00",
    "message": "[XLA:GPU] Fix type check in IsMatrixMultiplication\n\nPiperOrigin-RevId: 513638308",
    "changes": [
        {
            "name": "ir_emission_utils.cc",
            "path": "tensorflow/compiler/xla/service/gpu/ir_emission_utils.cc",
            "patches": [
                {
                    "old_start": 111,
                    "old_length": 7,
                    "new_start": 111,
                    "new_length": 7,
                    "hunk_buggy": "['        output_primitive_type == F32 || output_primitive_type == F64 ||\\n', '        output_primitive_type == C64 || output_primitive_type == C128) ||\\n', '       (output_primitive_type == S32 && lhs_shape.element_type() == S8 &&\\n', '-       lhs_shape.element_type() == S8);\\n', '   bool shapes_are_valid =\\n', '       type_is_allowed &&\\n', '       IsRank2(lhs_shape, dim_numbers.lhs_batch_dimensions_size()) &&']",
                    "hunk_fix": "@@ -111,7 +111,7 @@ bool IsMatrixMultiplication(const HloInstruction& dot) {\n        output_primitive_type == F32 || output_primitive_type == F64 ||\n        output_primitive_type == C64 || output_primitive_type == C128) ||\n       (output_primitive_type == S32 && lhs_shape.element_type() == S8 &&\n-       lhs_shape.element_type() == S8);\n+       rhs_shape.element_type() == S8);\n   bool shapes_are_valid =\n       type_is_allowed &&\n       IsRank2(lhs_shape, dim_numbers.lhs_batch_dimensions_size()) &&"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 91,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/a21ec782601aca6c7e0461093d72596f26229e44",
    "date": "2023-03-02T08:50:47-08:00",
    "message": "Use `getattr` instead of `isinstance` in `tensor_conversion_registry`.\n\nUsing `isinstance` to check if an object is\nan instance of a Python `typing.Protocol` instead\nof using `getattr`/`hasattr` has negative performance\nimplications.\n\nThis change reverts `tensor_conversion_registry.convert()`\nto use `getattr` for this reason.\n\nPiperOrigin-RevId: 513547008",
    "changes": [
        {
            "name": "tensor_conversion_registry.py",
            "path": "tensorflow/python/framework/tensor_conversion_registry.py",
            "patches": [
                {
                    "old_start": 204,
                    "old_length": 8,
                    "new_start": 204,
                    "new_length": 9,
                    "hunk_buggy": "['   if preferred_dtype is not None:\\n', '     preferred_dtype = dtypes.as_dtype(preferred_dtype)\\n', ' \\n', '-  if isinstance(value, core.TensorProtocol):\\n', '-    return value.__tf_tensor__(dtype, name)\\n', ' \\n', '   for base_type, conversion_func in get(type(value)):\\n', '     # If dtype is None but preferred_dtype is not None, we try to']",
                    "hunk_fix": "@@ -204,8 +204,9 @@ def convert(value,\n   if preferred_dtype is not None:\n     preferred_dtype = dtypes.as_dtype(preferred_dtype)\n \n-  if isinstance(value, core.TensorProtocol):\n-    return value.__tf_tensor__(dtype, name)\n+  overload = getattr(value, \"__tf_tensor__\", None)\n+  if overload is not None:\n+    return overload(dtype, name)  #  pylint: disable=not-callable\n \n   for base_type, conversion_func in get(type(value)):\n     # If dtype is None but preferred_dtype is not None, we try to"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 92,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/ab60b0ee51a8924a0f02b0152cd6a78ba64d3e94",
    "date": "2023-02-22T11:57:10-08:00",
    "message": "[tfg] Fix named-attribute token check.\n\nSince the name tokens are being indexed directly, we should check that list of tokens is not empty to prevent an out-of-bounds error.\n\nPiperOrigin-RevId: 511553573",
    "changes": [
        {
            "name": "convert_attributes.cc",
            "path": "tensorflow/core/ir/importexport/convert_attributes.cc",
            "patches": [
                {
                    "old_start": 16,
                    "old_length": 6,
                    "new_start": 16,
                    "new_length": 7,
                    "hunk_buggy": "[' #include \"tensorflow/core/ir/importexport/convert_attributes.h\"\\n', ' \\n', ' #include <string>\\n', ' \\n', ' #include \"llvm/ADT/StringSet.h\"\\n', ' #include \"llvm/ADT/TypeSwitch.h\"\\n']",
                    "hunk_fix": "@@ -16,6 +16,7 @@ limitations under the License.\n #include \"tensorflow/core/ir/importexport/convert_attributes.h\"\n \n #include <string>\n+#include <vector>\n \n #include \"llvm/ADT/StringSet.h\"\n #include \"llvm/ADT/TypeSwitch.h\"\n"
                },
                {
                    "old_start": 264,
                    "old_length": 6,
                    "new_start": 265,
                    "new_length": 7,
                    "hunk_buggy": "['     // calls.\\n', '     std::vector<std::string> name_tokens =\\n', \"         absl::StrSplit(name, '.', absl::SkipEmpty());\\n\", '     TF_RET_CHECK(name_tokens.size() <= 2);\\n', '     auto it = func_call_attrs.find(name_tokens[0]);\\n', '     if (it == func_call_attrs.end())']",
                    "hunk_fix": "@@ -264,6 +265,7 @@ Status ConvertAttributes(ArrayRef<NamedAttribute> attrs,\n     // calls.\n     std::vector<std::string> name_tokens =\n         absl::StrSplit(name, '.', absl::SkipEmpty());\n+    TF_RET_CHECK(!name_tokens.empty());\n     TF_RET_CHECK(name_tokens.size() <= 2);\n     auto it = func_call_attrs.find(name_tokens[0]);\n     if (it == func_call_attrs.end())"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 93,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/48d3e51a1bd128554dd129251a51b6e12918a604",
    "date": "2023-02-16T14:56:06-08:00",
    "message": "Add a check to HandleFromInput to ensure that the resource isn't empty.\n\nPiperOrigin-RevId: 510250667",
    "changes": [
        {
            "name": "resource_mgr.cc",
            "path": "tensorflow/core/framework/resource_mgr.cc",
            "patches": [
                {
                    "old_start": 380,
                    "old_length": 6,
                    "new_start": 380,
                    "new_length": 7,
                    "hunk_buggy": "['                          \"]\");\\n', ' }\\n', ' \\n', ' const ResourceHandle& HandleFromInput(OpKernelContext* ctx, int input) {\\n', '   return ctx->input(input).flat<ResourceHandle>()(0);\\n', ' }\\n']",
                    "hunk_fix": "@@ -380,6 +380,7 @@ string ContainerInfo::DebugString() const {\n                          \"]\");\n }\n \n+// TODO(b/228388547) users of this method should be migrated to the one below.\n const ResourceHandle& HandleFromInput(OpKernelContext* ctx, int input) {\n   return ctx->input(input).flat<ResourceHandle>()(0);\n }\n"
                },
                {
                    "old_start": 388,
                    "old_length": 6,
                    "new_start": 389,
                    "new_length": 9,
                    "hunk_buggy": "['                        ResourceHandle* handle) {\\n', '   const Tensor* tensor;\\n', '   TF_RETURN_IF_ERROR(ctx->input(input, &tensor));\\n', '   *handle = tensor->flat<ResourceHandle>()(0);\\n', '   return OkStatus();\\n', ' }']",
                    "hunk_fix": "@@ -388,6 +389,9 @@ Status HandleFromInput(OpKernelContext* ctx, StringPiece input,\n                        ResourceHandle* handle) {\n   const Tensor* tensor;\n   TF_RETURN_IF_ERROR(ctx->input(input, &tensor));\n+  if (tensor->NumElements() == 0) {\n+    return errors::InvalidArgument(\"Empty resouce handle\");\n+  }\n   *handle = tensor->flat<ResourceHandle>()(0);\n   return OkStatus();\n }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 94,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/a63f3006f703428ff980748cdbe24d6a13f761e2",
    "date": "2023-02-14T16:05:54-08:00",
    "message": "Skip checking for graph_key in V1 optimizer when running in eager mode.\n\nPiperOrigin-RevId: 509660850",
    "changes": [
        {
            "name": "optimizer.py",
            "path": "tensorflow/python/training/optimizer.py",
            "patches": [
                {
                    "old_start": 941,
                    "old_length": 7,
                    "new_start": 941,
                    "new_length": 12,
                    "hunk_buggy": "['     for (name, _), variable_object in sorted(self._non_slot_dict.items(),\\n', '                                              # Avoid comparing graphs\\n', '                                              key=lambda item: item[0][0]):\\n', '-      if variable_object._graph_key == current_graph_key:  # pylint: disable=protected-access\\n', '         current_graph_non_slot_variables[name] = variable_object\\n', '     current_graph_non_slot_variables.update(\\n', '         super(Optimizer, self)._trackable_children(save_type, **kwargs))']",
                    "hunk_fix": "@@ -941,7 +941,12 @@ class Optimizer(\n     for (name, _), variable_object in sorted(self._non_slot_dict.items(),\n                                              # Avoid comparing graphs\n                                              key=lambda item: item[0][0]):\n-      if variable_object._graph_key == current_graph_key:  # pylint: disable=protected-access\n+      # Skip checking for graph key for eager mode since there's only one graph.\n+      # This is necessary because there are cases where _trackable_children() is\n+      # called in a differenr thread from the main thread (e.g., async\n+      # checkpoint) and hence the default graph key would be different.\n+      if (context.executing_eagerly()\n+          or variable_object._graph_key == current_graph_key):  # pylint: disable=protected-access\n         current_graph_non_slot_variables[name] = variable_object\n     current_graph_non_slot_variables.update(\n         super(Optimizer, self)._trackable_children(save_type, **kwargs))"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 95,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/5ed3c7881f1f039b1bb502eb68c65250de3bbac8",
    "date": "2023-02-08T08:56:20-08:00",
    "message": "Fix ThreadPoolHandle 0 nthreads argument.\n\nIt was reported that a value of 0 leads to a check failure.  Using 0 to indicate\n`port::MaxParallelism`, for consistency with `Dataset`.\n\nFixes #59162\n\nPiperOrigin-RevId: 508092599",
    "changes": [
        {
            "name": "threadpool_dataset_op.cc",
            "path": "tensorflow/core/kernels/data/experimental/threadpool_dataset_op.cc",
            "patches": [
                {
                    "old_start": 100,
                    "old_length": 6,
                    "new_start": 100,
                    "new_length": 12,
                    "hunk_buggy": "['     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"max_intra_op_parallelism\",\\n', '                                      &max_intra_op_parallelism_));\\n', '     OP_REQUIRES_OK(ctx, ValidateNumThreads(num_threads_));\\n', '   }\\n', ' \\n', '   // The resource is deleted from the resource manager only when it is private']",
                    "hunk_fix": "@@ -100,6 +100,12 @@ class ThreadPoolHandleOp : public OpKernel {\n     OP_REQUIRES_OK(ctx, ctx->GetAttr(\"max_intra_op_parallelism\",\n                                      &max_intra_op_parallelism_));\n     OP_REQUIRES_OK(ctx, ValidateNumThreads(num_threads_));\n+\n+    // For consistency with Dataset, use MaxParallelism if 0 threads are\n+    // specified.\n+    if (num_threads_ == 0) {\n+      num_threads_ = port::MaxParallelism();\n+    }\n   }\n \n   // The resource is deleted from the resource manager only when it is private"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 96,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/b677392e4af8095dbde8068b0ceb60bca815e94b",
    "date": "2023-02-07T12:11:02-08:00",
    "message": "Reject non-PjRt devices in PjRtArray::Reshard()\n\nPjRt buffers traditionally support some degree of interoperability between PjRt\nclients (e.g., CPU and TPU). However, this is not universally true between\narbitrary IFRT clients that may use a non-PjRt-compatible runtime. This change\nadds extra checks to make sure that non-PjRt devices are not accidentally used\nin PjRtArray's destination devices.\n\nIn the future, ifrt::Device (currently aliasing to PjRtDevice) will introduce\nstronger type checking.\n\nPiperOrigin-RevId: 507846317",
    "changes": [
        {
            "name": "pjrt_array.cc",
            "path": "tensorflow/compiler/xla/python/pjrt_ifrt/pjrt_array.cc",
            "patches": [
                {
                    "old_start": 276,
                    "old_length": 6,
                    "new_start": 276,
                    "new_length": 13,
                    "hunk_buggy": "['           break;\\n', '       }\\n', '     } else {\\n', '       TF_ASSIGN_OR_RETURN(\\n', '           std::unique_ptr<xla::PjRtBuffer> copied_buffer,\\n', '           pjrt_buffers_[i]->CopyToDevice(new_sharding->devices()[i]));']",
                    "hunk_fix": "@@ -276,6 +276,13 @@ StatusOr<tsl::RCReference<Array>> PjRtArray::Reshard(\n           break;\n       }\n     } else {\n+      if (new_sharding->devices()[i]->client() == nullptr) {\n+        return InvalidArgument(\n+            \"The destination device is owned by a non-PjRt-compatible client. \"\n+            \"To use this Array on the destination device, the Array must be \"\n+            \"first fetched to the host and then sent to the destination \"\n+            \"device.\");\n+      }\n       TF_ASSIGN_OR_RETURN(\n           std::unique_ptr<xla::PjRtBuffer> copied_buffer,\n           pjrt_buffers_[i]->CopyToDevice(new_sharding->devices()[i]));"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 97,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/dd7d791e02396346d98b7b2c58137d7e51756c0c",
    "date": "2023-02-03T15:21:11-08:00",
    "message": "Add isinstance check for eager execution.\n\nPiperOrigin-RevId: 507003564",
    "changes": [
        {
            "name": "ops.py",
            "path": "tensorflow/python/framework/ops.py",
            "patches": [
                {
                    "old_start": 7211,
                    "old_length": 8,
                    "new_start": 7211,
                    "new_length": 9,
                    "hunk_buggy": "['       return graph.capture(v.handle).op, device_only_candidate\\n', '     else:\\n', '       return v.handle.op, device_only_candidate\\n', '-\\n', '-  if isinstance(v, internal.NativeObject):\\n', '     return v.op, None\\n', '   else:\\n', '     return convert_to_tensor(v, as_ref=True).op, None']",
                    "hunk_fix": "@@ -7211,8 +7211,9 @@ def _op_to_colocate_with(v, graph):\n       return graph.capture(v.handle).op, device_only_candidate\n     else:\n       return v.handle.op, device_only_candidate\n-\n-  if isinstance(v, internal.NativeObject):\n+  if isinstance(v, EagerTensor) and not context.executing_eagerly():\n+    return convert_to_tensor(v, as_ref=True).op, None\n+  elif isinstance(v, internal.NativeObject):\n     return v.op, None\n   else:\n     return convert_to_tensor(v, as_ref=True).op, None"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 98,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/9d3cce4c7525bad6743f84302e5f6355a3fd8fe5",
    "date": "2023-01-09T22:12:32+00:00",
    "message": "Fix crash in BlockLSTM\n\nThis PR tries to address the issue raised in 58175 in addressing\nthe crash of BlockLSTM when invalid input is provided.\n\nThis PR fixes 58175.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "changes": [
        {
            "name": "lstm_ops.cc",
            "path": "tensorflow/core/kernels/rnn/lstm_ops.cc",
            "patches": [
                {
                    "old_start": 901,
                    "old_length": 6,
                    "new_start": 901,
                    "new_length": 9,
                    "hunk_buggy": "['   void Compute(OpKernelContext* ctx) override {\\n', '     const Tensor* seq_len_max_tensor = nullptr;\\n', '     OP_REQUIRES_OK(ctx, ctx->input(\"seq_len_max\", &seq_len_max_tensor));\\n', ' \\n', '     const Tensor* x;\\n', '     OP_REQUIRES_OK(ctx, ctx->input(\"x\", &x));\\n']",
                    "hunk_fix": "@@ -901,6 +901,9 @@ class BlockLSTMOp : public OpKernel {\n   void Compute(OpKernelContext* ctx) override {\n     const Tensor* seq_len_max_tensor = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->input(\"seq_len_max\", &seq_len_max_tensor));\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(seq_len_max_tensor->shape()),\n+                errors::InvalidArgument(\"`seq_len_max_tensor` must be rank 0 but is rank \",\n+                                        seq_len_max_tensor->dims()));\n \n     const Tensor* x;\n     OP_REQUIRES_OK(ctx, ctx->input(\"x\", &x));\n"
                },
                {
                    "old_start": 1135,
                    "old_length": 6,
                    "new_start": 1138,
                    "new_length": 9,
                    "hunk_buggy": "['   void Compute(OpKernelContext* ctx) override {\\n', '     const Tensor* seq_len_max_tensor = nullptr;\\n', '     OP_REQUIRES_OK(ctx, ctx->input(\"seq_len_max\", &seq_len_max_tensor));\\n', ' \\n', '     const Tensor* x;\\n', '     OP_REQUIRES_OK(ctx, ctx->input(\"x\", &x));']",
                    "hunk_fix": "@@ -1135,6 +1138,9 @@ class BlockLSTMGradOp : public OpKernel {\n   void Compute(OpKernelContext* ctx) override {\n     const Tensor* seq_len_max_tensor = nullptr;\n     OP_REQUIRES_OK(ctx, ctx->input(\"seq_len_max\", &seq_len_max_tensor));\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(seq_len_max_tensor->shape()),\n+                errors::InvalidArgument(\"`seq_len_max_tensor` must be rank 0 but is rank \",\n+                                        seq_len_max_tensor->dims()));\n \n     const Tensor* x;\n     OP_REQUIRES_OK(ctx, ctx->input(\"x\", &x));"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 99,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/fe4f74018ec6a7dff2718ea59d0f317460c0b3ad",
    "date": "2023-01-04T12:08:26-08:00",
    "message": "Temporarily check for empty proto fields to avoid a crash for old cached traces.\n\nWe can remove this code once we land all the necessary changes and invalidate all the caches.\n\nPiperOrigin-RevId: 499545306",
    "changes": [
        {
            "name": "op_profile_builder.cc",
            "path": "tensorflow/core/profiler/convert/op_profile_builder.cc",
            "patches": [
                {
                    "old_start": 172,
                    "old_length": 6,
                    "new_start": 172,
                    "new_length": 17,
                    "hunk_buggy": "['     uint64_t total_time_ps, Node* node) {\\n', '   DCHECK_EQ(ChildrenTimePs(op_metrics), 0);\\n', ' \\n', '   Metrics* metrics = node->mutable_metrics();\\n', '   // The UI computes flops_rate = raw_flops / raw_time\\n', '   // and memory_bandwidth = raw_bytes_accessed / raw_time. See:']",
                    "hunk_fix": "@@ -172,6 +172,17 @@ void PopulateOpMetricsNode(\n     uint64_t total_time_ps, Node* node) {\n   DCHECK_EQ(ChildrenTimePs(op_metrics), 0);\n \n+  // TODO(dfinchel): remove this temporary change to avoid crash.\n+  // This is only needed while we make an update to proto version that is not\n+  // backwards compatible.\n+  if (peak_mem_gibibytes_per_second_per_core.size() !=\n+      (MemBwType_MAX - MemBwType_MIN + 1)) {\n+    peak_mem_gibibytes_per_second_per_core.clear();\n+    for (int i = MemBwType_MIN; i <= MemBwType_MAX; ++i) {\n+      peak_mem_gibibytes_per_second_per_core.push_back(0);\n+    }\n+  }\n+\n   Metrics* metrics = node->mutable_metrics();\n   // The UI computes flops_rate = raw_flops / raw_time\n   // and memory_bandwidth = raw_bytes_accessed / raw_time. See:"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 100,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/2c72ca8c439d64268e849ef81cde78f464e95ca2",
    "date": "2022-12-22T09:45:40+08:00",
    "message": "add rank checking for ADD, MUL, and DIV",
    "changes": [
        {
            "name": "nnapi_delegate.cc",
            "path": "tensorflow/lite/delegates/nnapi/nnapi_delegate.cc",
            "patches": [
                {
                    "old_start": 2313,
                    "old_length": 6,
                    "new_start": 2313,
                    "new_length": 13,
                    "hunk_buggy": "['       } else {\\n', '         ExpectIsFloatOrQuant8Operator(context, node, &val_ctx);\\n', '       }\\n', '     } break;\\n', '     case kTfLiteBuiltinArgMax:\\n', '     case kTfLiteBuiltinArgMin: {\\n']",
                    "hunk_fix": "@@ -2313,6 +2313,13 @@ bool NNAPIDelegateKernel::Validate(\n       } else {\n         ExpectIsFloatOrQuant8Operator(context, node, &val_ctx);\n       }\n+      const int input0_rank =\n+          context->tensors[node->inputs->data[0]].dims->size;\n+      const int input1_rank =\n+          context->tensors[node->inputs->data[1]].dims->size;\n+      Expect(input0_rank <= 4 && input1_rank <= 4,\n+             NNAPIValidationFailureType::kUnsupportedOperandRank,\n+             \"Input rank must be <= 4\", &val_ctx);\n     } break;\n     case kTfLiteBuiltinArgMax:\n     case kTfLiteBuiltinArgMin: {\n"
                },
                {
                    "old_start": 2372,
                    "old_length": 6,
                    "new_start": 2379,
                    "new_length": 13,
                    "hunk_buggy": "['       } else {\\n', '         ExpectIsFloatOrQuant8Operator(context, node, &val_ctx);\\n', '       }\\n', '     } break;\\n', '     case kTfLiteBuiltinAveragePool2d: {\\n', '       ExpectMaxOpVersion(version, 2, &val_ctx);\\n']",
                    "hunk_fix": "@@ -2372,6 +2379,13 @@ bool NNAPIDelegateKernel::Validate(\n       } else {\n         ExpectIsFloatOrQuant8Operator(context, node, &val_ctx);\n       }\n+      const int input0_rank =\n+          context->tensors[node->inputs->data[0]].dims->size;\n+      const int input1_rank =\n+          context->tensors[node->inputs->data[1]].dims->size;\n+      Expect(input0_rank <= 4 && input1_rank <= 4,\n+             NNAPIValidationFailureType::kUnsupportedOperandRank,\n+             \"Input rank must be <= 4\", &val_ctx);\n     } break;\n     case kTfLiteBuiltinAveragePool2d: {\n       ExpectMaxOpVersion(version, 2, &val_ctx);\n"
                },
                {
                    "old_start": 2804,
                    "old_length": 6,
                    "new_start": 2818,
                    "new_length": 13,
                    "hunk_buggy": "['       Expect(context->tensors[node->inputs->data[0]].type == kTfLiteFloat32,\\n', '              NNAPIValidationFailureType::kUnsupportedInputType,\\n', '              \"NNAPI only support float div.\", &val_ctx);\\n', '     } break;\\n', '     case kTfLiteBuiltinPad:\\n', '     case kTfLiteBuiltinPadv2: {']",
                    "hunk_fix": "@@ -2804,6 +2818,13 @@ bool NNAPIDelegateKernel::Validate(\n       Expect(context->tensors[node->inputs->data[0]].type == kTfLiteFloat32,\n              NNAPIValidationFailureType::kUnsupportedInputType,\n              \"NNAPI only support float div.\", &val_ctx);\n+      const int input0_rank =\n+          context->tensors[node->inputs->data[0]].dims->size;\n+      const int input1_rank =\n+          context->tensors[node->inputs->data[1]].dims->size;\n+      Expect(input0_rank <= 4 && input1_rank <= 4,\n+             NNAPIValidationFailureType::kUnsupportedOperandRank,\n+             \"Input rank must be <= 4\", &val_ctx);\n     } break;\n     case kTfLiteBuiltinPad:\n     case kTfLiteBuiltinPadv2: {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 101,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/662128e8ca3411286b234553a7efc1356353d0f5",
    "date": "2022-12-22T09:22:55+08:00",
    "message": "add rank checking for MEAN op\n\nThe MEAN op of NNAPI only supports a tensor with rank <= 4.\nCheck the rank of the input tensor before delegating the op.\n\n```\n...\n12-22 09:22:24.514  6130  6130 E ModelBuilder: Invalid Operation: NN_RET_CHECK failed (packages/modules/NeuralNetworks/common/types/operations/src/SimpleMath.cpp:30): inputRank <= 4u (inputRank = 5, 4u = 4) Unsupported input tensor rank for operation MEAN\n12-22 09:22:24.514  6130  6130 E tflite  : NN API returned error ANEURALNETWORKS_BAD_DATA at line 1131 while adding operation.\n12-22 09:22:24.515  6130  6130 E tflite  : Restored original execution plan after delegate application failure.\n...\n```",
    "changes": [
        {
            "name": "nnapi_delegate.cc",
            "path": "tensorflow/lite/delegates/nnapi/nnapi_delegate.cc",
            "patches": [
                {
                    "old_start": 3075,
                    "old_length": 6,
                    "new_start": 3075,
                    "new_length": 10,
                    "hunk_buggy": "['              NNAPIValidationFailureType::kUnsupportedOutputType,\\n', '              \"NNAPI does not support generating a scalar as output for MEAN.\",\\n', '              &val_ctx);\\n', '     } break;\\n', '     case kTfLiteBuiltinEmbeddingLookup: {\\n', '       ExpectOpVersion(version, 1, &val_ctx);']",
                    "hunk_fix": "@@ -3075,6 +3075,10 @@ bool NNAPIDelegateKernel::Validate(\n              NNAPIValidationFailureType::kUnsupportedOutputType,\n              \"NNAPI does not support generating a scalar as output for MEAN.\",\n              &val_ctx);\n+      Expect(context->tensors[node->inputs->data[0]].dims->size <= 4,\n+             NNAPIValidationFailureType::kUnsupportedOperandValue,\n+             \"NNAPI does not support mean of a tensor with rank > 4\",\n+             &val_ctx);\n     } break;\n     case kTfLiteBuiltinEmbeddingLookup: {\n       ExpectOpVersion(version, 1, &val_ctx);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 102,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/b8431494de404b5f4def7303fb8efd6ba3575ef9",
    "date": "2022-12-01T14:41:46-08:00",
    "message": "Fix error log messages in data type checks\n\nPiperOrigin-RevId: 492303173",
    "changes": [
        {
            "name": "xnnpack_delegate.cc",
            "path": "tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc",
            "patches": [
                {
                    "old_start": 131,
                    "old_length": 9,
                    "new_start": 131,
                    "new_length": 9,
                    "hunk_buggy": "['       if (zero_point < std::numeric_limits<uint8_t>::min() ||\\n', '           zero_point > std::numeric_limits<uint8_t>::max()) {\\n', '         TF_LITE_KERNEL_LOG(context,\\n', '-                           \"unsupported zero-point value (%f) for UINT8 tensor \"\\n', '                            \"%d in XNNPACK delegate\",\\n', '-                           scale, t);\\n', '         return xnn_datatype_invalid;\\n', '       }\\n', ' \\n']",
                    "hunk_fix": "@@ -131,9 +131,9 @@ xnn_datatype GetXNNPackDatatype(TfLiteContext* context,\n       if (zero_point < std::numeric_limits<uint8_t>::min() ||\n           zero_point > std::numeric_limits<uint8_t>::max()) {\n         TF_LITE_KERNEL_LOG(context,\n-                           \"unsupported zero-point value (%f) for UINT8 tensor \"\n+                           \"unsupported zero-point value (%d) for UINT8 tensor \"\n                            \"%d in XNNPACK delegate\",\n-                           scale, t);\n+                           zero_point, t);\n         return xnn_datatype_invalid;\n       }\n \n"
                },
                {
                    "old_start": 192,
                    "old_length": 7,
                    "new_start": 192,
                    "new_length": 7,
                    "hunk_buggy": "['         if (zero_point < std::numeric_limits<int8_t>::min() ||\\n', '             zero_point > std::numeric_limits<int8_t>::max()) {\\n', '           TF_LITE_KERNEL_LOG(context,\\n', '-                             \"unsupported zero-point value (%f) for INT8 \"\\n', '                              \"tensor %d in XNNPACK delegate\",\\n', '                              zero_point, t);\\n', '           return xnn_datatype_invalid;']",
                    "hunk_fix": "@@ -192,7 +192,7 @@ xnn_datatype GetXNNPackDatatype(TfLiteContext* context,\n         if (zero_point < std::numeric_limits<int8_t>::min() ||\n             zero_point > std::numeric_limits<int8_t>::max()) {\n           TF_LITE_KERNEL_LOG(context,\n-                             \"unsupported zero-point value (%f) for INT8 \"\n+                             \"unsupported zero-point value (%d) for INT8 \"\n                              \"tensor %d in XNNPACK delegate\",\n                              zero_point, t);\n           return xnn_datatype_invalid;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 103,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/1908d7ef706f0f3f8c7a300068355bf795fb3d17",
    "date": "2022-11-14T01:23:12-08:00",
    "message": "Fix out-of-bounds StringPiece access in ForwardNUTF8CharPositions()\n\nEven a simple invocation like 'int p = 0; ForwardNUTF8CharPositions(\"a\", 1, &p);' will cause an invalid access to in[1]. Checking for *pos < size before that access fixes this issue.\n\nLuckily the invalid access has only ever happened when the *pos < size part of the condition is false and thus the outcome of the IsTrailByte check is irrelevant. Thus this probably hasn't had any observable impact except when extra guards against invalid memory accesses are enabled.\n\nPiperOrigin-RevId: 488292704",
    "changes": [
        {
            "name": "string_util.h",
            "path": "tensorflow/core/kernels/string_util.h",
            "patches": [
                {
                    "old_start": 56,
                    "old_length": 7,
                    "new_start": 56,
                    "new_length": 7,
                    "hunk_buggy": "['     // move forward one utf-8 character\\n', '     do {\\n', '       ++*pos;\\n', '-    } while (IsTrailByte(in[*pos]) && *pos < size);\\n', '     ++utf8_chars_counted;\\n', '   }\\n', '   return utf8_chars_counted == num_utf8_chars_to_shift;']",
                    "hunk_fix": "@@ -56,7 +56,7 @@ bool ForwardNUTF8CharPositions(const StringPiece in,\n     // move forward one utf-8 character\n     do {\n       ++*pos;\n-    } while (IsTrailByte(in[*pos]) && *pos < size);\n+    } while (*pos < size && IsTrailByte(in[*pos]));\n     ++utf8_chars_counted;\n   }\n   return utf8_chars_counted == num_utf8_chars_to_shift;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 104,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/258233804f2bc92b4bdb9714b396aed34b53ff0d",
    "date": "2022-11-10T09:37:36-08:00",
    "message": "sanity check of empty tensor on avgpool3d_grad",
    "changes": [
        {
            "name": "mkl_avgpooling_op.cc",
            "path": "tensorflow/core/kernels/mkl/mkl_avgpooling_op.cc",
            "patches": [
                {
                    "old_start": 193,
                    "old_length": 6,
                    "new_start": 193,
                    "new_length": 11,
                    "hunk_buggy": "['       const Tensor& grad_tensor =\\n', '           MklGetInput(context, kInputTensorIndexInputGradient);\\n', ' \\n', '       MklDnnShape orig_input_mkl_shape, grad_mkl_shape;\\n', '       GetMklShape(context, kInputTensorIndexInputShape, &orig_input_mkl_shape,\\n', '                   this->native_format_);']",
                    "hunk_fix": "@@ -193,6 +193,11 @@ class MklAvgPoolingGradOp : public MklPoolingBackwardOpBase<T> {\n       const Tensor& grad_tensor =\n           MklGetInput(context, kInputTensorIndexInputGradient);\n \n+      // For empty tensor, avg_pool_3d_grad in oneDNN doesn't handle this case\n+      if (orig_input_tensor.NumElements() == 0 ||\n+          grad_tensor.NumElements() == 0)\n+        return;\n+      \n       MklDnnShape orig_input_mkl_shape, grad_mkl_shape;\n       GetMklShape(context, kInputTensorIndexInputShape, &orig_input_mkl_shape,\n                   this->native_format_);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 105,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/c676a2d7ce8884aad59ca9cd5f45e9b851574cac",
    "date": "2022-09-30T11:21:09-07:00",
    "message": "[tensorflow] Add a check that strided slice op strides argument has reasonable size\n\nPiperOrigin-RevId: 478036251",
    "changes": [
        {
            "name": "strided_slice_op.cc",
            "path": "tensorflow/core/util/strided_slice_op.cc",
            "patches": [
                {
                    "old_start": 81,
                    "old_length": 7,
                    "new_start": 81,
                    "new_length": 15,
                    "hunk_buggy": "[' static Status TF_MUST_USE_RESULT BuildDenseSpec(\\n', '     const StridedSliceSparseSpec& sparse, StridedSliceDenseSpec* dense) {\\n', '   if (dense->dims < 0) {\\n', '-    return errors::InvalidArgument(\"Unexpected negative dense.dims\");\\n', '   }\\n', ' \\n', '   // Build expanded begin, end, strides, begin_mask, end_mask']",
                    "hunk_fix": "@@ -81,7 +81,15 @@ template <class T>\n static Status TF_MUST_USE_RESULT BuildDenseSpec(\n     const StridedSliceSparseSpec& sparse, StridedSliceDenseSpec* dense) {\n   if (dense->dims < 0) {\n-    return errors::InvalidArgument(\"Unexpected negative dense.dims\");\n+    return errors::InvalidArgument(\"Unexpected negative dense.dims: %d\",\n+                                   dense->dims);\n+  }\n+\n+  if (dense->dims >= 1024) {\n+    // We do not expect to see tensors with rank >= 1024, it must mean that\n+    // there is a bug somewhere.\n+    return errors::InvalidArgument(\"Unexpected large dense.dims: %d\",\n+                                   dense->dims);\n   }\n \n   // Build expanded begin, end, strides, begin_mask, end_mask"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 106,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/c56d0cd8ce8239ee369fac1ae6b9cae67fd4c447",
    "date": "2022-09-29T12:48:46-07:00",
    "message": "Avoid signed integer overflow when loading tensors with both 0 and large dims.\n\n`TensorShapeBase` ensures `num_elements` doesn't overflow when adding a new\ndimension. However, this check is insufficient to prevent other functions that\nuse a different multiplication order from hitting an overflow _if any of the\ndimensions are 0_. For example, Eigen currently multiplies dimensions in\nreverse order, so dimensions of (0, 4294967296,4294967296) will trigger an\noverflow in Eigen code.\n\nTo prevent overflow for all multiplication orders, we can that `num_elements`\ndoesn't overflow if zero dimensions are skipped.\n\nPiperOrigin-RevId: 477796726",
    "changes": [
        {
            "name": "tensor_shape.cc",
            "path": "tensorflow/core/framework/tensor_shape.cc",
            "patches": [
                {
                    "old_start": 168,
                    "old_length": 12,
                    "new_start": 168,
                    "new_length": 27,
                    "hunk_buggy": "['   } else {\\n', '     out->set_ndims_byte(0);\\n', '     out->set_num_elements(1);\\n', '     Status s = OkStatus();\\n', '     for (const auto& d : proto.dim()) {\\n', '       s = out->AddDimWithStatus(d.size());\\n', '       if (!s.ok()) {\\n', '         return s;\\n', '       }\\n', '     }\\n', '   }\\n', '   return OkStatus();']",
                    "hunk_fix": "@@ -168,12 +168,27 @@ Status TensorShapeBase<Shape>::BuildTensorShapeBase(\n   } else {\n     out->set_ndims_byte(0);\n     out->set_num_elements(1);\n+    int64_t num_elements_excluding_zero_dims = 1;\n     Status s = OkStatus();\n     for (const auto& d : proto.dim()) {\n       s = out->AddDimWithStatus(d.size());\n       if (!s.ok()) {\n         return s;\n       }\n+      // If one of the dimensions has size 0, multiplying the dimensions in\n+      // ascending order isn't sufficient to prevent all multiplication orders\n+      // from overflowing. To do that, we need to check that there would be no\n+      // overflow if all zero-length dimensions were multiplied last, which is\n+      // equivalent to ensuring that there's no overflow if zero-length\n+      // dimensions are skipped. Unknown dimensions are also ignored.\n+      if (d.size() > 0) {\n+        num_elements_excluding_zero_dims =\n+            MultiplyWithoutOverflow(num_elements_excluding_zero_dims, d.size());\n+        if (TF_PREDICT_FALSE(num_elements_excluding_zero_dims < 0)) {\n+          return errors::InvalidArgument(\n+              \"Encountered overflow when multiplying shape dimensions\");\n+        }\n+      }\n     }\n   }\n   return OkStatus();"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 107,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/e6df768b81e973f2123bc83a18a60773fc4da99e",
    "date": "2022-08-22T16:49:41-07:00",
    "message": "[TFG] Fix IsAdd string type check in tf_op_names\n\nPiperOrigin-RevId: 469316572",
    "changes": [
        {
            "name": "tf_op_names.cc",
            "path": "tensorflow/core/ir/tf_op_names.cc",
            "patches": [
                {
                    "old_start": 25,
                    "old_length": 7,
                    "new_start": 25,
                    "new_length": 8,
                    "hunk_buggy": "['   StringAttr op_name = op->getName().getIdentifier();\\n', ' \\n', '   if (op_name == add_v2_) return true;\\n', '-  if (op_name == add_) return !op->getAttrOfType<StringAttr>(\"T\");\\n', '   return false;\\n', ' }\\n', ' ']",
                    "hunk_fix": "@@ -25,7 +25,8 @@ bool TFGraphDialect::IsAdd(TFOp op) const {\n   StringAttr op_name = op->getName().getIdentifier();\n \n   if (op_name == add_v2_) return true;\n-  if (op_name == add_) return !op->getAttrOfType<StringAttr>(\"T\");\n+  if (op_name == add_)\n+    return !op->getAttrOfType<TypeAttr>(\"T\").getValue().isa<StringType>();\n   return false;\n }\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 108,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/eb2ddc0debb7e1b0c9ea68c817ca05fd59dc7914",
    "date": "2022-07-28T22:48:28-07:00",
    "message": "In TF2XLA EnsureShape kernel, don't check whether the original tensor has dynamic shapes as it is much more expensive than just blindly clear out dynamic dimension.\n\nPiperOrigin-RevId: 464003792",
    "changes": [
        {
            "name": "ensure_shape_op.cc",
            "path": "tensorflow/compiler/tf2xla/kernels/ensure_shape_op.cc",
            "patches": [
                {
                    "old_start": 48,
                    "old_length": 14,
                    "new_start": 48,
                    "new_length": 15,
                    "hunk_buggy": "['                                 expected_shape_.DebugString(), \".\"));\\n', ' \\n', '     // If the shape dimension in `expected_shape_` is already static, we would\\n', '-    // remove the dynamic dimensions in XLA dynamic padder.\\n', '     xla::XlaOp tensor = ctx->Input(0);\\n', '-    std::vector<bool> dynamic_dims;\\n', '-    OP_REQUIRES_OK(ctx,\\n', '-                   ctx->ResolveInputDynamismIntoPredVector(0, &dynamic_dims));\\n', '     for (int i = 0; i < expected_shape_.dims(); ++i) {\\n', '-      if (expected_shape_.dim_size(i) > 0 && dynamic_dims[i]) {\\n', '-        VLOG(1) << \"RemoveDynamicDimension: \" << i;\\n', '         tensor = xla::RemoveDynamicDimension(tensor, i);\\n', '       }\\n', '     }']",
                    "hunk_fix": "@@ -48,14 +48,15 @@ class EnsureShapeOp : public XlaOpKernel {\n                                 expected_shape_.DebugString(), \".\"));\n \n     // If the shape dimension in `expected_shape_` is already static, we would\n-    // remove the dynamic dimensions in XLA dynamic padder.\n+    // remove the dynamic dimensions in XLA dynamic padder. Here we don't check\n+    // whether the original input has dynamic shapes, because\n+    // `ctx->ResolveInputDynamismIntoPredVector` runs a DFS underneath which is\n+    // more expensive.\n     xla::XlaOp tensor = ctx->Input(0);\n-    std::vector<bool> dynamic_dims;\n-    OP_REQUIRES_OK(ctx,\n-                   ctx->ResolveInputDynamismIntoPredVector(0, &dynamic_dims));\n     for (int i = 0; i < expected_shape_.dims(); ++i) {\n-      if (expected_shape_.dim_size(i) > 0 && dynamic_dims[i]) {\n-        VLOG(1) << \"RemoveDynamicDimension: \" << i;\n+      if (expected_shape_.dim_size(i) > 0) {\n+        VLOG(1) << \"RemoveDynamicDimension: \" << i << \" of shape \"\n+                << shape.DebugString();\n         tensor = xla::RemoveDynamicDimension(tensor, i);\n       }\n     }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 109,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/f22ca1dc88c70a0dc5696c37e6a2de6bcf8d60c7",
    "date": "2022-06-07T09:33:16-07:00",
    "message": "Avoid segfault when init_value is not on default_mesh.\n\nTo actually fix the segfault in lower level (e.g. directly users of VarHandleOp),\nI tried to add a validation in SPMD of AssignValueOp, but turns out it only\nknows the resource_layout is an 'empty' layout without any mesh information.\n\nSo there is not enough information to compare if the mesh is correct. We\nshall start tracking mesh of empty layout -- but changing the data model at\nthis point is not very easy to do or to justify.\n\nPiperOrigin-RevId: 453453817",
    "changes": [
        {
            "name": "d_variable.py",
            "path": "tensorflow/dtensor/python/d_variable.py",
            "patches": [
                {
                    "old_start": 14,
                    "old_length": 6,
                    "new_start": 14,
                    "new_length": 7,
                    "hunk_buggy": "[' # ==============================================================================\\n', ' \"\"\"DTensor variable and saveable.\"\"\"\\n', ' \\n', ' import functools\\n', ' \\n', ' from tensorflow.dtensor.python import api\\n']",
                    "hunk_fix": "@@ -14,6 +14,7 @@\n # ==============================================================================\n \"\"\"DTensor variable and saveable.\"\"\"\n \n+import contextlib\n import functools\n \n from tensorflow.dtensor.python import api\n"
                },
                {
                    "old_start": 169,
                    "old_length": 8,
                    "new_start": 170,
                    "new_length": 6,
                    "hunk_buggy": "['     #     # translate that into a placement for the eager VarHandleOp.\\n', '     #     variable_device = _dtensor_device().name\\n', '     with ops.device(variable_device):\\n', '-      super(DVariable, self).__init__(\\n', '-          initial_value, *args, dtype=dtype, **kwargs)\\n', '       # If initial tensor assigned to DVariable is DTensor, record the layout of\\n', '       # the resource so that this can be queried.\\n', '       self.layout = None\\n']",
                    "hunk_fix": "@@ -169,8 +170,6 @@ class DVariable(resource_variable_ops.ResourceVariable):\n     #     # translate that into a placement for the eager VarHandleOp.\n     #     variable_device = _dtensor_device().name\n     with ops.device(variable_device):\n-      super(DVariable, self).__init__(\n-          initial_value, *args, dtype=dtype, **kwargs)\n       # If initial tensor assigned to DVariable is DTensor, record the layout of\n       # the resource so that this can be queried.\n       self.layout = None\n"
                },
                {
                    "old_start": 183,
                    "old_length": 6,
                    "new_start": 182,
                    "new_length": 10,
                    "hunk_buggy": "['           # is called within DTensor device scope or not.\\n', '           self.layout = None\\n', '           pass\\n', ' \\n', '   @property\\n', '   def save_as_bf16(self):']",
                    "hunk_fix": "@@ -183,6 +182,10 @@ class DVariable(resource_variable_ops.ResourceVariable):\n           # is called within DTensor device scope or not.\n           self.layout = None\n           pass\n+      mesh = self.layout.mesh if self.layout else None\n+      with api.run_on(mesh) if mesh else contextlib.nullcontext():\n+        super(DVariable, self).__init__(\n+            initial_value, *args, dtype=dtype, **kwargs)\n \n   @property\n   def save_as_bf16(self):"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 110,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/580140611a47413dcf6373deb1250c0ed605e873",
    "date": "2022-05-23T13:53:05-07:00",
    "message": "[XLA] Do not check fail in proto copy from if the backend config proto and desired proto type do not match.\n\nPiperOrigin-RevId: 450516623",
    "changes": [
        {
            "name": "hlo_instruction.cc",
            "path": "tensorflow/compiler/xla/service/hlo_instruction.cc",
            "patches": [
                {
                    "old_start": 4272,
                    "old_length": 8,
                    "new_start": 4272,
                    "new_length": 10,
                    "hunk_buggy": "['   proto->Clear();\\n', ' \\n', '   if (auto* proto_ptr = backend_config_.GetProtoPtr()) {\\n', '-    proto->CopyFrom(*proto_ptr);\\n', '-    return Status::OK();\\n', '   }\\n', ' \\n', '   auto& raw_string = raw_backend_config_string();']",
                    "hunk_fix": "@@ -4272,8 +4272,10 @@ Status HloInstruction::GetBackendConfigInternal(\n   proto->Clear();\n \n   if (auto* proto_ptr = backend_config_.GetProtoPtr()) {\n-    proto->CopyFrom(*proto_ptr);\n-    return Status::OK();\n+    if (proto_ptr->GetDescriptor() == proto->GetDescriptor()) {\n+      proto->CopyFrom(*proto_ptr);\n+      return Status::OK();\n+    }\n   }\n \n   auto& raw_string = raw_backend_config_string();"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 111,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/a68f57a24203fd49c4a5c4a8f51098d4415a93f8",
    "date": "2022-05-11T11:21:59-07:00",
    "message": "[XNNPACK] Add missing return when output channels do not match in TransposeConvolution\n\nAdd a check that input channels in the filter and tensor match.\n\nPiperOrigin-RevId: 448040780",
    "changes": [
        {
            "name": "xnnpack_delegate.cc",
            "path": "tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc",
            "patches": [
                {
                    "old_start": 4296,
                    "old_length": 6,
                    "new_start": 4296,
                    "new_length": 15,
                    "hunk_buggy": "['           \"doesn\\'t match output shape channel dimension (%d) in node #%d: \"\\n', '           \"4 dimensions expected\",\\n', '           output_channels, output_tensor_channels, node_index);\\n', '     }\\n', ' \\n', '     int padding_top = 0;']",
                    "hunk_fix": "@@ -4296,6 +4296,15 @@ class Subgraph {\n           \"doesn't match output shape channel dimension (%d) in node #%d: \"\n           \"4 dimensions expected\",\n           output_channels, output_tensor_channels, node_index);\n+      return kTfLiteError;\n+    }\n+    if (input_channels != input_tensor_dims[3]) {\n+      TF_LITE_MAYBE_KERNEL_LOG(\n+          logging_context,\n+          \"transpose convolution kernel input channel dimension (%d) \"\n+          \"doesn't match filter input channel (%d) in node #%d\",\n+          input_channels, input_tensor_dims[3]);\n+      return kTfLiteError;\n     }\n \n     int padding_top = 0;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 112,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/08370e76523d5bece9ab28e7a9a902932e9a2cb9",
    "date": "2022-04-20T22:55:36+10:00",
    "message": "Fix small issues with DispatchToVectorized\n\n- Changes the base case of template recursion from VecSize=0 to\n  VecSize=1, because VecSize=0 will never be reached.\n- Adds handling of `alignment_of(zero/nullptr)`, which should be treated\n  as infinitely aligned.\n- Adds checks for preconditions.",
    "changes": [
        {
            "name": "gpu_kernel_helper.h",
            "path": "tensorflow/core/util/gpu_kernel_helper.h",
            "patches": [
                {
                    "old_start": 297,
                    "old_length": 6,
                    "new_start": 297,
                    "new_length": 10,
                    "hunk_buggy": "[' // Returns the maximum power-of-two alignment (in units of elements, not bytes)\\n', ' // of a stride or pointer value.\\n', ' inline int64_t alignment_of(int64_t element_stride) {\\n', '   return element_stride & -element_stride;\\n', ' }\\n', ' \\n']",
                    "hunk_fix": "@@ -297,6 +297,10 @@ class alignas(alignof(T) * N) AlignedVector {\n // Returns the maximum power-of-two alignment (in units of elements, not bytes)\n // of a stride or pointer value.\n inline int64_t alignment_of(int64_t element_stride) {\n+  // A zero/nullptr value means that the stride/pointer is not used, so it\n+  // effectively has infinite alignment.\n+  constexpr int64_t kMaxAlignment = 512;\n+  if (element_stride == 0) return kMaxAlignment;\n   return element_stride & -element_stride;\n }\n \n"
                },
                {
                    "old_start": 328,
                    "old_length": 7,
                    "new_start": 332,
                    "new_length": 7,
                    "hunk_buggy": "['   }\\n', ' };\\n', ' template <template <int vec_size> class Functor>\\n', '-struct DispatchToVectorizedHelper<0, Functor> {\\n', '   template <typename... Args>\\n', '   Status operator()(int64_t max_vec_size, Args&&... args) const {\\n', '     return Functor<1>()(std::forward<Args>(args)...);\\n']",
                    "hunk_fix": "@@ -328,7 +332,7 @@ struct DispatchToVectorizedHelper {\n   }\n };\n template <template <int vec_size> class Functor>\n-struct DispatchToVectorizedHelper<0, Functor> {\n+struct DispatchToVectorizedHelper<1, Functor> {\n   template <typename... Args>\n   Status operator()(int64_t max_vec_size, Args&&... args) const {\n     return Functor<1>()(std::forward<Args>(args)...);\n"
                },
                {
                    "old_start": 340,
                    "old_length": 8,
                    "new_start": 344,
                    "new_length": 16,
                    "hunk_buggy": "[' // Calls Functor<vec_size>()(args...) with vec_size set to the optimal GPU\\n', ' // vector instruction size for type T that is <= max_vec_size. The max_vec_size\\n', ' // argument should be set to the minimum alignment of all relevant parameters.\\n', ' template <typename T, template <int vec_size> class Functor, typename... Args>\\n', ' Status DispatchToVectorized(int64_t max_vec_size, Args&&... args) {\\n', '   constexpr const int kOptimalVecSizeBytes = 16;\\n', '   // The optimal number of (aligned) elements of T to load/store in a\\n', '   // single instruction inside a kernel.']",
                    "hunk_fix": "@@ -340,8 +344,16 @@ struct DispatchToVectorizedHelper<0, Functor> {\n // Calls Functor<vec_size>()(args...) with vec_size set to the optimal GPU\n // vector instruction size for type T that is <= max_vec_size. The max_vec_size\n // argument should be set to the minimum alignment of all relevant parameters.\n+// Requires sizeof(T) to be a power of 2.\n template <typename T, template <int vec_size> class Functor, typename... Args>\n Status DispatchToVectorized(int64_t max_vec_size, Args&&... args) {\n+  static_assert((sizeof(T) & (sizeof(T) - 1)) == 0,\n+                \"sizeof(T) must be a power of 2\");\n+  if (max_vec_size <= 0) {\n+    return errors::InvalidArgument(\"DispatchToVectorized: max_vec_size (\",\n+                                   max_vec_size,\n+                                   \") must be greater than zero.\");\n+  }\n   constexpr const int kOptimalVecSizeBytes = 16;\n   // The optimal number of (aligned) elements of T to load/store in a\n   // single instruction inside a kernel."
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 113,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/6b9189483513b0c663e23485834be64f51b076e4",
    "date": "2022-04-19T15:46:27-07:00",
    "message": "Add device compatibility check for fusion.\n\n_FusedMatMul is not supported by GPU currently.\n\nPiperOrigin-RevId: 442931704",
    "changes": [
        {
            "name": "fused_kernel_matcher.cc",
            "path": "tensorflow/compiler/mlir/tensorflow/transforms/fused_kernel_matcher.cc",
            "patches": [
                {
                    "old_start": 15,
                    "old_length": 6,
                    "new_start": 15,
                    "new_length": 7,
                    "hunk_buggy": "[' \\n', ' #include <cstdio>\\n', ' #include <iostream>\\n', ' \\n', ' #include \"llvm/ADT/StringRef.h\"\\n', ' #include \"mlir/Dialect/Func/IR/FuncOps.h\"  // from @llvm-project\\n']",
                    "hunk_fix": "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include <cstdio>\n #include <iostream>\n+#include <string>\n \n #include \"llvm/ADT/StringRef.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"  // from @llvm-project\n"
                },
                {
                    "old_start": 105,
                    "old_length": 6,
                    "new_start": 106,
                    "new_length": 13,
                    "hunk_buggy": "['     return true;\\n', '   }\\n', ' \\n', '   LogicalResult matchAndRewrite(SrcOpT contraction,\\n', '                                 PatternRewriter &rewriter) const override {\\n', '     auto context = rewriter.getContext();\\n']",
                    "hunk_fix": "@@ -105,6 +106,13 @@ class FuseContractionWithBiasAdd : public OpRewritePattern<SrcOpT> {\n     return true;\n   }\n \n+  // Class users should override this method if there are any op-specific\n+  // compatibility requirements for devices.\n+  virtual bool IsDeviceCompatible(SrcOpT contraction_op,\n+                                  PatternRewriter &rewriter) const {\n+    return true;\n+  }\n+\n   LogicalResult matchAndRewrite(SrcOpT contraction,\n                                 PatternRewriter &rewriter) const override {\n     auto context = rewriter.getContext();\n"
                },
                {
                    "old_start": 135,
                    "old_length": 6,
                    "new_start": 143,
                    "new_length": 13,
                    "hunk_buggy": "['           contraction, \"cannot fuse with the subsequent BiasAdd op\");\\n', '     }\\n', ' \\n', '     SmallVector<Location, 3> locations{contraction.getLoc(), bias_add.getLoc()};\\n', '     SmallVector<Attribute, 2> fused_ops{StringAttr::get(\\n', '         context, bias_add.getOperation()->getName().stripDialect())};\\n']",
                    "hunk_fix": "@@ -135,6 +143,13 @@ class FuseContractionWithBiasAdd : public OpRewritePattern<SrcOpT> {\n           contraction, \"cannot fuse with the subsequent BiasAdd op\");\n     }\n \n+    if (!IsDeviceCompatible(contraction, rewriter)) {\n+      return rewriter.notifyMatchFailure(\n+          contraction,\n+          \"cannot fuse with the subsequent op as it's not supported by the \"\n+          \"target device.\");\n+    }\n+\n     SmallVector<Location, 3> locations{contraction.getLoc(), bias_add.getLoc()};\n     SmallVector<Attribute, 2> fused_ops{StringAttr::get(\n         context, bias_add.getOperation()->getName().stripDialect())};\n"
                },
                {
                    "old_start": 192,
                    "old_length": 6,
                    "new_start": 207,
                    "new_length": 31,
                    "hunk_buggy": "['   }\\n', ' };\\n', ' \\n', ' // Performs a fusion of the following pattern(s), if possible:\\n', ' //   Conv2D + BiasAdd + <Activation> -> _FusedConv2D\\n', ' class FuseConv2DBiasAdd\\n']",
                    "hunk_fix": "@@ -192,6 +207,31 @@ class FuseContractionWithBiasAdd : public OpRewritePattern<SrcOpT> {\n   }\n };\n \n+const char kDeviceAttr[] = \"device\";\n+const char kDeviceGpu[] = \"GPU\";\n+\n+llvm::Optional<std::string> GetDevice(mlir::Operation *op) {\n+  mlir::StringAttr device = op->getAttrOfType<mlir::StringAttr>(kDeviceAttr);\n+  if (!device || device.getValue().empty()) {\n+    return llvm::None;\n+  }\n+  const std::string device_name = device.str();\n+  tensorflow::DeviceNameUtils::ParsedName parsed_name;\n+  if (!tensorflow::DeviceNameUtils::ParseFullName(device_name, &parsed_name)) {\n+    return llvm::None;\n+  }\n+  if (!parsed_name.has_type) {\n+    return llvm::None;\n+  }\n+  return parsed_name.type;\n+}\n+\n+bool IsGpuDevice(mlir::Operation *op) {\n+  llvm::Optional<std::string> device = GetDevice(op);\n+  if (!device) return false;\n+  return *device == kDeviceGpu;\n+}\n+\n // Performs a fusion of the following pattern(s), if possible:\n //   Conv2D + BiasAdd + <Activation> -> _FusedConv2D\n class FuseConv2DBiasAdd\n"
                },
                {
                    "old_start": 243,
                    "old_length": 6,
                    "new_start": 283,
                    "new_length": 17,
                    "hunk_buggy": "['     }\\n', '     return true;\\n', '   }\\n', ' };\\n', ' \\n', ' void FusedKernelMatcherPass::runOnOperation() {']",
                    "hunk_fix": "@@ -243,6 +283,17 @@ class FuseMatMulBiasAdd\n     }\n     return true;\n   }\n+\n+  bool IsDeviceCompatible(MatMulOp matmul,\n+                          PatternRewriter &rewriter) const override {\n+    if (IsGpuDevice(matmul)) {\n+      (void)rewriter.notifyMatchFailure(matmul, [&](Diagnostic &diag) {\n+        diag << \"_FusedMatMul is not supported by GPU\";\n+      });\n+      return false;\n+    }\n+    return true;\n+  }\n };\n \n void FusedKernelMatcherPass::runOnOperation() {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 114,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/0d5668cbdc6b46d099bd3abd93374c09b2e8121f",
    "date": "2022-04-06T16:43:53-07:00",
    "message": "[XLA:SHAPE_UTIL] Return nullopt instead of a check failure if the input dimensions are not sorted.\n\nPiperOrigin-RevId: 439966260",
    "changes": [
        {
            "name": "shape_util.cc",
            "path": "tensorflow/compiler/xla/shape_util.cc",
            "patches": [
                {
                    "old_start": 1246,
                    "old_length": 7,
                    "new_start": 1246,
                    "new_length": 9,
                    "hunk_buggy": "[' ShapeUtil::ReshapeLeavesDimensionsUnmodified(\\n', '     const Shape& from_shape, const Shape& to_shape,\\n', '     absl::Span<const int64_t> input_dim_indices) {\\n', '-  CHECK(std::is_sorted(input_dim_indices.begin(), input_dim_indices.end()));\\n', ' \\n', '   std::vector<int64_t> output_dim_indices;\\n', '   std::vector<std::pair<int64_t, int64_t>> unmodified_dims =']",
                    "hunk_fix": "@@ -1246,7 +1246,9 @@ ShapeUtil::DimensionsUnmodifiedByReshape(const Shape& input_shape,\n ShapeUtil::ReshapeLeavesDimensionsUnmodified(\n     const Shape& from_shape, const Shape& to_shape,\n     absl::Span<const int64_t> input_dim_indices) {\n-  CHECK(std::is_sorted(input_dim_indices.begin(), input_dim_indices.end()));\n+  if (!std::is_sorted(input_dim_indices.begin(), input_dim_indices.end())) {\n+    return absl::nullopt;\n+  }\n \n   std::vector<int64_t> output_dim_indices;\n   std::vector<std::pair<int64_t, int64_t>> unmodified_dims ="
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 115,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/824af2acfa0cdf897c08d91224aea0958c1afc02",
    "date": "2022-03-29T13:44:03-04:00",
    "message": "Add ndmin check\n\nAdded ndmin check to allow maximum 32 ndmin to make same behavior as numpy.\r\nCurrently it is crashing when very large ndmin is passed.",
    "changes": [
        {
            "name": "np_array_ops.py",
            "path": "tensorflow/python/ops/numpy_ops/np_array_ops.py",
            "patches": [
                {
                    "old_start": 169,
                    "old_length": 6,
                    "new_start": 169,
                    "new_length": 11,
                    "hunk_buggy": "['   if copy:\\n', '     result_t = array_ops.identity(result_t)\\n', ' \\n', '   if ndmin == 0:\\n', '     return result_t\\n', ' ']",
                    "hunk_fix": "@@ -169,6 +169,11 @@ def _array_internal(val, dtype=None, copy=True, ndmin=0):  # pylint: disable=red\n   if copy:\n     result_t = array_ops.identity(result_t)\n \n+  max_ndmin = 32\n+  if ndmin > max_ndmin:\n+    raise ValueError('ndmin bigger than allowable number of dimensions: '\n+                     f'{max_ndmin}.')\n+  \n   if ndmin == 0:\n     return result_t\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 116,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/9c92b50fc4b95985a0749101976d04896bf19bfe",
    "date": "2022-03-16T08:59:22+08:00",
    "message": "[conv3d_transpose] Fix dim check for bias\n\nPer discussion with @thaink, the previous way to do the dim check for bias is not correct. So we need this change.",
    "changes": [
        {
            "name": "conv3d_transpose.cc",
            "path": "tensorflow/lite/kernels/conv3d_transpose.cc",
            "patches": [
                {
                    "old_start": 171,
                    "old_length": 7,
                    "new_start": 171,
                    "new_length": 7,
                    "hunk_buggy": "['   const TfLiteTensor* bias = GetInput(context, node, 3);\\n', '   if (bias) {\\n', '     TF_LITE_ENSURE_TYPES_EQ(context, bias->type, input->type);\\n', '-    TF_LITE_ENSURE_EQ(context, NumElements(bias), SizeOfDimension(filter, 4));\\n', '   }\\n', ' \\n', \"   // GenericOptimized kernel currently doesn't support dilation.\"]",
                    "hunk_fix": "@@ -171,7 +171,7 @@ TfLiteStatus Prepare(KernelType kernel_type, TfLiteContext* context,\n   const TfLiteTensor* bias = GetInput(context, node, 3);\n   if (bias) {\n     TF_LITE_ENSURE_TYPES_EQ(context, bias->type, input->type);\n-    TF_LITE_ENSURE_EQ(context, NumElements(bias), SizeOfDimension(filter, 4));\n+    TF_LITE_ENSURE_EQ(context, NumElements(bias), SizeOfDimension(filter, 3));\n   }\n \n   // GenericOptimized kernel currently doesn't support dilation."
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 117,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/18dd91ccd4b1817cd5c34e40f76823a162bea029",
    "date": "2022-03-02T18:18:49-08:00",
    "message": "[XLA] Report that real -> complex bitcast_convert is not allowed\n\nThe check as exists is bidirectional: it prevents conversions from complex to real and real to complex alike, but the reported error message was unidirectional.\n\nPiperOrigin-RevId: 432075224",
    "changes": [
        {
            "name": "shape_inference.cc",
            "path": "tensorflow/compiler/xla/service/shape_inference.cc",
            "patches": [
                {
                    "old_start": 457,
                    "old_length": 7,
                    "new_start": 457,
                    "new_length": 7,
                    "hunk_buggy": "['   auto old_element_type = operand_shape.element_type();\\n', '   if (primitive_util::IsComplexType(old_element_type) !=\\n', '       primitive_util::IsComplexType(new_element_type)) {\\n', '-    return InvalidArgument(\"Conversion from complex to real type %s => %s.\",\\n', '                            ShapeUtil::HumanString(operand_shape),\\n', '                            PrimitiveType_Name(new_element_type));\\n', '   }']",
                    "hunk_fix": "@@ -457,7 +457,7 @@ StatusOr<PrimitiveType> MaybeUpcast(\n   auto old_element_type = operand_shape.element_type();\n   if (primitive_util::IsComplexType(old_element_type) !=\n       primitive_util::IsComplexType(new_element_type)) {\n-    return InvalidArgument(\"Conversion from complex to real type %s => %s.\",\n+    return InvalidArgument(\"Conversion between complex and real type %s => %s.\",\n                            ShapeUtil::HumanString(operand_shape),\n                            PrimitiveType_Name(new_element_type));\n   }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 118,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/19b2e1b5868a044df4622ef7e26fa5570ca52e5e",
    "date": "2022-02-11T15:15:25-08:00",
    "message": "Only perform scalar check for a tensor shape if it's not empty.\n\nPiperOrigin-RevId: 428100731\nChange-Id: I4b908cf448bcbb853023899f8b501b82c8ac03f0",
    "changes": [
        {
            "name": "weights.cc",
            "path": "tensorflow/compiler/tf2tensorrt/convert/weights.cc",
            "patches": [
                {
                    "old_start": 37,
                    "old_length": 7,
                    "new_start": 37,
                    "new_length": 7,
                    "hunk_buggy": "['   weights.tensor_ = std::forward<Tensor>(tensor);\\n', '   weights.volume_ = weights.shape_.Volume();\\n', '   if (weights.shape_.NumDims() == 0) {\\n', '-    DCHECK(weights.shape_.IsScalar());\\n', '   }\\n', '   return weights;\\n', ' }']",
                    "hunk_fix": "@@ -37,7 +37,7 @@ StatusOr<TRT_ShapedWeights> TRT_ShapedWeights::CreateWithTensor(\n   weights.tensor_ = std::forward<Tensor>(tensor);\n   weights.volume_ = weights.shape_.Volume();\n   if (weights.shape_.NumDims() == 0) {\n-    DCHECK(weights.shape_.IsScalar());\n+    DCHECK(weights.shape_.IsEmpty() || weights.shape_.IsScalar());\n   }\n   return weights;\n }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 119,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/e6390bc13471f28f211cab874cc49a123505dc3e",
    "date": "2022-01-29T04:52:05+05:30",
    "message": "Update histogram_ops.py\n\nAdded the condition to check the negative value of nbins input",
    "changes": [
        {
            "name": "histogram_ops.py",
            "path": "tensorflow/python/ops/histogram_ops.py",
            "patches": [
                {
                    "old_start": 57,
                    "old_length": 6,
                    "new_start": 57,
                    "new_length": 7,
                    "hunk_buggy": "['     TypeError: If any unsupported dtype is provided.\\n', '     tf.errors.InvalidArgumentError: If value_range does not\\n', '         satisfy value_range[0] < value_range[1].\\n', ' \\n', '   Examples:\\n', ' \\n']",
                    "hunk_fix": "@@ -57,6 +57,7 @@ def histogram_fixed_width_bins(values,\n     TypeError: If any unsupported dtype is provided.\n     tf.errors.InvalidArgumentError: If value_range does not\n         satisfy value_range[0] < value_range[1].\n+    ValueError: If the value of nbins is negative.\n \n   Examples:\n \n"
                },
                {
                    "old_start": 69,
                    "old_length": 6,
                    "new_start": 70,
                    "new_length": 9,
                    "hunk_buggy": "['   >>> indices.numpy()\\n', '   array([0, 0, 1, 2, 4, 4], dtype=int32)\\n', '   \"\"\"\\n', \"   with ops.name_scope(name, 'histogram_fixed_width_bins',\\n\", '                       [values, value_range, nbins]):\\n', \"     values = ops.convert_to_tensor(values, name='values')\\n\"]",
                    "hunk_fix": "@@ -69,6 +70,9 @@ def histogram_fixed_width_bins(values,\n   >>> indices.numpy()\n   array([0, 0, 1, 2, 4, 4], dtype=int32)\n   \"\"\"\n+  if nbins < 0:\n+    raise ValueError(\"nbins should be a positive number.\")\n+    \n   with ops.name_scope(name, 'histogram_fixed_width_bins',\n                       [values, value_range, nbins]):\n     values = ops.convert_to_tensor(values, name='values')\n"
                },
                {
                    "old_start": 124,
                    "old_length": 6,
                    "new_start": 128,
                    "new_length": 7,
                    "hunk_buggy": "['     TypeError: If any unsupported dtype is provided.\\n', '     tf.errors.InvalidArgumentError: If value_range does not\\n', '         satisfy value_range[0] < value_range[1].\\n', ' \\n', '   Examples:\\n', ' \\n']",
                    "hunk_fix": "@@ -124,6 +128,7 @@ def histogram_fixed_width(values,\n     TypeError: If any unsupported dtype is provided.\n     tf.errors.InvalidArgumentError: If value_range does not\n         satisfy value_range[0] < value_range[1].\n+    ValueError: If the value of nbins is negative.\n \n   Examples:\n \n"
                },
                {
                    "old_start": 136,
                    "old_length": 6,
                    "new_start": 141,
                    "new_length": 9,
                    "hunk_buggy": "['   >>> hist.numpy()\\n', '   array([2, 1, 1, 0, 2], dtype=int32)\\n', '   \"\"\"\\n', \"   with ops.name_scope(name, 'histogram_fixed_width',\\n\", '                       [values, value_range, nbins]) as name:\\n', '     # pylint: disable=protected-access']",
                    "hunk_fix": "@@ -136,6 +141,9 @@ def histogram_fixed_width(values,\n   >>> hist.numpy()\n   array([2, 1, 1, 0, 2], dtype=int32)\n   \"\"\"\n+  if nbins < 0:\n+    raise ValueError(\"nbins should be a positive number.\")\n+\n   with ops.name_scope(name, 'histogram_fixed_width',\n                       [values, value_range, nbins]) as name:\n     # pylint: disable=protected-access"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 120,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/6364463d6f5b6254cac3d6aedf999b6a96225038",
    "date": "2021-12-16T15:46:23-08:00",
    "message": "[lite] Add some safety checks to avoid out of bound access for sparsity format\n\nPiperOrigin-RevId: 416910386\nChange-Id: Ic0b4dc048dc4b5a6309c572b8c4c9f776e4db60a",
    "changes": [
        {
            "name": "sparsity_format_converter.cc",
            "path": "tensorflow/lite/kernels/internal/utils/sparsity_format_converter.cc",
            "patches": [
                {
                    "old_start": 282,
                    "old_length": 10,
                    "new_start": 282,
                    "new_length": 12,
                    "hunk_buggy": "['   block_size_.resize(block_map_.size());\\n', '   for (int i = 0; i < original_rank; i++) {\\n', '     if (block_dim < block_map_.size() && block_map_[block_dim] == i) {\\n', '-      int orig_dim = traversal_order_[original_rank + block_dim];\\n', '-      block_size_[block_dim] = dense_size[orig_dim];\\n', '-      blocked_shape_[i] = dense_shape_[i] / dense_size[orig_dim];\\n', '-      block_dim++;\\n', '     } else {\\n', '       blocked_shape_[i] = dense_shape_[i];\\n', '     }\\n']",
                    "hunk_fix": "@@ -282,10 +282,12 @@ void FormatConverter<T>::InitSparseToDenseConverter(\n   block_size_.resize(block_map_.size());\n   for (int i = 0; i < original_rank; i++) {\n     if (block_dim < block_map_.size() && block_map_[block_dim] == i) {\n-      int orig_dim = traversal_order_[original_rank + block_dim];\n-      block_size_[block_dim] = dense_size[orig_dim];\n-      blocked_shape_[i] = dense_shape_[i] / dense_size[orig_dim];\n-      block_dim++;\n+      if (original_rank + block_dim < traversal_order_.size()) {\n+        int orig_dim = traversal_order_[original_rank + block_dim];\n+        block_size_[block_dim] = dense_size[orig_dim];\n+        blocked_shape_[i] = dense_shape_[i] / dense_size[orig_dim];\n+        block_dim++;\n+      }\n     } else {\n       blocked_shape_[i] = dense_shape_[i];\n     }\n"
                },
                {
                    "old_start": 328,
                    "old_length": 13,
                    "new_start": 330,
                    "new_length": 15,
                    "hunk_buggy": "['       Populate(src_data, indices, level + 1, prev_idx * shape_of_level + i,\\n', '                src_data_ptr, dest_data);\\n', '     }\\n', '-  } else {\\n', '     const auto& array_segments = dim_metadata_[metadata_idx];\\n', '     const auto& array_indices = dim_metadata_[metadata_idx + 1];\\n', '     for (int i = array_segments[prev_idx]; i < array_segments[prev_idx + 1];\\n', '          i++) {\\n', '-      indices[level] = array_indices[i];\\n', '-      Populate(src_data, indices, level + 1, i, src_data_ptr, dest_data);\\n', '     }\\n', '   }\\n', ' }']",
                    "hunk_fix": "@@ -328,13 +330,15 @@ void FormatConverter<T>::Populate(const T* src_data, std::vector<int> indices,\n       Populate(src_data, indices, level + 1, prev_idx * shape_of_level + i,\n                src_data_ptr, dest_data);\n     }\n-  } else {\n+  } else if (prev_idx + 1 < dim_metadata_[metadata_idx].size()) {\n     const auto& array_segments = dim_metadata_[metadata_idx];\n     const auto& array_indices = dim_metadata_[metadata_idx + 1];\n     for (int i = array_segments[prev_idx]; i < array_segments[prev_idx + 1];\n          i++) {\n-      indices[level] = array_indices[i];\n-      Populate(src_data, indices, level + 1, i, src_data_ptr, dest_data);\n+      if (i < array_indices.size() && level < indices.size()) {\n+        indices[level] = array_indices[i];\n+        Populate(src_data, indices, level + 1, i, src_data_ptr, dest_data);\n+      }\n     }\n   }\n }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 121,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/e5b0eec199c2d03de54fd6a7fd9275692218e2bc",
    "date": "2021-12-14T17:13:47-08:00",
    "message": "[lite] Add validation check for dilation height/width to be positive integers.\n\nPiperOrigin-RevId: 416429178\nChange-Id: If7cdcddca54486434d9b2f06e7e2b401d7c3ee25",
    "changes": [
        {
            "name": "depthwise_conv.cc",
            "path": "tensorflow/lite/kernels/depthwise_conv.cc",
            "patches": [
                {
                    "old_start": 115,
                    "old_length": 6,
                    "new_start": 115,
                    "new_length": 8,
                    "hunk_buggy": "[' \\n', '   TF_LITE_ENSURE_EQ(context, NumDimensions(input), 4);\\n', '   TF_LITE_ENSURE_EQ(context, NumDimensions(filter), 4);\\n', ' \\n', '   const TfLiteType data_type = input->type;\\n', ' ']",
                    "hunk_fix": "@@ -115,6 +115,8 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n \n   TF_LITE_ENSURE_EQ(context, NumDimensions(input), 4);\n   TF_LITE_ENSURE_EQ(context, NumDimensions(filter), 4);\n+  TF_LITE_ENSURE(context, params->dilation_height_factor > 0);\n+  TF_LITE_ENSURE(context, params->dilation_width_factor > 0);\n \n   const TfLiteType data_type = input->type;\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 122,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/8c6f391a2282684a25cbfec7687bd5d35261a209",
    "date": "2021-12-14T13:45:48-08:00",
    "message": "[lite] Add check for bias_size is zero to avoid division by zero. This shouldn't happen for properly converted models. Just safety check\n\nPiperOrigin-RevId: 416383645\nChange-Id: If8e508bf696ae8ecfb927e69c139a8ccf7fe60cb",
    "changes": [
        {
            "name": "common.h",
            "path": "tensorflow/lite/kernels/internal/common.h",
            "patches": [
                {
                    "old_start": 75,
                    "old_length": 6,
                    "new_start": 75,
                    "new_length": 7,
                    "hunk_buggy": "[' inline void BiasAndClamp(float clamp_min, float clamp_max, int bias_size,\\n', '                          const float* bias_data, int array_size,\\n', '                          float* array_data) {\\n', '   // Note: see b/132215220: in May 2019 we thought it would be OK to replace\\n', '   // this with the Eigen one-liner:\\n', '   //   return (array.colwise() + bias).cwiseMin(clamp_max).cwiseMin(clamp_max).']",
                    "hunk_fix": "@@ -75,6 +75,7 @@ float ActivationFunction(float x) {\n inline void BiasAndClamp(float clamp_min, float clamp_max, int bias_size,\n                          const float* bias_data, int array_size,\n                          float* array_data) {\n+  if (bias_size == 0) return;\n   // Note: see b/132215220: in May 2019 we thought it would be OK to replace\n   // this with the Eigen one-liner:\n   //   return (array.colwise() + bias).cwiseMin(clamp_max).cwiseMin(clamp_max)."
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 123,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/1b54cadd19391b60b6fcccd8d076426f7221d5e8",
    "date": "2021-12-10T09:51:09-08:00",
    "message": "Add missing validation to sparse dense cwise ops.\n\nPiperOrigin-RevId: 415543133\nChange-Id: I5baf3284e919338afb96178c468ad3d3cb0d956c",
    "changes": [
        {
            "name": "sparse_dense_binary_op_shared.cc",
            "path": "tensorflow/core/kernels/sparse_dense_binary_op_shared.cc",
            "patches": [
                {
                    "old_start": 78,
                    "old_length": 11,
                    "new_start": 78,
                    "new_length": 24,
                    "hunk_buggy": "['                     \"but received shapes: \",\\n', '                     values_t->shape().DebugString(), \" and \",\\n', '                     shape_t->shape().DebugString()));\\n', '     OP_REQUIRES(\\n', '         ctx, values_t->dim_size(0) == indices_t->dim_size(0),\\n', '         errors::InvalidArgument(\\n', '             \"The first dimension of values and indices should match. (\",\\n', '             values_t->dim_size(0), \" vs. \", indices_t->dim_size(0), \")\"));\\n', ' \\n', '     const auto indices_mat = indices_t->matrix<int64_t>();\\n', '     const auto shape_vec = shape_t->vec<int64_t>();']",
                    "hunk_fix": "@@ -78,11 +78,24 @@ class SparseDenseBinaryOpShared : public OpKernel {\n                     \"but received shapes: \",\n                     values_t->shape().DebugString(), \" and \",\n                     shape_t->shape().DebugString()));\n+    OP_REQUIRES(\n+        ctx, TensorShapeUtils::IsVector(shape_t->shape()),\n+        errors::InvalidArgument(\"Input sp_shape must be a vector. Got: \",\n+                                shape_t->shape().DebugString()));\n     OP_REQUIRES(\n         ctx, values_t->dim_size(0) == indices_t->dim_size(0),\n         errors::InvalidArgument(\n             \"The first dimension of values and indices should match. (\",\n             values_t->dim_size(0), \" vs. \", indices_t->dim_size(0), \")\"));\n+    OP_REQUIRES(\n+        ctx, shape_t->shape().dim_size(0) == indices_t->shape().dim_size(1),\n+        errors::InvalidArgument(\n+            \"Number of dimensions must match second dimension of indices. \",\n+            \"Got \", shape_t->shape().dim_size(0),\n+            \" dimensions, indices shape: \", indices_t->shape().DebugString()));\n+    OP_REQUIRES(ctx, shape_t->NumElements() > 0,\n+                errors::InvalidArgument(\n+                    \"The shape argument requires at least one element.\"));\n \n     const auto indices_mat = indices_t->matrix<int64_t>();\n     const auto shape_vec = shape_t->vec<int64_t>();"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 124,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/b51b82fe65ebace4475e3c54eb089c18a4403f1c",
    "date": "2021-12-09T14:41:08-08:00",
    "message": "Add missing validation to `AddManySparseToTensorsMap`.\n\nSparse tensors have a set of requirements for the 3 components and not all of them were checked.\n\nPiperOrigin-RevId: 415358027\nChange-Id: I96cbb672999cd1da772c22fabbd15507e32e12dc",
    "changes": [
        {
            "name": "sparse_tensors_map_ops.cc",
            "path": "tensorflow/core/kernels/sparse_tensors_map_ops.cc",
            "patches": [
                {
                    "old_start": 241,
                    "old_length": 6,
                    "new_start": 239,
                    "new_length": 21,
                    "hunk_buggy": "['                 errors::InvalidArgument(\\n', '                     \"Input shape should be a vector but received shape \",\\n', '                     input_shape->shape().DebugString()));\\n', ' \\n', '     int rank = input_shape->NumElements();\\n', ' ']",
                    "hunk_fix": "@@ -241,6 +239,21 @@ class AddManySparseToTensorsMapOp : public SparseTensorAccessingOp {\n                 errors::InvalidArgument(\n                     \"Input shape should be a vector but received shape \",\n                     input_shape->shape().DebugString()));\n+    OP_REQUIRES(\n+        context,\n+        input_values->shape().dim_size(0) == input_indices->shape().dim_size(0),\n+        errors::InvalidArgument(\n+            \"Number of values must match first dimension of indices. \", \"Got \",\n+            input_values->shape().dim_size(0),\n+            \" values, indices shape: \", input_indices->shape().DebugString()));\n+    OP_REQUIRES(\n+        context,\n+        input_shape->shape().dim_size(0) == input_indices->shape().dim_size(1),\n+        errors::InvalidArgument(\n+            \"Number of dimensions must match second dimension of indices. \",\n+            \"Got \", input_shape->shape().dim_size(0),\n+            \" dimensions, indices shape: \",\n+            input_indices->shape().DebugString()));\n \n     int rank = input_shape->NumElements();\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 125,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/d23458fdd2655c83ff9d54725062ded31b644ba4",
    "date": "2021-11-23T16:51:35-08:00",
    "message": "[XLA:CPU] Do not check that the size of the XLA parameter buffer is exactly equal to the size of the underlying given buffer\n\nInstead, check that the underlying allocation is \"large enough\". This is also\nmore consistent with XLA:GPU behavior.\n\nThe mismatch can happen when the input comes from tf.where, which is backed by\nan allocation larger than is actually required.\n\nProviding tests is a bit of a chicken-and-an-egg problem: this is tested in\nwhere ops tests, which are disabled due to this issue.\n\nPiperOrigin-RevId: 411923802\nChange-Id: Iee3da305b9692ff37bdecba058629419e3f495c3",
    "changes": [
        {
            "name": "cpu_executable.cc",
            "path": "tensorflow/compiler/xla/service/cpu/cpu_executable.cc",
            "patches": [
                {
                    "old_start": 109,
                    "old_length": 7,
                    "new_start": 109,
                    "new_length": 7,
                    "hunk_buggy": "['     se::DeviceMemoryBase out = arguments[allocation.parameter_number()]\\n', '                                    .Buffer(allocation.param_shape_index())\\n', '                                    .AsDeviceMemoryBase();\\n', '-    CHECK_EQ(allocation.size(), out.size())\\n', '         << \"Size mismatch on param \" << allocation.parameter_number()\\n', '         << \" at shape index \" << allocation.param_shape_index().ToString();\\n', '     VLOG(3) << \"allocation is a parameter\";']",
                    "hunk_fix": "@@ -109,7 +109,7 @@ static StatusOr<MaybeOwningDeviceMemory> MemoryForAllocation(\n     se::DeviceMemoryBase out = arguments[allocation.parameter_number()]\n                                    .Buffer(allocation.param_shape_index())\n                                    .AsDeviceMemoryBase();\n-    CHECK_EQ(allocation.size(), out.size())\n+    CHECK_LE(allocation.size(), out.size())\n         << \"Size mismatch on param \" << allocation.parameter_number()\n         << \" at shape index \" << allocation.param_shape_index().ToString();\n     VLOG(3) << \"allocation is a parameter\";"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 126,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/23968a8bf65b009120c43b5ebcceaf52dbc9e943",
    "date": "2021-11-19T23:22:12-08:00",
    "message": "Fix out of bound access in DequantizeOp by adding check for axis < input dimension\n\nPiperOrigin-RevId: 411214268\nChange-Id: I3249d2a69ddc82f182c589a3a5bbfb71543f4b29",
    "changes": [
        {
            "name": "dequantize_op.cc",
            "path": "tensorflow/core/kernels/dequantize_op.cc",
            "patches": [
                {
                    "old_start": 94,
                    "old_length": 6,
                    "new_start": 94,
                    "new_length": 11,
                    "hunk_buggy": "['     const Tensor& input_min_tensor = ctx->input(1);\\n', '     const Tensor& input_max_tensor = ctx->input(2);\\n', ' \\n', '     int num_slices = 1;\\n', '     if (axis_ > -1) {\\n', '       num_slices = input.dim_size(axis_);']",
                    "hunk_fix": "@@ -94,6 +94,11 @@ class DequantizeOp : public OpKernel {\n     const Tensor& input_min_tensor = ctx->input(1);\n     const Tensor& input_max_tensor = ctx->input(2);\n \n+    OP_REQUIRES(\n+        ctx, axis_ < input.dims(),\n+        errors::InvalidArgument(\"Axis must be less than input dimension(\",\n+                                input.dims(), \"), got \", axis_));\n+\n     int num_slices = 1;\n     if (axis_ > -1) {\n       num_slices = input.dim_size(axis_);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 127,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/1610f391833738972b538e4ee97f90dbd30fc745",
    "date": "2021-11-19T14:14:37-08:00",
    "message": "Replace `DCHECK` with actual validation in `AddRangeStats`\n\nPiperOrigin-RevId: 411138402\nChange-Id: I123eafe70e292f286ba384d4ed73222edf61a6ea",
    "changes": [
        {
            "name": "stats_ops.cc",
            "path": "tensorflow/core/kernels/boosted_trees/stats_ops.cc",
            "patches": [
                {
                    "old_start": 1730,
                    "old_length": 9,
                    "new_start": 1730,
                    "new_length": 15,
                    "hunk_buggy": "['                           const TTypes<int32>::ConstVec& node_ids,\\n', '                           const int32_t feature_dims, const int32_t bucket_id,\\n', '                           const int32_t logits_dims, const int32_t stats_dims) {\\n', '-  DCHECK_LE(start_instance, end_instance);\\n', '   if (start_instance == end_instance) {\\n', '-    DCHECK_LT(start_feature_dim, end_feature_dim);\\n', '   }\\n', '   for (int32_t instance = start_instance; instance <= end_instance;\\n', '        ++instance) {']",
                    "hunk_fix": "@@ -1730,9 +1730,15 @@ static void AddRangeStats(OpKernelContext* const context,\n                           const TTypes<int32>::ConstVec& node_ids,\n                           const int32_t feature_dims, const int32_t bucket_id,\n                           const int32_t logits_dims, const int32_t stats_dims) {\n-  DCHECK_LE(start_instance, end_instance);\n+  OP_REQUIRES(context, start_instance <= end_instance,\n+              errors::InvalidArgument(\n+                  \"start_instance = \", start_instance,\n+                  \" which is not at most end_instance=\", end_instance));\n   if (start_instance == end_instance) {\n-    DCHECK_LT(start_feature_dim, end_feature_dim);\n+    OP_REQUIRES(context, start_feature_dim < end_feature_dim,\n+                errors::InvalidArgument(\n+                    \"start_feature_dim = \", start_feature_dim,\n+                    \" which is not at most end_feature_dim=\", end_feature_dim));\n   }\n   for (int32_t instance = start_instance; instance <= end_instance;\n        ++instance) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 128,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/41ab69692ede0db3422fa70bc5889d470741e69c",
    "date": "2021-11-19T07:24:55-08:00",
    "message": "Check for tensors to be vectors in `BoostedTreesSparseAggregateStatsOp`.\n\nCalling `tensor->vec` should only happen after checking that the tensor shape implies a vector. Otherwise, we can get denial of service via `CHECK`-fails\n\nPiperOrigin-RevId: 411054482\nChange-Id: Idc7a624dcc9e84685bf328b2d0e4842b904229dd",
    "changes": [
        {
            "name": "stats_ops.cc",
            "path": "tensorflow/core/kernels/boosted_trees/stats_ops.cc",
            "patches": [
                {
                    "old_start": 1649,
                    "old_length": 6,
                    "new_start": 1649,
                    "new_length": 10,
                    "hunk_buggy": "['     const Tensor* feature_values_t;\\n', '     OP_REQUIRES_OK(context,\\n', '                    context->input(\"feature_values\", &feature_values_t));\\n', '     const auto feature_values = feature_values_t->vec<int32>();\\n', ' \\n', '     // feature shape.']",
                    "hunk_fix": "@@ -1649,6 +1649,10 @@ class BoostedTreesSparseAggregateStatsOp : public OpKernel {\n     const Tensor* feature_values_t;\n     OP_REQUIRES_OK(context,\n                    context->input(\"feature_values\", &feature_values_t));\n+    OP_REQUIRES(context, TensorShapeUtils::IsVector(feature_values_t->shape()),\n+                errors::InvalidArgument(\n+                    \"feature_values must be a vector, received shape \",\n+                    feature_values_t->shape().DebugString()));\n     const auto feature_values = feature_values_t->vec<int32>();\n \n     // feature shape."
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 129,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/8d733ecdb270dd90b2b5f53fd220d5ce17a5e20f",
    "date": "2021-11-18T20:21:35-08:00",
    "message": "Check for tensors to be vectors in `BoostedTreesSparseAggregateStatsOp`.\n\nCalling `tensor->vec` should only happen after checking that the tensor shape implies a vector. Otherwise, we can get denial of service via `CHECK`-fails\n\nPiperOrigin-RevId: 410960878\nChange-Id: I7b26bec796cbaebde4696862eb855160402b4b0d",
    "changes": [
        {
            "name": "stats_ops.cc",
            "path": "tensorflow/core/kernels/boosted_trees/stats_ops.cc",
            "patches": [
                {
                    "old_start": 1611,
                    "old_length": 6,
                    "new_start": 1611,
                    "new_length": 10,
                    "hunk_buggy": "['     // node_ids.\\n', '     const Tensor* node_ids_t;\\n', '     OP_REQUIRES_OK(context, context->input(\"node_ids\", &node_ids_t));\\n', '     const auto node_ids = node_ids_t->vec<int32>();\\n', ' \\n', '     // gradients.']",
                    "hunk_fix": "@@ -1611,6 +1611,10 @@ class BoostedTreesSparseAggregateStatsOp : public OpKernel {\n     // node_ids.\n     const Tensor* node_ids_t;\n     OP_REQUIRES_OK(context, context->input(\"node_ids\", &node_ids_t));\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsVector(node_ids_t->shape()),\n+        errors::InvalidArgument(\"node_ids must be a vector, received shape \",\n+                                node_ids_t->shape().DebugString()));\n     const auto node_ids = node_ids_t->vec<int32>();\n \n     // gradients."
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 130,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/1595906c2192b7f402f746652042a592ad290378",
    "date": "2021-11-18T19:27:23-08:00",
    "message": "Prevent `CHECK`-fail DOS in `BoostedTreesSparseAggregateStatsOp`.\n\nCalling `tensor->matrix` should only happen after checking that the tensor shape implies a matrix.\n\nPiperOrigin-RevId: 410952517\nChange-Id: Ica269fc5695d2a467c2e3c7b0681d717d152e2a3",
    "changes": [
        {
            "name": "stats_ops.cc",
            "path": "tensorflow/core/kernels/boosted_trees/stats_ops.cc",
            "patches": [
                {
                    "old_start": 1635,
                    "old_length": 6,
                    "new_start": 1635,
                    "new_length": 10,
                    "hunk_buggy": "['     const Tensor* feature_indices_t;\\n', '     OP_REQUIRES_OK(context,\\n', '                    context->input(\"feature_indices\", &feature_indices_t));\\n', '     const auto feature_indices = feature_indices_t->matrix<int32>();\\n', ' \\n', '     // feature values.']",
                    "hunk_fix": "@@ -1635,6 +1635,10 @@ class BoostedTreesSparseAggregateStatsOp : public OpKernel {\n     const Tensor* feature_indices_t;\n     OP_REQUIRES_OK(context,\n                    context->input(\"feature_indices\", &feature_indices_t));\n+    OP_REQUIRES(context, TensorShapeUtils::IsMatrix(feature_indices_t->shape()),\n+                errors::InvalidArgument(\n+                    \"feature_indices must be a matrix, received shape \",\n+                    feature_indices_t->shape().DebugString()));\n     const auto feature_indices = feature_indices_t->matrix<int32>();\n \n     // feature values."
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 131,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/54c94431e5dd17fc46d99da1a3f132c76414c161",
    "date": "2021-11-18T19:22:30-08:00",
    "message": "Prevent `CHECK`-fail DOS in `BoostedTreesSparseAggregateStatsOp`.\n\nCalling `tensor->matrix` should only happen after checking that the tensor shape implies a matrix.\n\nPiperOrigin-RevId: 410951880\nChange-Id: Id26099f022d68366eec03cc878e57bf6237ecccf",
    "changes": [
        {
            "name": "stats_ops.cc",
            "path": "tensorflow/core/kernels/boosted_trees/stats_ops.cc",
            "patches": [
                {
                    "old_start": 1625,
                    "old_length": 6,
                    "new_start": 1625,
                    "new_length": 10,
                    "hunk_buggy": "['     // hessians.\\n', '     const Tensor* hessians_t;\\n', '     OP_REQUIRES_OK(context, context->input(\"hessians\", &hessians_t));\\n', '     const auto hessians = hessians_t->matrix<float>();\\n', ' \\n', '     // feature indices.']",
                    "hunk_fix": "@@ -1625,6 +1625,10 @@ class BoostedTreesSparseAggregateStatsOp : public OpKernel {\n     // hessians.\n     const Tensor* hessians_t;\n     OP_REQUIRES_OK(context, context->input(\"hessians\", &hessians_t));\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsMatrix(hessians_t->shape()),\n+        errors::InvalidArgument(\"hessians must be a matrix, received shape \",\n+                                hessians_t->shape().DebugString()));\n     const auto hessians = hessians_t->matrix<float>();\n \n     // feature indices."
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 132,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/5d96267d907ac2119cbccf1416b749195e8fd8de",
    "date": "2021-11-18T19:15:24-08:00",
    "message": "Prevent `CHECK`-fail DOS in `BoostedTreesSparseAggregateStatsOp`.\n\nCalling `tensor->matrix` should only happen after checking that the tensor shape implies a matrix.\n\nPiperOrigin-RevId: 410951067\nChange-Id: I73a968f2116cc14b3e0e868d8a188aa232b47643",
    "changes": [
        {
            "name": "stats_ops.cc",
            "path": "tensorflow/core/kernels/boosted_trees/stats_ops.cc",
            "patches": [
                {
                    "old_start": 1616,
                    "old_length": 6,
                    "new_start": 1616,
                    "new_length": 10,
                    "hunk_buggy": "['     // gradients.\\n', '     const Tensor* gradients_t;\\n', '     OP_REQUIRES_OK(context, context->input(\"gradients\", &gradients_t));\\n', '     const auto gradients = gradients_t->matrix<float>();\\n', ' \\n', '     // hessians.']",
                    "hunk_fix": "@@ -1616,6 +1616,10 @@ class BoostedTreesSparseAggregateStatsOp : public OpKernel {\n     // gradients.\n     const Tensor* gradients_t;\n     OP_REQUIRES_OK(context, context->input(\"gradients\", &gradients_t));\n+    OP_REQUIRES(\n+        context, TensorShapeUtils::IsMatrix(gradients_t->shape()),\n+        errors::InvalidArgument(\"gradients must be a matrix, received shape \",\n+                                gradients_t->shape().DebugString()));\n     const auto gradients = gradients_t->matrix<float>();\n \n     // hessians."
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 133,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/28dacabab5aac2963e37e622f4b157cf00d82662",
    "date": "2021-11-17T07:06:17-08:00",
    "message": "[tf] Explicitly check that runner index is in bounds and runner is available\n\nPiperOrigin-RevId: 410516176\nChange-Id: I8caecaffb958f3cd5251d85a9f6dc7cbc5c06742",
    "changes": [
        {
            "name": "op_kernel_runner.h",
            "path": "tensorflow/core/runtime_fallback/kernel/op_kernel_runner.h",
            "patches": [
                {
                    "old_start": 156,
                    "old_length": 9,
                    "new_start": 156,
                    "new_length": 14,
                    "hunk_buggy": "['   // not in the table. Note that the returned pointer will be invalidated if\\n', '   // Insert() is called.\\n', '   const OpKernelRunner* Get(int64_t index) const {\\n', '-    DCHECK_GT(runners_.size(), index);\\n', '     auto& result = runners_.at(index);\\n', '-    DCHECK(result.has_value());\\n', '     return &(*result);\\n', '   }\\n', ' ']",
                    "hunk_fix": "@@ -156,9 +156,14 @@ class OpKernelRunnerTable {\n   // not in the table. Note that the returned pointer will be invalidated if\n   // Insert() is called.\n   const OpKernelRunner* Get(int64_t index) const {\n-    DCHECK_GT(runners_.size(), index);\n+    // Out of bounds vector access will throw an exception and anyway will crash\n+    // the binary, prefer a more readable error message.\n+    CHECK_GT(runners_.size(), index)  // Crash OK\n+        << \"runner index is out of bounds: index=\" << index\n+        << \" size=\" << runners_.size();\n     auto& result = runners_.at(index);\n-    DCHECK(result.has_value());\n+    CHECK(result.has_value())  // Crash OK\n+        << \"runner is not available: index=\" << index;\n     return &(*result);\n   }\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 134,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/6381a7b127bd276a3817a93e5423b15a06c33419",
    "date": "2021-11-15T14:00:13-08:00",
    "message": "[tf.data] Add a check for ram_budget == 0 to avoid division by 0 exception when ram_budget is not set.\n\nPiperOrigin-RevId: 410071934\nChange-Id: Ida9fb401ba24367e48066c8a899962877429c3da",
    "changes": [
        {
            "name": "model.cc",
            "path": "tensorflow/core/framework/model.cc",
            "patches": [
                {
                    "old_start": 46,
                    "old_length": 6,
                    "new_start": 46,
                    "new_length": 9,
                    "hunk_buggy": "[' \\n', ' // Records the ram usage of hill climbing algorithm.\\n', ' void RecordAutotuneRamUsage(int64 ram_budget, double max_buffered_bytes) {\\n', '   const auto memory_info = port::GetMemoryInfo();\\n', '   // Records ratio of memory used since RootDataset was created over the ram\\n', '   // budget.']",
                    "hunk_fix": "@@ -46,6 +46,9 @@ bool AreAllParametersMax(const Model::ModelParameters& parameters) {\n \n // Records the ram usage of hill climbing algorithm.\n void RecordAutotuneRamUsage(int64 ram_budget, double max_buffered_bytes) {\n+  if (ram_budget == 0) {\n+    return;\n+  }\n   const auto memory_info = port::GetMemoryInfo();\n   // Records ratio of memory used since RootDataset was created over the ram\n   // budget."
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 135,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/a7c02f1a9bbc35473969618a09ee5f9f5d3e52d9",
    "date": "2021-11-12T00:29:24-08:00",
    "message": "Validate real and expected type of arguments to cwise ops.\n\nWithout this validation, it is possible to trigger a `CHECK`-fail denial of service.\n\nThis is a rollforward of a previous commit which was rolled back as it was relying on RTTI. This time we don't use RTTI, we replace `typeid(Tin).name()` with a double function call, `DataTypeString(DataTypeToEnum<Tin>::v())`.\n\nPiperOrigin-RevId: 409340416\nChange-Id: I96080b2796729a3a9b65e7c68307ac276070f2f0",
    "changes": [
        {
            "name": "cwise_ops_common.h",
            "path": "tensorflow/core/kernels/cwise_ops_common.h",
            "patches": [
                {
                    "old_start": 87,
                    "old_length": 7,
                    "new_start": 87,
                    "new_length": 17,
                    "hunk_buggy": "[' \\n', '   void Compute(OpKernelContext* ctx) override {\\n', '     const Tensor& input_0 = ctx->input(0);\\n', '     const Tensor& input_1 = ctx->input(1);\\n', '     const Device& eigen_device = ctx->eigen_device<Device>();\\n', '     bool error = false;\\n', '     bool* const error_ptr = Functor::has_errors ? &error : nullptr;']",
                    "hunk_fix": "@@ -87,7 +87,17 @@ class BinaryOp : public BinaryOpShared {\n \n   void Compute(OpKernelContext* ctx) override {\n     const Tensor& input_0 = ctx->input(0);\n+    OP_REQUIRES(ctx, input_0.dtype() == DataTypeToEnum<Tin>::v(),\n+                errors::InvalidArgument(\n+                    \"Expected tensor of type \",\n+                    DataTypeString(DataTypeToEnum<Tin>::v()), \" but got type \",\n+                    DataTypeString(input_0.dtype())));\n     const Tensor& input_1 = ctx->input(1);\n+    OP_REQUIRES(ctx, input_1.dtype() == DataTypeToEnum<Tin>::v(),\n+                errors::InvalidArgument(\n+                    \"Expected tensor of type \",\n+                    DataTypeString(DataTypeToEnum<Tin>::v()), \" but got type \",\n+                    DataTypeString(input_1.dtype())));\n     const Device& eigen_device = ctx->eigen_device<Device>();\n     bool error = false;\n     bool* const error_ptr = Functor::has_errors ? &error : nullptr;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 136,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/4f38b1ac8e42727e18a2f0bde06d3bee8e77b250",
    "date": "2021-11-09T17:03:42-08:00",
    "message": "Prevent null dereference read in `GetInitOp`.\n\nWe have a map of maps. We test that the key exists in the first map but then we don't have any validation that this also means the second map has the needed key. In the scenarios where this is not the case, we'll dereference a nullptr, if we don't have this check\n\nPiperOrigin-RevId: 408739325\nChange-Id: If9bb7ed759aba1f3b56a34913f209508dbaf65ce",
    "changes": [
        {
            "name": "loader_util.cc",
            "path": "tensorflow/cc/saved_model/loader_util.cc",
            "patches": [
                {
                    "old_start": 34,
                    "old_length": 9,
                    "new_start": 34,
                    "new_length": 14,
                    "hunk_buggy": "['   const auto& init_op_sig_it =\\n', '       meta_graph_def.signature_def().find(kSavedModelInitOpSignatureKey);\\n', '   if (init_op_sig_it != sig_def_map.end()) {\\n', '-    *init_op_name = init_op_sig_it->second.outputs()\\n', '-                        .find(kSavedModelInitOpSignatureKey)\\n', '-                        ->second.name();\\n', '     return Status::OK();\\n', '   }\\n', ' ']",
                    "hunk_fix": "@@ -34,9 +34,14 @@ Status GetInitOp(const string& export_dir, const MetaGraphDef& meta_graph_def,\n   const auto& init_op_sig_it =\n       meta_graph_def.signature_def().find(kSavedModelInitOpSignatureKey);\n   if (init_op_sig_it != sig_def_map.end()) {\n-    *init_op_name = init_op_sig_it->second.outputs()\n-                        .find(kSavedModelInitOpSignatureKey)\n-                        ->second.name();\n+    const auto& sig_def_outputs = init_op_sig_it->second.outputs();\n+    const auto& sig_def_outputs_it =\n+        sig_def_outputs.find(kSavedModelInitOpSignatureKey);\n+    if (sig_def_outputs_it == sig_def_outputs.end()) {\n+      return errors::FailedPrecondition(\"Could not find output \",\n+                                        kSavedModelInitOpSignatureKey);\n+    }\n+    *init_op_name = sig_def_outputs_it->second.name();\n     return Status::OK();\n   }\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 137,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/cb164786dc891ea11d3a900e90367c339305dc7b",
    "date": "2021-11-08T10:35:56-08:00",
    "message": "Properly handle the case where `SpecializeType()` returns an error `Status`.\n\nIf the error case in `SpecializeType()` is reached, then we would get a crash when trying to access the value of an errorenous `StatusOr` object\n\nPiperOrigin-RevId: 408380069\nChange-Id: If3c3fc876dcf9384d5ec7a4985adc68c23ea7318",
    "changes": [
        {
            "name": "shape_inference.cc",
            "path": "tensorflow/core/framework/shape_inference.cc",
            "patches": [
                {
                    "old_start": 170,
                    "old_length": 7,
                    "new_start": 170,
                    "new_length": 10,
                    "hunk_buggy": "['     const std::vector<ShapeHandle>& input_tensors_as_shapes) {\\n', '   // TODO(mdan): This is also done at graph construction. Run only here instead?\\n', '   const auto ret = full_type::SpecializeType(attrs_, op_def);\\n', '-  DCHECK(ret.status().ok()) << \"while instantiating types: \" << ret.status();\\n', '   ret_types_ = ret.ValueOrDie();\\n', ' \\n', '   input_tensors_ = input_tensors;']",
                    "hunk_fix": "@@ -170,7 +170,10 @@ void InferenceContext::PreInputInit(\n     const std::vector<ShapeHandle>& input_tensors_as_shapes) {\n   // TODO(mdan): This is also done at graph construction. Run only here instead?\n   const auto ret = full_type::SpecializeType(attrs_, op_def);\n-  DCHECK(ret.status().ok()) << \"while instantiating types: \" << ret.status();\n+  if (!ret.status().ok()) {\n+    construction_status_ = ret.status();\n+    return;\n+  }\n   ret_types_ = ret.ValueOrDie();\n \n   input_tensors_ = input_tensors;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 138,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/b73a3c21a224f479af8d3b8af320c220a091906c",
    "date": "2021-11-03T07:25:02-07:00",
    "message": "[XLA] Add check for potential out-of-bound access.\n\nPiperOrigin-RevId: 407330138\nChange-Id: Ib32dfd4ac963bbb79e3dde999334c81c8b87f55f",
    "changes": [
        {
            "name": "hlo_evaluator.cc",
            "path": "tensorflow/compiler/xla/service/hlo_evaluator.cc",
            "patches": [
                {
                    "old_start": 2234,
                    "old_length": 6,
                    "new_start": 2234,
                    "new_length": 9,
                    "hunk_buggy": "['   std::vector<int64_t> increment(rank, 1);\\n', '   int64_t sort_dim = sort->dimensions(0);\\n', '   int64_t sort_dim_elements = key_shape.dimensions(sort_dim);\\n', '   increment[sort_dim] = sort_dim_elements;\\n', '   HloEvaluator embedded_evaluator(max_loop_iterations_);\\n', \"   // Iterate through each dimension except 'sort_dim'.\"]",
                    "hunk_fix": "@@ -2234,6 +2234,9 @@ Status HloEvaluator::HandleSort(HloInstruction* sort) {\n   std::vector<int64_t> increment(rank, 1);\n   int64_t sort_dim = sort->dimensions(0);\n   int64_t sort_dim_elements = key_shape.dimensions(sort_dim);\n+  TF_RET_CHECK(sort_dim >= 0 && sort_dim < increment.size())\n+      << \"Unexpected out-of-bound sort dimension \" << sort_dim\n+      << \" accessing increment of size \" << increment.size();\n   increment[sort_dim] = sort_dim_elements;\n   HloEvaluator embedded_evaluator(max_loop_iterations_);\n   // Iterate through each dimension except 'sort_dim'."
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 139,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/aaa3fb49374d59c89115730c8e2f672e70b9e3fa",
    "date": "2021-10-21T15:20:22-07:00",
    "message": "[TFLite] Bucketize op: Fix processing of bucket boundary array.\n\nThe param value may be a nullptr, which is an error; we should catch this and\navoid dereferencing it.\n\nPiperOrigin-RevId: 404889441\nChange-Id: I022d0d175c83e0498ab47ed93d7040ea3f3f3987",
    "changes": [
        {
            "name": "flatbuffer_conversions.cc",
            "path": "tensorflow/lite/core/api/flatbuffer_conversions.cc",
            "patches": [
                {
                    "old_start": 804,
                    "old_length": 7,
                    "new_start": 804,
                    "new_length": 19,
                    "hunk_buggy": "['               op->builtin_options_as_BucketizeOptions()) {\\n', '         const flatbuffers::Vector<float>* boundaries =\\n', '             bucketize_params->boundaries();\\n', '         params->num_boundaries = boundaries->size();\\n', '         params->boundaries = boundaries->data();\\n', '       }\\n', '       *builtin_data = params.release();']",
                    "hunk_fix": "@@ -804,7 +804,19 @@ TfLiteStatus ParseOpDataTfLite(const Operator* op, BuiltinOperator op_type,\n               op->builtin_options_as_BucketizeOptions()) {\n         const flatbuffers::Vector<float>* boundaries =\n             bucketize_params->boundaries();\n+        if (boundaries == nullptr) {\n+          TF_LITE_REPORT_ERROR(\n+              error_reporter,\n+              \"boundaries array not provided for operation 'bucketize'.\\n\");\n+          return kTfLiteError;\n+        }\n         params->num_boundaries = boundaries->size();\n+        if (boundaries->data() == nullptr) {\n+          TF_LITE_REPORT_ERROR(error_reporter,\n+                               \"boundaries.data() returned nullptr for \"\n+                               \"operation 'bucketize'.\\n\");\n+          return kTfLiteError;\n+        }\n         params->boundaries = boundaries->data();\n       }\n       *builtin_data = params.release();"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 140,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/67b6c880e39ba02ba53c7d499e45fd136090ee32",
    "date": "2021-09-21T07:34:46-07:00",
    "message": "In tf.map_fn: skip sanity check for `shape` of first value in `elems` if it doesn't have a shape attribute.  (E.g., this can happen if it's a CompsiteTensor.)\n\nPiperOrigin-RevId: 397990479\nChange-Id: I5e8547880dee811ab534ba9f25ef7b1b4ec75510",
    "changes": [
        {
            "name": "map_fn.py",
            "path": "tensorflow/python/ops/map_fn.py",
            "patches": [
                {
                    "old_start": 421,
                    "old_length": 10,
                    "new_start": 421,
                    "new_length": 11,
                    "hunk_buggy": "[' \\n', '     # Check that inputs are not scalars.\\n', '     first_elem = elems_flat[0]\\n', '-    elems_static_shape = first_elem.shape\\n', '-    if elems_static_shape.ndims is not None and elems_static_shape.ndims < 1:\\n', '-      raise ValueError(\\n', '-          \"Elements in elems must be 1+ dimensional Tensors, not scalars\")\\n', ' \\n', '     # Box any composite tensors into tensor lists.\\n', '     elems_batchable = _elems_flat_to_batchable(elems_flat)']",
                    "hunk_fix": "@@ -421,10 +421,11 @@ def map_fn(fn,\n \n     # Check that inputs are not scalars.\n     first_elem = elems_flat[0]\n-    elems_static_shape = first_elem.shape\n-    if elems_static_shape.ndims is not None and elems_static_shape.ndims < 1:\n-      raise ValueError(\n-          \"Elements in elems must be 1+ dimensional Tensors, not scalars\")\n+    if hasattr(first_elem, \"shape\"):\n+      elems_static_shape = first_elem.shape\n+      if elems_static_shape.ndims is not None and elems_static_shape.ndims < 1:\n+        raise ValueError(\n+            \"Elements in elems must be 1+ dimensional Tensors, not scalars\")\n \n     # Box any composite tensors into tensor lists.\n     elems_batchable = _elems_flat_to_batchable(elems_flat)"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 141,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/c5019e2156c749d35ec786ff7946a55006d9ba91",
    "date": "2021-08-30T08:23:38-07:00",
    "message": "Add a check.",
    "changes": [
        {
            "name": "gpu_cudamallocasync_allocator.cc",
            "path": "tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc",
            "patches": [
                {
                    "old_start": 381,
                    "old_length": 6,
                    "new_start": 381,
                    "new_length": 11,
                    "hunk_buggy": "[' \\n', ' void GpuCudaMallocAsyncAllocator::SetStream(void* stream) {\\n', ' #if TF_CUDA_MALLOC_ASYNC_SUPPORTED\\n', '   uint64_t pool_size_64 = 0;\\n', '   if (auto status = cuMemPoolGetAttribute(\\n', '           pool_, CU_MEMPOOL_ATTR_RELEASE_THRESHOLD, &pool_size_64)) {']",
                    "hunk_fix": "@@ -381,6 +381,11 @@ bool GpuCudaMallocAsyncAllocator::ClearStats() {\n \n void GpuCudaMallocAsyncAllocator::SetStream(void* stream) {\n #if TF_CUDA_MALLOC_ASYNC_SUPPORTED\n+  if (cuda_stream_ != nullptr) {\n+    LOG(FATAL) <<  // Crash OK.\n+        \"Trying to set the stream twice. This isn't supported. \";\n+  }\n+\n   uint64_t pool_size_64 = 0;\n   if (auto status = cuMemPoolGetAttribute(\n           pool_, CU_MEMPOOL_ATTR_RELEASE_THRESHOLD, &pool_size_64)) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 142,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/63753d5f1531b17cf8cbbf1d8b77c16edcfb9711",
    "date": "2021-08-18T17:02:15-07:00",
    "message": "Change DCHECK_LE to DCHECK_LT when checking invariant on original indices for sorted items\n\nIndices of items should be strictly smaller than the size of the vector.\n\nPiperOrigin-RevId: 391640935\nChange-Id: I110f2a8d269e90d9853094f68ada700d0c346c12",
    "changes": [
        {
            "name": "saved_model.cc",
            "path": "tensorflow/core/tfrt/saved_model/saved_model.cc",
            "patches": [
                {
                    "old_start": 749,
                    "old_length": 7,
                    "new_start": 749,
                    "new_length": 7,
                    "hunk_buggy": "['   // Use sorted indices to generate sorted names.\\n', '   sorted_names.reserve(names.size());\\n', '   for (int original_index : original_indices) {\\n', '-    DCHECK_LE(original_index, names.size());\\n', '     sorted_names.push_back(names[original_index]);\\n', '   }\\n', ' }']",
                    "hunk_fix": "@@ -749,7 +749,7 @@ void CreateSortedNamesAndOriginalIndices(absl::Span<const std::string> names,\n   // Use sorted indices to generate sorted names.\n   sorted_names.reserve(names.size());\n   for (int original_index : original_indices) {\n-    DCHECK_LE(original_index, names.size());\n+    DCHECK_LT(original_index, names.size());\n     sorted_names.push_back(names[original_index]);\n   }\n }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 143,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/4923de56ec94fff7770df259ab7f2288a74feb41",
    "date": "2021-08-02T13:57:19-07:00",
    "message": "Don't do any work when reshaping 0 elements sparse tensor.\n\nIf reshaping to 0 elements tensor, check that input has no elements.\nIf reshaping no elements input, check that output has no elements.\n\nPiperOrigin-RevId: 388296986\nChange-Id: Iadc9fe7252e14313ca987e69bf0d7042fd10232a",
    "changes": [
        {
            "name": "reshape_util.cc",
            "path": "tensorflow/core/kernels/reshape_util.cc",
            "patches": [
                {
                    "old_start": 174,
                    "old_length": 6,
                    "new_start": 174,
                    "new_length": 12,
                    "hunk_buggy": "['                                           TensorShape({nnz, output_rank}),\\n', '                                           &result_indices));\\n', '   if (nnz > 0) {\\n', '     OP_REQUIRES_OK(context, functor::ReshapeSparseTensorFunctor<Device>()(\\n', '                                 context, input_shape, output_shape,\\n', '                                 input_indices_in.matrix<int64>(),']",
                    "hunk_fix": "@@ -174,6 +174,12 @@ void ReshapeSparseTensor(OpKernelContext *context,\n                                           TensorShape({nnz, output_rank}),\n                                           &result_indices));\n   if (nnz > 0) {\n+    OP_REQUIRES(\n+        context, dense_size > 0 && product > 0,\n+        errors::InvalidArgument(\n+            \"Input tensor has \", nnz, \" non zero elements but input shape (\",\n+            input_shape.DebugString(), \") or output shape (\",\n+            output_shape.DebugString(), \") is empty\"));\n     OP_REQUIRES_OK(context, functor::ReshapeSparseTensorFunctor<Device>()(\n                                 context, input_shape, output_shape,\n                                 input_indices_in.matrix<int64>(),"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 144,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/f2a673bd34f0d64b8e40a551ac78989d16daad09",
    "date": "2021-07-30T19:12:41-07:00",
    "message": "Add missing validation to `matrix_diag_op.cc`\n\nPiperOrigin-RevId: 387923533\nChange-Id: Idfffeb328d5f9c6748d992d28a56d6e9e45103a0",
    "changes": [
        {
            "name": "matrix_diag_op.cc",
            "path": "tensorflow/core/kernels/linalg/matrix_diag_op.cc",
            "patches": [
                {
                    "old_start": 73,
                    "old_length": 6,
                    "new_start": 73,
                    "new_length": 9,
                    "hunk_buggy": "['                   errors::InvalidArgument(\\n', '                       \"diag_index must be a scalar or vector, received shape: \",\\n', '                       diag_index.shape().DebugString()));\\n', '       lower_diag_index = diag_index.flat<int32>()(0);\\n', '       upper_diag_index = lower_diag_index;\\n', '       if (TensorShapeUtils::IsVector(diag_index.shape())) {\\n']",
                    "hunk_fix": "@@ -73,6 +73,9 @@ class MatrixDiagPartOp : public OpKernel {\n                   errors::InvalidArgument(\n                       \"diag_index must be a scalar or vector, received shape: \",\n                       diag_index.shape().DebugString()));\n+      OP_REQUIRES(context, diag_index.NumElements() > 0,\n+                  errors::InvalidArgument(\n+                      \"Expected diag_index to have at least 1 element\"));\n       lower_diag_index = diag_index.flat<int32>()(0);\n       upper_diag_index = lower_diag_index;\n       if (TensorShapeUtils::IsVector(diag_index.shape())) {\n"
                },
                {
                    "old_start": 179,
                    "old_length": 6,
                    "new_start": 182,
                    "new_length": 9,
                    "hunk_buggy": "['                   errors::InvalidArgument(\\n', '                       \"diag_index must be a scalar or vector, received shape: \",\\n', '                       diag_index.shape().DebugString()));\\n', '       lower_diag_index = diag_index.flat<int32>()(0);\\n', '       upper_diag_index = lower_diag_index;\\n', '       if (TensorShapeUtils::IsVector(diag_index.shape())) {']",
                    "hunk_fix": "@@ -179,6 +182,9 @@ class MatrixDiagOp : public OpKernel {\n                   errors::InvalidArgument(\n                       \"diag_index must be a scalar or vector, received shape: \",\n                       diag_index.shape().DebugString()));\n+      OP_REQUIRES(context, diag_index.NumElements() > 0,\n+                  errors::InvalidArgument(\n+                      \"Expected diag_index to have at least 1 element\"));\n       lower_diag_index = diag_index.flat<int32>()(0);\n       upper_diag_index = lower_diag_index;\n       if (TensorShapeUtils::IsVector(diag_index.shape())) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 145,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/532f5c5a547126c634fefd43bbad1dc6417678ac",
    "date": "2021-07-29T22:54:59-07:00",
    "message": "Prevent nullptr deref in validation of indexes in map ops.\n\nPiperOrigin-RevId: 387738023\nChange-Id: I83d18d36a7b82ffd2a40b5124a4e5b4c72238f27",
    "changes": [
        {
            "name": "map_stage_op.cc",
            "path": "tensorflow/core/kernels/map_stage_op.cc",
            "patches": [
                {
                    "old_start": 210,
                    "old_length": 9,
                    "new_start": 210,
                    "new_length": 9,
                    "hunk_buggy": "['                                    const OptionalTuple& tuple)\\n', '       TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {\\n', '     if (tuple[index].has_value()) {\\n', '-      return Status(errors::InvalidArgument(\\n', '           \"The tensor for index \\'\", index, \"\\' for key \\'\", key.scalar<int64>()(),\\n', '-          \"\\' was already initialized \\'\", dtypes_.size(), \"\\'.\"));\\n', '     }\\n', ' \\n', '     return Status::OK();\\n']",
                    "hunk_fix": "@@ -210,9 +210,9 @@ class StagingMap : public ResourceBase {\n                                    const OptionalTuple& tuple)\n       TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {\n     if (tuple[index].has_value()) {\n-      return Status(errors::InvalidArgument(\n+      return errors::InvalidArgument(\n           \"The tensor for index '\", index, \"' for key '\", key.scalar<int64>()(),\n-          \"' was already initialized '\", dtypes_.size(), \"'.\"));\n+          \"' was already initialized '\", dtypes_.size(), \"'.\");\n     }\n \n     return Status::OK();\n"
                },
                {
                    "old_start": 220,
                    "old_length": 6,
                    "new_start": 220,
                    "new_length": 10,
                    "hunk_buggy": "[' \\n', '   // Check that the indices are strictly ordered\\n', '   Status check_index_ordering(const Tensor& indices) {\\n', '     auto findices = indices.flat<int>();\\n', ' \\n', '     for (std::size_t i = 0; i < findices.dimension(0) - 1; ++i) {\\n']",
                    "hunk_fix": "@@ -220,6 +220,10 @@ class StagingMap : public ResourceBase {\n \n   // Check that the indices are strictly ordered\n   Status check_index_ordering(const Tensor& indices) {\n+    if (indices.NumElements() == 0) {\n+      return errors::InvalidArgument(\"Indices are empty\");\n+    }\n+\n     auto findices = indices.flat<int>();\n \n     for (std::size_t i = 0; i < findices.dimension(0) - 1; ++i) {\n"
                },
                {
                    "old_start": 227,
                    "old_length": 8,
                    "new_start": 231,
                    "new_length": 7,
                    "hunk_buggy": "['         continue;\\n', '       }\\n', ' \\n', '-      return Status(\\n', '-          errors::InvalidArgument(\"Indices are not strictly ordered\"));\\n', '     }\\n', ' \\n', '     return Status::OK();\\n']",
                    "hunk_fix": "@@ -227,8 +231,7 @@ class StagingMap : public ResourceBase {\n         continue;\n       }\n \n-      return Status(\n-          errors::InvalidArgument(\"Indices are not strictly ordered\"));\n+      return errors::InvalidArgument(\"Indices are not strictly ordered\");\n     }\n \n     return Status::OK();\n"
                },
                {
                    "old_start": 238,
                    "old_length": 10,
                    "new_start": 241,
                    "new_length": 10,
                    "hunk_buggy": "['   Status check_memory_limit(std::size_t bytes)\\n', '       TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {\\n', '     if (has_memory_limit() && bytes > memory_limit_) {\\n', '-      return Status(errors::ResourceExhausted(\\n', '           \"Attempted to insert tensors with combined size of \\'\", bytes,\\n', '           \"\\' bytes into Staging Area with a memory limit of \\'\", memory_limit_,\\n', '-          \"\\'.\"));\\n', '     }\\n', ' \\n', '     return Status::OK();']",
                    "hunk_fix": "@@ -238,10 +241,10 @@ class StagingMap : public ResourceBase {\n   Status check_memory_limit(std::size_t bytes)\n       TF_EXCLUSIVE_LOCKS_REQUIRED(mu_) {\n     if (has_memory_limit() && bytes > memory_limit_) {\n-      return Status(errors::ResourceExhausted(\n+      return errors::ResourceExhausted(\n           \"Attempted to insert tensors with combined size of '\", bytes,\n           \"' bytes into Staging Area with a memory limit of '\", memory_limit_,\n-          \"'.\"));\n+          \"'.\");\n     }\n \n     return Status::OK();"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 146,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/102cacf28ad5a9e7f00b5a195d1995ead8870006",
    "date": "2021-07-29T22:31:35-07:00",
    "message": "Add missing validation to `maxpooling_op.cc`\n\nPiperOrigin-RevId: 387737899\nChange-Id: I95d33dac25a3c8e1d23934e623c43e81b7da0ece",
    "changes": [
        {
            "name": "maxpooling_op.cc",
            "path": "tensorflow/core/kernels/maxpooling_op.cc",
            "patches": [
                {
                    "old_start": 271,
                    "old_length": 6,
                    "new_start": 271,
                    "new_length": 10,
                    "hunk_buggy": "['                 errors::InvalidArgument(\"tensor_in must be 4-dimensional\"));\\n', '     OP_REQUIRES(context, tensor_out.dims() == 4,\\n', '                 errors::InvalidArgument(\"tensor_out must be 4-dimensional\"));\\n', '     // For maxpooling, out_backprop should have 4 dimensions.\\n', '     OP_REQUIRES(context, out_backprop.dims() == 4,\\n', '                 errors::InvalidArgument(\"out_backprop must be 4-dimensional\"));\\n']",
                    "hunk_fix": "@@ -271,6 +271,10 @@ class MaxPoolingGradOp : public OpKernel {\n                 errors::InvalidArgument(\"tensor_in must be 4-dimensional\"));\n     OP_REQUIRES(context, tensor_out.dims() == 4,\n                 errors::InvalidArgument(\"tensor_out must be 4-dimensional\"));\n+    OP_REQUIRES(context, tensor_in.NumElements() > 0,\n+                errors::InvalidArgument(\"tensor_in must not be empty\"));\n+    OP_REQUIRES(context, tensor_out.NumElements() > 0,\n+                errors::InvalidArgument(\"tensor_out must not be empty\"));\n     // For maxpooling, out_backprop should have 4 dimensions.\n     OP_REQUIRES(context, out_backprop.dims() == 4,\n                 errors::InvalidArgument(\"out_backprop must be 4-dimensional\"));\n"
                },
                {
                    "old_start": 949,
                    "old_length": 6,
                    "new_start": 953,
                    "new_length": 10,
                    "hunk_buggy": "[' \\n', '   void Compute(OpKernelContext* context) override {\\n', '     const Tensor& tensor_in = context->input(0);\\n', ' \\n', '     PoolParameters params{context,\\n', '                           ksize_,']",
                    "hunk_fix": "@@ -949,6 +953,10 @@ class MaxPoolingWithArgmaxOp : public OpKernel {\n \n   void Compute(OpKernelContext* context) override {\n     const Tensor& tensor_in = context->input(0);\n+    OP_REQUIRES(context, tensor_in.dims() == 4,\n+                errors::InvalidArgument(\"tensor_in must be 4-dimensional\"));\n+    OP_REQUIRES(context, tensor_in.NumElements() > 0,\n+                errors::InvalidArgument(\"tensor_in must not be empty\"));\n \n     PoolParameters params{context,\n                           ksize_,"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 147,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/1071f554dbd09f7e101324d366eec5f4fe5a3ece",
    "date": "2021-07-29T18:27:51-07:00",
    "message": "Add missing validation to `RaggedTensorToSparse`.\n\nThere needs to be a check that the splits allow for valid ragged tensors.\n\nPiperOrigin-RevId: 387712169\nChange-Id: I2499175324b82b65d159a260c7f83b98ceb5cc7d",
    "changes": [
        {
            "name": "ragged_tensor_to_sparse_kernel.cc",
            "path": "tensorflow/core/kernels/ragged_tensor_to_sparse_kernel.cc",
            "patches": [
                {
                    "old_start": 21,
                    "old_length": 6,
                    "new_start": 21,
                    "new_length": 7,
                    "hunk_buggy": "[' #include \"tensorflow/core/framework/register_types.h\"\\n', ' #include \"tensorflow/core/framework/tensor.h\"\\n', ' #include \"tensorflow/core/framework/tensor_shape.h\"\\n', ' \\n', ' namespace tensorflow {\\n', ' \\n']",
                    "hunk_fix": "@@ -21,6 +21,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/register_types.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n #include \"tensorflow/core/framework/tensor_shape.h\"\n+#include \"tensorflow/core/platform/errors.h\"\n \n namespace tensorflow {\n \n"
                },
                {
                    "old_start": 38,
                    "old_length": 7,
                    "new_start": 39,
                    "new_length": 8,
                    "hunk_buggy": "['     OP_REQUIRES_OK(\\n', '         context, context->input_list(\"rt_nested_splits\", &rt_nested_splits_in));\\n', '     const int rt_nested_splits_len = rt_nested_splits_in.size();\\n', '-    DCHECK_GT(rt_nested_splits_len, 0);  // Enforced by REGISTER_OP.\\n', '     std::vector<ConstFlatSplits> rt_nested_splits;\\n', '     rt_nested_splits.reserve(rt_nested_splits_len);\\n', '     for (int i = 0; i < rt_nested_splits_len; ++i) {\\n']",
                    "hunk_fix": "@@ -38,7 +39,8 @@ class RaggedTensorToSparseOp : public OpKernel {\n     OP_REQUIRES_OK(\n         context, context->input_list(\"rt_nested_splits\", &rt_nested_splits_in));\n     const int rt_nested_splits_len = rt_nested_splits_in.size();\n-    DCHECK_GT(rt_nested_splits_len, 0);  // Enforced by REGISTER_OP.\n+    OP_REQUIRES(context, rt_nested_splits_len > 0,\n+                errors::InvalidArgument(\"rt_nested_splits must be non empty\"));\n     std::vector<ConstFlatSplits> rt_nested_splits;\n     rt_nested_splits.reserve(rt_nested_splits_len);\n     for (int i = 0; i < rt_nested_splits_len; ++i) {\n"
                },
                {
                    "old_start": 162,
                    "old_length": 6,
                    "new_start": 164,
                    "new_length": 14,
                    "hunk_buggy": "['       if (rt_nested_splits[i](0) != 0) {\\n', '         return InvalidArgument(\"First value of ragged splits must be 0.\");\\n', '       }\\n', '       if (i > 0) {\\n', '         SPLITS_TYPE last_split =\\n', '             rt_nested_splits[i - 1](rt_nested_splits[i - 1].size() - 1);']",
                    "hunk_fix": "@@ -162,6 +164,14 @@ class RaggedTensorToSparseOp : public OpKernel {\n       if (rt_nested_splits[i](0) != 0) {\n         return InvalidArgument(\"First value of ragged splits must be 0.\");\n       }\n+      for (int j = 1; j < rt_nested_splits[i].size(); ++j) {\n+        if (rt_nested_splits[i](j) < rt_nested_splits[i](j - 1)) {\n+          return InvalidArgument(\n+              \"Ragged splits should be non decreasing, but we got \",\n+              rt_nested_splits[i](j - 1), \" followed by \",\n+              rt_nested_splits[i](j));\n+        }\n+      }\n       if (i > 0) {\n         SPLITS_TYPE last_split =\n             rt_nested_splits[i - 1](rt_nested_splits[i - 1].size() - 1);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 148,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/9e62869465573cb2d9b5053f1fa02a81fce21d69",
    "date": "2021-07-29T16:35:05-07:00",
    "message": "Add more validation to `RequantizationRangePerChannel`.\n\nPiperOrigin-RevId: 387693946\nChange-Id: Ife8dcbdb021bec4787eef6a4361dd08f17c14bd6",
    "changes": [
        {
            "name": "mkl_requantization_range_per_channel_op.cc",
            "path": "tensorflow/core/kernels/mkl/mkl_requantization_range_per_channel_op.cc",
            "patches": [
                {
                    "old_start": 57,
                    "old_length": 6,
                    "new_start": 57,
                    "new_length": 20,
                    "hunk_buggy": "['         ctx, input_max.dim_size(0) == depth,\\n', '         errors::InvalidArgument(\"input_max has incorrect size, expected \",\\n', '                                 depth, \" was \", input_max.dim_size(0)));\\n', ' \\n', '     const float* input_min_data = input_min.flat<float>().data();\\n', '     const float* input_max_data = input_max.flat<float>().data();']",
                    "hunk_fix": "@@ -57,6 +57,20 @@ class MklRequantizationRangePerChannelOp : public OpKernel {\n         ctx, input_max.dim_size(0) == depth,\n         errors::InvalidArgument(\"input_max has incorrect size, expected \",\n                                 depth, \" was \", input_max.dim_size(0)));\n+    OP_REQUIRES(\n+        ctx, input_min.NumElements() == depth,\n+        errors::InvalidArgument(\"input_min must have the same number of \"\n+                                \"elements as input_max, got \",\n+                                input_min.NumElements(), \" and \", depth));\n+    OP_REQUIRES(ctx, input.NumElements() > 0,\n+                errors::InvalidArgument(\"input must not be empty\"));\n+    OP_REQUIRES(ctx, input.dims() == 4,\n+                errors::InvalidArgument(\"input must be in NHWC format\"));\n+    OP_REQUIRES(\n+        ctx, input.dim_size(3) == depth,\n+        errors::InvalidArgument(\n+            \"input must have same number of channels as length of input_min: \",\n+            input.dim_size(3), \" vs \", depth));\n \n     const float* input_min_data = input_min.flat<float>().data();\n     const float* input_max_data = input_max.flat<float>().data();"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 149,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/429f009d2b2c09028647dd4bb7b3f6f414bbaad7",
    "date": "2021-07-28T13:43:55-07:00",
    "message": "Add remaining missing validation to `BoostedTreesCalculateBestFeatureSplit`\n\nPiperOrigin-RevId: 387423006\nChange-Id: I8eaf30efb223011519e60707bfa751b275d3a443",
    "changes": [
        {
            "name": "stats_ops.cc",
            "path": "tensorflow/core/kernels/boosted_trees/stats_ops.cc",
            "patches": [
                {
                    "old_start": 14,
                    "old_length": 6,
                    "new_start": 14,
                    "new_length": 7,
                    "hunk_buggy": "[' ==============================================================================*/\\n', ' \\n', ' #include <limits>\\n', ' #include <vector>\\n', ' \\n', ' #include \"third_party/eigen3/Eigen/Core\"\\n']",
                    "hunk_fix": "@@ -14,6 +14,7 @@ limitations under the License.\n ==============================================================================*/\n \n #include <limits>\n+#include <string>\n #include <vector>\n \n #include \"third_party/eigen3/Eigen/Core\"\n"
                },
                {
                    "old_start": 22,
                    "old_length": 6,
                    "new_start": 23,
                    "new_length": 7,
                    "hunk_buggy": "[' #include \"tensorflow/core/framework/tensor_shape.h\"\\n', ' #include \"tensorflow/core/kernels/boosted_trees/boosted_trees.pb.h\"\\n', ' #include \"tensorflow/core/kernels/boosted_trees/tree_helper.h\"\\n', ' #include \"tensorflow/core/platform/logging.h\"\\n', ' \\n', ' namespace tensorflow {\\n']",
                    "hunk_fix": "@@ -22,6 +23,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/tensor_shape.h\"\n #include \"tensorflow/core/kernels/boosted_trees/boosted_trees.pb.h\"\n #include \"tensorflow/core/kernels/boosted_trees/tree_helper.h\"\n+#include \"tensorflow/core/platform/errors.h\"\n #include \"tensorflow/core/platform/logging.h\"\n \n namespace tensorflow {\n"
                },
                {
                    "old_start": 254,
                    "old_length": 12,
                    "new_start": 256,
                    "new_length": 18,
                    "hunk_buggy": "['     // node_id_range\\n', '     const Tensor* node_id_range_t;\\n', '     OP_REQUIRES_OK(context, context->input(\"node_id_range\", &node_id_range_t));\\n', '     const auto node_id_range = node_id_range_t->vec<int32>();\\n', '     const int32_t node_id_first = node_id_range(0);  // inclusive\\n', '     const int32_t node_id_last = node_id_range(1);   // exclusive\\n', ' \\n', '     const Tensor* stats_summary_t;\\n', '     OP_REQUIRES_OK(context, context->input(\"stats_summary\", &stats_summary_t));\\n', '     TTypes<float, 4>::ConstTensor stats_summary =\\n', '         stats_summary_t->tensor<float, 4>();\\n', '     const int32_t feature_dims = stats_summary_t->dim_size(1);\\n']",
                    "hunk_fix": "@@ -254,12 +256,18 @@ class BoostedTreesCalculateBestFeatureSplitOp : public OpKernel {\n     // node_id_range\n     const Tensor* node_id_range_t;\n     OP_REQUIRES_OK(context, context->input(\"node_id_range\", &node_id_range_t));\n+    OP_REQUIRES(\n+        context, node_id_range_t->NumElements() == 2,\n+        errors::InvalidArgument(\"node_id_range argument must have shape [2]\"));\n     const auto node_id_range = node_id_range_t->vec<int32>();\n     const int32_t node_id_first = node_id_range(0);  // inclusive\n     const int32_t node_id_last = node_id_range(1);   // exclusive\n \n     const Tensor* stats_summary_t;\n     OP_REQUIRES_OK(context, context->input(\"stats_summary\", &stats_summary_t));\n+    OP_REQUIRES(\n+        context, stats_summary_t->shape().dims() == 4,\n+        errors::InvalidArgument(\"stats_summary argument must have rank 4\"));\n     TTypes<float, 4>::ConstTensor stats_summary =\n         stats_summary_t->tensor<float, 4>();\n     const int32_t feature_dims = stats_summary_t->dim_size(1);\n"
                },
                {
                    "old_start": 272,
                    "old_length": 6,
                    "new_start": 280,
                    "new_length": 8,
                    "hunk_buggy": "[' \\n', '     const Tensor* l1_t;\\n', '     OP_REQUIRES_OK(context, context->input(\"l1\", &l1_t));\\n', '     const auto l1 = l1_t->scalar<float>()();\\n', '     DCHECK_GE(l1, 0);\\n', '     if (logits_dim_ > 1) {\\n']",
                    "hunk_fix": "@@ -272,6 +280,8 @@ class BoostedTreesCalculateBestFeatureSplitOp : public OpKernel {\n \n     const Tensor* l1_t;\n     OP_REQUIRES_OK(context, context->input(\"l1\", &l1_t));\n+    OP_REQUIRES(context, l1_t->NumElements() == 1,\n+                errors::InvalidArgument(\"l1 argument must be a scalar\"));\n     const auto l1 = l1_t->scalar<float>()();\n     DCHECK_GE(l1, 0);\n     if (logits_dim_ > 1) {\n"
                },
                {
                    "old_start": 281,
                    "old_length": 17,
                    "new_start": 291,
                    "new_length": 25,
                    "hunk_buggy": "[' \\n', '     const Tensor* l2_t;\\n', '     OP_REQUIRES_OK(context, context->input(\"l2\", &l2_t));\\n', '     const auto l2 = l2_t->scalar<float>()();\\n', '     DCHECK_GE(l2, 0);\\n', ' \\n', '     const Tensor* tree_complexity_t;\\n', '     OP_REQUIRES_OK(context,\\n', '                    context->input(\"tree_complexity\", &tree_complexity_t));\\n', '     const auto tree_complexity = tree_complexity_t->scalar<float>()();\\n', ' \\n', '     const Tensor* min_node_weight_t;\\n', '     OP_REQUIRES_OK(context,\\n', '                    context->input(\"min_node_weight\", &min_node_weight_t));\\n', '     const auto min_node_weight = min_node_weight_t->scalar<float>()();\\n', ' \\n', '     std::vector<int32> output_node_ids;\\n']",
                    "hunk_fix": "@@ -281,17 +291,25 @@ class BoostedTreesCalculateBestFeatureSplitOp : public OpKernel {\n \n     const Tensor* l2_t;\n     OP_REQUIRES_OK(context, context->input(\"l2\", &l2_t));\n+    OP_REQUIRES(context, l2_t->NumElements() == 1,\n+                errors::InvalidArgument(\"l2 argument must be a scalar\"));\n     const auto l2 = l2_t->scalar<float>()();\n     DCHECK_GE(l2, 0);\n \n     const Tensor* tree_complexity_t;\n     OP_REQUIRES_OK(context,\n                    context->input(\"tree_complexity\", &tree_complexity_t));\n+    OP_REQUIRES(\n+        context, tree_complexity_t->NumElements() == 1,\n+        errors::InvalidArgument(\"tree_complexity argument must be a scalar\"));\n     const auto tree_complexity = tree_complexity_t->scalar<float>()();\n \n     const Tensor* min_node_weight_t;\n     OP_REQUIRES_OK(context,\n                    context->input(\"min_node_weight\", &min_node_weight_t));\n+    OP_REQUIRES(\n+        context, min_node_weight_t->NumElements() == 1,\n+        errors::InvalidArgument(\"min_node_weight argument must be a scalar\"));\n     const auto min_node_weight = min_node_weight_t->scalar<float>()();\n \n     std::vector<int32> output_node_ids;\n"
                },
                {
                    "old_start": 300,
                    "old_length": 7,
                    "new_start": 318,
                    "new_length": 7,
                    "hunk_buggy": "['     std::vector<int32> output_thresholds;\\n', '     std::vector<Eigen::VectorXf> output_left_node_contribs;\\n', '     std::vector<Eigen::VectorXf> output_right_node_contribs;\\n', '-    std::vector<string> output_split_types;\\n', ' \\n', '     // TODO(tanzheny) parallelize the computation.\\n', '     // Iterate each node and find the best gain per node.']",
                    "hunk_fix": "@@ -300,7 +318,7 @@ class BoostedTreesCalculateBestFeatureSplitOp : public OpKernel {\n     std::vector<int32> output_thresholds;\n     std::vector<Eigen::VectorXf> output_left_node_contribs;\n     std::vector<Eigen::VectorXf> output_right_node_contribs;\n-    std::vector<string> output_split_types;\n+    std::vector<std::string> output_split_types;\n \n     // TODO(tanzheny) parallelize the computation.\n     // Iterate each node and find the best gain per node."
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 150,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/eb921122119a6b6e470ee98b89e65d721663179d",
    "date": "2021-07-27T17:20:51-07:00",
    "message": "Prevent heap OOB read in TFLite's `gather.cc`.\n\nPassing negative indices is illegal but there was a missing check so that resulted in OOB accesses.\n\nPiperOrigin-RevId: 387231300\nChange-Id: I3111b54b2f232638d795be17efc46abe4ede6bf8",
    "changes": [
        {
            "name": "gather.cc",
            "path": "tensorflow/lite/kernels/gather.cc",
            "patches": [
                {
                    "old_start": 117,
                    "old_length": 8,
                    "new_start": 117,
                    "new_length": 20,
                    "hunk_buggy": "[' }\\n', ' \\n', ' template <typename InputT, typename PositionsT>\\n', '-TfLiteStatus Gather(const TfLiteGatherParams& params, const TfLiteTensor* input,\\n', '-                    const TfLiteTensor* positions, TfLiteTensor* output) {\\n', '   tflite::GatherParams op_params;\\n', '   op_params.axis = params.axis;\\n', '   op_params.batch_dims = params.batch_dims;\\n']",
                    "hunk_fix": "@@ -117,8 +117,20 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n }\n \n template <typename InputT, typename PositionsT>\n-TfLiteStatus Gather(const TfLiteGatherParams& params, const TfLiteTensor* input,\n-                    const TfLiteTensor* positions, TfLiteTensor* output) {\n+TfLiteStatus Gather(TfLiteContext* context, const TfLiteGatherParams& params,\n+                    const TfLiteTensor* input, const TfLiteTensor* positions,\n+                    TfLiteTensor* output) {\n+  const PositionsT* indexes = GetTensorData<PositionsT>(positions);\n+  bool indices_has_only_positive_elements = true;\n+  const size_t num_indices = positions->bytes / sizeof(PositionsT);\n+  for (size_t i = 0; i < num_indices; i++) {\n+    if (indexes[i] < 0) {\n+      indices_has_only_positive_elements = false;\n+      break;\n+    }\n+  }\n+  TF_LITE_ENSURE(context, indices_has_only_positive_elements);\n+\n   tflite::GatherParams op_params;\n   op_params.axis = params.axis;\n   op_params.batch_dims = params.batch_dims;\n"
                },
                {
                    "old_start": 134,
                    "old_length": 7,
                    "new_start": 146,
                    "new_length": 18,
                    "hunk_buggy": "['                            const TfLiteTensor* positions,\\n', '                            TfLiteTensor* output) {\\n', '   DynamicBuffer buffer;\\n', '   const PositionT* indexes = GetTensorData<PositionT>(positions);\\n', '   const PositionT num_strings = GetStringCount(input);\\n', '   const int num_indexes = NumElements(positions);\\n', ' \\n']",
                    "hunk_fix": "@@ -134,7 +146,18 @@ TfLiteStatus GatherStrings(TfLiteContext* context, const TfLiteTensor* input,\n                            const TfLiteTensor* positions,\n                            TfLiteTensor* output) {\n   DynamicBuffer buffer;\n+\n   const PositionT* indexes = GetTensorData<PositionT>(positions);\n+  bool indices_has_only_positive_elements = true;\n+  const size_t num_indices = positions->bytes / sizeof(PositionT);\n+  for (size_t i = 0; i < num_indices; i++) {\n+    if (indexes[i] < 0) {\n+      indices_has_only_positive_elements = false;\n+      break;\n+    }\n+  }\n+  TF_LITE_ENSURE(context, indices_has_only_positive_elements);\n+\n   const PositionT num_strings = GetStringCount(input);\n   const int num_indexes = NumElements(positions);\n \n"
                },
                {
                    "old_start": 163,
                    "old_length": 19,
                    "new_start": 186,
                    "new_length": 26,
                    "hunk_buggy": "['   if (positions->type == kTfLiteInt32) {\\n', '     switch (input->type) {\\n', '       case kTfLiteFloat32:\\n', '-        return Gather<float, int32_t>(*params, input, positions, output);\\n', '       case kTfLiteUInt8:\\n', '-        return Gather<uint8_t, int32_t>(*params, input, positions, output);\\n', '       case kTfLiteInt8:\\n', '-        return Gather<int8_t, int32_t>(*params, input, positions, output);\\n', '       case kTfLiteInt16:\\n', '-        return Gather<int16_t, int32_t>(*params, input, positions, output);\\n', '       case kTfLiteInt32:\\n', '-        return Gather<int32_t, int32_t>(*params, input, positions, output);\\n', '       case kTfLiteInt64:\\n', '-        return Gather<int64_t, int32_t>(*params, input, positions, output);\\n', '       case kTfLiteBool:\\n', '-        return Gather<bool, int32_t>(*params, input, positions, output);\\n', '       case kTfLiteString:\\n', '         return GatherStrings<int32_t>(context, input, positions, output);\\n', '       default:\\n']",
                    "hunk_fix": "@@ -163,19 +186,26 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n   if (positions->type == kTfLiteInt32) {\n     switch (input->type) {\n       case kTfLiteFloat32:\n-        return Gather<float, int32_t>(*params, input, positions, output);\n+        return Gather<float, int32_t>(context, *params, input, positions,\n+                                      output);\n       case kTfLiteUInt8:\n-        return Gather<uint8_t, int32_t>(*params, input, positions, output);\n+        return Gather<uint8_t, int32_t>(context, *params, input, positions,\n+                                        output);\n       case kTfLiteInt8:\n-        return Gather<int8_t, int32_t>(*params, input, positions, output);\n+        return Gather<int8_t, int32_t>(context, *params, input, positions,\n+                                       output);\n       case kTfLiteInt16:\n-        return Gather<int16_t, int32_t>(*params, input, positions, output);\n+        return Gather<int16_t, int32_t>(context, *params, input, positions,\n+                                        output);\n       case kTfLiteInt32:\n-        return Gather<int32_t, int32_t>(*params, input, positions, output);\n+        return Gather<int32_t, int32_t>(context, *params, input, positions,\n+                                        output);\n       case kTfLiteInt64:\n-        return Gather<int64_t, int32_t>(*params, input, positions, output);\n+        return Gather<int64_t, int32_t>(context, *params, input, positions,\n+                                        output);\n       case kTfLiteBool:\n-        return Gather<bool, int32_t>(*params, input, positions, output);\n+        return Gather<bool, int32_t>(context, *params, input, positions,\n+                                     output);\n       case kTfLiteString:\n         return GatherStrings<int32_t>(context, input, positions, output);\n       default:\n"
                },
                {
                    "old_start": 187,
                    "old_length": 19,
                    "new_start": 217,
                    "new_length": 26,
                    "hunk_buggy": "['   if (positions->type == kTfLiteInt64) {\\n', '     switch (input->type) {\\n', '       case kTfLiteFloat32:\\n', '-        return Gather<float, int64_t>(*params, input, positions, output);\\n', '       case kTfLiteUInt8:\\n', '-        return Gather<uint8_t, int64_t>(*params, input, positions, output);\\n', '       case kTfLiteInt8:\\n', '-        return Gather<int8_t, int64_t>(*params, input, positions, output);\\n', '       case kTfLiteInt16:\\n', '-        return Gather<int16_t, int64_t>(*params, input, positions, output);\\n', '       case kTfLiteInt32:\\n', '-        return Gather<int32_t, int64_t>(*params, input, positions, output);\\n', '       case kTfLiteInt64:\\n', '-        return Gather<int64_t, int64_t>(*params, input, positions, output);\\n', '       case kTfLiteBool:\\n', '-        return Gather<bool, int64_t>(*params, input, positions, output);\\n', '       case kTfLiteString:\\n', '         return GatherStrings<int64_t>(context, input, positions, output);\\n', '       default:']",
                    "hunk_fix": "@@ -187,19 +217,26 @@ TfLiteStatus Eval(TfLiteContext* context, TfLiteNode* node) {\n   if (positions->type == kTfLiteInt64) {\n     switch (input->type) {\n       case kTfLiteFloat32:\n-        return Gather<float, int64_t>(*params, input, positions, output);\n+        return Gather<float, int64_t>(context, *params, input, positions,\n+                                      output);\n       case kTfLiteUInt8:\n-        return Gather<uint8_t, int64_t>(*params, input, positions, output);\n+        return Gather<uint8_t, int64_t>(context, *params, input, positions,\n+                                        output);\n       case kTfLiteInt8:\n-        return Gather<int8_t, int64_t>(*params, input, positions, output);\n+        return Gather<int8_t, int64_t>(context, *params, input, positions,\n+                                       output);\n       case kTfLiteInt16:\n-        return Gather<int16_t, int64_t>(*params, input, positions, output);\n+        return Gather<int16_t, int64_t>(context, *params, input, positions,\n+                                        output);\n       case kTfLiteInt32:\n-        return Gather<int32_t, int64_t>(*params, input, positions, output);\n+        return Gather<int32_t, int64_t>(context, *params, input, positions,\n+                                        output);\n       case kTfLiteInt64:\n-        return Gather<int64_t, int64_t>(*params, input, positions, output);\n+        return Gather<int64_t, int64_t>(context, *params, input, positions,\n+                                        output);\n       case kTfLiteBool:\n-        return Gather<bool, int64_t>(*params, input, positions, output);\n+        return Gather<bool, int64_t>(context, *params, input, positions,\n+                                     output);\n       case kTfLiteString:\n         return GatherStrings<int64_t>(context, input, positions, output);\n       default:"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 151,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/bb6a0383ed553c286f87ca88c207f6774d5c4a8f",
    "date": "2021-07-27T15:25:33-07:00",
    "message": "Prevent heap OOB read in TFLite's `gather_nd.cc`.\n\nPassing negative indices is illegal but there was a missing check so that resulted in OOB accesses.\n\nPiperOrigin-RevId: 387208551\nChange-Id: I6b7a8a62d3e7c13a16d81619e5bc23ae2cdbc7fd",
    "changes": [
        {
            "name": "gather_nd.cc",
            "path": "tensorflow/lite/kernels/gather_nd.cc",
            "patches": [
                {
                    "old_start": 123,
                    "old_length": 6,
                    "new_start": 123,
                    "new_length": 17,
                    "hunk_buggy": "[' template <typename IndicesT>\\n', ' TfLiteStatus EvalGatherNd(TfLiteContext* context, const TfLiteTensor* params,\\n', '                           const TfLiteTensor* indices, TfLiteTensor* output) {\\n', '   switch (params->type) {\\n', '     case kTfLiteFloat32:\\n', '       return GatherNd<float, IndicesT>(params, indices, output);']",
                    "hunk_fix": "@@ -123,6 +123,17 @@ TfLiteStatus GatherNdString(const TfLiteTensor* params,\n template <typename IndicesT>\n TfLiteStatus EvalGatherNd(TfLiteContext* context, const TfLiteTensor* params,\n                           const TfLiteTensor* indices, TfLiteTensor* output) {\n+  bool indices_has_only_positive_elements = true;\n+  const auto* indices_values = GetTensorData<IndicesT>(indices);\n+  const size_t num_indices = indices->bytes / sizeof(IndicesT);\n+  for (size_t i = 0; i < num_indices; i++) {\n+    if (indices_values[i] < 0) {\n+      indices_has_only_positive_elements = false;\n+      break;\n+    }\n+  }\n+  TF_LITE_ENSURE(context, indices_has_only_positive_elements);\n+\n   switch (params->type) {\n     case kTfLiteFloat32:\n       return GatherNd<float, IndicesT>(params, indices, output);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 152,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/d94ffe08a65400f898241c0374e9edc6fa8ed257",
    "date": "2021-07-27T14:52:03-07:00",
    "message": "Prevent an OOB read in `expand_dims.cc`\n\nThe for loop that follows this check assumes that `axis` is between `0` and `input_dims.size`. If user supplied `axis` is negative, the if code before this check is supposed to bring it back to positive (similar to how in Python one can do `l[-3]` to mean `l[-3 + len(l)]`).\n\nPiperOrigin-RevId: 387200206\nChange-Id: I162f4feba12d547c3a4340833ae682016a2ebfab",
    "changes": [
        {
            "name": "expand_dims.cc",
            "path": "tensorflow/lite/kernels/expand_dims.cc",
            "patches": [
                {
                    "old_start": 37,
                    "old_length": 6,
                    "new_start": 37,
                    "new_length": 7,
                    "hunk_buggy": "['     axis = input_dims.size + 1 + axis;\\n', '   }\\n', '   TF_LITE_ENSURE(context, axis <= input_dims.size);\\n', ' \\n', '   TfLiteIntArray* output_dims = TfLiteIntArrayCreate(input_dims.size + 1);\\n', '   for (int i = 0; i < output_dims->size; ++i) {']",
                    "hunk_fix": "@@ -37,6 +37,7 @@ TfLiteStatus ExpandTensorDim(TfLiteContext* context, const TfLiteTensor& input,\n     axis = input_dims.size + 1 + axis;\n   }\n   TF_LITE_ENSURE(context, axis <= input_dims.size);\n+  TF_LITE_ENSURE(context, axis >= 0);\n \n   TfLiteIntArray* output_dims = TfLiteIntArrayCreate(input_dims.size + 1);\n   for (int i = 0; i < output_dims->size; ++i) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 153,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/8933b8a21280696ab119b63263babdb54c298538",
    "date": "2021-07-16T10:27:37-07:00",
    "message": "Fix a null pointer exception caused by branching on uninitialized data.\n\nThis is due to not checking that the params for the quantization exists. If there is no quantization, we should not access the `.params` field.\n\nPiperOrigin-RevId: 385173491\nChange-Id: I8fc476c4b274fdb21ba741caa0fbc6d1b8840663",
    "changes": [
        {
            "name": "depthwise_conv.cc",
            "path": "tensorflow/lite/kernels/depthwise_conv.cc",
            "patches": [
                {
                    "old_start": 176,
                    "old_length": 6,
                    "new_start": 176,
                    "new_length": 7,
                    "hunk_buggy": "['   if (data_type != kTfLiteFloat32) {\\n', '     TF_LITE_ENSURE_EQ(context, filter->quantization.type,\\n', '                       kTfLiteAffineQuantization);\\n', '     const auto* affine_quantization =\\n', '         reinterpret_cast<TfLiteAffineQuantization*>(\\n', '             filter->quantization.params);\\n']",
                    "hunk_fix": "@@ -176,6 +176,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   if (data_type != kTfLiteFloat32) {\n     TF_LITE_ENSURE_EQ(context, filter->quantization.type,\n                       kTfLiteAffineQuantization);\n+    TF_LITE_ENSURE(context, filter->quantization.type != kTfLiteNoQuantization);\n     const auto* affine_quantization =\n         reinterpret_cast<TfLiteAffineQuantization*>(\n             filter->quantization.params);\n"
                },
                {
                    "old_start": 195,
                    "old_length": 6,
                    "new_start": 196,
                    "new_length": 7,
                    "hunk_buggy": "['   }\\n', ' \\n', '   if (is_hybrid) {\\n', '     const auto* affine_quantization =\\n', '         reinterpret_cast<TfLiteAffineQuantization*>(\\n', '             filter->quantization.params);\\n']",
                    "hunk_fix": "@@ -195,6 +196,7 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   }\n \n   if (is_hybrid) {\n+    TF_LITE_ENSURE(context, filter->quantization.type != kTfLiteNoQuantization);\n     const auto* affine_quantization =\n         reinterpret_cast<TfLiteAffineQuantization*>(\n             filter->quantization.params);\n"
                },
                {
                    "old_start": 495,
                    "old_length": 6,
                    "new_start": 497,
                    "new_length": 7,
                    "hunk_buggy": "['   op_params.weights_offset = 0;\\n', '   op_params.float_activation_min = output_activation_min;\\n', '   op_params.float_activation_max = output_activation_max;\\n', '   const auto* affine_quantization =\\n', '       reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params);\\n', '   if (kernel_type == kReference) {']",
                    "hunk_fix": "@@ -495,6 +497,7 @@ TfLiteStatus EvalHybridPerChannel(TfLiteContext* context, TfLiteNode* node,\n   op_params.weights_offset = 0;\n   op_params.float_activation_min = output_activation_min;\n   op_params.float_activation_max = output_activation_max;\n+  TF_LITE_ENSURE(context, filter->quantization.type != kTfLiteNoQuantization);\n   const auto* affine_quantization =\n       reinterpret_cast<TfLiteAffineQuantization*>(filter->quantization.params);\n   if (kernel_type == kReference) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 154,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/d7ec7b9415181fce88ea8fde39af9e8be5a8be97",
    "date": "2021-06-22T11:20:17-07:00",
    "message": "Added generic check that shape has not more than 4 dimensions.\n\nPiperOrigin-RevId: 380849977\nChange-Id: I94da5e4e5f557e1649f8ca5284ebf1ad363ba211",
    "changes": [
        {
            "name": "model_builder.cc",
            "path": "tensorflow/lite/delegates/gpu/common/model_builder.cc",
            "patches": [
                {
                    "old_start": 2834,
                    "old_length": 6,
                    "new_start": 2834,
                    "new_length": 9,
                    "hunk_buggy": "['     int tensor_idx = tensor_indices->data[i];\\n', '     if (tensor_idx == kTfLiteOptionalTensor) continue;\\n', '     const TfLiteTensor* t = &context->tensors[tensor_idx];\\n', '     bool type_supported = false;\\n', '     for (auto allowed_type : allowed_types) {\\n', '       if (t->type == allowed_type) {\\n']",
                    "hunk_fix": "@@ -2834,6 +2834,9 @@ bool IsAllAllowedTensors(TfLiteContext* context,\n     int tensor_idx = tensor_indices->data[i];\n     if (tensor_idx == kTfLiteOptionalTensor) continue;\n     const TfLiteTensor* t = &context->tensors[tensor_idx];\n+    if (t->dims && t->dims->size >= 5) {\n+      return false;\n+    }\n     bool type_supported = false;\n     for (auto allowed_type : allowed_types) {\n       if (t->type == allowed_type) {\n"
                },
                {
                    "old_start": 3066,
                    "old_length": 7,
                    "new_start": 3069,
                    "new_length": 7,
                    "hunk_buggy": "['         !IsAllAllowedTensors(context, node->outputs, allowed_out_types)) {\\n', '       if (unsupported_details) {\\n', '         *unsupported_details =\\n', '-            \"OP is supported, but tensor type doesn\\'t match.\";\\n', '       }\\n', '       return false;\\n', '     }']",
                    "hunk_fix": "@@ -3066,7 +3069,7 @@ TfLiteIntArray* GetOpsToReplace(TfLiteContext* context, bool allow_quant_ops,\n         !IsAllAllowedTensors(context, node->outputs, allowed_out_types)) {\n       if (unsupported_details) {\n         *unsupported_details =\n-            \"OP is supported, but tensor type doesn't match.\";\n+            \"OP is supported, but tensor type/shape doesn't supported.\";\n       }\n       return false;\n     }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 155,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/9f8ad5ff118166537d42f87f1ee254f83ba553f0",
    "date": "2021-06-08T10:15:31-07:00",
    "message": "Fix CUDA version check (format is 1000 * major + 10 * minor).\n\nRef: https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART____VERSION.html\nPiperOrigin-RevId: 378180901\nChange-Id: Ief1c62ea9ed6df95195a7439ae3315e62222b56a",
    "changes": [
        {
            "name": "cuda_asm_compiler.cc",
            "path": "tensorflow/stream_executor/cuda/cuda_asm_compiler.cc",
            "patches": [
                {
                    "old_start": 36,
                    "old_length": 7,
                    "new_start": 36,
                    "new_length": 7,
                    "hunk_buggy": "[' port::StatusOr<std::vector<uint8>> LinkGpuAsm(\\n', '     gpu::GpuContext* context, std::vector<CubinOrPTXImage> images) {\\n', '   const bool linking_supported = [] {\\n', '-    if (CUDA_VERSION < 11300) {\\n', '       return true;\\n', '     }\\n', '     auto version_or_status = gpu::Diagnostician::FindKernelDriverVersion();']",
                    "hunk_fix": "@@ -36,7 +36,7 @@ namespace stream_executor {\n port::StatusOr<std::vector<uint8>> LinkGpuAsm(\n     gpu::GpuContext* context, std::vector<CubinOrPTXImage> images) {\n   const bool linking_supported = [] {\n-    if (CUDA_VERSION < 11300) {\n+    if (CUDA_VERSION < 11030) {\n       return true;\n     }\n     auto version_or_status = gpu::Diagnostician::FindKernelDriverVersion();"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 156,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/75c45e54bd37932f26d6e7cb36920c06a7833d52",
    "date": "2021-05-11T11:51:01-07:00",
    "message": "Added sanity checks on the existance of nodes to ConstantFolding.\n\nThis should fix null dereferences in ConstantFolding on fuzzed inputs.\n\nPiperOrigin-RevId: 373199608\nChange-Id: I3ab59dacf349f28edfb9ca900a625cade6266633",
    "changes": [
        {
            "name": "constant_folding.cc",
            "path": "tensorflow/core/grappler/optimizers/constant_folding.cc",
            "patches": [
                {
                    "old_start": 212,
                    "old_length": 9,
                    "new_start": 212,
                    "new_length": 13,
                    "hunk_buggy": "['   if (IsControlInput(input_name)) {\\n', '     return input_name;\\n', '   }\\n', '-  const NodeDef& node = *node_map->GetNode(input_name);\\n', '-  if (!IsSwitch(node)) {\\n', '-    return AsControlDependency(node);\\n', '   } else {\\n', \"     // We can't anchor control dependencies directly on the switch node: unlike\\n\", '     // other nodes only one of the outputs of the switch node will be generated\\n']",
                    "hunk_fix": "@@ -212,9 +212,13 @@ string ConstantFolding::AddControlDependency(const string& input_name,\n   if (IsControlInput(input_name)) {\n     return input_name;\n   }\n-  const NodeDef& node = *node_map->GetNode(input_name);\n-  if (!IsSwitch(node)) {\n-    return AsControlDependency(node);\n+  const NodeDef* node = node_map->GetNode(input_name);\n+  // Sanity check for missing node.\n+  if (!node) {\n+    return input_name;\n+  }\n+  if (!IsSwitch(*node)) {\n+    return AsControlDependency(*node);\n   } else {\n     // We can't anchor control dependencies directly on the switch node: unlike\n     // other nodes only one of the outputs of the switch node will be generated\n"
                },
                {
                    "old_start": 222,
                    "old_length": 9,
                    "new_start": 226,
                    "new_length": 9,
                    "hunk_buggy": "['     // dependency is only triggered when the corresponding output is triggered.\\n', '     // We start by looking for an identity node connected to the output of the\\n', '     // switch node, and use it to anchor the control dependency.\\n', '-    for (const NodeDef* output : node_map->GetOutputs(node.name())) {\\n', '       if (IsIdentity(*output) || IsIdentityNSingleInput(*output)) {\\n', '-        if (IsSameInput(node.input(0), input_name)) {\\n', '           return AsControlDependency(*output);\\n', '         }\\n', '       }\\n']",
                    "hunk_fix": "@@ -222,9 +226,9 @@ string ConstantFolding::AddControlDependency(const string& input_name,\n     // dependency is only triggered when the corresponding output is triggered.\n     // We start by looking for an identity node connected to the output of the\n     // switch node, and use it to anchor the control dependency.\n-    for (const NodeDef* output : node_map->GetOutputs(node.name())) {\n+    for (const NodeDef* output : node_map->GetOutputs(node->name())) {\n       if (IsIdentity(*output) || IsIdentityNSingleInput(*output)) {\n-        if (IsSameInput(node.input(0), input_name)) {\n+        if (IsSameInput(node->input(0), input_name)) {\n           return AsControlDependency(*output);\n         }\n       }\n"
                },
                {
                    "old_start": 235,
                    "old_length": 19,
                    "new_start": 239,
                    "new_length": 19,
                    "hunk_buggy": "['     string ctrl_dep_name = ParseNodeName(input_name, &port);\\n', '     strings::StrAppend(&ctrl_dep_name, \"_\", port);\\n', '     ctrl_dep_name = AddPrefixToNodeName(ctrl_dep_name, kConstantFoldingCtrl);\\n', '-    const DataType output_type = node.attr().at(\"T\").type();\\n', ' \\n', '     NodeDef* added_node = node_map->GetNode(ctrl_dep_name);\\n', '     if (added_node == nullptr) {\\n', '       added_node = graph->add_node();\\n', '       added_node->set_name(ctrl_dep_name);\\n', '       added_node->set_op(\"Identity\");\\n', '-      added_node->set_device(node.device());\\n', ' \\n', '       (*added_node->mutable_attr())[\"T\"].set_type(output_type);\\n', '       *added_node->add_input() = input_name;\\n', '       node_map->AddNode(added_node->name(), added_node);\\n', '-      node_map->AddOutput(node.name(), added_node->name());\\n', '     }\\n', '     return AsControlDependency(*added_node);\\n', '   }\\n']",
                    "hunk_fix": "@@ -235,19 +239,19 @@ string ConstantFolding::AddControlDependency(const string& input_name,\n     string ctrl_dep_name = ParseNodeName(input_name, &port);\n     strings::StrAppend(&ctrl_dep_name, \"_\", port);\n     ctrl_dep_name = AddPrefixToNodeName(ctrl_dep_name, kConstantFoldingCtrl);\n-    const DataType output_type = node.attr().at(\"T\").type();\n+    const DataType output_type = node->attr().at(\"T\").type();\n \n     NodeDef* added_node = node_map->GetNode(ctrl_dep_name);\n     if (added_node == nullptr) {\n       added_node = graph->add_node();\n       added_node->set_name(ctrl_dep_name);\n       added_node->set_op(\"Identity\");\n-      added_node->set_device(node.device());\n+      added_node->set_device(node->device());\n \n       (*added_node->mutable_attr())[\"T\"].set_type(output_type);\n       *added_node->add_input() = input_name;\n       node_map->AddNode(added_node->name(), added_node);\n-      node_map->AddOutput(node.name(), added_node->name());\n+      node_map->AddOutput(node->name(), added_node->name());\n     }\n     return AsControlDependency(*added_node);\n   }\n"
                },
                {
                    "old_start": 3098,
                    "old_length": 6,
                    "new_start": 3102,
                    "new_length": 12,
                    "hunk_buggy": "['   }\\n', '   NodeDef* left_child = node_map_->GetNode(parent.input(0));\\n', '   NodeDef* right_child = node_map_->GetNode(parent.input(1));\\n', '   ctx->left_child_is_const = IsReallyConstant(*left_child);\\n', '   ctx->right_child_is_const = IsReallyConstant(*right_child);\\n', '   ctx->op_child = ctx->left_child_is_const ? right_child : left_child;']",
                    "hunk_fix": "@@ -3098,6 +3102,12 @@ bool ConstantFolding::PrepareConstantPushDown(\n   }\n   NodeDef* left_child = node_map_->GetNode(parent.input(0));\n   NodeDef* right_child = node_map_->GetNode(parent.input(1));\n+\n+  // Sanity check for missing children.\n+  if (left_child == nullptr || right_child == nullptr) {\n+    return false;\n+  }\n+\n   ctx->left_child_is_const = IsReallyConstant(*left_child);\n   ctx->right_child_is_const = IsReallyConstant(*right_child);\n   ctx->op_child = ctx->left_child_is_const ? right_child : left_child;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 157,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/63c6a29d0f2d692b247f7bf81f8732d6442fad09",
    "date": "2021-05-05T18:18:17-07:00",
    "message": "Add missing validation, prevent heap OOB\n\nPiperOrigin-RevId: 372246723\nChange-Id: I1a454a643810e77d7d14821b342098c56a09fbbf",
    "changes": [
        {
            "name": "pooling_ops_3d.cc",
            "path": "tensorflow/core/kernels/pooling_ops_3d.cc",
            "patches": [
                {
                    "old_start": 693,
                    "old_length": 6,
                    "new_start": 693,
                    "new_length": 7,
                    "hunk_buggy": "[' \\n', '     Pool3dParameters params{context,  ksize_,       stride_,\\n', '                             padding_, data_format_, tensor_in.shape()};\\n', ' \\n', '     Tensor* output = nullptr;\\n', '     OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\\n']",
                    "hunk_fix": "@@ -693,6 +693,7 @@ class MaxPooling3dGradGradOp : public OpKernel {\n \n     Pool3dParameters params{context,  ksize_,       stride_,\n                             padding_, data_format_, tensor_in.shape()};\n+    if (!context->status().ok()) return;  // params is invalid\n \n     Tensor* output = nullptr;\n     OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n"
                },
                {
                    "old_start": 710,
                    "old_length": 6,
                    "new_start": 711,
                    "new_length": 17,
                    "hunk_buggy": "['         context, out_grad_backprop.NumElements() > 0,\\n', '         errors::InvalidArgument(\"received empty tensor out_grad_backprop: \",\\n', '                                 out_grad_backprop.DebugString()));\\n', ' \\n', '     LaunchMaxPooling3dGradGradOp<Device, T>::launch(\\n', '         context, params, tensor_in, tensor_out, out_grad_backprop, output);']",
                    "hunk_fix": "@@ -710,6 +711,17 @@ class MaxPooling3dGradGradOp : public OpKernel {\n         context, out_grad_backprop.NumElements() > 0,\n         errors::InvalidArgument(\"received empty tensor out_grad_backprop: \",\n                                 out_grad_backprop.DebugString()));\n+    OP_REQUIRES(context,\n+                tensor_in.NumElements() == out_grad_backprop.NumElements(),\n+                errors::InvalidArgument(\"tensor_in and out_grad_backprop must \"\n+                                        \"have same number of elements, got <\",\n+                                        tensor_in.DebugString(), \"> and <\",\n+                                        out_grad_backprop.DebugString(), \">\"));\n+    OP_REQUIRES(\n+        context, tensor_out.NumElements() == output->NumElements(),\n+        errors::InvalidArgument(\n+            \"tensor_out and output must have same number of elements, got <\",\n+            tensor_out.DebugString(), \"> and <\", output->DebugString(), \">\"));\n \n     LaunchMaxPooling3dGradGradOp<Device, T>::launch(\n         context, params, tensor_in, tensor_out, out_grad_backprop, output);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 158,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/f6f62119587baf8ccb7378ceac86bacd2db2863d",
    "date": "2021-05-05T16:09:09-07:00",
    "message": "Add missing validation in maxpooling_op.cc\n\nPiperOrigin-RevId: 372225911\nChange-Id: I85d5e2c5640a659cda0e7cdc0dcb6dc82564210d",
    "changes": [
        {
            "name": "maxpooling_op.cc",
            "path": "tensorflow/core/kernels/maxpooling_op.cc",
            "patches": [
                {
                    "old_start": 326,
                    "old_length": 6,
                    "new_start": 326,
                    "new_length": 15,
                    "hunk_buggy": "['     OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\\n', '                                 {0}, 0, output_shape, &output));\\n', ' \\n', '     SpatialMaxPoolWithArgMaxHelper<CPUDevice, T, int64>(\\n', '         context, &tensor_out_dup, &tensor_out_arg_max, output, tensor_in,\\n', '         out_backprop, params, true);']",
                    "hunk_fix": "@@ -326,6 +326,15 @@ class MaxPoolingGradOp : public OpKernel {\n     OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                 {0}, 0, output_shape, &output));\n \n+    // Given access patterns in SpatialMaxPoolWithArgMaxHelper, these tensors\n+    // must have elements.\n+    OP_REQUIRES(\n+        context, tensor_out_arg_max.NumElements() > 0,\n+        errors::InvalidArgument(\"tensor_out_arg_max must not be empty, got \",\n+                                tensor_out_arg_max.DebugString()));\n+    OP_REQUIRES(context, out_backprop.NumElements() > 0,\n+                errors::InvalidArgument(\"out_backprop must not be empty, got \",\n+                                        out_backprop.DebugString()));\n     SpatialMaxPoolWithArgMaxHelper<CPUDevice, T, int64>(\n         context, &tensor_out_dup, &tensor_out_arg_max, output, tensor_in,\n         out_backprop, params, true);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 159,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/a3d9f9be9ac2296615644061b40cefcee341dcc4",
    "date": "2021-05-05T15:24:00-07:00",
    "message": "Add missing validation to pooling_ops_3d\n\nPiperOrigin-RevId: 372218727\nChange-Id: I6b9ed4266aa7286c02f1f230d7bea922c1be547e",
    "changes": [
        {
            "name": "pooling_ops_3d.cc",
            "path": "tensorflow/core/kernels/pooling_ops_3d.cc",
            "patches": [
                {
                    "old_start": 698,
                    "old_length": 6,
                    "new_start": 698,
                    "new_length": 19,
                    "hunk_buggy": "['     OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\\n', '                                 {2}, 0, tensor_out.shape(), &output));\\n', ' \\n', '     LaunchMaxPooling3dGradGradOp<Device, T>::launch(\\n', '         context, params, tensor_in, tensor_out, out_grad_backprop, output);\\n', '   }']",
                    "hunk_fix": "@@ -698,6 +698,19 @@ class MaxPooling3dGradGradOp : public OpKernel {\n     OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                 {2}, 0, tensor_out.shape(), &output));\n \n+    // Given access patterns in LaunchMaxPooling3dGradGradOp, these tensors must\n+    // have elements.\n+    OP_REQUIRES(context, tensor_in.NumElements() > 0,\n+                errors::InvalidArgument(\"received empty tensor tensor_in: \",\n+                                        tensor_in.DebugString()));\n+    OP_REQUIRES(context, tensor_out.NumElements() > 0,\n+                errors::InvalidArgument(\"received empty tensor tensor_out: \",\n+                                        tensor_out.DebugString()));\n+    OP_REQUIRES(\n+        context, out_grad_backprop.NumElements() > 0,\n+        errors::InvalidArgument(\"received empty tensor out_grad_backprop: \",\n+                                out_grad_backprop.DebugString()));\n+\n     LaunchMaxPooling3dGradGradOp<Device, T>::launch(\n         context, params, tensor_in, tensor_out, out_grad_backprop, output);\n   }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 160,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/31bd5026304677faa8a0b77602c6154171b9aec1",
    "date": "2021-05-04T17:46:54-07:00",
    "message": "Prevent check fail in FFT\n\nPiperOrigin-RevId: 372031044\nChange-Id: I50994e3e8a5d1342d01bde80256f6bf2730ca299",
    "changes": [
        {
            "name": "fft_ops.cc",
            "path": "tensorflow/core/kernels/fft_ops.cc",
            "patches": [
                {
                    "old_start": 222,
                    "old_length": 6,
                    "new_start": 222,
                    "new_length": 9,
                    "hunk_buggy": "['       input_slice_sizes[i] = fft_shape[i - 1];\\n', '       temp_shape.AddDim(fft_shape[i - 1]);\\n', '     }\\n', ' \\n', '     auto output = out->flat_inner_dims<ComplexT, FFTRank + 1>();\\n', '     const Eigen::DSizes<Eigen::DenseIndex, FFTRank + 1> zero_start_indices;']",
                    "hunk_fix": "@@ -222,6 +222,9 @@ class FFTCPU : public FFTBase {\n       input_slice_sizes[i] = fft_shape[i - 1];\n       temp_shape.AddDim(fft_shape[i - 1]);\n     }\n+    OP_REQUIRES(ctx, temp_shape.num_elements() > 0,\n+                errors::InvalidArgument(\"Obtained a FFT shape of 0 elements: \",\n+                                        temp_shape.DebugString()));\n \n     auto output = out->flat_inner_dims<ComplexT, FFTRank + 1>();\n     const Eigen::DSizes<Eigen::DenseIndex, FFTRank + 1> zero_start_indices;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 161,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/1c56f53be0b722ca657cbc7df461ed676c8642a2",
    "date": "2021-05-04T17:18:39-07:00",
    "message": "Fix a check fail in Fast Fourier implementation\n\nPiperOrigin-RevId: 372026629\nChange-Id: Id05c3362aa575271bc3e06b16316c9037085fc11",
    "changes": [
        {
            "name": "fft_ops.cc",
            "path": "tensorflow/core/kernels/fft_ops.cc",
            "patches": [
                {
                    "old_start": 13,
                    "old_length": 6,
                    "new_start": 13,
                    "new_length": 7,
                    "hunk_buggy": "[' limitations under the License.\\n', ' ==============================================================================*/\\n', ' \\n', ' #define EIGEN_USE_THREADS\\n', ' \\n', ' // See docs in ../ops/fft_ops.cc.\\n']",
                    "hunk_fix": "@@ -13,6 +13,7 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n+#include \"tensorflow/core/platform/errors.h\"\n #define EIGEN_USE_THREADS\n \n // See docs in ../ops/fft_ops.cc.\n"
                },
                {
                    "old_start": 261,
                    "old_length": 6,
                    "new_start": 262,
                    "new_length": 9,
                    "hunk_buggy": "['           i == FFTRank ? fft_shape[i - 1] / 2 + 1 : fft_shape[i - 1];\\n', '       full_fft_shape.AddDim(fft_shape[i - 1]);\\n', '     }\\n', ' \\n', '     Tensor temp;\\n', '     OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<ComplexT>::v(),']",
                    "hunk_fix": "@@ -261,6 +262,9 @@ class FFTCPU : public FFTBase {\n           i == FFTRank ? fft_shape[i - 1] / 2 + 1 : fft_shape[i - 1];\n       full_fft_shape.AddDim(fft_shape[i - 1]);\n     }\n+    OP_REQUIRES(ctx, full_fft_shape.num_elements() > 0,\n+                errors::InvalidArgument(\"Obtained a FFT shape of 0 elements: \",\n+                                        full_fft_shape.DebugString()));\n \n     Tensor temp;\n     OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<ComplexT>::v(),"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 162,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/27bd8aaa7b58d2591fed43a6c245f3037664cfb1",
    "date": "2021-05-03T20:40:32-07:00",
    "message": "Fix another Eigen missing validation\n\nPiperOrigin-RevId: 371833155\nChange-Id: I5a23d451132cb1624ad916ef46ea01d0e88ec82c",
    "changes": [
        {
            "name": "banded_triangular_solve_op.cc",
            "path": "tensorflow/core/kernels/linalg/banded_triangular_solve_op.cc",
            "patches": [
                {
                    "old_start": 275,
                    "old_length": 6,
                    "new_start": 275,
                    "new_length": 14,
                    "hunk_buggy": "['     OP_REQUIRES(\\n', '         ctx, in1.dims() >= 2,\\n', '         errors::InvalidArgument(\"In[1] ndims must be >= 2: \", in1.dims()));\\n', '   }\\n', '   bool lower_;\\n', '   bool adjoint_;']",
                    "hunk_fix": "@@ -275,6 +275,14 @@ class BandedTriangularSolveOpCpu : public OpKernel {\n     OP_REQUIRES(\n         ctx, in1.dims() >= 2,\n         errors::InvalidArgument(\"In[1] ndims must be >= 2: \", in1.dims()));\n+\n+    OP_REQUIRES(ctx, in0.NumElements() > 0,\n+                errors::InvalidArgument(\"In[0] must not be an empty tensor: \",\n+                                        in0.DebugString()));\n+\n+    OP_REQUIRES(ctx, in1.NumElements() > 0,\n+                errors::InvalidArgument(\"In[1] must not be an empty tensor: \",\n+                                        in1.DebugString()));\n   }\n   bool lower_;\n   bool adjoint_;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 163,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/22783fdf812b700f7de9980038ab41ee0a4a2284",
    "date": "2021-04-30T15:28:52-07:00",
    "message": "Add checks recently removed\n\nPiperOrigin-RevId: 371415079\nChange-Id: I25fa185165541e940d749ba0ff76b12b7ce27394",
    "changes": [
        {
            "name": "object_reader.h",
            "path": "tensorflow/lite/delegates/gpu/common/object_reader.h",
            "patches": [
                {
                    "old_start": 62,
                    "old_length": 7,
                    "new_start": 62,
                    "new_length": 17,
                    "hunk_buggy": "[' \\n', '   template <typename TensorT>\\n', '   absl::Status ReadTensor(uint32_t index, TensorT* tensor) const {\\n', '     const int32_t tensor_id = node_->inputs->data[index];\\n', '     const TfLiteTensor* tflite_tensor = context_->tensors + tensor_id;\\n', '     tensor->data.resize(NumElements(tflite_tensor));\\n', '     if (tflite_tensor->sparsity) {']",
                    "hunk_fix": "@@ -62,7 +62,17 @@ class ObjectReader {\n \n   template <typename TensorT>\n   absl::Status ReadTensor(uint32_t index, TensorT* tensor) const {\n+    if (index < 0 || index >= node_->inputs->size) {\n+      // If larger, this can be an older model with fewer input tensors than the\n+      // current implementation.\n+      return absl::OutOfRangeError(\"Invalid data index found.\");\n+    }\n     const int32_t tensor_id = node_->inputs->data[index];\n+    if (tensor_id < 0) {\n+      return absl::InvalidArgumentError(\n+          \"Invalid data index found. Possibly an unset optional tensor is \"\n+          \"being read.\");\n+    }\n     const TfLiteTensor* tflite_tensor = context_->tensors + tensor_id;\n     tensor->data.resize(NumElements(tflite_tensor));\n     if (tflite_tensor->sparsity) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 164,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/c59c37e7b2d563967da813fa50fe20b21f4da683",
    "date": "2021-04-28T17:54:22-07:00",
    "message": "Prevent array write out-of-bounds.\n\nIf user passes an invalid axis, then we copy one too many dimensions to the output in the loop below these checks. Even if we didn't do that, there will be further issues with an invalid axis, so we check for that right now.\n\nPiperOrigin-RevId: 371023299\nChange-Id: I9eca37ffc2b29e8e48710f500701270ef0790224",
    "changes": [
        {
            "name": "arg_min_max.cc",
            "path": "tensorflow/lite/kernels/arg_min_max.cc",
            "patches": [
                {
                    "old_start": 48,
                    "old_length": 6,
                    "new_start": 48,
                    "new_length": 9,
                    "hunk_buggy": "['     axis_value += NumDimensions(input);\\n', '   }\\n', ' \\n', '   // Copy the input dimensions to output except the axis dimension.\\n', '   TfLiteIntArray* output_dims = TfLiteIntArrayCreate(NumDimensions(input) - 1);\\n', '   int j = 0;']",
                    "hunk_fix": "@@ -48,6 +48,9 @@ TfLiteStatus ResizeOutput(TfLiteContext* context, const TfLiteTensor* input,\n     axis_value += NumDimensions(input);\n   }\n \n+  TF_LITE_ENSURE(context, axis_value >= 0);\n+  TF_LITE_ENSURE(context, axis_value < NumDimensions(input));\n+\n   // Copy the input dimensions to output except the axis dimension.\n   TfLiteIntArray* output_dims = TfLiteIntArrayCreate(NumDimensions(input) - 1);\n   int j = 0;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 165,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/d6ed5bcfe1dcab9e85a4d39931bd18d99018e75b",
    "date": "2021-04-23T11:54:50-07:00",
    "message": "Add missing validation in `QuantizedBatchNormWithGlobalNormalization`\n\nPiperOrigin-RevId: 370123451\nChange-Id: Id234d6dab1ec21230bb8e503dba30f899af87f33",
    "changes": [
        {
            "name": "quantized_batch_norm_op.cc",
            "path": "tensorflow/core/kernels/quantized_batch_norm_op.cc",
            "patches": [
                {
                    "old_start": 173,
                    "old_length": 20,
                    "new_start": 173,
                    "new_length": 50,
                    "hunk_buggy": "[' \\n', '   void Compute(OpKernelContext* context) override {\\n', '     const Tensor& input = context->input(0);\\n', '-    const float input_min = context->input(1).flat<float>()(0);\\n', '-    const float input_max = context->input(2).flat<float>()(0);\\n', '     const Tensor& mean = context->input(3);\\n', '-    const float mean_min = context->input(4).flat<float>()(0);\\n', '-    const float mean_max = context->input(5).flat<float>()(0);\\n', '     const Tensor& var = context->input(6);\\n', '-    const float var_min = context->input(7).flat<float>()(0);\\n', '-    const float var_max = context->input(8).flat<float>()(0);\\n', '     const Tensor& beta = context->input(9);\\n', '-    const float beta_min = context->input(10).flat<float>()(0);\\n', '-    const float beta_max = context->input(11).flat<float>()(0);\\n', '     const Tensor& gamma = context->input(12);\\n', '-    const float gamma_min = context->input(13).flat<float>()(0);\\n', '-    const float gamma_max = context->input(14).flat<float>()(0);\\n', ' \\n', '     OP_REQUIRES(context, input.dims() == 4,\\n', '                 errors::InvalidArgument(\"input must be 4-dimensional\",\\n']",
                    "hunk_fix": "@@ -173,20 +173,50 @@ class QuantizedBatchNormOp : public OpKernel {\n \n   void Compute(OpKernelContext* context) override {\n     const Tensor& input = context->input(0);\n-    const float input_min = context->input(1).flat<float>()(0);\n-    const float input_max = context->input(2).flat<float>()(0);\n+    const auto& input_min_tensor = context->input(1);\n+    OP_REQUIRES(context, input_min_tensor.NumElements() == 1,\n+                errors::InvalidArgument(\"input_min must have 1 element\"));\n+    const float input_min = input_min_tensor.flat<float>()(0);\n+    const auto& input_max_tensor = context->input(2);\n+    OP_REQUIRES(context, input_max_tensor.NumElements() == 1,\n+                errors::InvalidArgument(\"input_max must have 1 element\"));\n+    const float input_max = input_max_tensor.flat<float>()(0);\n     const Tensor& mean = context->input(3);\n-    const float mean_min = context->input(4).flat<float>()(0);\n-    const float mean_max = context->input(5).flat<float>()(0);\n+    const auto& mean_min_tensor = context->input(4);\n+    OP_REQUIRES(context, mean_min_tensor.NumElements() == 1,\n+                errors::InvalidArgument(\"mean_min must have 1 element\"));\n+    const float mean_min = mean_min_tensor.flat<float>()(0);\n+    const auto& mean_max_tensor = context->input(5);\n+    OP_REQUIRES(context, mean_max_tensor.NumElements() == 1,\n+                errors::InvalidArgument(\"mean_max must have 1 element\"));\n+    const float mean_max = mean_max_tensor.flat<float>()(0);\n     const Tensor& var = context->input(6);\n-    const float var_min = context->input(7).flat<float>()(0);\n-    const float var_max = context->input(8).flat<float>()(0);\n+    const auto& var_min_tensor = context->input(7);\n+    OP_REQUIRES(context, var_min_tensor.NumElements() == 1,\n+                errors::InvalidArgument(\"var_min must have 1 element\"));\n+    const float var_min = var_min_tensor.flat<float>()(0);\n+    const auto& var_max_tensor = context->input(8);\n+    OP_REQUIRES(context, var_max_tensor.NumElements() == 1,\n+                errors::InvalidArgument(\"var_max must have 1 element\"));\n+    const float var_max = var_max_tensor.flat<float>()(0);\n     const Tensor& beta = context->input(9);\n-    const float beta_min = context->input(10).flat<float>()(0);\n-    const float beta_max = context->input(11).flat<float>()(0);\n+    const auto& beta_min_tensor = context->input(10);\n+    OP_REQUIRES(context, beta_min_tensor.NumElements() == 1,\n+                errors::InvalidArgument(\"beta_min must have 1 element\"));\n+    const float beta_min = beta_min_tensor.flat<float>()(0);\n+    const auto& beta_max_tensor = context->input(11);\n+    OP_REQUIRES(context, beta_max_tensor.NumElements() == 1,\n+                errors::InvalidArgument(\"beta_max must have 1 element\"));\n+    const float beta_max = beta_max_tensor.flat<float>()(0);\n     const Tensor& gamma = context->input(12);\n-    const float gamma_min = context->input(13).flat<float>()(0);\n-    const float gamma_max = context->input(14).flat<float>()(0);\n+    const auto& gamma_min_tensor = context->input(13);\n+    OP_REQUIRES(context, gamma_min_tensor.NumElements() == 1,\n+                errors::InvalidArgument(\"gamma_min must have 1 element\"));\n+    const float gamma_min = gamma_min_tensor.flat<float>()(0);\n+    const auto& gamma_max_tensor = context->input(14);\n+    OP_REQUIRES(context, gamma_max_tensor.NumElements() == 1,\n+                errors::InvalidArgument(\"gamma_max must have 1 element\"));\n+    const float gamma_max = gamma_max_tensor.flat<float>()(0);\n \n     OP_REQUIRES(context, input.dims() == 4,\n                 errors::InvalidArgument(\"input must be 4-dimensional\",\n"
                },
                {
                    "old_start": 203,
                    "old_length": 6,
                    "new_start": 233,
                    "new_length": 33,
                    "hunk_buggy": "['     OP_REQUIRES(context, gamma.dims() == 1,\\n', '                 errors::InvalidArgument(\"gamma must be 1-dimensional\",\\n', '                                         gamma.shape().DebugString()));\\n', ' \\n', '     Tensor* output = nullptr;\\n', '     OP_REQUIRES_OK(context,']",
                    "hunk_fix": "@@ -203,6 +233,33 @@ class QuantizedBatchNormOp : public OpKernel {\n     OP_REQUIRES(context, gamma.dims() == 1,\n                 errors::InvalidArgument(\"gamma must be 1-dimensional\",\n                                         gamma.shape().DebugString()));\n+    OP_REQUIRES(context, mean.NumElements() > 1,\n+                errors::InvalidArgument(\"Must have at least a mean value\",\n+                                        gamma.shape().DebugString()));\n+    OP_REQUIRES(context, mean.NumElements() > 1,\n+                errors::InvalidArgument(\"Must have at least a mean value\"));\n+    const auto last_dim = input.shape().dims() - 1;\n+    OP_REQUIRES(context,\n+                mean.shape().dim_size(0) == input.shape().dim_size(last_dim),\n+                errors::InvalidArgument(\"Must provide as many means as the \"\n+                                        \"last dimension of the input tensor: \",\n+                                        mean.shape().DebugString(), \" vs. \",\n+                                        input.shape().DebugString()));\n+    OP_REQUIRES(\n+        context, mean.shape().dim_size(0) == var.shape().dim_size(0),\n+        errors::InvalidArgument(\n+            \"Mean and variance tensors must have the same shape: \",\n+            mean.shape().DebugString(), \" vs. \", var.shape().DebugString()));\n+    OP_REQUIRES(\n+        context, mean.shape().dim_size(0) == beta.shape().dim_size(0),\n+        errors::InvalidArgument(\n+            \"Mean and beta tensors must have the same shape: \",\n+            mean.shape().DebugString(), \" vs. \", beta.shape().DebugString()));\n+    OP_REQUIRES(\n+        context, mean.shape().dim_size(0) == gamma.shape().dim_size(0),\n+        errors::InvalidArgument(\n+            \"Mean and gamma tensors must have the same shape: \",\n+            mean.shape().DebugString(), \" vs. \", gamma.shape().DebugString()));\n \n     Tensor* output = nullptr;\n     OP_REQUIRES_OK(context,"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 166,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/4ea68093eeaf4c4157368668afd7f809b806a504",
    "date": "2021-04-23T14:38:51-04:00",
    "message": "Add negative parameter validation to convolution layers.",
    "changes": [
        {
            "name": "convolutional.py",
            "path": "tensorflow/python/keras/layers/convolutional.py",
            "patches": [
                {
                    "old_start": 135,
                    "old_length": 6,
                    "new_start": 135,
                    "new_length": 9,
                    "hunk_buggy": "[' \\n', '     if isinstance(filters, float):\\n', '       filters = int(filters)\\n', '     self.filters = filters\\n', '     self.groups = groups or 1\\n', '     self.kernel_size = conv_utils.normalize_tuple(']",
                    "hunk_fix": "@@ -135,6 +135,9 @@ class Conv(Layer):\n \n     if isinstance(filters, float):\n       filters = int(filters)\n+    if filters < 0:\n+      raise ValueError(\"Recieved a negative value for `filters`,\n+                       \"was expecting a positive value.\")\n     self.filters = filters\n     self.groups = groups or 1\n     self.kernel_size = conv_utils.normalize_tuple("
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 167,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/1223335a8d34a8ce656dbd10b2a236ef6204ff47",
    "date": "2021-04-23T14:35:07-04:00",
    "message": "Add negative parameter validation for recurrent layers.",
    "changes": [
        {
            "name": "recurrent.py",
            "path": "tensorflow/python/keras/layers/recurrent.py",
            "patches": [
                {
                    "old_start": 1320,
                    "old_length": 6,
                    "new_start": 1320,
                    "new_length": 9,
                    "hunk_buggy": "['     else:\\n', \"       self._enable_caching_device = kwargs.pop('enable_caching_device', False)\\n\", '     super(SimpleRNNCell, self).__init__(**kwargs)\\n', '     self.units = units\\n', '     self.activation = activations.get(activation)\\n', '     self.use_bias = use_bias\\n']",
                    "hunk_fix": "@@ -1320,6 +1320,9 @@ class SimpleRNNCell(DropoutRNNCellMixin, Layer):\n     else:\n       self._enable_caching_device = kwargs.pop('enable_caching_device', False)\n     super(SimpleRNNCell, self).__init__(**kwargs)\n+    if units < 0:\n+      raise ValueError(\"Received a negative value for `units`, \",\n+                       \"expected a positive value.\")\n     self.units = units\n     self.activation = activations.get(activation)\n     self.use_bias = use_bias\n"
                },
                {
                    "old_start": 1758,
                    "old_length": 6,
                    "new_start": 1761,
                    "new_length": 9,
                    "hunk_buggy": "['     else:\\n', \"       self._enable_caching_device = kwargs.pop('enable_caching_device', False)\\n\", '     super(GRUCell, self).__init__(**kwargs)\\n', '     self.units = units\\n', '     self.activation = activations.get(activation)\\n', '     self.recurrent_activation = activations.get(recurrent_activation)\\n']",
                    "hunk_fix": "@@ -1758,6 +1761,9 @@ class GRUCell(DropoutRNNCellMixin, Layer):\n     else:\n       self._enable_caching_device = kwargs.pop('enable_caching_device', False)\n     super(GRUCell, self).__init__(**kwargs)\n+    if units < 0:\n+      raise ValueError(\"Received an negative value for `units`, \"\n+                       \"expected a positive value.\")\n     self.units = units\n     self.activation = activations.get(activation)\n     self.recurrent_activation = activations.get(recurrent_activation)\n"
                },
                {
                    "old_start": 2318,
                    "old_length": 6,
                    "new_start": 2324,
                    "new_length": 9,
                    "hunk_buggy": "['     else:\\n', \"       self._enable_caching_device = kwargs.pop('enable_caching_device', False)\\n\", '     super(LSTMCell, self).__init__(**kwargs)\\n', '     self.units = units\\n', '     self.activation = activations.get(activation)\\n', '     self.recurrent_activation = activations.get(recurrent_activation)']",
                    "hunk_fix": "@@ -2318,6 +2324,9 @@ class LSTMCell(DropoutRNNCellMixin, Layer):\n     else:\n       self._enable_caching_device = kwargs.pop('enable_caching_device', False)\n     super(LSTMCell, self).__init__(**kwargs)\n+    if units < 0:\n+      raise ValueError(\"Received a negative value for `units`, \"\n+                       \"expected a postiive value.\")\n     self.units = units\n     self.activation = activations.get(activation)\n     self.recurrent_activation = activations.get(recurrent_activation)"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 168,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/64afe2d199ec4513223bbf5176835bf681cf056b",
    "date": "2021-04-23T14:28:05-04:00",
    "message": "Add negative parameter validation to Core Keras layers.",
    "changes": [
        {
            "name": "core.py",
            "path": "tensorflow/python/keras/layers/core.py",
            "patches": [
                {
                    "old_start": 195,
                    "old_length": 6,
                    "new_start": 195,
                    "new_length": 9,
                    "hunk_buggy": "[' \\n', '   def __init__(self, rate, noise_shape=None, seed=None, **kwargs):\\n', '     super(Dropout, self).__init__(**kwargs)\\n', '     self.rate = rate\\n', '     if isinstance(rate, (int, float)) and not rate:\\n', '       keras_temporary_dropout_rate.get_cell().set(True)\\n']",
                    "hunk_fix": "@@ -195,6 +195,9 @@ class Dropout(Layer):\n \n   def __init__(self, rate, noise_shape=None, seed=None, **kwargs):\n     super(Dropout, self).__init__(**kwargs)\n+    if isinstance(rate, (int, float)) and rate < 0:\n+      raise ValueError(\"Invalid value received for `rate`, expected \"\n+                       \"a value between 0 and 1.\")\n     self.rate = rate\n     if isinstance(rate, (int, float)) and not rate:\n       keras_temporary_dropout_rate.get_cell().set(True)\n"
                },
                {
                    "old_start": 739,
                    "old_length": 6,
                    "new_start": 742,
                    "new_length": 8,
                    "hunk_buggy": "['   def __init__(self, n, **kwargs):\\n', '     super(RepeatVector, self).__init__(**kwargs)\\n', '     self.n = n\\n', '     self.input_spec = InputSpec(ndim=2)\\n', ' \\n', '   def compute_output_shape(self, input_shape):\\n']",
                    "hunk_fix": "@@ -739,6 +742,8 @@ class RepeatVector(Layer):\n   def __init__(self, n, **kwargs):\n     super(RepeatVector, self).__init__(**kwargs)\n     self.n = n\n+    if not isinstance(n, int):\n+      raise TypeError(\"Expected an integer value for `n`.\")\n     self.input_spec = InputSpec(ndim=2)\n \n   def compute_output_shape(self, input_shape):\n"
                },
                {
                    "old_start": 1165,
                    "old_length": 6,
                    "new_start": 1170,
                    "new_length": 9,
                    "hunk_buggy": "['         activity_regularizer=activity_regularizer, **kwargs)\\n', '     \\n', '     self.units = int(units) if not isinstance(units, int) else units\\n', '     self.activation = activations.get(activation)\\n', '     self.use_bias = use_bias\\n', '     self.kernel_initializer = initializers.get(kernel_initializer)']",
                    "hunk_fix": "@@ -1165,6 +1170,9 @@ class Dense(Layer):\n         activity_regularizer=activity_regularizer, **kwargs)\n     \n     self.units = int(units) if not isinstance(units, int) else units\n+    if self.units < 0:\n+      raise ValueError(f\"Received an invalid value for `units`, expected\n+                       f\"a positive integer, got {units}.\")\n     self.activation = activations.get(activation)\n     self.use_bias = use_bias\n     self.kernel_initializer = initializers.get(kernel_initializer)"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 169,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/551a90f2e3d20420d68a2796d19f1c42b6636e0d",
    "date": "2021-04-07T05:50:41-07:00",
    "message": "Add checks in ReduceWindowOpOnTensorsConversion.\n\nThe pattern does not support ops with non-zero padding config. Add a check to\nprevent unexpected lowering.\n\nIt is not easy to add tests because other patterns will convert body ops, and\nit causes issues like invalid IRs.\n\nPiperOrigin-RevId: 367202450\nChange-Id: Ibe3a7c904cda09c2fff122e303acdd9daea208ac",
    "changes": [
        {
            "name": "legalize_to_linalg.cc",
            "path": "tensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/transforms/legalize_to_linalg.cc",
            "patches": [
                {
                    "old_start": 1712,
                    "old_length": 6,
                    "new_start": 1712,
                    "new_length": 10,
                    "hunk_buggy": "['       return rewriter.notifyMatchFailure(op, \"expected NHWC pooling-based op\");\\n', '     }\\n', ' \\n', '     SmallVector<int64_t, 2> shapes;\\n', '     shapes.push_back(op.window_dimensions().getValue<int64_t>(1));\\n', '     shapes.push_back(op.window_dimensions().getValue<int64_t>(2));']",
                    "hunk_fix": "@@ -1712,6 +1712,10 @@ struct ReduceWindowOpOnTensorsConversion\n       return rewriter.notifyMatchFailure(op, \"expected NHWC pooling-based op\");\n     }\n \n+    if (op.padding() && !isSplatValue(*op.padding(), 0)) {\n+      return rewriter.notifyMatchFailure(op, \"require paddings are all zero\");\n+    }\n+\n     SmallVector<int64_t, 2> shapes;\n     shapes.push_back(op.window_dimensions().getValue<int64_t>(1));\n     shapes.push_back(op.window_dimensions().getValue<int64_t>(2));"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 170,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/1e5c11676dce37bb7c8eb58b35fd298a655c6fd3",
    "date": "2021-04-05T11:21:08-07:00",
    "message": "[tf.data service] Include dispatcher address in version check error message.\n\nThis is the error message that happens when the address was specified incorrectly, so it is useful to include the potentially-incorrect address in the error message.\n\nPiperOrigin-RevId: 366831311\nChange-Id: I000307bb15759f3d52635f047e62d04b0618c9c3",
    "changes": [
        {
            "name": "data_service.cc",
            "path": "tensorflow/core/data/service/data_service.cc",
            "patches": [
                {
                    "old_start": 264,
                    "old_length": 7,
                    "new_start": 264,
                    "new_length": 11,
                    "hunk_buggy": "['         grpc::ClientContext ctx;\\n', '         grpc::Status s = stub_->GetVersion(&ctx, req, &resp);\\n', '         if (!s.ok()) {\\n', '-          return grpc_util::WrapError(\"Failed to get dispatcher version\", s);\\n', '         }\\n', '         return Status::OK();\\n', '       },']",
                    "hunk_fix": "@@ -264,7 +264,11 @@ Status DataServiceDispatcherClient::EnsureInitialized() {\n         grpc::ClientContext ctx;\n         grpc::Status s = stub_->GetVersion(&ctx, req, &resp);\n         if (!s.ok()) {\n-          return grpc_util::WrapError(\"Failed to get dispatcher version\", s);\n+          return grpc_util::WrapError(\n+              absl::StrCat(\"Failed to get dispatcher version from dispatcher \"\n+                           \"running at \",\n+                           address_),\n+              s);\n         }\n         return Status::OK();\n       },"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 171,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/e07e48b2e0908333a36f1c5726a9406a83b3ec90",
    "date": "2021-03-15T13:04:19-07:00",
    "message": "Added a check on literal_.has_value() to avoid segfault.\n\nPiperOrigin-RevId: 363008898\nChange-Id: I1a17b58f7e05ccb7db70ded82522030258239fef",
    "changes": [
        {
            "name": "hlo_instructions.cc",
            "path": "tensorflow/compiler/xla/service/hlo_instructions.cc",
            "patches": [
                {
                    "old_start": 1322,
                    "old_length": 6,
                    "new_start": 1322,
                    "new_length": 9,
                    "hunk_buggy": "['     const HloPrintOptions& options,\\n', '     CanonicalNameMap* canonical_name_map) const {\\n', '   if (options.print_only_essential_constants()) {\\n', '     if (literal().IsAll(0)) {\\n', '       return \"0\";\\n', '     }']",
                    "hunk_fix": "@@ -1322,6 +1322,9 @@ string HloConstantInstruction::OperandsToStringWithCanonicalNameMap(\n     const HloPrintOptions& options,\n     CanonicalNameMap* canonical_name_map) const {\n   if (options.print_only_essential_constants()) {\n+    if (!literal_.has_value()) {\n+      return \"{...}\";\n+    }\n     if (literal().IsAll(0)) {\n       return \"0\";\n     }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 172,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/582bf0d3ac33fc10156f737c0d42f3adee54409a",
    "date": "2021-03-11T23:39:10-08:00",
    "message": "Update the tflite model \"buffers\" field checking rule.\n\nIf we don't use \"--force-empty-vectors\" flag[1] for flatc, the buffers\nmight be a null ptr if we serialize a model with zero buffers size(e.g.\nall ops in model doesn't use const weights in model).\nThis commit relaxs the \"buffers\" null ptr checking for this situation,\nand also updates the \"subgraphs\" checking for null ptr dereference.\n\n[1]\nhttps://github.com/google/flatbuffers/blob/fc4fffea41f5b682198edfc72eca536d33fd848c/src/flatc.cpp#L167",
    "changes": [
        {
            "name": "interpreter_builder.cc",
            "path": "tensorflow/lite/interpreter_builder.cc",
            "patches": [
                {
                    "old_start": 564,
                    "old_length": 11,
                    "new_start": 564,
                    "new_length": 13,
                    "hunk_buggy": "['       // TODO(aselle): Check what happens if we have an unspecified size\\n', '       // constant.\\n', '       *buffer_data = nullptr;\\n', '-      if (tensor->buffer() == 0) return kTfLiteOk;\\n', '-      if (tensor->buffer() >= buffers->size()) {\\n', '         error_reporter_->Report(\\n', '             \"Tensor %d specifies out of range buffer %d (only %d buffers).\\\\n\",\\n', '-            i, tensor->buffer(), buffers->size());\\n', '         return kTfLiteError;\\n', '       }\\n', '       if (auto* buffer = (*buffers)[tensor->buffer()]) {\\n']",
                    "hunk_fix": "@@ -564,11 +564,13 @@ TfLiteStatus InterpreterBuilder::ParseTensors(\n       // TODO(aselle): Check what happens if we have an unspecified size\n       // constant.\n       *buffer_data = nullptr;\n-      if (tensor->buffer() == 0) return kTfLiteOk;\n-      if (tensor->buffer() >= buffers->size()) {\n+      if (tensor->buffer() == 0) {\n+        return kTfLiteOk;\n+      }\n+      if (!buffers || tensor->buffer() >= buffers->size()) {\n         error_reporter_->Report(\n             \"Tensor %d specifies out of range buffer %d (only %d buffers).\\n\",\n-            i, tensor->buffer(), buffers->size());\n+            i, tensor->buffer(), (buffers) ? buffers->size() : 0);\n         return kTfLiteError;\n       }\n       if (auto* buffer = (*buffers)[tensor->buffer()]) {\n"
                },
                {
                    "old_start": 710,
                    "old_length": 15,
                    "new_start": 712,
                    "new_length": 11,
                    "hunk_buggy": "['   auto* subgraphs = model_->subgraphs();\\n', '   auto* buffers = model_->buffers();\\n', ' \\n', '-  if (subgraphs->size() == 0) {\\n', '     TF_LITE_REPORT_ERROR(error_reporter_, \"No subgraph in the model.\\\\n\");\\n', '     return cleanup_and_error();\\n', '   }\\n', ' \\n', '-  if (!buffers) {\\n', '-    TF_LITE_REPORT_ERROR(error_reporter_, \"No buffers in the model.\\\\n\");\\n', '-    return cleanup_and_error();\\n', '-  }\\n', ' \\n', '   interpreter->reset(new Interpreter(error_reporter_));\\n', '   (*interpreter)->SetNumThreads(num_threads);']",
                    "hunk_fix": "@@ -710,15 +712,11 @@ TfLiteStatus InterpreterBuilder::operator()(\n   auto* subgraphs = model_->subgraphs();\n   auto* buffers = model_->buffers();\n \n-  if (subgraphs->size() == 0) {\n+  if (!subgraphs || subgraphs->size() == 0) {\n     TF_LITE_REPORT_ERROR(error_reporter_, \"No subgraph in the model.\\n\");\n     return cleanup_and_error();\n   }\n \n-  if (!buffers) {\n-    TF_LITE_REPORT_ERROR(error_reporter_, \"No buffers in the model.\\n\");\n-    return cleanup_and_error();\n-  }\n \n   interpreter->reset(new Interpreter(error_reporter_));\n   (*interpreter)->SetNumThreads(num_threads);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 173,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/9b0f99ddd27e7738732a154be5469391ee8fc977",
    "date": "2021-02-15T18:11:49-08:00",
    "message": "Add check to ensure element sizes are the same",
    "changes": [
        {
            "name": "optimized_ops.h",
            "path": "tensorflow/lite/kernels/internal/optimized/optimized_ops.h",
            "patches": [
                {
                    "old_start": 2724,
                    "old_length": 7,
                    "new_start": 2724,
                    "new_length": 7,
                    "hunk_buggy": "['     const T* input1_data, const RuntimeShape& input2_shape,\\n', '     const T* input2_data, const RuntimeShape& output_shape, T* output_data) {\\n', '   ruy::profiler::ScopeLabel label(\"SubWithActivation_optimized\");\\n', '-\\n', '   auto input1_map = MapAsVector(input1_data, input1_shape);\\n', '   auto input2_map = MapAsVector(input2_data, input2_shape);\\n', '   auto output_map = MapAsVector(output_data, output_shape);']",
                    "hunk_fix": "@@ -2724,7 +2724,7 @@ inline void SubWithActivation(\n     const T* input1_data, const RuntimeShape& input2_shape,\n     const T* input2_data, const RuntimeShape& output_shape, T* output_data) {\n   ruy::profiler::ScopeLabel label(\"SubWithActivation_optimized\");\n-\n+  TFLITE_DCHECK_EQ(input1_shape.FlatSize(), input2_shape.FlatSize());\n   auto input1_map = MapAsVector(input1_data, input1_shape);\n   auto input2_map = MapAsVector(input2_data, input2_shape);\n   auto output_map = MapAsVector(output_data, output_shape);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 174,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/b1c9e600e02b93885dbebfa5dae92436c63d6c03",
    "date": "2021-02-11T08:43:58-08:00",
    "message": "[XLA] Add range check for xla::Array<> indexing.\n\nPiperOrigin-RevId: 356981991\nChange-Id: I73343a8776b0df0f2570bcd596247164c8588cb9",
    "changes": [
        {
            "name": "array.h",
            "path": "tensorflow/compiler/xla/array.h",
            "patches": [
                {
                    "old_start": 561,
                    "old_length": 6,
                    "new_start": 561,
                    "new_length": 7,
                    "hunk_buggy": "['       index *= sizes_[i];\\n', '       index += indexes[i];\\n', '     }\\n', '     return index;\\n', '   }\\n', ' ']",
                    "hunk_fix": "@@ -561,6 +561,7 @@ class Array {\n       index *= sizes_[i];\n       index += indexes[i];\n     }\n+    DCHECK_LT(index, this->num_elements());\n     return index;\n   }\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 175,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/3c80be9f2cfece929f5858e7df0e7f4503c9baec",
    "date": "2021-01-14T16:01:39-08:00",
    "message": "[tf.data service] Support num_consumers being a Tensor.\n\nThe `if num_consumers >= 0:` check causes an error when num_consumers is a Tensor in graph mode:\n\nOperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution\n\nComparing to `None` accomplishes the same thing without this issue. Once the forward compatibility window expires we can remove the check completely.\n\nPiperOrigin-RevId: 351894589\nChange-Id: I0f6e8b93f6ae14b92b08c7c438d6a111ae0efff7",
    "changes": [
        {
            "name": "data_service_ops.py",
            "path": "tensorflow/python/data/experimental/ops/data_service_ops.py",
            "patches": [
                {
                    "old_start": 111,
                    "old_length": 10,
                    "new_start": 111,
                    "new_length": 6,
                    "hunk_buggy": "['       max_outstanding_requests = dataset_ops.AUTOTUNE\\n', '     if task_refresh_interval_hint_ms is None:\\n', '       task_refresh_interval_hint_ms = dataset_ops.AUTOTUNE\\n', '-    if consumer_index is None:\\n', '-      consumer_index = -1\\n', '-    if num_consumers is None:\\n', '-      num_consumers = -1\\n', ' \\n', '     self._dataset_id = ops.convert_to_tensor(\\n', '         dataset_id, dtype=dtypes.int64, name=\"dataset_id\")\\n']",
                    "hunk_fix": "@@ -111,10 +111,6 @@ class _DataServiceDatasetV2(dataset_ops.DatasetSource):\n       max_outstanding_requests = dataset_ops.AUTOTUNE\n     if task_refresh_interval_hint_ms is None:\n       task_refresh_interval_hint_ms = dataset_ops.AUTOTUNE\n-    if consumer_index is None:\n-      consumer_index = -1\n-    if num_consumers is None:\n-      num_consumers = -1\n \n     self._dataset_id = ops.convert_to_tensor(\n         dataset_id, dtype=dtypes.int64, name=\"dataset_id\")\n"
                },
                {
                    "old_start": 127,
                    "old_length": 9,
                    "new_start": 123,
                    "new_length": 13,
                    "hunk_buggy": "['     self._job_name = ops.convert_to_tensor(\\n', '         job_name, dtype=dtypes.string, name=\"job_name\")\\n', '     self._consumer_index = ops.convert_to_tensor(\\n', '-        consumer_index, dtype=dtypes.int64, name=\"consumer_index\")\\n', '     self._num_consumers = ops.convert_to_tensor(\\n', '-        num_consumers, dtype=dtypes.int64, name=\"num_consumers\")\\n', '     self._max_outstanding_requests = ops.convert_to_tensor(\\n', '         max_outstanding_requests,\\n', '         dtype=dtypes.int64,\\n']",
                    "hunk_fix": "@@ -127,9 +123,13 @@ class _DataServiceDatasetV2(dataset_ops.DatasetSource):\n     self._job_name = ops.convert_to_tensor(\n         job_name, dtype=dtypes.string, name=\"job_name\")\n     self._consumer_index = ops.convert_to_tensor(\n-        consumer_index, dtype=dtypes.int64, name=\"consumer_index\")\n+        -1 if consumer_index is None else consumer_index,\n+        dtype=dtypes.int64,\n+        name=\"consumer_index\")\n     self._num_consumers = ops.convert_to_tensor(\n-        num_consumers, dtype=dtypes.int64, name=\"num_consumers\")\n+        -1 if num_consumers is None else num_consumers,\n+        dtype=dtypes.int64,\n+        name=\"num_consumers\")\n     self._max_outstanding_requests = ops.convert_to_tensor(\n         max_outstanding_requests,\n         dtype=dtypes.int64,\n"
                },
                {
                    "old_start": 138,
                    "old_length": 27,
                    "new_start": 138,
                    "new_length": 27,
                    "hunk_buggy": "['     # represented by scalar DT_VARIANTs.\\n', '     self._element_spec = tensor_spec.TensorSpec(shape=(), dtype=dtypes.variant)\\n', ' \\n', '-    if num_consumers >= 0:\\n', '-      variant_tensor = gen_experimental_dataset_ops.data_service_dataset_v2(\\n', '           dataset_id=self._dataset_id,\\n', '           processing_mode=self._processing_mode,\\n', '           address=self._address,\\n', '           protocol=self._protocol,\\n', '           job_name=self._job_name,\\n', '-          consumer_index=self._consumer_index,\\n', '-          num_consumers=self._num_consumers,\\n', '           max_outstanding_requests=self._max_outstanding_requests,\\n', '           task_refresh_interval_hint_ms=task_refresh_interval_hint_ms,\\n', '           iteration_counter=gen_experimental_dataset_ops\\n', '           .dummy_iteration_counter(),\\n', '           **self._flat_structure)\\n', '     else:\\n', '-      variant_tensor = gen_experimental_dataset_ops.data_service_dataset(\\n', '           dataset_id=self._dataset_id,\\n', '           processing_mode=self._processing_mode,\\n', '           address=self._address,\\n', '           protocol=self._protocol,\\n', '           job_name=self._job_name,\\n', '           max_outstanding_requests=self._max_outstanding_requests,\\n', '           task_refresh_interval_hint_ms=task_refresh_interval_hint_ms,\\n', '           iteration_counter=gen_experimental_dataset_ops']",
                    "hunk_fix": "@@ -138,27 +138,27 @@ class _DataServiceDatasetV2(dataset_ops.DatasetSource):\n     # represented by scalar DT_VARIANTs.\n     self._element_spec = tensor_spec.TensorSpec(shape=(), dtype=dtypes.variant)\n \n-    if num_consumers >= 0:\n-      variant_tensor = gen_experimental_dataset_ops.data_service_dataset_v2(\n+    if num_consumers is None:\n+      variant_tensor = gen_experimental_dataset_ops.data_service_dataset(\n           dataset_id=self._dataset_id,\n           processing_mode=self._processing_mode,\n           address=self._address,\n           protocol=self._protocol,\n           job_name=self._job_name,\n-          consumer_index=self._consumer_index,\n-          num_consumers=self._num_consumers,\n           max_outstanding_requests=self._max_outstanding_requests,\n           task_refresh_interval_hint_ms=task_refresh_interval_hint_ms,\n           iteration_counter=gen_experimental_dataset_ops\n           .dummy_iteration_counter(),\n           **self._flat_structure)\n     else:\n-      variant_tensor = gen_experimental_dataset_ops.data_service_dataset(\n+      variant_tensor = gen_experimental_dataset_ops.data_service_dataset_v2(\n           dataset_id=self._dataset_id,\n           processing_mode=self._processing_mode,\n           address=self._address,\n           protocol=self._protocol,\n           job_name=self._job_name,\n+          consumer_index=self._consumer_index,\n+          num_consumers=self._num_consumers,\n           max_outstanding_requests=self._max_outstanding_requests,\n           task_refresh_interval_hint_ms=task_refresh_interval_hint_ms,\n           iteration_counter=gen_experimental_dataset_ops"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 176,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/b6f3366a716ca9b5a1e6114a3bea050c80d8a475",
    "date": "2020-12-30T14:42:46-08:00",
    "message": "Don't check for if null after already dereferenced\n\nI'm not sure how it could be null at this point (and obviously it is nowhere else we'd have seen failures), but keeping the check as is and just moving it to where it would catch it before dereferencing.\n\nPiperOrigin-RevId: 349603765\nChange-Id: Ifa792bed92ffd2652c6946d3a48bfb53aafc3477",
    "changes": [
        {
            "name": "function_def_utils.cc",
            "path": "tensorflow/core/common_runtime/function_def_utils.cc",
            "patches": [
                {
                    "old_start": 55,
                    "old_length": 9,
                    "new_start": 55,
                    "new_length": 11,
                    "hunk_buggy": "['   const StackTracesMap& stack_traces =\\n', '       lib_def->GetStackTraces(fdef.signature().name());\\n', '   for (Node* n : graph->nodes()) {\\n', '-    auto it = stack_traces.find(n->name());\\n', '-    if (n && it != stack_traces.end()) {\\n', '-      n->SetStackTrace(it->second);\\n', '     }\\n', '   }\\n', ' ']",
                    "hunk_fix": "@@ -55,9 +55,11 @@ Status FunctionDefToBodyHelper(\n   const StackTracesMap& stack_traces =\n       lib_def->GetStackTraces(fdef.signature().name());\n   for (Node* n : graph->nodes()) {\n-    auto it = stack_traces.find(n->name());\n-    if (n && it != stack_traces.end()) {\n-      n->SetStackTrace(it->second);\n+    if (n) {\n+      auto it = stack_traces.find(n->name());\n+      if (it != stack_traces.end()) {\n+        n->SetStackTrace(it->second);\n+      }\n     }\n   }\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 177,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/8cef4cda26e08256b6698e942820d9a3ac1bcc94",
    "date": "2020-12-11T11:52:43+05:30",
    "message": "Add minor checks for data_format and padding value",
    "changes": [
        {
            "name": "tf_ops_a_m.cc",
            "path": "tensorflow/compiler/mlir/tensorflow/ir/tf_ops_a_m.cc",
            "patches": [
                {
                    "old_start": 1467,
                    "old_length": 9,
                    "new_start": 1467,
                    "new_length": 15,
                    "hunk_buggy": "['   ArrayRef<Attribute> dilations = op.dilations().getValue();\\n', ' \\n', '   tensorflow::TensorFormat format;\\n', '-  FormatFromString(data_format.str(), &format);\\n', '   tensorflow::Padding padding;\\n', '-  GetPaddingFromString(paddings.str(), &padding);\\n', '   auto get_int = [](Attribute attr) {\\n', '     return attr.template cast<IntegerAttr>().getInt();\\n', '   };']",
                    "hunk_fix": "@@ -1467,9 +1467,15 @@ static LogicalResult inferConvReturnTypes(\n   ArrayRef<Attribute> dilations = op.dilations().getValue();\n \n   tensorflow::TensorFormat format;\n-  FormatFromString(data_format.str(), &format);\n+  auto data_format_is_valid = FormatFromString(data_format.str(), &format);\n+  if (!data_format_is_valid) {\n+    return emitOptionalError(location, \"Invalid data format provided\");\n+  }\n   tensorflow::Padding padding;\n-  GetPaddingFromString(paddings.str(), &padding);\n+  auto padding_is_valid = GetPaddingFromString(paddings.str(), &padding);\n+  if (!padding_is_valid.ok()) {\n+    return emitOptionalError(location, \"Invalid padding format provided\");\n+  }\n   auto get_int = [](Attribute attr) {\n     return attr.template cast<IntegerAttr>().getInt();\n   };"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 178,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/e7de472681079932b2547024f31c876da54f61a0",
    "date": "2020-12-07T15:36:46-08:00",
    "message": "Fix a bug in flatbuffer importer that use tensor quantization before checking.\n\nPiperOrigin-RevId: 346193249\nChange-Id: I10fabc67ee5339c40db640477116127fb50bc575",
    "changes": [
        {
            "name": "flatbuffer_import.cc",
            "path": "tensorflow/compiler/mlir/lite/flatbuffer_import.cc",
            "patches": [
                {
                    "old_start": 509,
                    "old_length": 7,
                    "new_start": 509,
                    "new_length": 7,
                    "hunk_buggy": "['     return op.getOperation();\\n', '   }\\n', '   auto op = builder.create<tfl::ConstOp>(loc, value);\\n', '-  if (!tensor.quantization->min.empty()) {\\n', '     if (auto stats_op =\\n', '             ConvertMinMaxToStatsOp(tensor, builder, op.getResult())) {\\n', '       return stats_op;']",
                    "hunk_fix": "@@ -509,7 +509,7 @@ Operation* BuildVariableOp(const tflite::TensorT& tensor,\n     return op.getOperation();\n   }\n   auto op = builder.create<tfl::ConstOp>(loc, value);\n-  if (!tensor.quantization->min.empty()) {\n+  if (tensor.quantization && !tensor.quantization->min.empty()) {\n     if (auto stats_op =\n             ConvertMinMaxToStatsOp(tensor, builder, op.getResult())) {\n       return stats_op;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 179,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/15c186bffe51901e4a48b4b6bf1316832533743f",
    "date": "2020-12-04T18:17:13-08:00",
    "message": "Correctly handle the case if static maximum dimension size = 0.\n\nPiperOrigin-RevId: 345794998\nChange-Id: Ic112e144731c1a23879c44aa7c5e3c180ccccc1c",
    "changes": [
        {
            "name": "tpu.py",
            "path": "tensorflow/python/tpu/tpu.py",
            "patches": [
                {
                    "old_start": 1032,
                    "old_length": 7,
                    "new_start": 1032,
                    "new_length": 7,
                    "hunk_buggy": "['         need_padding.append(np.full_like(input_shape, False, dtype=bool))\\n', '       else:\\n', '         for i, s in enumerate(input_shape):\\n', '-          if not s or s != maximum_static_shapes[idx][i]:\\n', '             need_padding[idx][i] = True\\n', '         maximum_static_shapes[idx] = max(input_shape,\\n', '                                          maximum_static_shapes[idx])\\n']",
                    "hunk_fix": "@@ -1032,7 +1032,7 @@ def _pad_all_input(\n         need_padding.append(np.full_like(input_shape, False, dtype=bool))\n       else:\n         for i, s in enumerate(input_shape):\n-          if not s or s != maximum_static_shapes[idx][i]:\n+          if s is None or s != maximum_static_shapes[idx][i]:\n             need_padding[idx][i] = True\n         maximum_static_shapes[idx] = max(input_shape,\n                                          maximum_static_shapes[idx])\n"
                },
                {
                    "old_start": 1081,
                    "old_length": 7,
                    "new_start": 1081,
                    "new_length": 7,
                    "hunk_buggy": "[\"             # The minimum padded dimension size is 2 as XLA doesn't support size\\n\", '             # 1 dynamic size.\\n', '             minimum_dynamic_dim_size = 2\\n', '-            if s.value:\\n', '               # Pad to the given maximum value.\\n', '               max_dim_size = max(s.value, minimum_dynamic_dim_size)\\n', '             else:']",
                    "hunk_fix": "@@ -1081,7 +1081,7 @@ def _pad_all_input(\n             # The minimum padded dimension size is 2 as XLA doesn't support size\n             # 1 dynamic size.\n             minimum_dynamic_dim_size = 2\n-            if s.value:\n+            if s.value is not None:\n               # Pad to the given maximum value.\n               max_dim_size = max(s.value, minimum_dynamic_dim_size)\n             else:"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 180,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/d414a925a73553e4dd0d559d2d275668a298dab4",
    "date": "2020-11-18T02:49:04-08:00",
    "message": "Check against the size of a std::vector to prevent out-of-boundary access.\n\nPiperOrigin-RevId: 343043192\nChange-Id: Icc6f7338791709c1d9dfe20e985fbcdd50ed3336",
    "changes": [
        {
            "name": "verifier.cc",
            "path": "tensorflow/lite/tools/verifier.cc",
            "patches": [
                {
                    "old_start": 351,
                    "old_length": 6,
                    "new_start": 351,
                    "new_length": 9,
                    "hunk_buggy": "['   for (int i = 0; i < block_rank; i++) {\\n', '     int original_block_dim =\\n', '         sparsity->traversal_order()->Get(i + original_rank);\\n', '     int block_dim_size =\\n', '         sparsity->dim_metadata()->Get(i + original_rank)->dense_size();\\n', '     if (block_dim_size == 0) {\\n']",
                    "hunk_fix": "@@ -351,6 +351,9 @@ absl::optional<uint64_t> VerifyAndCountSparseElements(const Tensor& tensor) {\n   for (int i = 0; i < block_rank; i++) {\n     int original_block_dim =\n         sparsity->traversal_order()->Get(i + original_rank);\n+    if (original_block_dim < 0 || original_block_dim >= total_dims) {\n+      return absl::nullopt;\n+    }\n     int block_dim_size =\n         sparsity->dim_metadata()->Get(i + original_rank)->dense_size();\n     if (block_dim_size == 0) {\n"
                },
                {
                    "old_start": 358,
                    "old_length": 7,
                    "new_start": 361,
                    "new_length": 12,
                    "hunk_buggy": "['     }\\n', ' \\n', '     expanded_dim_sizes[original_block_dim] = block_dim_size;\\n', '-    expanded_dim_sizes[sparsity->block_map()->Get(i)] /= block_dim_size;\\n', '   }\\n', ' \\n', '   return VerifyAndCountElements(*sparsity, expanded_dim_sizes);']",
                    "hunk_fix": "@@ -358,7 +361,12 @@ absl::optional<uint64_t> VerifyAndCountSparseElements(const Tensor& tensor) {\n     }\n \n     expanded_dim_sizes[original_block_dim] = block_dim_size;\n-    expanded_dim_sizes[sparsity->block_map()->Get(i)] /= block_dim_size;\n+\n+    int mapped_block_dim = sparsity->block_map()->Get(i);\n+    if (mapped_block_dim < 0 || mapped_block_dim >= total_dims) {\n+      return absl::nullopt;\n+    }\n+    expanded_dim_sizes[mapped_block_dim] /= block_dim_size;\n   }\n \n   return VerifyAndCountElements(*sparsity, expanded_dim_sizes);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 181,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/459b4bfe1f73737fae23aa1499b06a69605d0f65",
    "date": "2020-10-26T17:56:11-07:00",
    "message": "Added a check in EagerExecutor to avoid getting invalid range.\n\nPiperOrigin-RevId: 339155581\nChange-Id: Id3dd028f2ea6ae1e2889b1ef6661796d44203c5a",
    "changes": [
        {
            "name": "eager_executor.cc",
            "path": "tensorflow/core/common_runtime/eager/eager_executor.cc",
            "patches": [
                {
                    "old_start": 300,
                    "old_length": 6,
                    "new_start": 300,
                    "new_length": 9,
                    "hunk_buggy": "['     } else {\\n', '       upperbound_id = next_node_id_ - 1;\\n', '     }\\n', '     DVLOG(3) << \"Notify node done: [id \" << id << \" to \" << upperbound_id\\n', '              << \"] \";\\n', '     // Note that we notify all waiting threads in case an error has']",
                    "hunk_fix": "@@ -300,6 +300,9 @@ void EagerExecutor::NotifyWaiters(uint64 id) {\n     } else {\n       upperbound_id = next_node_id_ - 1;\n     }\n+    if (upperbound_id < id) {\n+      return;\n+    }\n     DVLOG(3) << \"Notify node done: [id \" << id << \" to \" << upperbound_id\n              << \"] \";\n     // Note that we notify all waiting threads in case an error has"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 182,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/e1eb6d9cfa14368442f0d172a40f87ce4f094386",
    "date": "2020-10-18T18:45:15-07:00",
    "message": "Add CheckArraySegments() to model verifier\n\nAdd additional logic to check nullness of array segments vector.\n\nPiperOrigin-RevId: 337776748\nChange-Id: Ia6a7a9f1773b211a54e7598d85768aaf3e136753",
    "changes": [
        {
            "name": "verifier.cc",
            "path": "tensorflow/lite/tools/verifier.cc",
            "patches": [
                {
                    "old_start": 126,
                    "old_length": 6,
                    "new_start": 126,
                    "new_length": 25,
                    "hunk_buggy": "['   return true;\\n', ' }\\n', ' \\n', ' int GetSizeOfSegments(const DimensionMetadata* dim_metadata) {\\n', '   switch (dim_metadata->array_segments_type()) {\\n', '     case SparseIndexVector_Int32Vector:\\n']",
                    "hunk_fix": "@@ -126,6 +126,25 @@ bool VerifyStringTensorBuffer(const Tensor& tensor, const Buffer& buffer,\n   return true;\n }\n \n+bool CheckArraySegments(const DimensionMetadata* dim_metadata) {\n+  if (dim_metadata->array_segments() == nullptr) {\n+    return false;\n+  }\n+  switch (dim_metadata->array_segments_type()) {\n+    case SparseIndexVector_Int32Vector:\n+      return (dim_metadata->array_segments_as_Int32Vector()->values() !=\n+              nullptr);\n+    case SparseIndexVector_Uint16Vector:\n+      return (dim_metadata->array_segments_as_Uint16Vector()->values() !=\n+              nullptr);\n+    case SparseIndexVector_Uint8Vector:\n+      return (dim_metadata->array_segments_as_Uint8Vector()->values() !=\n+              nullptr);\n+    default:\n+      return false;\n+  }\n+}\n+\n int GetSizeOfSegments(const DimensionMetadata* dim_metadata) {\n   switch (dim_metadata->array_segments_type()) {\n     case SparseIndexVector_Int32Vector:\n"
                },
                {
                    "old_start": 155,
                    "old_length": 6,
                    "new_start": 174,
                    "new_length": 25,
                    "hunk_buggy": "['   }\\n', ' }\\n', ' \\n', ' int GetSizeOfIndices(const DimensionMetadata* dim_metadata) {\\n', '   switch (dim_metadata->array_indices_type()) {\\n', '     case SparseIndexVector_Int32Vector:\\n']",
                    "hunk_fix": "@@ -155,6 +174,25 @@ int GetValueOfSegmentsAt(const DimensionMetadata* dim_metadata, const int i) {\n   }\n }\n \n+bool CheckArrayIndices(const DimensionMetadata* dim_metadata) {\n+  if (dim_metadata->array_indices() == nullptr) {\n+    return false;\n+  }\n+  switch (dim_metadata->array_indices_type()) {\n+    case SparseIndexVector_Int32Vector:\n+      return (dim_metadata->array_indices_as_Int32Vector()->values() !=\n+              nullptr);\n+    case SparseIndexVector_Uint16Vector:\n+      return (dim_metadata->array_indices_as_Uint16Vector()->values() !=\n+              nullptr);\n+    case SparseIndexVector_Uint8Vector:\n+      return (dim_metadata->array_indices_as_Uint8Vector()->values() !=\n+              nullptr);\n+    default:\n+      return false;\n+  }\n+}\n+\n int GetSizeOfIndices(const DimensionMetadata* dim_metadata) {\n   switch (dim_metadata->array_indices_type()) {\n     case SparseIndexVector_Int32Vector:\n"
                },
                {
                    "old_start": 205,
                    "old_length": 9,
                    "new_start": 243,
                    "new_length": 8,
                    "hunk_buggy": "['       // Each index in a dense dimension is stored implicitly.\\n', '       num_elements *= dim_metadata->dense_size();\\n', '     } else {\\n', '-      const auto* array_segments = dim_metadata->array_segments();\\n', '-      const auto* array_indices = dim_metadata->array_indices();\\n', '-      if (array_segments == nullptr || array_indices == nullptr) {\\n', '         return absl::nullopt;\\n', '       }\\n', ' ']",
                    "hunk_fix": "@@ -205,9 +243,8 @@ absl::optional<uint64_t> VerifyAndCountElements(\n       // Each index in a dense dimension is stored implicitly.\n       num_elements *= dim_metadata->dense_size();\n     } else {\n-      const auto* array_segments = dim_metadata->array_segments();\n-      const auto* array_indices = dim_metadata->array_indices();\n-      if (array_segments == nullptr || array_indices == nullptr) {\n+      if (!CheckArraySegments(dim_metadata) ||\n+          !CheckArrayIndices(dim_metadata)) {\n         return absl::nullopt;\n       }\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 183,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/0d65cfaab050295c311d9f2fb28388435359db27",
    "date": "2020-10-12T11:00:39-07:00",
    "message": "Add an additional `NoneType` check when converting a traced tensor to a `KerasTensor`.\n\nPiperOrigin-RevId: 336694750\nChange-Id: Ic79bc3b46b81d4816a7108ed9b1aa426e7f4d3d5",
    "changes": [
        {
            "name": "keras_tensor.py",
            "path": "tensorflow/python/keras/engine/keras_tensor.py",
            "patches": [
                {
                    "old_start": 170,
                    "old_length": 7,
                    "new_start": 170,
                    "new_length": 8,
                    "hunk_buggy": "[\"       name = getattr(tensor, 'name', None)\\n\", '       type_spec = type_spec_module.type_spec_from_value(tensor)\\n', '       inferred_value = None\\n', '-      if (type_spec.dtype == dtypes.int32 and type_spec.shape.rank < 2):\\n', '         # If this tensor might be representing shape information,\\n', '         # (dtype=int32, rank of 0 or 1, not too large to represent a shape)\\n', \"         # we attempt to capture any value information tensorflow's\"]",
                    "hunk_fix": "@@ -170,7 +170,8 @@ class KerasTensor(object):\n       name = getattr(tensor, 'name', None)\n       type_spec = type_spec_module.type_spec_from_value(tensor)\n       inferred_value = None\n-      if (type_spec.dtype == dtypes.int32 and type_spec.shape.rank < 2):\n+      if (type_spec.dtype == dtypes.int32 and type_spec.shape.rank is not None\n+          and type_spec.shape.rank < 2):\n         # If this tensor might be representing shape information,\n         # (dtype=int32, rank of 0 or 1, not too large to represent a shape)\n         # we attempt to capture any value information tensorflow's"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 184,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/bd1f1ac1fec05d38f1b8fc98f650c1c55ac06790",
    "date": "2020-10-06T15:04:33+02:00",
    "message": "Fix operator check",
    "changes": [
        {
            "name": "registrations_util.py",
            "path": "tensorflow/python/ops/linalg/registrations_util.py",
            "patches": [
                {
                    "old_start": 56,
                    "old_length": 7,
                    "new_start": 56,
                    "new_length": 7,
                    "hunk_buggy": "['       return m == l\\n', ' \\n', '   if (operator_a.is_square != operator_b.is_square) and (\\n', '-      operator_a.is_square is not None and operator_a.is_square is not None):\\n', '     return False\\n', ' \\n', '   return None']",
                    "hunk_fix": "@@ -56,7 +56,7 @@ def is_square(operator_a, operator_b):\n       return m == l\n \n   if (operator_a.is_square != operator_b.is_square) and (\n-      operator_a.is_square is not None and operator_a.is_square is not None):\n+      operator_a.is_square is not None and operator_b.is_square is not None):\n     return False\n \n   return None"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 185,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/402d478a107e2931fb0e9b2f08f973997cae7f98",
    "date": "2020-09-28T14:04:28-07:00",
    "message": "Move the checking of ranks for early exit",
    "changes": [
        {
            "name": "generic_layout_optimizer_transposer.cc",
            "path": "tensorflow/core/grappler/optimizers/generic_layout_optimizer_transposer.cc",
            "patches": [
                {
                    "old_start": 1049,
                    "old_length": 6,
                    "new_start": 1049,
                    "new_length": 9,
                    "hunk_buggy": "['   const auto* output_shape_attr = node->GetAttr(kAttrOutputShape);\\n', '   const auto& shape = output_shape_attr->list().shape(0);\\n', '   const int rank = shape.dim_size();\\n', '   std::string src_format = context->src_format;\\n', '   std::string dst_format = context->dst_format;\\n', '   // Update the format from 4D to 5D layout if necessary.\\n']",
                    "hunk_fix": "@@ -1049,6 +1049,9 @@ Status DefaultLayoutAgnosticOpTransposer::TransposeNode(\n   const auto* output_shape_attr = node->GetAttr(kAttrOutputShape);\n   const auto& shape = output_shape_attr->list().shape(0);\n   const int rank = shape.dim_size();\n+  if (rank != 4 && rank != 5) {\n+    return Status::OK();\n+  }\n   std::string src_format = context->src_format;\n   std::string dst_format = context->dst_format;\n   // Update the format from 4D to 5D layout if necessary.\n"
                },
                {
                    "old_start": 1060,
                    "old_length": 7,
                    "new_start": 1063,
                    "new_length": 7,
                    "hunk_buggy": "['     context->AssignDeviceAndDataFormats(context->target_device, src_format_3d,\\n', '                                         dst_format_3d);\\n', '   }\\n', '-  if (!ShouldProcess(*context, *node) || (rank != 4 && rank != 5) ||\\n', '       !IsAfterDstToSrcTransform(*context, *node)) {\\n', '     if (allow_5d) {\\n', '       context->AssignDeviceAndDataFormats(context->target_device, src_format,']",
                    "hunk_fix": "@@ -1060,7 +1063,7 @@ Status DefaultLayoutAgnosticOpTransposer::TransposeNode(\n     context->AssignDeviceAndDataFormats(context->target_device, src_format_3d,\n                                         dst_format_3d);\n   }\n-  if (!ShouldProcess(*context, *node) || (rank != 4 && rank != 5) ||\n+  if (!ShouldProcess(*context, *node) ||\n       !IsAfterDstToSrcTransform(*context, *node)) {\n     if (allow_5d) {\n       context->AssignDeviceAndDataFormats(context->target_device, src_format,"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 186,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/076ea8d84c2058b0d01d56dd9ddc3221a2e0c817",
    "date": "2020-09-28T11:07:24-07:00",
    "message": "Also check dst_format",
    "changes": [
        {
            "name": "generic_layout_optimizer_transposer.cc",
            "path": "tensorflow/core/grappler/optimizers/generic_layout_optimizer_transposer.cc",
            "patches": [
                {
                    "old_start": 1052,
                    "old_length": 7,
                    "new_start": 1052,
                    "new_length": 8,
                    "hunk_buggy": "['   std::string src_format = context->src_format;\\n', '   std::string dst_format = context->dst_format;\\n', '   // Update the format from 4D to 5D layout if necessary.\\n', '-  bool allow_5d = rank == 5 && (src_format == \"NHWC\" || src_format == \"NCHW\");\\n', '   if (allow_5d) {\\n', '     std::string src_format_3d = src_format == \"NHWC\" ? \"NDHWC\" : \"NCDHW\";\\n', '     std::string dst_format_3d = dst_format == \"NHWC\" ? \"NDHWC\" : \"NCDHW\";\\n']",
                    "hunk_fix": "@@ -1052,7 +1052,8 @@ Status DefaultLayoutAgnosticOpTransposer::TransposeNode(\n   std::string src_format = context->src_format;\n   std::string dst_format = context->dst_format;\n   // Update the format from 4D to 5D layout if necessary.\n-  bool allow_5d = rank == 5 && (src_format == \"NHWC\" || src_format == \"NCHW\");\n+  bool allow_5d = rank == 5 && (src_format == \"NHWC\" || src_format == \"NCHW\") &&\n+                  (dst_format == \"NHWC\" || dst_format == \"NCHW\");\n   if (allow_5d) {\n     std::string src_format_3d = src_format == \"NHWC\" ? \"NDHWC\" : \"NCDHW\";\n     std::string dst_format_3d = dst_format == \"NHWC\" ? \"NDHWC\" : \"NCDHW\";\n"
                },
                {
                    "old_start": 1229,
                    "old_length": 7,
                    "new_start": 1230,
                    "new_length": 8,
                    "hunk_buggy": "['   std::string src_format = context->src_format;\\n', '   std::string dst_format = context->dst_format;\\n', '   // Update the format from 4D to 5D layout if necessary.\\n', '-  bool allow_5d = rank == 5 && (src_format == \"NHWC\" || src_format == \"NCHW\");\\n', '   if (allow_5d) {\\n', '     std::string src_format_3d = src_format == \"NHWC\" ? \"NDHWC\" : \"NCDHW\";\\n', '     std::string dst_format_3d = dst_format == \"NHWC\" ? \"NDHWC\" : \"NCDHW\";']",
                    "hunk_fix": "@@ -1229,7 +1230,8 @@ Status BinaryOpTransposer::TransposeNode(TransposeContext* context,\n   std::string src_format = context->src_format;\n   std::string dst_format = context->dst_format;\n   // Update the format from 4D to 5D layout if necessary.\n-  bool allow_5d = rank == 5 && (src_format == \"NHWC\" || src_format == \"NCHW\");\n+  bool allow_5d = rank == 5 && (src_format == \"NHWC\" || src_format == \"NCHW\") &&\n+                  (dst_format == \"NHWC\" || dst_format == \"NCHW\");\n   if (allow_5d) {\n     std::string src_format_3d = src_format == \"NHWC\" ? \"NDHWC\" : \"NCDHW\";\n     std::string dst_format_3d = dst_format == \"NHWC\" ? \"NDHWC\" : \"NCDHW\";"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 187,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/9b947dd6377c022091c8aa005cdcff52c53ff5f0",
    "date": "2020-09-23T12:04:10-07:00",
    "message": "Also check dst_format",
    "changes": [
        {
            "name": "generic_layout_optimizer_transposer.cc",
            "path": "tensorflow/core/grappler/optimizers/generic_layout_optimizer_transposer.cc",
            "patches": [
                {
                    "old_start": 1379,
                    "old_length": 7,
                    "new_start": 1379,
                    "new_length": 8,
                    "hunk_buggy": "['   std::string src_format = context->src_format;\\n', '   std::string dst_format = context->dst_format;\\n', '   // Update the format from 4D to 5D layout if necessary.\\n', '-  bool allow_5d = rank == 5 && (src_format == \"NHWC\" || src_format == \"NCHW\");\\n', '   if (allow_5d) {\\n', '     std::string src_format_3d = src_format == \"NHWC\" ? \"NDHWC\" : \"NCDHW\";\\n', '     std::string dst_format_3d = dst_format == \"NHWC\" ? \"NDHWC\" : \"NCDHW\";']",
                    "hunk_fix": "@@ -1379,7 +1379,8 @@ Status ReduceTransposer::TransposeNode(TransposeContext* context,\n   std::string src_format = context->src_format;\n   std::string dst_format = context->dst_format;\n   // Update the format from 4D to 5D layout if necessary.\n-  bool allow_5d = rank == 5 && (src_format == \"NHWC\" || src_format == \"NCHW\");\n+  bool allow_5d = rank == 5 && (src_format == \"NHWC\" || src_format == \"NCHW\") &&\n+                  (dst_format == \"NHWC\" || dst_format == \"NCHW\");\n   if (allow_5d) {\n     std::string src_format_3d = src_format == \"NHWC\" ? \"NDHWC\" : \"NCDHW\";\n     std::string dst_format_3d = dst_format == \"NHWC\" ? \"NDHWC\" : \"NCDHW\";"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 188,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/ce589223a5fa78cb12efaf1efd1d8d0e5507bd08",
    "date": "2020-08-13T21:34:20+05:30",
    "message": "Update nn_ops.py\n\nAdded check for pooling_ratio",
    "changes": [
        {
            "name": "nn_ops.py",
            "path": "tensorflow/python/ops/nn_ops.py",
            "patches": [
                {
                    "old_start": 5334,
                    "old_length": 6,
                    "new_start": 5334,
                    "new_length": 8,
                    "hunk_buggy": "['       [Graham, 2015](https://arxiv.org/abs/1412.6071)\\n', '       ([pdf](https://arxiv.org/pdf/1412.6071.pdf))\\n', '   \"\"\"\\n', '   pooling_ratio = _get_sequence(pooling_ratio, 2, 3, \"pooling_ratio\")\\n', ' \\n', '   if seed == 0:']",
                    "hunk_fix": "@@ -5334,6 +5334,8 @@ def fractional_max_pool_v2(value,\n       [Graham, 2015](https://arxiv.org/abs/1412.6071)\n       ([pdf](https://arxiv.org/pdf/1412.6071.pdf))\n   \"\"\"\n+  if pooling_ratio < 1.0:\n+    raise ValueError(\"pooling_ratio should be >= 1.0.\")\n   pooling_ratio = _get_sequence(pooling_ratio, 2, 3, \"pooling_ratio\")\n \n   if seed == 0:"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 189,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/cddca76312f5ae4fb92a101e79eeff6d5ac16932",
    "date": "2020-08-03T11:41:16-07:00",
    "message": "Add check for reading input tensors at an index that is out of range.\n\nPiperOrigin-RevId: 324646398\nChange-Id: I602b23b2f28504c20a6d099874cdba2ddbf5ca83",
    "changes": [
        {
            "name": "object_reader.h",
            "path": "tensorflow/lite/delegates/gpu/common/object_reader.h",
            "patches": [
                {
                    "old_start": 58,
                    "old_length": 6,
                    "new_start": 58,
                    "new_length": 11,
                    "hunk_buggy": "[' \\n', '   template <typename TensorT>\\n', '   absl::Status ReadTensor(uint32_t idx, TensorT* t) const {\\n', '     const int32_t tensor_idx = node_->inputs->data[idx];\\n', '     if (tensor_idx < 0) {\\n', '       return absl::InvalidArgumentError(']",
                    "hunk_fix": "@@ -58,6 +58,11 @@ class ObjectReader {\n \n   template <typename TensorT>\n   absl::Status ReadTensor(uint32_t idx, TensorT* t) const {\n+    if (idx < 0 || idx >= node_->inputs->size) {\n+      // If larger, this can be an older model with fewer input tensors than the\n+      // current implementation.\n+      return absl::OutOfRangeError(\"Invalid data index found.\");\n+    }\n     const int32_t tensor_idx = node_->inputs->data[idx];\n     if (tensor_idx < 0) {\n       return absl::InvalidArgumentError("
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 190,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/e82a377de614fed51da8a7c5242a90a7967169f2",
    "date": "2020-07-24T20:29:28-07:00",
    "message": "Correct axis check",
    "changes": [
        {
            "name": "tfl_ops.cc",
            "path": "tensorflow/compiler/mlir/lite/ir/tfl_ops.cc",
            "patches": [
                {
                    "old_start": 1028,
                    "old_length": 9,
                    "new_start": 1028,
                    "new_length": 13,
                    "hunk_buggy": "['   // Check axis bounds.\\n', '   if (input_type.hasRank()) {\\n', '     int64_t axis_value = op.axis().getSExtValue();\\n', '-    if (abs(axis_value) > input_type.getRank())\\n', '-      return op.emitOpError(\"op attribute \\'axis\\' is out of bounds, got \")\\n', '-             << axis_value;\\n', '   }\\n', ' \\n', '   // Make sure all inputs have the same shape and element type.']",
                    "hunk_fix": "@@ -1028,9 +1028,13 @@ static LogicalResult Verify(PackOp op) {\n   // Check axis bounds.\n   if (input_type.hasRank()) {\n     int64_t axis_value = op.axis().getSExtValue();\n-    if (abs(axis_value) > input_type.getRank())\n-      return op.emitOpError(\"op attribute 'axis' is out of bounds, got \")\n-             << axis_value;\n+    if (axis_value < 0)\n+      axis_value += input_type.getRank() + 1;\n+    if (axis_value < 0 || axis_value >= input_type.getRank() + 1)\n+      return op.emitOpError()\n+             << \"op attribute 'axis' should be in range [-rank - 1, rank + 1), \"\n+             << \"got rank = \" << input_type.getRank()\n+             << \", and axis = \" << op.axis().getSExtValue();\n   }\n \n   // Make sure all inputs have the same shape and element type."
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 191,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/264eb6ed1dbfb5e078c7dd977da8d7e633106fc5",
    "date": "2020-07-13T11:33:03-07:00",
    "message": "Fixed add bias transformation.\nAdded check for convolution with dynamic weights.\n\nPiperOrigin-RevId: 320996352\nChange-Id: Ie88eb026151c8ce49e9987867bc2807e13176cea",
    "changes": [
        {
            "name": "add_bias.cc",
            "path": "tensorflow/lite/delegates/gpu/common/transformations/add_bias.cc",
            "patches": [
                {
                    "old_start": 48,
                    "old_length": 6,
                    "new_start": 48,
                    "new_length": 11,
                    "hunk_buggy": "['  public:\\n', '   TransformResult ApplyToNode(Node* node, GraphFloat32* graph) final {\\n', '     if (node->operation.type == ToString(OperationType::CONVOLUTION_2D)) {\\n', '       auto& attr =\\n', '           absl::any_cast<Convolution2DAttributes&>(node->operation.attributes);\\n', '       return FillBias(attr.weights.shape.o, &attr.bias);']",
                    "hunk_fix": "@@ -48,6 +48,11 @@ class AddBias : public NodeTransformation {\n  public:\n   TransformResult ApplyToNode(Node* node, GraphFloat32* graph) final {\n     if (node->operation.type == ToString(OperationType::CONVOLUTION_2D)) {\n+      if (graph->FindInputs(node->id).size() != 1) {\n+        return {TransformStatus::DECLINED,\n+                \"This transformation is only applicable to conv with one \"\n+                \"runtime input.\"};\n+      }\n       auto& attr =\n           absl::any_cast<Convolution2DAttributes&>(node->operation.attributes);\n       return FillBias(attr.weights.shape.o, &attr.bias);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 192,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/26cd260fac5fa98ade11ff2a5ec38ede65631cc0",
    "date": "2020-06-30T12:48:22-07:00",
    "message": "Add additional data validation while saving and restoring iterators.\n\nPiperOrigin-RevId: 319078544\nChange-Id: I4a439934e1ba35d5eab38513cae735372d62c8d6",
    "changes": [
        {
            "name": "iterator_ops.cc",
            "path": "tensorflow/core/kernels/data/iterator_ops.cc",
            "patches": [
                {
                    "old_start": 331,
                    "old_length": 6,
                    "new_start": 331,
                    "new_length": 12,
                    "hunk_buggy": "['     data.reserve(num_tensors);\\n', '     for (int i = 0; i < num_tensors; ++i) {\\n', '       auto* w = serialized_vec(i).get<IteratorStateVariant>();\\n', '       data.push_back(w->GetData());\\n', '     }\\n', '     reader_ = absl::make_unique<VariantTensorDataReader>(data);\\n']",
                    "hunk_fix": "@@ -331,6 +331,12 @@ class IteratorVariantSerializer {\n     data.reserve(num_tensors);\n     for (int i = 0; i < num_tensors; ++i) {\n       auto* w = serialized_vec(i).get<IteratorStateVariant>();\n+      if (!w) {\n+        return errors::Internal(\n+            \"Cannot initialize an iterator from tensor \",\n+            serialized_vec(i).DebugString(),\n+            \". Expected a variant tensor of type IteratorStateVariant\");\n+      }\n       data.push_back(w->GetData());\n     }\n     reader_ = absl::make_unique<VariantTensorDataReader>(data);\n"
                },
                {
                    "old_start": 349,
                    "old_length": 6,
                    "new_start": 355,
                    "new_length": 10,
                    "hunk_buggy": "['     }\\n', '     int64 size = variants_.size();\\n', '     for (int64 i = 0; i < size; ++i) {\\n', '       serialized->vec<Variant>()(i) = variants_[i];\\n', '     }\\n', '     return Status::OK();']",
                    "hunk_fix": "@@ -349,6 +355,10 @@ class IteratorVariantSerializer {\n     }\n     int64 size = variants_.size();\n     for (int64 i = 0; i < size; ++i) {\n+      if (variants_[i].GetData() == nullptr) {\n+        return errors::Internal(\n+            \"Cannot serialize an empty IteratorStateVariant\");\n+      }\n       serialized->vec<Variant>()(i) = variants_[i];\n     }\n     return Status::OK();"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 193,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/a17858f3cc5e7ab4ebc2c166d71e7f85b2dad05d",
    "date": "2020-06-24T09:54:27-07:00",
    "message": "Avoid undefined behavior by checking for null Operation in TF_Input/TF_Output\n\nPiperOrigin-RevId: 318082756\nChange-Id: I5b5e9bc716cf159f22a4b89083e00efb93d9fecb",
    "changes": [
        {
            "name": "c_api_function.cc",
            "path": "tensorflow/c/c_api_function.cc",
            "patches": [
                {
                    "old_start": 54,
                    "old_length": 7,
                    "new_start": 54,
                    "new_length": 7,
                    "hunk_buggy": "['     TF_EXCLUSIVE_LOCKS_REQUIRED(fn_body->mu) {\\n', '   input_tensors->reserve(ninputs);\\n', '   for (int i = 0; i < ninputs; ++i) {\\n', '-    Node* node = &inputs[i].oper->node;\\n', '     int idx = inputs[i].index;\\n', ' \\n', '     TF_RETURN_WITH_CONTEXT_IF_ERROR(\\n']",
                    "hunk_fix": "@@ -54,7 +54,7 @@ Status ProcessInputs(\n     TF_EXCLUSIVE_LOCKS_REQUIRED(fn_body->mu) {\n   input_tensors->reserve(ninputs);\n   for (int i = 0; i < ninputs; ++i) {\n-    Node* node = &inputs[i].oper->node;\n+    Node* node = inputs[i].oper ? &inputs[i].oper->node : nullptr;\n     int idx = inputs[i].index;\n \n     TF_RETURN_WITH_CONTEXT_IF_ERROR(\n"
                },
                {
                    "old_start": 90,
                    "old_length": 7,
                    "new_start": 90,
                    "new_length": 7,
                    "hunk_buggy": "['     TF_EXCLUSIVE_LOCKS_REQUIRED(fn_body->mu) {\\n', '   output_tensors->reserve(noutputs);\\n', '   for (int i = 0; i < noutputs; ++i) {\\n', '-    Node* node = &outputs[i].oper->node;\\n', '     int idx = outputs[i].index;\\n', '     TF_RETURN_WITH_CONTEXT_IF_ERROR(\\n', '         fn_body->graph.IsValidOutputTensor(node, idx),']",
                    "hunk_fix": "@@ -90,7 +90,7 @@ Status ProcessOutputs(const TF_Graph* fn_body, const char* fn_name,\n     TF_EXCLUSIVE_LOCKS_REQUIRED(fn_body->mu) {\n   output_tensors->reserve(noutputs);\n   for (int i = 0; i < noutputs; ++i) {\n-    Node* node = &outputs[i].oper->node;\n+    Node* node = outputs[i].oper ? &outputs[i].oper->node : nullptr;\n     int idx = outputs[i].index;\n     TF_RETURN_WITH_CONTEXT_IF_ERROR(\n         fn_body->graph.IsValidOutputTensor(node, idx),"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 194,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/1835465ac5a9c823f7187cb0dd5786da9c360838",
    "date": "2020-06-09T14:07:19-07:00",
    "message": "Add error_reporter DCHECK back into SimpleMemoryAllocator.\n\nThis check was removed due to an internal build problem.\n\nPiperOrigin-RevId: 315555154\nChange-Id: I0f211aa284b2d327df52941bfbcd998a1daf9656",
    "changes": [
        {
            "name": "simple_memory_allocator.cc",
            "path": "tensorflow/lite/micro/simple_memory_allocator.cc",
            "patches": [
                {
                    "old_start": 42,
                    "old_length": 6,
                    "new_start": 42,
                    "new_length": 7,
                    "hunk_buggy": "[' /* static */\\n', ' SimpleMemoryAllocator* SimpleMemoryAllocator::Create(\\n', '     ErrorReporter* error_reporter, uint8_t* buffer_head, size_t buffer_size) {\\n', '   TFLITE_DCHECK(buffer_head != nullptr);\\n', '   SimpleMemoryAllocator tmp =\\n', '       SimpleMemoryAllocator(error_reporter, buffer_head, buffer_size);']",
                    "hunk_fix": "@@ -42,6 +42,7 @@ SimpleMemoryAllocator::SimpleMemoryAllocator(ErrorReporter* error_reporter,\n /* static */\n SimpleMemoryAllocator* SimpleMemoryAllocator::Create(\n     ErrorReporter* error_reporter, uint8_t* buffer_head, size_t buffer_size) {\n+  TFLITE_DCHECK(error_reporter != nullptr);\n   TFLITE_DCHECK(buffer_head != nullptr);\n   SimpleMemoryAllocator tmp =\n       SimpleMemoryAllocator(error_reporter, buffer_head, buffer_size);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 195,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/b68b869e75916e6de37c2ca23a93643faf333011",
    "date": "2020-06-09T13:59:23-07:00",
    "message": "Fix invalid keras tensor isinstance check\n\nPiperOrigin-RevId: 315553346\nChange-Id: I120234e58cb0fb9dce007e7739639519719a9764",
    "changes": [
        {
            "name": "input_layer.py",
            "path": "tensorflow/python/keras/engine/input_layer.py",
            "patches": [
                {
                    "old_start": 164,
                    "old_length": 7,
                    "new_start": 164,
                    "new_length": 7,
                    "hunk_buggy": "['     else:\\n', '       raise_eager_tensor_error = False\\n', '       if keras_tensor.keras_tensors_enabled():\\n', '-        if not isinstance(input_tensor, keras_tensor.keras_tensors_enabled()):\\n', '           raise_eager_tensor_error = True\\n', '       else:\\n', '         if not tf_utils.is_symbolic_tensor(input_tensor):']",
                    "hunk_fix": "@@ -164,7 +164,7 @@ class InputLayer(base_layer.Layer):\n     else:\n       raise_eager_tensor_error = False\n       if keras_tensor.keras_tensors_enabled():\n-        if not isinstance(input_tensor, keras_tensor.keras_tensors_enabled()):\n+        if not isinstance(input_tensor, keras_tensor.KerasTensor):\n           raise_eager_tensor_error = True\n       else:\n         if not tf_utils.is_symbolic_tensor(input_tensor):"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 196,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/0317f64491ba42376d96b157983a02d8b31b679e",
    "date": "2020-06-05T23:02:56-07:00",
    "message": "Update RNNCell._rnn_get_variable to use Variable._trainable in TF2 mode.\n\nWhen using a legacy RNNCell in TF2 mode within a tf.function the \"var in trainable_variables()\" check led to treating a tf.bool tensor as a Python bool. This change makes use within a tf.function use the same logic that is used in Eager mode.\n\nPiperOrigin-RevId: 315052160\nChange-Id: I2fb89690d780e9823ab7e399d8e5cd6fe41d9300",
    "changes": [
        {
            "name": "rnn_cell_impl.py",
            "path": "tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py",
            "patches": [
                {
                    "old_start": 245,
                    "old_length": 8,
                    "new_start": 245,
                    "new_length": 8,
                    "hunk_buggy": "[' \\n', '   def _rnn_get_variable(self, getter, *args, **kwargs):\\n', '     variable = getter(*args, **kwargs)\\n', '-    if context.executing_eagerly():\\n', '-      trainable = variable._trainable  # pylint: disable=protected-access\\n', '     else:\\n', '       trainable = (\\n', '           variable in tf_variables.trainable_variables() or']",
                    "hunk_fix": "@@ -245,8 +245,8 @@ class RNNCell(base_layer.Layer):\n \n   def _rnn_get_variable(self, getter, *args, **kwargs):\n     variable = getter(*args, **kwargs)\n-    if context.executing_eagerly():\n-      trainable = variable._trainable  # pylint: disable=protected-access\n+    if ops.executing_eagerly_outside_functions():\n+      trainable = variable.trainable\n     else:\n       trainable = (\n           variable in tf_variables.trainable_variables() or"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 197,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/70fd126d3afb8a1d00299c28ab234623d2b88704",
    "date": "2020-06-03T16:35:48-07:00",
    "message": "Merge pull request #32202 from Huawei-MRC-OSI:check-name-types\n\nPiperOrigin-RevId: 314626868\nChange-Id: I58cf3f4f12b7f36b4ae226d3f371da3f429f20e3",
    "changes": [
        {
            "name": "util.py",
            "path": "tensorflow/lite/python/util.py",
            "patches": [
                {
                    "old_start": 117,
                    "old_length": 6,
                    "new_start": 117,
                    "new_length": 12,
                    "hunk_buggy": "['   tensors = []\\n', '   invalid_tensors = []\\n', '   for name in tensor_names:\\n', '     tensor = tensor_name_to_tensor.get(name)\\n', '     if tensor is None:\\n', '       invalid_tensors.append(name)']",
                    "hunk_fix": "@@ -117,6 +117,12 @@ def get_tensors_from_tensor_names(graph, tensor_names):\n   tensors = []\n   invalid_tensors = []\n   for name in tensor_names:\n+    if not isinstance(name, six.string_types):\n+      raise ValueError(\"Invalid type for a tensor name in the provided graph. \"\n+                       \"Expected type for a tensor name is 'str', instead got \"\n+                       \"type '{}' for tensor name '{}'\".format(\n+                           type(name), name))\n+\n     tensor = tensor_name_to_tensor.get(name)\n     if tensor is None:\n       invalid_tensors.append(name)"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 198,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/dedac5053f1ca2d6a7820e330714e50d2d724cee",
    "date": "2020-05-27T21:45:36-07:00",
    "message": "Fix edge case bug in handling FP16 weights in XNNPACK delegate\n\nQuasi-static tensors may become subgraph outputs after partitioning; we need to\nexplicitly exclude them from outputs and treat as static tensors.\n\nPiperOrigin-RevId: 313522428\nChange-Id: I621cb575b52caa59910a281078c3c505e796880f",
    "changes": [
        {
            "name": "xnnpack_delegate.cc",
            "path": "tensorflow/lite/delegates/xnnpack/xnnpack_delegate.cc",
            "patches": [
                {
                    "old_start": 101,
                    "old_length": 9,
                    "new_start": 101,
                    "new_length": 15,
                    "hunk_buggy": "['     const std::unordered_set<int> inputs(\\n', '         &params->input_tensors->data[0],\\n', '         &params->input_tensors->data[params->input_tensors->size]);\\n', '-    const std::unordered_set<int> outputs(\\n', '-        &params->output_tensors->data[0],\\n', '-        &params->output_tensors->data[params->output_tensors->size]);\\n', '     std::unordered_set<int> externals(outputs);\\n', ' \\n', '     TfLiteIntArray* execution_plan;']",
                    "hunk_fix": "@@ -101,9 +101,15 @@ class Subgraph {\n     const std::unordered_set<int> inputs(\n         &params->input_tensors->data[0],\n         &params->input_tensors->data[params->input_tensors->size]);\n-    const std::unordered_set<int> outputs(\n-        &params->output_tensors->data[0],\n-        &params->output_tensors->data[params->output_tensors->size]);\n+    std::unordered_set<int> outputs;\n+    for (int o = 0; o < params->output_tensors->size; o++) {\n+      const int output_tensor_idx = params->output_tensors->data[o];\n+      // Exclude quasi-static tensors which may have become subgraph outputs\n+      // after partitioning.\n+      if (delegate->static_unpacked_data_map_.count(output_tensor_idx) == 0) {\n+        outputs.insert(output_tensor_idx);\n+      }\n+    }\n     std::unordered_set<int> externals(outputs);\n \n     TfLiteIntArray* execution_plan;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 199,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/07898e752cf02518508f193a0be2e451450044bd",
    "date": "2020-05-20T11:31:08-07:00",
    "message": "Provide a more informative error message when the bazel version check fails\n\nCurrently, if the version check fails, the error message is:\n\n```\nsubprocess.CalledProcessError: Command '['bazel', '--batch', '--bazelrc=/dev/null', 'version']' returned non-zero exit status 1.\n```\n\nAfter this patch, it becomes:\n\n```\nError checking bazel version:  ERROR: The project you're trying to build requires Bazel 3.0.0 (specified in /usr/local/google/home/cheshire/code/opensource/docker_tf/tensorflow/.bazelversion), but it wasn't found in /usr/bin.\n\nYou can install the required Bazel version via apt:\n  sudo apt update && sudo apt install bazel-3.0.0\n```\n\nPiperOrigin-RevId: 312520687\nChange-Id: I41523f7defa3db10aa34b6b313d6b65c792b2020",
    "changes": [
        {
            "name": "configure.py",
            "path": "configure.py",
            "patches": [
                {
                    "old_start": 1368,
                    "old_length": 8,
                    "new_start": 1368,
                    "new_length": 13,
                    "hunk_buggy": "['   # environment variables.\\n', '   environ_cp = dict(os.environ)\\n', ' \\n', '-  current_bazel_version = check_bazel_version(_TF_MIN_BAZEL_VERSION,\\n', '-                                              _TF_MAX_BAZEL_VERSION)\\n', '   _TF_CURRENT_BAZEL_VERSION = convert_version_to_int(current_bazel_version)\\n', ' \\n', '   reset_tf_configure_bazelrc()']",
                    "hunk_fix": "@@ -1368,8 +1368,13 @@ def main():\n   # environment variables.\n   environ_cp = dict(os.environ)\n \n-  current_bazel_version = check_bazel_version(_TF_MIN_BAZEL_VERSION,\n-                                              _TF_MAX_BAZEL_VERSION)\n+  try:\n+    current_bazel_version = check_bazel_version(_TF_MIN_BAZEL_VERSION,\n+                                                _TF_MAX_BAZEL_VERSION)\n+  except subprocess.CalledProcessError as e:\n+    print(\"Error checking bazel version: \", e.output.decode('UTF-8').strip())\n+    raise e\n+\n   _TF_CURRENT_BAZEL_VERSION = convert_version_to_int(current_bazel_version)\n \n   reset_tf_configure_bazelrc()"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 200,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/db9b247cd1f3ff046359f7b64ca60c2d697fe2e1",
    "date": "2020-05-13T19:56:36-07:00",
    "message": "Fix the functional model loading with nested sequential model.\n\nThe nested sequential model is created with _is_graph_network = False, the current instance check is not strong enough.\n\nPiperOrigin-RevId: 311454248\nChange-Id: I3b36cc037474587c134eab567d42694129c5cf52",
    "changes": [
        {
            "name": "functional.py",
            "path": "tensorflow/python/keras/engine/functional.py",
            "patches": [
                {
                    "old_start": 1017,
                    "old_length": 7,
                    "new_start": 1017,
                    "new_length": 9,
                    "hunk_buggy": "[' def _should_skip_first_node(layer):\\n', '   \"\"\"Returns True if the first layer node should not be saved or loaded.\"\"\"\\n', '   # Networks start with a pre-existing node linking their input to output.\\n', '-  return isinstance(layer, Functional)\\n', ' \\n', ' \\n', ' def _deserialize_keras_tensors(kwargs, layer_map):']",
                    "hunk_fix": "@@ -1017,7 +1017,9 @@ def _map_subgraph_network(inputs, outputs):\n def _should_skip_first_node(layer):\n   \"\"\"Returns True if the first layer node should not be saved or loaded.\"\"\"\n   # Networks start with a pre-existing node linking their input to output.\n-  return isinstance(layer, Functional)\n+  # For a sequential model, it is first created with _is_graph_network = False,\n+  # we have to keep the _is_graph_network check here.\n+  return isinstance(layer, Functional) and layer._is_graph_network\n \n \n def _deserialize_keras_tensors(kwargs, layer_map):"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 201,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/0e3574d39c66d937fa9f9d2e25554aab0066f250",
    "date": "2020-04-22T08:33:35-07:00",
    "message": "Add rank check to Sub op delegation to NNAPI\n\nPiperOrigin-RevId: 307821863\nChange-Id: Ib98448d67e9948576e6c9fb43a98d364ab434e37",
    "changes": [
        {
            "name": "nnapi_delegate.cc",
            "path": "tensorflow/lite/delegates/nnapi/nnapi_delegate.cc",
            "patches": [
                {
                    "old_start": 1799,
                    "old_length": 7,
                    "new_start": 1799,
                    "new_length": 7,
                    "hunk_buggy": "['              \" NNAPI only support float tanh.\", &val_ctx);\\n', '     } break;\\n', '     case kTfLiteBuiltinSub: {\\n', '-      ExpectMaxOpVersion(version, 2, &val_ctx);\\n', '       const TfLiteType input_type =\\n', '           context->tensors[node->inputs->data[0]].type;\\n', '       Expect((android_sdk_version >= kMinSdkVersionForNNAPI11 &&\\n']",
                    "hunk_fix": "@@ -1799,7 +1799,7 @@ bool NNAPIDelegateKernel::Validate(\n              \" NNAPI only support float tanh.\", &val_ctx);\n     } break;\n     case kTfLiteBuiltinSub: {\n-      ExpectMaxOpVersion(version, 2, &val_ctx);\n+      ExpectMaxOpVersion(version, 3, &val_ctx);\n       const TfLiteType input_type =\n           context->tensors[node->inputs->data[0]].type;\n       Expect((android_sdk_version >= kMinSdkVersionForNNAPI11 &&\n"
                },
                {
                    "old_start": 1808,
                    "old_length": 6,
                    "new_start": 1808,
                    "new_length": 13,
                    "hunk_buggy": "['                   IsQuantized(input_type)),\\n', '              NNAPIValidationFailureType::kUnsupportedInputType,\\n', '              \"NNAPI only support float sub.\", &val_ctx);\\n', '     } break;\\n', '     case kTfLiteBuiltinDiv: {\\n', '       ExpectOpVersion(version, 1, &val_ctx);\\n']",
                    "hunk_fix": "@@ -1808,6 +1808,13 @@ bool NNAPIDelegateKernel::Validate(\n                   IsQuantized(input_type)),\n              NNAPIValidationFailureType::kUnsupportedInputType,\n              \"NNAPI only support float sub.\", &val_ctx);\n+      const int input0_rank =\n+          context->tensors[node->inputs->data[0]].dims->size;\n+      const int input1_rank =\n+          context->tensors[node->inputs->data[1]].dims->size;\n+      Expect(input0_rank <= 4 && input1_rank <= 4,\n+             NNAPIValidationFailureType::kUnsupportedOperandRank,\n+             \"Input rank must be <= 4\", &val_ctx);\n     } break;\n     case kTfLiteBuiltinDiv: {\n       ExpectOpVersion(version, 1, &val_ctx);\n"
                },
                {
                    "old_start": 2327,
                    "old_length": 7,
                    "new_start": 2334,
                    "new_length": 7,
                    "hunk_buggy": "['                            \"Unsupported operation type.\", &val_ctx);\\n', '   }\\n', '   return val_ctx.is_valid;\\n', '-}\\n', ' \\n', ' TfLiteStatus NNAPIDelegateKernel::Map(\\n', '     TfLiteContext* context, int builtin_code, int version,']",
                    "hunk_fix": "@@ -2327,7 +2334,7 @@ bool NNAPIDelegateKernel::Validate(\n                            \"Unsupported operation type.\", &val_ctx);\n   }\n   return val_ctx.is_valid;\n-}\n+}  // NOLINT(readability/fn_size)\n \n TfLiteStatus NNAPIDelegateKernel::Map(\n     TfLiteContext* context, int builtin_code, int version,"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 202,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/4a8d8518fba1d70f63633775695f1a5189cd252f",
    "date": "2020-04-15T16:00:49-07:00",
    "message": "Add checks that the Allocate function returned successfully.\n\nPiperOrigin-RevId: 306737078\nChange-Id: I74abe5b6db04ed850e98117d32c6be3aa06c0211",
    "changes": [
        {
            "name": "flatbuffer_conversions.cc",
            "path": "tensorflow/lite/core/api/flatbuffer_conversions.cc",
            "patches": [
                {
                    "old_start": 193,
                    "old_length": 6,
                    "new_start": 193,
                    "new_length": 7,
                    "hunk_buggy": "['   switch (op_type) {\\n', '     case BuiltinOperator_CONV_2D: {\\n', '       auto params = safe_allocator.Allocate<TfLiteConvParams>();\\n', '       if (auto* conv_params = op->builtin_options_as_Conv2DOptions()) {\\n', '         params->padding = parse_padding(conv_params->padding());\\n', '         params->stride_width = conv_params->stride_w();\\n']",
                    "hunk_fix": "@@ -193,6 +193,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n   switch (op_type) {\n     case BuiltinOperator_CONV_2D: {\n       auto params = safe_allocator.Allocate<TfLiteConvParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (auto* conv_params = op->builtin_options_as_Conv2DOptions()) {\n         params->padding = parse_padding(conv_params->padding());\n         params->stride_width = conv_params->stride_w();\n"
                },
                {
                    "old_start": 208,
                    "old_length": 6,
                    "new_start": 209,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_CAST: {\\n', '       auto params = safe_allocator.Allocate<TfLiteCastParams>();\\n', '       if (const auto* schema_params = op->builtin_options_as_CastOptions()) {\\n', '         TF_LITE_ENSURE_STATUS(ConvertTensorType(schema_params->in_data_type(),\\n', '                                                 &params->in_data_type,\\n']",
                    "hunk_fix": "@@ -208,6 +209,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_CAST: {\n       auto params = safe_allocator.Allocate<TfLiteCastParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* schema_params = op->builtin_options_as_CastOptions()) {\n         TF_LITE_ENSURE_STATUS(ConvertTensorType(schema_params->in_data_type(),\n                                                 &params->in_data_type,\n"
                },
                {
                    "old_start": 221,
                    "old_length": 6,
                    "new_start": 223,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_LSH_PROJECTION: {\\n', '       auto params = safe_allocator.Allocate<TfLiteLSHProjectionParams>();\\n', '       if (const auto* lshParams =\\n', '               op->builtin_options_as_LSHProjectionOptions()) {\\n', '         params->type = parseLSHProjectionType(lshParams->type());\\n']",
                    "hunk_fix": "@@ -221,6 +223,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_LSH_PROJECTION: {\n       auto params = safe_allocator.Allocate<TfLiteLSHProjectionParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* lshParams =\n               op->builtin_options_as_LSHProjectionOptions()) {\n         params->type = parseLSHProjectionType(lshParams->type());\n"
                },
                {
                    "old_start": 232,
                    "old_length": 6,
                    "new_start": 235,
                    "new_length": 7,
                    "hunk_buggy": "['     case BuiltinOperator_MAX_POOL_2D:\\n', '     case BuiltinOperator_L2_POOL_2D: {\\n', '       auto params = safe_allocator.Allocate<TfLitePoolParams>();\\n', '       if (const auto* pool_params = op->builtin_options_as_Pool2DOptions()) {\\n', '         params->padding = parse_padding(pool_params->padding());\\n', '         params->stride_width = pool_params->stride_w();\\n']",
                    "hunk_fix": "@@ -232,6 +235,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     case BuiltinOperator_MAX_POOL_2D:\n     case BuiltinOperator_L2_POOL_2D: {\n       auto params = safe_allocator.Allocate<TfLitePoolParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* pool_params = op->builtin_options_as_Pool2DOptions()) {\n         params->padding = parse_padding(pool_params->padding());\n         params->stride_width = pool_params->stride_w();\n"
                },
                {
                    "old_start": 246,
                    "old_length": 6,
                    "new_start": 250,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_DEPTHWISE_CONV_2D: {\\n', '       auto params = safe_allocator.Allocate<TfLiteDepthwiseConvParams>();\\n', '       if (const auto* conv_params =\\n', '               op->builtin_options_as_DepthwiseConv2DOptions()) {\\n', '         params->padding = parse_padding(conv_params->padding());\\n']",
                    "hunk_fix": "@@ -246,6 +250,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_DEPTHWISE_CONV_2D: {\n       auto params = safe_allocator.Allocate<TfLiteDepthwiseConvParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* conv_params =\n               op->builtin_options_as_DepthwiseConv2DOptions()) {\n         params->padding = parse_padding(conv_params->padding());\n"
                },
                {
                    "old_start": 263,
                    "old_length": 6,
                    "new_start": 268,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_SVDF: {\\n', '       auto params = safe_allocator.Allocate<TfLiteSVDFParams>();\\n', '       if (const auto* svdf_params = op->builtin_options_as_SVDFOptions()) {\\n', '         params->rank = svdf_params->rank();\\n', '         params->activation =\\n']",
                    "hunk_fix": "@@ -263,6 +268,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_SVDF: {\n       auto params = safe_allocator.Allocate<TfLiteSVDFParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* svdf_params = op->builtin_options_as_SVDFOptions()) {\n         params->rank = svdf_params->rank();\n         params->activation =\n"
                },
                {
                    "old_start": 275,
                    "old_length": 6,
                    "new_start": 281,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_UNIDIRECTIONAL_SEQUENCE_RNN: {\\n', '       auto params = safe_allocator.Allocate<TfLiteSequenceRNNParams>();\\n', '       if (const auto* sequence_rnn_params =\\n', '               op->builtin_options_as_SequenceRNNOptions()) {\\n', '         params->activation =\\n']",
                    "hunk_fix": "@@ -275,6 +281,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_UNIDIRECTIONAL_SEQUENCE_RNN: {\n       auto params = safe_allocator.Allocate<TfLiteSequenceRNNParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* sequence_rnn_params =\n               op->builtin_options_as_SequenceRNNOptions()) {\n         params->activation =\n"
                },
                {
                    "old_start": 289,
                    "old_length": 6,
                    "new_start": 296,
                    "new_length": 7,
                    "hunk_buggy": "['     case BuiltinOperator_BIDIRECTIONAL_SEQUENCE_RNN: {\\n', '       auto params =\\n', '           safe_allocator.Allocate<TfLiteBidirectionalSequenceRNNParams>();\\n', '       if (const auto* bidi_sequence_rnn_params =\\n', '               op->builtin_options_as_BidirectionalSequenceRNNOptions()) {\\n', '         params->activation = parse_activation(\\n']",
                    "hunk_fix": "@@ -289,6 +296,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     case BuiltinOperator_BIDIRECTIONAL_SEQUENCE_RNN: {\n       auto params =\n           safe_allocator.Allocate<TfLiteBidirectionalSequenceRNNParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* bidi_sequence_rnn_params =\n               op->builtin_options_as_BidirectionalSequenceRNNOptions()) {\n         params->activation = parse_activation(\n"
                },
                {
                    "old_start": 303,
                    "old_length": 6,
                    "new_start": 311,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_RNN: {\\n', '       auto params = safe_allocator.Allocate<TfLiteRNNParams>();\\n', '       if (const auto* rnn_params = op->builtin_options_as_RNNOptions()) {\\n', '         params->activation =\\n', '             parse_activation(rnn_params->fused_activation_function());\\n']",
                    "hunk_fix": "@@ -303,6 +311,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_RNN: {\n       auto params = safe_allocator.Allocate<TfLiteRNNParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* rnn_params = op->builtin_options_as_RNNOptions()) {\n         params->activation =\n             parse_activation(rnn_params->fused_activation_function());\n"
                },
                {
                    "old_start": 315,
                    "old_length": 6,
                    "new_start": 324,
                    "new_length": 7,
                    "hunk_buggy": "['     case BuiltinOperator_EMBEDDING_LOOKUP_SPARSE: {\\n', '       auto params =\\n', '           safe_allocator.Allocate<TfLiteEmbeddingLookupSparseParams>();\\n', '       if (const auto* embedding_params =\\n', '               op->builtin_options_as_EmbeddingLookupSparseOptions()) {\\n', '         params->combiner = parseCombinerType(embedding_params->combiner());\\n']",
                    "hunk_fix": "@@ -315,6 +324,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     case BuiltinOperator_EMBEDDING_LOOKUP_SPARSE: {\n       auto params =\n           safe_allocator.Allocate<TfLiteEmbeddingLookupSparseParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* embedding_params =\n               op->builtin_options_as_EmbeddingLookupSparseOptions()) {\n         params->combiner = parseCombinerType(embedding_params->combiner());\n"
                },
                {
                    "old_start": 324,
                    "old_length": 6,
                    "new_start": 334,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_FULLY_CONNECTED: {\\n', '       auto params = safe_allocator.Allocate<TfLiteFullyConnectedParams>();\\n', '       if (const auto* fully_connected_params =\\n', '               op->builtin_options_as_FullyConnectedOptions()) {\\n', '         params->activation = parse_activation(\\n']",
                    "hunk_fix": "@@ -324,6 +334,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_FULLY_CONNECTED: {\n       auto params = safe_allocator.Allocate<TfLiteFullyConnectedParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* fully_connected_params =\n               op->builtin_options_as_FullyConnectedOptions()) {\n         params->activation = parse_activation(\n"
                },
                {
                    "old_start": 353,
                    "old_length": 6,
                    "new_start": 364,
                    "new_length": 7,
                    "hunk_buggy": "['       return kTfLiteOk;\\n', '     case BuiltinOperator_SOFTMAX: {\\n', '       auto params = safe_allocator.Allocate<TfLiteSoftmaxParams>();\\n', '       if (const auto* softmax_params =\\n', '               op->builtin_options_as_SoftmaxOptions()) {\\n', '         params->beta = softmax_params->beta();\\n']",
                    "hunk_fix": "@@ -353,6 +364,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n       return kTfLiteOk;\n     case BuiltinOperator_SOFTMAX: {\n       auto params = safe_allocator.Allocate<TfLiteSoftmaxParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* softmax_params =\n               op->builtin_options_as_SoftmaxOptions()) {\n         params->beta = softmax_params->beta();\n"
                },
                {
                    "old_start": 362,
                    "old_length": 6,
                    "new_start": 374,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_CONCATENATION: {\\n', '       auto params = safe_allocator.Allocate<TfLiteConcatenationParams>();\\n', '       if (const auto* concatenation_params =\\n', '               op->builtin_options_as_ConcatenationOptions()) {\\n', '         params->activation =\\n']",
                    "hunk_fix": "@@ -362,6 +374,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_CONCATENATION: {\n       auto params = safe_allocator.Allocate<TfLiteConcatenationParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* concatenation_params =\n               op->builtin_options_as_ConcatenationOptions()) {\n         params->activation =\n"
                },
                {
                    "old_start": 373,
                    "old_length": 6,
                    "new_start": 386,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_MUL: {\\n', '       auto params = safe_allocator.Allocate<TfLiteMulParams>();\\n', '       if (const auto* schema_params = op->builtin_options_as_MulOptions()) {\\n', '         params->activation =\\n', '             parse_activation(schema_params->fused_activation_function());\\n']",
                    "hunk_fix": "@@ -373,6 +386,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_MUL: {\n       auto params = safe_allocator.Allocate<TfLiteMulParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* schema_params = op->builtin_options_as_MulOptions()) {\n         params->activation =\n             parse_activation(schema_params->fused_activation_function());\n"
                },
                {
                    "old_start": 382,
                    "old_length": 6,
                    "new_start": 396,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_ADD: {\\n', '       auto params = safe_allocator.Allocate<TfLiteAddParams>();\\n', '       if (const auto* schema_params = op->builtin_options_as_AddOptions()) {\\n', '         params->activation =\\n', '             parse_activation(schema_params->fused_activation_function());\\n']",
                    "hunk_fix": "@@ -382,6 +396,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_ADD: {\n       auto params = safe_allocator.Allocate<TfLiteAddParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* schema_params = op->builtin_options_as_AddOptions()) {\n         params->activation =\n             parse_activation(schema_params->fused_activation_function());\n"
                },
                {
                    "old_start": 391,
                    "old_length": 6,
                    "new_start": 406,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_DIV: {\\n', '       auto params = safe_allocator.Allocate<TfLiteDivParams>();\\n', '       if (const auto* schema_params = op->builtin_options_as_DivOptions()) {\\n', '         params->activation =\\n', '             parse_activation(schema_params->fused_activation_function());\\n']",
                    "hunk_fix": "@@ -391,6 +406,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_DIV: {\n       auto params = safe_allocator.Allocate<TfLiteDivParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* schema_params = op->builtin_options_as_DivOptions()) {\n         params->activation =\n             parse_activation(schema_params->fused_activation_function());\n"
                },
                {
                    "old_start": 400,
                    "old_length": 6,
                    "new_start": 416,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_SUB: {\\n', '       auto params = safe_allocator.Allocate<TfLiteSubParams>();\\n', '       if (const auto* schema_params = op->builtin_options_as_SubOptions()) {\\n', '         params->activation =\\n', '             parse_activation(schema_params->fused_activation_function());\\n']",
                    "hunk_fix": "@@ -400,6 +416,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_SUB: {\n       auto params = safe_allocator.Allocate<TfLiteSubParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* schema_params = op->builtin_options_as_SubOptions()) {\n         params->activation =\n             parse_activation(schema_params->fused_activation_function());\n"
                },
                {
                    "old_start": 409,
                    "old_length": 6,
                    "new_start": 426,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_L2_NORMALIZATION: {\\n', '       auto params = safe_allocator.Allocate<TfLiteL2NormParams>();\\n', '       if (const auto* schema_params = op->builtin_options_as_L2NormOptions()) {\\n', '         params->activation =\\n', '             parse_activation(schema_params->fused_activation_function());\\n']",
                    "hunk_fix": "@@ -409,6 +426,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_L2_NORMALIZATION: {\n       auto params = safe_allocator.Allocate<TfLiteL2NormParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* schema_params = op->builtin_options_as_L2NormOptions()) {\n         params->activation =\n             parse_activation(schema_params->fused_activation_function());\n"
                },
                {
                    "old_start": 418,
                    "old_length": 6,
                    "new_start": 436,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_LOCAL_RESPONSE_NORMALIZATION: {\\n', '       auto params = safe_allocator.Allocate<TfLiteLocalResponseNormParams>();\\n', '       if (const auto* schema_params =\\n', '               op->builtin_options_as_LocalResponseNormalizationOptions()) {\\n', '         params->radius = schema_params->radius();\\n']",
                    "hunk_fix": "@@ -418,6 +436,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_LOCAL_RESPONSE_NORMALIZATION: {\n       auto params = safe_allocator.Allocate<TfLiteLocalResponseNormParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* schema_params =\n               op->builtin_options_as_LocalResponseNormalizationOptions()) {\n         params->radius = schema_params->radius();\n"
                },
                {
                    "old_start": 430,
                    "old_length": 6,
                    "new_start": 449,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_LSTM: {\\n', '       auto params = safe_allocator.Allocate<TfLiteLSTMParams>();\\n', '       if (const auto* lstm_params = op->builtin_options_as_LSTMOptions()) {\\n', '         params->activation =\\n', '             parse_activation(lstm_params->fused_activation_function());\\n']",
                    "hunk_fix": "@@ -430,6 +449,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_LSTM: {\n       auto params = safe_allocator.Allocate<TfLiteLSTMParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* lstm_params = op->builtin_options_as_LSTMOptions()) {\n         params->activation =\n             parse_activation(lstm_params->fused_activation_function());\n"
                },
                {
                    "old_start": 461,
                    "old_length": 6,
                    "new_start": 481,
                    "new_length": 7,
                    "hunk_buggy": "['     case BuiltinOperator_UNIDIRECTIONAL_SEQUENCE_LSTM: {\\n', '       auto params =\\n', '           safe_allocator.Allocate<TfLiteUnidirectionalSequenceLSTMParams>();\\n', '       if (const auto* seq_lstm_params =\\n', '               op->builtin_options_as_UnidirectionalSequenceLSTMOptions()) {\\n', '         params->activation =\\n']",
                    "hunk_fix": "@@ -461,6 +481,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     case BuiltinOperator_UNIDIRECTIONAL_SEQUENCE_LSTM: {\n       auto params =\n           safe_allocator.Allocate<TfLiteUnidirectionalSequenceLSTMParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* seq_lstm_params =\n               op->builtin_options_as_UnidirectionalSequenceLSTMOptions()) {\n         params->activation =\n"
                },
                {
                    "old_start": 477,
                    "old_length": 6,
                    "new_start": 498,
                    "new_length": 7,
                    "hunk_buggy": "['     case BuiltinOperator_BIDIRECTIONAL_SEQUENCE_LSTM: {\\n', '       auto params =\\n', '           safe_allocator.Allocate<TfLiteBidirectionalSequenceLSTMParams>();\\n', '       if (const auto* bidi_lstm_params =\\n', '               op->builtin_options_as_BidirectionalSequenceLSTMOptions()) {\\n', '         params->activation =\\n']",
                    "hunk_fix": "@@ -477,6 +498,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     case BuiltinOperator_BIDIRECTIONAL_SEQUENCE_LSTM: {\n       auto params =\n           safe_allocator.Allocate<TfLiteBidirectionalSequenceLSTMParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* bidi_lstm_params =\n               op->builtin_options_as_BidirectionalSequenceLSTMOptions()) {\n         params->activation =\n"
                },
                {
                    "old_start": 493,
                    "old_length": 6,
                    "new_start": 515,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_RESIZE_BILINEAR: {\\n', '       auto params = safe_allocator.Allocate<TfLiteResizeBilinearParams>();\\n', '       if (const auto* schema_params =\\n', '               op->builtin_options_as_ResizeBilinearOptions()) {\\n', '         params->align_corners = schema_params->align_corners();\\n']",
                    "hunk_fix": "@@ -493,6 +515,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_RESIZE_BILINEAR: {\n       auto params = safe_allocator.Allocate<TfLiteResizeBilinearParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* schema_params =\n               op->builtin_options_as_ResizeBilinearOptions()) {\n         params->align_corners = schema_params->align_corners();\n"
                },
                {
                    "old_start": 509,
                    "old_length": 6,
                    "new_start": 532,
                    "new_length": 7,
                    "hunk_buggy": "['     case BuiltinOperator_RESIZE_NEAREST_NEIGHBOR: {\\n', '       auto params =\\n', '           safe_allocator.Allocate<TfLiteResizeNearestNeighborParams>();\\n', '       if (const auto* schema_params =\\n', '               op->builtin_options_as_ResizeNearestNeighborOptions()) {\\n', '         params->align_corners = schema_params->align_corners();\\n']",
                    "hunk_fix": "@@ -509,6 +532,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     case BuiltinOperator_RESIZE_NEAREST_NEIGHBOR: {\n       auto params =\n           safe_allocator.Allocate<TfLiteResizeNearestNeighborParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* schema_params =\n               op->builtin_options_as_ResizeNearestNeighborOptions()) {\n         params->align_corners = schema_params->align_corners();\n"
                },
                {
                    "old_start": 518,
                    "old_length": 6,
                    "new_start": 542,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_RESHAPE: {\\n', '       auto params = safe_allocator.Allocate<TfLiteReshapeParams>();\\n', '       if (const auto* schema_params = op->builtin_options_as_ReshapeOptions()) {\\n', '         auto* new_shape = schema_params->new_shape();\\n', '         // TODO(b/147203660): We need to figure out when dynamic reshape\\n']",
                    "hunk_fix": "@@ -518,6 +542,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_RESHAPE: {\n       auto params = safe_allocator.Allocate<TfLiteReshapeParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* schema_params = op->builtin_options_as_ReshapeOptions()) {\n         auto* new_shape = schema_params->new_shape();\n         // TODO(b/147203660): We need to figure out when dynamic reshape\n"
                },
                {
                    "old_start": 535,
                    "old_length": 6,
                    "new_start": 560,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_SKIP_GRAM: {\\n', '       auto params = safe_allocator.Allocate<TfLiteSkipGramParams>();\\n', '       if (const auto* skip_gram_params =\\n', '               op->builtin_options_as_SkipGramOptions()) {\\n', '         params->ngram_size = skip_gram_params->ngram_size();\\n']",
                    "hunk_fix": "@@ -535,6 +560,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_SKIP_GRAM: {\n       auto params = safe_allocator.Allocate<TfLiteSkipGramParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* skip_gram_params =\n               op->builtin_options_as_SkipGramOptions()) {\n         params->ngram_size = skip_gram_params->ngram_size();\n"
                },
                {
                    "old_start": 546,
                    "old_length": 6,
                    "new_start": 572,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_SPACE_TO_DEPTH: {\\n', '       auto params = safe_allocator.Allocate<TfLiteSpaceToDepthParams>();\\n', '       if (const auto* schema_params =\\n', '               op->builtin_options_as_SpaceToDepthOptions()) {\\n', '         params->block_size = schema_params->block_size();\\n']",
                    "hunk_fix": "@@ -546,6 +572,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_SPACE_TO_DEPTH: {\n       auto params = safe_allocator.Allocate<TfLiteSpaceToDepthParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* schema_params =\n               op->builtin_options_as_SpaceToDepthOptions()) {\n         params->block_size = schema_params->block_size();\n"
                },
                {
                    "old_start": 555,
                    "old_length": 6,
                    "new_start": 582,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_DEPTH_TO_SPACE: {\\n', '       auto params = safe_allocator.Allocate<TfLiteDepthToSpaceParams>();\\n', '       if (const auto* schema_params =\\n', '               op->builtin_options_as_DepthToSpaceOptions()) {\\n', '         params->block_size = schema_params->block_size();\\n']",
                    "hunk_fix": "@@ -555,6 +582,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_DEPTH_TO_SPACE: {\n       auto params = safe_allocator.Allocate<TfLiteDepthToSpaceParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* schema_params =\n               op->builtin_options_as_DepthToSpaceOptions()) {\n         params->block_size = schema_params->block_size();\n"
                },
                {
                    "old_start": 564,
                    "old_length": 6,
                    "new_start": 592,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_GATHER: {\\n', '       auto params = safe_allocator.Allocate<TfLiteGatherParams>();\\n', '       params->axis = 0;\\n', '       if (const auto* gather_params = op->builtin_options_as_GatherOptions()) {\\n', '         params->axis = gather_params->axis();\\n']",
                    "hunk_fix": "@@ -564,6 +592,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_GATHER: {\n       auto params = safe_allocator.Allocate<TfLiteGatherParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       params->axis = 0;\n       if (const auto* gather_params = op->builtin_options_as_GatherOptions()) {\n         params->axis = gather_params->axis();\n"
                },
                {
                    "old_start": 579,
                    "old_length": 6,
                    "new_start": 608,
                    "new_length": 7,
                    "hunk_buggy": "['     case BuiltinOperator_REDUCE_ANY:\\n', '     case BuiltinOperator_SUM: {\\n', '       auto params = safe_allocator.Allocate<TfLiteReducerParams>();\\n', '       if (const auto* schema_params = op->builtin_options_as_ReducerOptions()) {\\n', '         params->keep_dims = schema_params->keep_dims();\\n', '       }\\n']",
                    "hunk_fix": "@@ -579,6 +608,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     case BuiltinOperator_REDUCE_ANY:\n     case BuiltinOperator_SUM: {\n       auto params = safe_allocator.Allocate<TfLiteReducerParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* schema_params = op->builtin_options_as_ReducerOptions()) {\n         params->keep_dims = schema_params->keep_dims();\n       }\n"
                },
                {
                    "old_start": 587,
                    "old_length": 6,
                    "new_start": 617,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_SPLIT: {\\n', '       auto params = safe_allocator.Allocate<TfLiteSplitParams>();\\n', '       if (const auto* schema_params = op->builtin_options_as_SplitOptions()) {\\n', '         params->num_splits = schema_params->num_splits();\\n', '       }\\n']",
                    "hunk_fix": "@@ -587,6 +617,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_SPLIT: {\n       auto params = safe_allocator.Allocate<TfLiteSplitParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* schema_params = op->builtin_options_as_SplitOptions()) {\n         params->num_splits = schema_params->num_splits();\n       }\n"
                },
                {
                    "old_start": 595,
                    "old_length": 6,
                    "new_start": 626,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_SPLIT_V: {\\n', '       auto params = safe_allocator.Allocate<TfLiteSplitParams>();\\n', '       if (const auto* schema_params = op->builtin_options_as_SplitVOptions()) {\\n', '         params->num_splits = schema_params->num_splits();\\n', '       }\\n']",
                    "hunk_fix": "@@ -595,6 +626,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_SPLIT_V: {\n       auto params = safe_allocator.Allocate<TfLiteSplitParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* schema_params = op->builtin_options_as_SplitVOptions()) {\n         params->num_splits = schema_params->num_splits();\n       }\n"
                },
                {
                    "old_start": 603,
                    "old_length": 6,
                    "new_start": 635,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_SQUEEZE: {\\n', '       auto params = safe_allocator.Allocate<TfLiteSqueezeParams>();\\n', '       if (const auto* schema_params = op->builtin_options_as_SqueezeOptions()) {\\n', '         const auto* squeeze_dims = schema_params->squeeze_dims();\\n', '         TF_LITE_ENSURE_STATUS(FlatBufferIntVectorToArray(\\n']",
                    "hunk_fix": "@@ -603,6 +635,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_SQUEEZE: {\n       auto params = safe_allocator.Allocate<TfLiteSqueezeParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* schema_params = op->builtin_options_as_SqueezeOptions()) {\n         const auto* squeeze_dims = schema_params->squeeze_dims();\n         TF_LITE_ENSURE_STATUS(FlatBufferIntVectorToArray(\n"
                },
                {
                    "old_start": 615,
                    "old_length": 6,
                    "new_start": 648,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_STRIDED_SLICE: {\\n', '       auto params = safe_allocator.Allocate<TfLiteStridedSliceParams>();\\n', '       if (const auto* schema_params =\\n', '               op->builtin_options_as_StridedSliceOptions()) {\\n', '         params->begin_mask = schema_params->begin_mask();\\n']",
                    "hunk_fix": "@@ -615,6 +648,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_STRIDED_SLICE: {\n       auto params = safe_allocator.Allocate<TfLiteStridedSliceParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* schema_params =\n               op->builtin_options_as_StridedSliceOptions()) {\n         params->begin_mask = schema_params->begin_mask();\n"
                },
                {
                    "old_start": 628,
                    "old_length": 6,
                    "new_start": 662,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_ARG_MAX: {\\n', '       auto params = safe_allocator.Allocate<TfLiteArgMaxParams>();\\n', '       if (const auto* schema_params = op->builtin_options_as_ArgMaxOptions()) {\\n', '         TF_LITE_ENSURE_STATUS(ConvertTensorType(schema_params->output_type(),\\n', '                                                 &params->output_type,\\n']",
                    "hunk_fix": "@@ -628,6 +662,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_ARG_MAX: {\n       auto params = safe_allocator.Allocate<TfLiteArgMaxParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* schema_params = op->builtin_options_as_ArgMaxOptions()) {\n         TF_LITE_ENSURE_STATUS(ConvertTensorType(schema_params->output_type(),\n                                                 &params->output_type,\n"
                },
                {
                    "old_start": 638,
                    "old_length": 6,
                    "new_start": 673,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_ARG_MIN: {\\n', '       auto params = safe_allocator.Allocate<TfLiteArgMinParams>();\\n', '       if (const auto* schema_params = op->builtin_options_as_ArgMinOptions()) {\\n', '         TF_LITE_ENSURE_STATUS(ConvertTensorType(schema_params->output_type(),\\n', '                                                 &params->output_type,\\n']",
                    "hunk_fix": "@@ -638,6 +673,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_ARG_MIN: {\n       auto params = safe_allocator.Allocate<TfLiteArgMinParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* schema_params = op->builtin_options_as_ArgMinOptions()) {\n         TF_LITE_ENSURE_STATUS(ConvertTensorType(schema_params->output_type(),\n                                                 &params->output_type,\n"
                },
                {
                    "old_start": 648,
                    "old_length": 6,
                    "new_start": 684,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_TRANSPOSE_CONV: {\\n', '       auto params = safe_allocator.Allocate<TfLiteTransposeConvParams>();\\n', '       if (const auto* transpose_conv_params =\\n', '               op->builtin_options_as_TransposeConvOptions()) {\\n', '         params->padding = parse_padding(transpose_conv_params->padding());\\n']",
                    "hunk_fix": "@@ -648,6 +684,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_TRANSPOSE_CONV: {\n       auto params = safe_allocator.Allocate<TfLiteTransposeConvParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* transpose_conv_params =\n               op->builtin_options_as_TransposeConvOptions()) {\n         params->padding = parse_padding(transpose_conv_params->padding());\n"
                },
                {
                    "old_start": 659,
                    "old_length": 6,
                    "new_start": 696,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_SPARSE_TO_DENSE: {\\n', '       auto params = safe_allocator.Allocate<TfLiteSparseToDenseParams>();\\n', '       if (const auto* sparse_to_dense_params =\\n', '               op->builtin_options_as_SparseToDenseOptions()) {\\n', '         params->validate_indices = sparse_to_dense_params->validate_indices();\\n']",
                    "hunk_fix": "@@ -659,6 +696,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_SPARSE_TO_DENSE: {\n       auto params = safe_allocator.Allocate<TfLiteSparseToDenseParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* sparse_to_dense_params =\n               op->builtin_options_as_SparseToDenseOptions()) {\n         params->validate_indices = sparse_to_dense_params->validate_indices();\n"
                },
                {
                    "old_start": 668,
                    "old_length": 6,
                    "new_start": 706,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_SHAPE: {\\n', '       auto params = safe_allocator.Allocate<TfLiteShapeParams>();\\n', '       if (const auto* schema_params = op->builtin_options_as_ShapeOptions()) {\\n', '         TF_LITE_ENSURE_STATUS(ConvertTensorType(\\n', '             schema_params->out_type(), &params->out_type, error_reporter));\\n']",
                    "hunk_fix": "@@ -668,6 +706,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_SHAPE: {\n       auto params = safe_allocator.Allocate<TfLiteShapeParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* schema_params = op->builtin_options_as_ShapeOptions()) {\n         TF_LITE_ENSURE_STATUS(ConvertTensorType(\n             schema_params->out_type(), &params->out_type, error_reporter));\n"
                },
                {
                    "old_start": 677,
                    "old_length": 6,
                    "new_start": 716,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_PACK: {\\n', '       auto params = safe_allocator.Allocate<TfLitePackParams>();\\n', '       if (const auto* pack_params = op->builtin_options_as_PackOptions()) {\\n', '         params->values_count = pack_params->values_count();\\n', '         params->axis = pack_params->axis();\\n']",
                    "hunk_fix": "@@ -677,6 +716,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_PACK: {\n       auto params = safe_allocator.Allocate<TfLitePackParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* pack_params = op->builtin_options_as_PackOptions()) {\n         params->values_count = pack_params->values_count();\n         params->axis = pack_params->axis();\n"
                },
                {
                    "old_start": 692,
                    "old_length": 6,
                    "new_start": 732,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_FAKE_QUANT: {\\n', '       auto params = safe_allocator.Allocate<TfLiteFakeQuantParams>();\\n', '       if (const auto* schema_params =\\n', '               op->builtin_options_as_FakeQuantOptions()) {\\n', '         params->min = schema_params->min();\\n']",
                    "hunk_fix": "@@ -692,6 +732,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_FAKE_QUANT: {\n       auto params = safe_allocator.Allocate<TfLiteFakeQuantParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* schema_params =\n               op->builtin_options_as_FakeQuantOptions()) {\n         params->min = schema_params->min();\n"
                },
                {
                    "old_start": 704,
                    "old_length": 6,
                    "new_start": 745,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_ONE_HOT: {\\n', '       auto params = safe_allocator.Allocate<TfLiteOneHotParams>();\\n', '       if (const auto* schema_params = op->builtin_options_as_OneHotOptions()) {\\n', '         params->axis = schema_params->axis();\\n', '       }\\n']",
                    "hunk_fix": "@@ -704,6 +745,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_ONE_HOT: {\n       auto params = safe_allocator.Allocate<TfLiteOneHotParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* schema_params = op->builtin_options_as_OneHotOptions()) {\n         params->axis = schema_params->axis();\n       }\n"
                },
                {
                    "old_start": 712,
                    "old_length": 6,
                    "new_start": 754,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_UNPACK: {\\n', '       auto params = safe_allocator.Allocate<TfLiteUnpackParams>();\\n', '       if (const auto* unpack_params = op->builtin_options_as_UnpackOptions()) {\\n', '         params->num = unpack_params->num();\\n', '         params->axis = unpack_params->axis();\\n']",
                    "hunk_fix": "@@ -712,6 +754,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_UNPACK: {\n       auto params = safe_allocator.Allocate<TfLiteUnpackParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* unpack_params = op->builtin_options_as_UnpackOptions()) {\n         params->num = unpack_params->num();\n         params->axis = unpack_params->axis();\n"
                },
                {
                    "old_start": 721,
                    "old_length": 6,
                    "new_start": 764,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_LEAKY_RELU: {\\n', '       auto params = safe_allocator.Allocate<TfLiteLeakyReluParams>();\\n', '       if (const auto* leaky_relu_params =\\n', '               op->builtin_options_as_LeakyReluOptions()) {\\n', '         params->alpha = leaky_relu_params->alpha();\\n']",
                    "hunk_fix": "@@ -721,6 +764,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_LEAKY_RELU: {\n       auto params = safe_allocator.Allocate<TfLiteLeakyReluParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* leaky_relu_params =\n               op->builtin_options_as_LeakyReluOptions()) {\n         params->alpha = leaky_relu_params->alpha();\n"
                },
                {
                    "old_start": 730,
                    "old_length": 6,
                    "new_start": 774,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_MIRROR_PAD: {\\n', '       auto params = safe_allocator.Allocate<TfLiteMirrorPaddingParams>();\\n', '       const auto* mirror_pad_params = op->builtin_options_as_MirrorPadOptions();\\n', '       if (mirror_pad_params != nullptr) {\\n', '         params->mode =\\n']",
                    "hunk_fix": "@@ -730,6 +774,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_MIRROR_PAD: {\n       auto params = safe_allocator.Allocate<TfLiteMirrorPaddingParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       const auto* mirror_pad_params = op->builtin_options_as_MirrorPadOptions();\n       if (mirror_pad_params != nullptr) {\n         params->mode =\n"
                },
                {
                    "old_start": 742,
                    "old_length": 6,
                    "new_start": 787,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_UNIQUE: {\\n', '       auto params = safe_allocator.Allocate<TfLiteUniqueParams>();\\n', '       const auto* unique_params = op->builtin_options_as_UniqueOptions();\\n', '       if (unique_params != nullptr) {\\n', '         params->index_out_type =\\n']",
                    "hunk_fix": "@@ -742,6 +787,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_UNIQUE: {\n       auto params = safe_allocator.Allocate<TfLiteUniqueParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       const auto* unique_params = op->builtin_options_as_UniqueOptions();\n       if (unique_params != nullptr) {\n         params->index_out_type =\n"
                },
                {
                    "old_start": 754,
                    "old_length": 6,
                    "new_start": 800,
                    "new_length": 7,
                    "hunk_buggy": "['     }\\n', '     case BuiltinOperator_REVERSE_SEQUENCE: {\\n', '       auto params = safe_allocator.Allocate<TfLiteReverseSequenceParams>();\\n', '       if (const auto* reverse_seq_params =\\n', '               op->builtin_options_as_ReverseSequenceOptions()) {\\n', '         params->seq_dim = reverse_seq_params->seq_dim();\\n']",
                    "hunk_fix": "@@ -754,6 +800,7 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n     }\n     case BuiltinOperator_REVERSE_SEQUENCE: {\n       auto params = safe_allocator.Allocate<TfLiteReverseSequenceParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* reverse_seq_params =\n               op->builtin_options_as_ReverseSequenceOptions()) {\n         params->seq_dim = reverse_seq_params->seq_dim();\n"
                },
                {
                    "old_start": 763,
                    "old_length": 32,
                    "new_start": 810,
                    "new_length": 34,
                    "hunk_buggy": "['       return kTfLiteOk;\\n', '     }\\n', '     case BuiltinOperator_IF: {\\n', '-      TfLiteIfParams* params = allocator->AllocatePOD<TfLiteIfParams>();\\n', '       if (const auto* if_params = op->builtin_options_as_IfOptions()) {\\n', '         params->then_subgraph_index = if_params->then_subgraph_index();\\n', '         params->else_subgraph_index = if_params->else_subgraph_index();\\n', '       }\\n', '-      *builtin_data = params;\\n', '       return kTfLiteOk;\\n', '     }\\n', '     case BuiltinOperator_WHILE: {\\n', '-      TfLiteWhileParams* params = allocator->AllocatePOD<TfLiteWhileParams>();\\n', '       if (const auto* while_params = op->builtin_options_as_WhileOptions()) {\\n', '         params->cond_subgraph_index = while_params->cond_subgraph_index();\\n', '         params->body_subgraph_index = while_params->body_subgraph_index();\\n', '       }\\n', '-      *builtin_data = params;\\n', '       return kTfLiteOk;\\n', '     }\\n', '     case BuiltinOperator_BATCH_MATMUL: {\\n', '-      TfLiteBatchMatMulParams* params =\\n', '-          allocator->AllocatePOD<TfLiteBatchMatMulParams>();\\n', '       if (const auto* bmm_params =\\n', '               op->builtin_options_as_BatchMatMulOptions()) {\\n', '         params->adjoint_lhs = bmm_params->adjoint_lhs();\\n', '         params->adjoint_rhs = bmm_params->adjoint_rhs();\\n', '       }\\n', '-      *builtin_data = params;\\n', '       return kTfLiteOk;\\n', '     }\\n', '     // Below are the ops with no builtin_data structure.']",
                    "hunk_fix": "@@ -763,32 +810,34 @@ TfLiteStatus ParseOpData(const Operator* op, BuiltinOperator op_type,\n       return kTfLiteOk;\n     }\n     case BuiltinOperator_IF: {\n-      TfLiteIfParams* params = allocator->AllocatePOD<TfLiteIfParams>();\n+      auto params = safe_allocator.Allocate<TfLiteIfParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* if_params = op->builtin_options_as_IfOptions()) {\n         params->then_subgraph_index = if_params->then_subgraph_index();\n         params->else_subgraph_index = if_params->else_subgraph_index();\n       }\n-      *builtin_data = params;\n+      *builtin_data = params.release();\n       return kTfLiteOk;\n     }\n     case BuiltinOperator_WHILE: {\n-      TfLiteWhileParams* params = allocator->AllocatePOD<TfLiteWhileParams>();\n+      auto params = safe_allocator.Allocate<TfLiteWhileParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* while_params = op->builtin_options_as_WhileOptions()) {\n         params->cond_subgraph_index = while_params->cond_subgraph_index();\n         params->body_subgraph_index = while_params->body_subgraph_index();\n       }\n-      *builtin_data = params;\n+      *builtin_data = params.release();\n       return kTfLiteOk;\n     }\n     case BuiltinOperator_BATCH_MATMUL: {\n-      TfLiteBatchMatMulParams* params =\n-          allocator->AllocatePOD<TfLiteBatchMatMulParams>();\n+      auto params = safe_allocator.Allocate<TfLiteBatchMatMulParams>();\n+      TF_LITE_ENSURE(error_reporter, params != nullptr);\n       if (const auto* bmm_params =\n               op->builtin_options_as_BatchMatMulOptions()) {\n         params->adjoint_lhs = bmm_params->adjoint_lhs();\n         params->adjoint_rhs = bmm_params->adjoint_rhs();\n       }\n-      *builtin_data = params;\n+      *builtin_data = params.release();\n       return kTfLiteOk;\n     }\n     // Below are the ops with no builtin_data structure."
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 203,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/01e84d7cc214dbf5a7a21bc418ad43afb5694fbc",
    "date": "2020-03-31T09:34:49-07:00",
    "message": "Update error message for data_adapter with validation split.\n\nRemove the user provided value in the error string in case it contains large amount of data. Dump large input data to log might crash on user side.\n\nhttps://github.com/tensorflow/tensorflow/issues/37840\n\nPiperOrigin-RevId: 303980671\nChange-Id: I1e2a6d3091c98bad0fcdd0e59c0eb88cd1d36956",
    "changes": [
        {
            "name": "data_adapter.py",
            "path": "tensorflow/python/keras/engine/data_adapter.py",
            "patches": [
                {
                    "old_start": 1397,
                    "old_length": 10,
                    "new_start": 1397,
                    "new_length": 11,
                    "hunk_buggy": "['     return isinstance(t, tensor_types) or t is None\\n', ' \\n', '   flat_arrays = nest.flatten(arrays)\\n', '-  if not all(_can_split(t) for t in flat_arrays):\\n', '     raise ValueError(\\n', '         \"`validation_split` is only supported for Tensors or NumPy \"\\n', '-        \"arrays, found: {}\".format(arrays))\\n', ' \\n', '   if all(t is None for t in flat_arrays):\\n', '     return arrays, arrays']",
                    "hunk_fix": "@@ -1397,10 +1397,11 @@ def train_validation_split(arrays, validation_split, shuffle=True):\n     return isinstance(t, tensor_types) or t is None\n \n   flat_arrays = nest.flatten(arrays)\n-  if not all(_can_split(t) for t in flat_arrays):\n+  unsplitable = [type(t) for t in flat_arrays if not _can_split(t)]\n+  if unsplitable:\n     raise ValueError(\n         \"`validation_split` is only supported for Tensors or NumPy \"\n-        \"arrays, found: {}\".format(arrays))\n+        \"arrays, found following types in the input: {}\".format(unsplitable))\n \n   if all(t is None for t in flat_arrays):\n     return arrays, arrays"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 204,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/f61175812426009a4c96e51befb2951612990903",
    "date": "2020-03-26T11:25:22-07:00",
    "message": "To add a check of `input_dims` greater than zero in embedding layers.\nFixes: https://github.com/tensorflow/tensorflow/issues/37777\n\nPiperOrigin-RevId: 303155222\nChange-Id: Ib5a693b7dc6058981f530bc78954dd5430f6982f",
    "changes": [
        {
            "name": "embeddings.py",
            "path": "tensorflow/python/keras/layers/embeddings.py",
            "patches": [
                {
                    "old_start": 112,
                    "old_length": 6,
                    "new_start": 112,
                    "new_length": 9,
                    "hunk_buggy": "['     super(Embedding, self).__init__(dtype=dtype, **kwargs)\\n', ' \\n', '     self.input_dim = input_dim\\n', '     self.output_dim = output_dim\\n', '     self.embeddings_initializer = initializers.get(embeddings_initializer)\\n', '     self.embeddings_regularizer = regularizers.get(embeddings_regularizer)']",
                    "hunk_fix": "@@ -112,6 +112,9 @@ class Embedding(Layer):\n     super(Embedding, self).__init__(dtype=dtype, **kwargs)\n \n     self.input_dim = input_dim\n+    if self.input_dim <= 0:\n+      raise ValueError('The argument `input_dim` should be greater than zero. '\n+                       'Received: %s' % input_dim)\n     self.output_dim = output_dim\n     self.embeddings_initializer = initializers.get(embeddings_initializer)\n     self.embeddings_regularizer = regularizers.get(embeddings_regularizer)"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 205,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/48393637f8154be16088d84742485a0e153ecbb2",
    "date": "2020-03-02T22:58:36-08:00",
    "message": "Change check to allow tensors with up to 6 dims.\n\nPiperOrigin-RevId: 298531979\nChange-Id: I6b4d6196d68c32fb93c84c9fbd980197c118ebaa",
    "changes": [
        {
            "name": "propagate_fixed_sizes.cc",
            "path": "tensorflow/lite/toco/graph_transformations/propagate_fixed_sizes.cc",
            "patches": [
                {
                    "old_start": 358,
                    "old_length": 8,
                    "new_start": 358,
                    "new_length": 8,
                    "hunk_buggy": "['     return;\\n', '   }\\n', '   CHECK(dims_array.data_type == ArrayDataType::kInt32) << \"dims must be int32\";\\n', '-  CHECK_LE(RequiredBufferSizeForShape(dims_array.shape()), 4)\\n', '-      << \"dims vector can be no larger than 4 values\";\\n', ' \\n', '   std::vector<int32> const& dims =\\n', '       dims_array.GetBuffer<ArrayDataType::kInt32>().data;']",
                    "hunk_fix": "@@ -358,8 +358,8 @@ void ProcessOpWithShapeInput(Model* model, Operator* op) {\n     return;\n   }\n   CHECK(dims_array.data_type == ArrayDataType::kInt32) << \"dims must be int32\";\n-  CHECK_LE(RequiredBufferSizeForShape(dims_array.shape()), 4)\n-      << \"dims vector can be no larger than 4 values\";\n+  CHECK_LE(RequiredBufferSizeForShape(dims_array.shape()), 6)\n+      << \"dims vector can be no larger than 6 values\";\n \n   std::vector<int32> const& dims =\n       dims_array.GetBuffer<ArrayDataType::kInt32>().data;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 206,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/a0fe44410e875e8e7775c6c256496bafb1a41b25",
    "date": "2020-03-02T09:07:54-08:00",
    "message": "Remove the check of NodeItem exists in unfinished_nodes_ in node callback.\n\nThis fixes the failure of RemoteAsyncTest.test_out_of_range_with_while_loop in DEBUG mode.\n\nPiperOrigin-RevId: 298364642\nChange-Id: Iabaa53bb2ee68c84893c07d5bd45dafb2b99cd69",
    "changes": [
        {
            "name": "eager_executor.cc",
            "path": "tensorflow/core/common_runtime/eager/eager_executor.cc",
            "patches": [
                {
                    "old_start": 240,
                    "old_length": 8,
                    "new_start": 240,
                    "new_length": 17,
                    "hunk_buggy": "['       // nodes list. However we only notify if we are at the front of the list\\n', \"       // since we don't want to notify any waiters of earlier nodes.\\n\", '       need_notification = item->id == unfinished_nodes_.begin()->first;\\n', '       auto result = unfinished_nodes_.erase(item->id);\\n', '-      DCHECK_GT(result, 0);\\n', '     }\\n', ' \\n', '     if (!status.ok() && item->node->Fatal()) {']",
                    "hunk_fix": "@@ -240,8 +240,17 @@ void EagerExecutor::NodeDone(const core::RefCountPtr<NodeItem>& item,\n       // nodes list. However we only notify if we are at the front of the list\n       // since we don't want to notify any waiters of earlier nodes.\n       need_notification = item->id == unfinished_nodes_.begin()->first;\n+      // Remove item if it exists in unfinished_nodes_.\n+      // With async execution, if two separate nodes failed and enter this\n+      // callback, then the second node might not find itself in\n+      // unfinished_nodes_ in the following senario:\n+      //   1) Callback of the first failed node clears unfinished_nodes_\n+      //   2) ClearError is called and executor status_ is set to OK\n+      //   3) Callback of the second failed node is triggered\n+      // In this case, do not taint the executor status or other note items\n+      // because they are inserted after the ClearError.\n       auto result = unfinished_nodes_.erase(item->id);\n-      DCHECK_GT(result, 0);\n+      if (result == 0) return;\n     }\n \n     if (!status.ok() && item->node->Fatal()) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 207,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/a7908e924af3105c3007988e219855174b26774f",
    "date": "2020-02-25T18:55:18-05:00",
    "message": "[core] Added check for output buffer",
    "changes": [
        {
            "name": "jpeg_mem.cc",
            "path": "tensorflow/core/lib/jpeg/jpeg_mem.cc",
            "patches": [
                {
                    "old_start": 593,
                    "old_length": 6,
                    "new_start": 593,
                    "new_length": 11,
                    "hunk_buggy": "[' namespace {\\n', ' bool CompressInternal(const uint8* srcdata, int width, int height,\\n', '                       const CompressFlags& flags, tstring* output) {\\n', '   output->clear();\\n', '   const int components = (static_cast<int>(flags.format) & 0xff);\\n', ' ']",
                    "hunk_fix": "@@ -593,6 +593,11 @@ bool GetImageInfo(const void* srcdata, int datasize, int* width, int* height,\n namespace {\n bool CompressInternal(const uint8* srcdata, int width, int height,\n                       const CompressFlags& flags, tstring* output) {\n+  if (output == nullptr)\n+    LOG(ERROR) << \"Output buffer is null: \";\n+    return false;\n+  }\n+\n   output->clear();\n   const int components = (static_cast<int>(flags.format) & 0xff);\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 208,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/2adf1114d4dc7ca30e5117acd2dc7aeb3279feb7",
    "date": "2020-02-14T18:35:27-08:00",
    "message": "Make NNAPI delegate only apply overflow check to quantized average_pool\n\nPiperOrigin-RevId: 295269801\nChange-Id: Iea8dc5e2da5496aca76500c207f02f67196c14b1",
    "changes": [
        {
            "name": "nnapi_delegate.cc",
            "path": "tensorflow/lite/delegates/nnapi/nnapi_delegate.cc",
            "patches": [
                {
                    "old_start": 1444,
                    "old_length": 12,
                    "new_start": 1444,
                    "new_length": 14,
                    "hunk_buggy": "['       ExpectIsFloatOrQuant8Operator(context, node, &val_ctx);\\n', '       auto builtin = reinterpret_cast<TfLitePoolParams*>(node->builtin_data);\\n', '       // TODO(b/138756912): Large filter window would overflow on the\\n', '-      // reference CPU path.\\n', '-      Expect(is_accelerator_specified ||\\n', '-                 (builtin->filter_width * builtin->filter_height <= 256),\\n', '-             NNAPIValidationFailureType::kUnsupportedOperandSize,\\n', '-             \"Large filter window would overflow on the reference CPU path\",\\n', '-             &val_ctx);\\n', '     } break;\\n', '     case kTfLiteBuiltinMaxPool2d: {\\n', '       ExpectMaxOpVersion(version, 2, &val_ctx);']",
                    "hunk_fix": "@@ -1444,12 +1444,14 @@ bool NNAPIDelegateKernel::Validate(\n       ExpectIsFloatOrQuant8Operator(context, node, &val_ctx);\n       auto builtin = reinterpret_cast<TfLitePoolParams*>(node->builtin_data);\n       // TODO(b/138756912): Large filter window would overflow on the\n-      // reference CPU path.\n-      Expect(is_accelerator_specified ||\n-                 (builtin->filter_width * builtin->filter_height <= 256),\n-             NNAPIValidationFailureType::kUnsupportedOperandSize,\n-             \"Large filter window would overflow on the reference CPU path\",\n-             &val_ctx);\n+      // quantized reference CPU path.\n+      if (IsQuantized(context->tensors[node->inputs->data[0]].type)) {\n+        Expect(is_accelerator_specified ||\n+                   (builtin->filter_width * builtin->filter_height <= 256),\n+               NNAPIValidationFailureType::kUnsupportedOperandSize,\n+               \"Large filter window would overflow on the reference CPU path\",\n+               &val_ctx);\n+      }\n     } break;\n     case kTfLiteBuiltinMaxPool2d: {\n       ExpectMaxOpVersion(version, 2, &val_ctx);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 209,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/d79c4d435fc6e7be6cc69a3ca446716ebf6190b9",
    "date": "2020-02-15T10:24:46+08:00",
    "message": "address review concerns\n\n1. change default value of num_threads to Non\n2. set num_threads before delegate\n3. check the type of num_threads before setting it",
    "changes": [
        {
            "name": "interpreter.py",
            "path": "tensorflow/lite/python/interpreter.py",
            "patches": [
                {
                    "old_start": 184,
                    "old_length": 7,
                    "new_start": 184,
                    "new_length": 7,
                    "hunk_buggy": "['                model_path=None,\\n', '                model_content=None,\\n', '                experimental_delegates=None,\\n', '-               num_threads=1):\\n', '     \"\"\"Constructor.\\n', ' \\n', '     Args:\\n']",
                    "hunk_fix": "@@ -184,7 +184,7 @@ class Interpreter(object):\n                model_path=None,\n                model_content=None,\n                experimental_delegates=None,\n-               num_threads=1):\n+               num_threads=None):\n     \"\"\"Constructor.\n \n     Args:\n"
                },
                {
                    "old_start": 221,
                    "old_length": 6,
                    "new_start": 221,
                    "new_length": 11,
                    "hunk_buggy": "['     else:\\n', \"       raise ValueError('Can\\\\'t both provide `model_path` and `model_content`')\\n\", ' \\n', '     # Each delegate is a wrapper that owns the delegates that have been loaded\\n', '     # as plugins. The interpreter wrapper will be using them, but we need to\\n', '     # hold them in a list so that the lifetime is preserved at least as long as\\n']",
                    "hunk_fix": "@@ -221,6 +221,11 @@ class Interpreter(object):\n     else:\n       raise ValueError('Can\\'t both provide `model_path` and `model_content`')\n \n+    if num_threads:\n+      if not isinstance(num_threads, int):\n+        raise ValueError('type of num_threads should be int')\n+      self._interpreter.SetNumThreads(num_threads)\n+\n     # Each delegate is a wrapper that owns the delegates that have been loaded\n     # as plugins. The interpreter wrapper will be using them, but we need to\n     # hold them in a list so that the lifetime is preserved at least as long as\n"
                },
                {
                    "old_start": 232,
                    "old_length": 8,
                    "new_start": 237,
                    "new_length": 6,
                    "hunk_buggy": "['         self._interpreter.ModifyGraphWithDelegate(\\n', '             delegate._get_native_delegate_pointer())  # pylint: disable=protected-access\\n', ' \\n', '-    self._interpreter.SetNumThreads(num_threads)\\n', '-\\n', '   def __del__(self):\\n', '     # Must make sure the interpreter is destroyed before things that\\n', '     # are used by it like the delegates. NOTE this only works on CPython']",
                    "hunk_fix": "@@ -232,8 +237,6 @@ class Interpreter(object):\n         self._interpreter.ModifyGraphWithDelegate(\n             delegate._get_native_delegate_pointer())  # pylint: disable=protected-access\n \n-    self._interpreter.SetNumThreads(num_threads)\n-\n   def __del__(self):\n     # Must make sure the interpreter is destroyed before things that\n     # are used by it like the delegates. NOTE this only works on CPython"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 210,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/be5116dd131a92da298dbb68d26e0d47f66f2fe5",
    "date": "2020-02-13T12:15:20-08:00",
    "message": "Correct graph check in broadcast_to gradient.\n\nPiperOrigin-RevId: 294970783\nChange-Id: I2774ddf6949a1ba6814cefd939695010ecda0861",
    "changes": [
        {
            "name": "array_grad.py",
            "path": "tensorflow/python/ops/array_grad.py",
            "patches": [
                {
                    "old_start": 1135,
                    "old_length": 7,
                    "new_start": 1135,
                    "new_length": 7,
                    "hunk_buggy": "['   input_value = op.inputs[0]\\n', '   broadcast_shape = op.inputs[1]\\n', '   input_value_shape = array_ops.shape(input_value)\\n', '-  if not context.executing_eagerly():\\n', '     broadcast_shape_static = tensor_shape.TensorShape(\\n', '         pywrap_tf_session.TF_TryEvaluateConstant_wrapper(\\n', '             broadcast_shape.graph._c_graph, broadcast_shape._as_tf_output()))  # pylint: disable=protected-access']",
                    "hunk_fix": "@@ -1135,7 +1135,7 @@ def _BroadcastToGrad(op, grad):\n   input_value = op.inputs[0]\n   broadcast_shape = op.inputs[1]\n   input_value_shape = array_ops.shape(input_value)\n-  if not context.executing_eagerly():\n+  if not isinstance(broadcast_shape, ops.EagerTensor):\n     broadcast_shape_static = tensor_shape.TensorShape(\n         pywrap_tf_session.TF_TryEvaluateConstant_wrapper(\n             broadcast_shape.graph._c_graph, broadcast_shape._as_tf_output()))  # pylint: disable=protected-access"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 211,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/5bc536f1afbaff5d3d5a14a9185cd1e3cc31b302",
    "date": "2020-01-07T17:21:14-08:00",
    "message": "[Fix] bug fix during check static shape.\n\nPiperOrigin-RevId: 288604974\nChange-Id: I5b22b754c5396c3a6d148159642500993a5c8215",
    "changes": [
        {
            "name": "legalize_tf.cc",
            "path": "tensorflow/compiler/mlir/lite/transforms/legalize_tf.cc",
            "patches": [
                {
                    "old_start": 73,
                    "old_length": 7,
                    "new_start": 73,
                    "new_length": 7,
                    "hunk_buggy": "['   ArrayRef<int64_t> shape;\\n', '   for (Value value : values) {\\n', '     auto shaped_type = value.getType().dyn_cast<ShapedType>();\\n', '-    if (!shaped_type && !shaped_type.hasStaticShape()) {\\n', '       return false;\\n', '     }\\n', '     if (index == 0) {']",
                    "hunk_fix": "@@ -73,7 +73,7 @@ bool HasSameStaticShapes(Operation* op) {\n   ArrayRef<int64_t> shape;\n   for (Value value : values) {\n     auto shaped_type = value.getType().dyn_cast<ShapedType>();\n-    if (!shaped_type && !shaped_type.hasStaticShape()) {\n+    if (!shaped_type || !shaped_type.hasStaticShape()) {\n       return false;\n     }\n     if (index == 0) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 212,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/ba91c04e001f417641e757a6417e5325c1c4e15e",
    "date": "2019-12-19T12:09:00-08:00",
    "message": "Add more check to sparsity parameter verifier.\n\nPiperOrigin-RevId: 286436553\nChange-Id: I43913b2a16ad7bb3b3e22fa65cd47462159b8f67",
    "changes": [
        {
            "name": "verifier.cc",
            "path": "tensorflow/lite/tools/verifier.cc",
            "patches": [
                {
                    "old_start": 179,
                    "old_length": 7,
                    "new_start": 179,
                    "new_length": 8,
                    "hunk_buggy": "[' \\n', '   const int total_dims = sparsity->traversal_order()->size();\\n', ' \\n', '-  if (sparsity->dim_metadata()->size() != total_dims) {\\n', '     return absl::nullopt;\\n', '   }\\n', ' ']",
                    "hunk_fix": "@@ -179,7 +179,8 @@ absl::optional<uint64_t> VerifyAndCountSparseElements(const Tensor& tensor) {\n \n   const int total_dims = sparsity->traversal_order()->size();\n \n-  if (sparsity->dim_metadata()->size() != total_dims) {\n+  if (total_dims < tensor.shape()->size() ||\n+      sparsity->dim_metadata()->size() != total_dims) {\n     return absl::nullopt;\n   }\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 213,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/cc560f64b6e3e6724517757e9789c52cde224ee9",
    "date": "2019-12-13T17:22:18-08:00",
    "message": "Profiler: restore correct behavior of StartTracing with empty workers list.\n\nabsl::StrSplit behaves differently from str_util::Split when the passed string is empty. Restore previous behavior by explicitly checking for an empty string.\nPiperOrigin-RevId: 285460069\nChange-Id: I8fd2768d32c8ee566ce5dcd9ab9e404242703597",
    "changes": [
        {
            "name": "capture_profile.cc",
            "path": "tensorflow/core/profiler/rpc/client/capture_profile.cc",
            "patches": [
                {
                    "old_start": 201,
                    "old_length": 7,
                    "new_start": 201,
                    "new_length": 10,
                    "hunk_buggy": "['   constexpr char kProfilePluginDirectory[] = \"plugins/profile/\";\\n', '   tensorflow::string repository_root =\\n', '       io::JoinPath(logdir, kProfilePluginDirectory);\\n', \"-  std::vector<tensorflow::string> hostnames = absl::StrSplit(workers_list, ',');\\n\", ' \\n', '   TF_RETURN_IF_ERROR(MaybeCreateEmptyEventFile(logdir));\\n', ' ']",
                    "hunk_fix": "@@ -201,7 +201,10 @@ Status StartTracing(const tensorflow::string& service_addr,\n   constexpr char kProfilePluginDirectory[] = \"plugins/profile/\";\n   tensorflow::string repository_root =\n       io::JoinPath(logdir, kProfilePluginDirectory);\n-  std::vector<tensorflow::string> hostnames = absl::StrSplit(workers_list, ',');\n+  std::vector<tensorflow::string> hostnames;\n+  if (!workers_list.empty()) {\n+    hostnames = absl::StrSplit(workers_list, ',');\n+  }\n \n   TF_RETURN_IF_ERROR(MaybeCreateEmptyEventFile(logdir));\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 214,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/4863013a3ec5b97c042a38ab567bcc4a62ccde5c",
    "date": "2019-12-13T16:34:20-08:00",
    "message": "Add checking for number of inputs in GetOptionalInputTensor to avoid indexing out of array bounds.\n\nPiperOrigin-RevId: 285438822\nChange-Id: Icbb7d7c3ffbffc574ff54082107b2fbb3fa751e4",
    "changes": [
        {
            "name": "kernel_util.h",
            "path": "tensorflow/lite/kernels/kernel_util.h",
            "patches": [
                {
                    "old_start": 77,
                    "old_length": 7,
                    "new_start": 77,
                    "new_length": 8,
                    "hunk_buggy": "[' inline const TfLiteTensor* GetOptionalInputTensor(TfLiteContext* context,\\n', '                                                   const TfLiteNode* node,\\n', '                                                   int index) {\\n', '-  const bool use_tensor = node->inputs->data[index] != kTfLiteOptionalTensor;\\n', '   if (use_tensor) {\\n', '     return &context\\n', '                 ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];']",
                    "hunk_fix": "@@ -77,7 +77,8 @@ inline int64_t NumElements(const TfLiteTensor* t) {\n inline const TfLiteTensor* GetOptionalInputTensor(TfLiteContext* context,\n                                                   const TfLiteNode* node,\n                                                   int index) {\n-  const bool use_tensor = node->inputs->data[index] != kTfLiteOptionalTensor;\n+  const bool use_tensor = index < node->inputs->size &&\n+                          node->inputs->data[index] != kTfLiteOptionalTensor;\n   if (use_tensor) {\n     return &context\n                 ->tensors[flatbuffers::EndianScalar(node->inputs->data[index])];"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 215,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/3e0152a8b4aad03dd06274e0dd3b94bd5f8bf5d3",
    "date": "2019-12-05T03:40:35+00:00",
    "message": "Fix `invalid syntax` error when `import carla` is present\n\nThis fix tries to address the issue raised in 34828 where\n`import carla` followed by `import tensorflow` caused the\nfollowing:\n```\nSyntaxError: invalid syntax\n```\n\nThe issue is that, when `import carla` is invoked,\nI/O operation for `std::ostringstream s` might fail,\nwhich caused the conversion of AttrValue to string as empty.\n\nThis PR check `s.good()` to make sure the I/O operation\nis OK, and, fallback to normal conversion if locale-neutral I/O\noperation fails.\n\nThis PR fixes 34828.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "changes": [
        {
            "name": "python_op_gen_internal.cc",
            "path": "tensorflow/python/framework/python_op_gen_internal.cc",
            "patches": [
                {
                    "old_start": 449,
                    "old_length": 7,
                    "new_start": 449,
                    "new_length": 12,
                    "hunk_buggy": "['       std::ostringstream s;\\n', '       s.imbue(std::locale::classic());\\n', '       s << std::setprecision(FLT_DIG) << value.f();\\n', '-      return s.str();\\n', '     }\\n', '   } else if (type == \"bool\") {\\n', '     return value.b() ? \"True\" : \"False\";']",
                    "hunk_fix": "@@ -449,7 +449,12 @@ string AttrValueToPython(const string& type, const AttrValue& value,\n       std::ostringstream s;\n       s.imbue(std::locale::classic());\n       s << std::setprecision(FLT_DIG) << value.f();\n-      return s.str();\n+      // If there is no I/O error for `std::ostringstream s` return s.str(),\n+      // otherwise fallback to strings::StrCat(value.f()).\n+      if (s.good()) {\n+        return s.str();\n+      }\n+      return strings::StrCat(value.f());\n     }\n   } else if (type == \"bool\") {\n     return value.b() ? \"True\" : \"False\";"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 216,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/f482488b481a799ca07e7e2d153cf47b8e91a60c",
    "date": "2019-11-21T16:37:21-08:00",
    "message": "TFLite OpenGL ES delegate: out of boundary writes fixed for bhwc->phwc4 conversion.\n\nPiperOrigin-RevId: 281857556\nChange-Id: I94d17ca85965cf0ada292ad1ac3f7d21e36126a1",
    "changes": [
        {
            "name": "bhwc_to_phwc4.cc",
            "path": "tensorflow/lite/delegates/gpu/gl/converters/bhwc_to_phwc4.cc",
            "patches": [
                {
                    "old_start": 86,
                    "old_length": 7,
                    "new_start": 86,
                    "new_length": 7,
                    "hunk_buggy": "['   if (shape.b != 1) {\\n', '     return UnimplementedError(\"BhwcToPhwc4: Batch size is not equal to 1.\");\\n', '   }\\n', '-  uint3 workload = uint3(shape.w, shape.h, shape.c);\\n', '   uint3 num_workgroups = IntegralDivideRoundUp(workload, workgroup_size_);\\n', ' \\n', '   RETURN_IF_ERROR(program_.SetParameter(']",
                    "hunk_fix": "@@ -86,7 +86,7 @@ Status ConverterBhwcToPhwc4::Convert(const BHWC& shape, const GlBuffer& source,\n   if (shape.b != 1) {\n     return UnimplementedError(\"BhwcToPhwc4: Batch size is not equal to 1.\");\n   }\n-  uint3 workload = uint3(shape.w, shape.h, shape.c);\n+  uint3 workload = uint3(shape.w, shape.h, IntegralDivideRoundUp(shape.c, 4));\n   uint3 num_workgroups = IntegralDivideRoundUp(workload, workgroup_size_);\n \n   RETURN_IF_ERROR(program_.SetParameter("
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 217,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/677866210941431b82c95d58d0798976bb40a415",
    "date": "2019-10-28T00:30:19-07:00",
    "message": "Add a nullptr check for the tensor quantization field\n\nPiperOrigin-RevId: 277009093\nChange-Id: Id7ff4ee3939f0faf1652393c021317d82b0b179f",
    "changes": [
        {
            "name": "flatbuffer_import.cc",
            "path": "tensorflow/compiler/mlir/lite/flatbuffer_import.cc",
            "patches": [
                {
                    "old_start": 197,
                    "old_length": 7,
                    "new_start": 197,
                    "new_length": 7,
                    "hunk_buggy": "['                                         Value* res) {\\n', '   // If the `tensor` has scale/zero_point, it must have been quantized, then the\\n', '   // min/max stats is just for comments, so ignore it.\\n', '-  if (IsQuantized(tensor)) return nullptr;\\n', ' \\n', '   auto mins = tensor.quantization->min;\\n', '   auto maxs = tensor.quantization->max;\\n']",
                    "hunk_fix": "@@ -197,7 +197,7 @@ mlir::Operation* ConvertMinMaxToStatsOp(const TensorT& tensor, OpBuilder b,\n                                         Value* res) {\n   // If the `tensor` has scale/zero_point, it must have been quantized, then the\n   // min/max stats is just for comments, so ignore it.\n-  if (IsQuantized(tensor)) return nullptr;\n+  if (!tensor.quantization || IsQuantized(tensor)) return nullptr;\n \n   auto mins = tensor.quantization->min;\n   auto maxs = tensor.quantization->max;\n"
                },
                {
                    "old_start": 223,
                    "old_length": 6,
                    "new_start": 223,
                    "new_length": 7,
                    "hunk_buggy": "['     axis_stats = mlir::DenseFPElementsAttr::get(\\n', '         mlir::RankedTensorType::get(axis_stats_shape, b.getF32Type()),\\n', '         min_maxs);\\n', '     axis = b.getI64IntegerAttr(tensor.quantization->quantized_dimension);\\n', '   }\\n', '   return b.create<mlir::quant::StatisticsOp>(b.getUnknownLoc(), res,']",
                    "hunk_fix": "@@ -223,6 +223,7 @@ mlir::Operation* ConvertMinMaxToStatsOp(const TensorT& tensor, OpBuilder b,\n     axis_stats = mlir::DenseFPElementsAttr::get(\n         mlir::RankedTensorType::get(axis_stats_shape, b.getF32Type()),\n         min_maxs);\n+    // TODO(fengliuai): this quantization dimension isn't correct.\n     axis = b.getI64IntegerAttr(tensor.quantization->quantized_dimension);\n   }\n   return b.create<mlir::quant::StatisticsOp>(b.getUnknownLoc(), res,"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 218,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/a680ed0bf03d5ca3b2c4a70c0d95eeebc20da6d6",
    "date": "2019-10-17T17:10:41-07:00",
    "message": "For Substr check pos and len rank equality only when their rank is known.\n\nThis fixes a bug where len has unknown rank, while pos has known shape. The WithRank(...) check returned error in such a case. Here we compare their ranks only when both pos and len have known rank.\n\nPiperOrigin-RevId: 275370109\nChange-Id: I8df36f3d4dcf3104e246e8605689b9ded3d9c783",
    "changes": [
        {
            "name": "string_ops.cc",
            "path": "tensorflow/core/ops/string_ops.cc",
            "patches": [
                {
                    "old_start": 261,
                    "old_length": 8,
                    "new_start": 261,
                    "new_length": 10,
                    "hunk_buggy": "['       ShapeHandle pos_shape = c->input(1);\\n', '       ShapeHandle len_shape = c->input(2);\\n', '       ShapeHandle unused;\\n', '-      // Check that pos/len have same rank\\n', '-      TF_RETURN_IF_ERROR(c->WithRank(pos_shape, c->Rank(len_shape), &unused));\\n', '       // Check that dimensions are equal\\n', '       for (int32 i = 0; i < c->Rank(pos_shape); ++i) {\\n', '         DimensionHandle pos_dim = c->Dim(pos_shape, i);']",
                    "hunk_fix": "@@ -261,8 +261,10 @@ REGISTER_OP(\"Substr\")\n       ShapeHandle pos_shape = c->input(1);\n       ShapeHandle len_shape = c->input(2);\n       ShapeHandle unused;\n-      // Check that pos/len have same rank\n-      TF_RETURN_IF_ERROR(c->WithRank(pos_shape, c->Rank(len_shape), &unused));\n+      // If len rank is known, check that pos and len have the same rank\n+      if (c->RankKnown(len_shape)) {\n+        TF_RETURN_IF_ERROR(c->WithRank(pos_shape, c->Rank(len_shape), &unused));\n+      }\n       // Check that dimensions are equal\n       for (int32 i = 0; i < c->Rank(pos_shape); ++i) {\n         DimensionHandle pos_dim = c->Dim(pos_shape, i);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 219,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/7f9929732ced22fe8ef42a695dae39c1caf44608",
    "date": "2019-10-14T20:20:18-07:00",
    "message": "For gather op, if params.shape[:batch_dims] is not the same as indices.shape[:batch_dims], return an error instead of check fail (according to https://www.tensorflow.org/api_docs/python/tf/gather).\n\nPiperOrigin-RevId: 274672054",
    "changes": [
        {
            "name": "gather_op.cc",
            "path": "tensorflow/core/kernels/gather_op.cc",
            "patches": [
                {
                    "old_start": 109,
                    "old_length": 6,
                    "new_start": 109,
                    "new_length": 13,
                    "hunk_buggy": "['                   errors::InvalidArgument(\"batch_dims (\", batch_dims_,\\n', '                                           \") must be less than or equal to \",\\n', '                                           \"axis (\", axis, \").\"));\\n', '     }\\n', ' \\n', '     // Check that we have enough index space']",
                    "hunk_fix": "@@ -109,6 +109,13 @@ class GatherOp : public OpKernel {\n                   errors::InvalidArgument(\"batch_dims (\", batch_dims_,\n                                           \") must be less than or equal to \",\n                                           \"axis (\", axis, \").\"));\n+      for (int i = 0; i < batch_dims_; ++i) {\n+        OP_REQUIRES(c, params.dim_size(i) == indices.dim_size(i),\n+                    errors::InvalidArgument(\n+                        \"params.shape[\", i, \"]: \", params.dim_size(i),\n+                        \" should be equal to indices.shape[\", i,\n+                        \"]: \", indices.dim_size(i)));\n+      }\n     }\n \n     // Check that we have enough index space"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 220,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/ab0a5278d81ef34096775d5d56f11694cca2a785",
    "date": "2019-10-05T07:18:42+00:00",
    "message": "Fix tf.assert_equal issue when one tenor is empty and another is non-empty\n\nThis fix tries to address the issue raised in 32082 where\ntf.assert_equal([], [1.0]) doesn't raise error.\nThe reason was that in assert_equal `[1.0]` was broadcasted\nas `[]` and equal was in place in that situation.\n\nThis PR updates the _binary_asesert so that it will check if\nx, y are both empty or both non-empty. If one is empty and another is\nnon-empty, then assertion throws exception. This change is to not impact\nother ops that depends on the broadcast behavior.\n\nThis fix fixes 32082.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "changes": [
        {
            "name": "check_ops.py",
            "path": "tensorflow/python/ops/check_ops.py",
            "patches": [
                {
                    "old_start": 295,
                    "old_length": 9,
                    "new_start": 295,
                    "new_length": 27,
                    "hunk_buggy": "['   else:\\n', '     return str(data_item)\\n', ' \\n', ' \\n', ' def _binary_assert(sym, opname, op_func, static_func, x, y, data, summarize,\\n', '-                   message, name):\\n', '   \"\"\"Generic binary elementwise assertion.\\n', ' \\n', '   Implements the behavior described in _binary_assert_doc() above.\\n']",
                    "hunk_fix": "@@ -295,9 +295,27 @@ def _pretty_print(data_item, summarize):\n   else:\n     return str(data_item)\n \n+def _binary_all_empty_or_all_non_empty(x, y):\n+  \"\"\"Chek if x and y are either all empty or all non-empty.\n+\n+  Args:\n+    x:  A `Tensor`.\n+    y:  A `Tensor`.\n+\n+  Returns:\n+    True if x and y are either all empty or all non-empty\n+  \"\"\"\n+  all_empty = math_ops.logical_and(\n+      math_ops.equal(array_ops.size(x), 0),\n+      math_ops.equal(array_ops.size(y), 0))\n+  all_non_empty = math_ops.logical_and(\n+      math_ops.not_equal(array_ops.size(x), 0),\n+      math_ops.not_equal(array_ops.size(y), 0))\n+  return math_ops.logical_or(all_empty, all_non_empty)\n+\n \n def _binary_assert(sym, opname, op_func, static_func, x, y, data, summarize,\n-                   message, name):\n+                   message, name, allow_empty=False):\n   \"\"\"Generic binary elementwise assertion.\n \n   Implements the behavior described in _binary_assert_doc() above.\n"
                },
                {
                    "old_start": 329,
                    "old_length": 7,
                    "new_start": 347,
                    "new_length": 12,
                    "hunk_buggy": "[' \\n', '     if context.executing_eagerly():\\n', '       test_op = op_func(x, y)\\n', '-      condition = math_ops.reduce_all(test_op)\\n', '       if condition:\\n', '         return\\n', ' \\n']",
                    "hunk_fix": "@@ -329,7 +347,12 @@ def _binary_assert(sym, opname, op_func, static_func, x, y, data, summarize,\n \n     if context.executing_eagerly():\n       test_op = op_func(x, y)\n-      condition = math_ops.reduce_all(test_op)\n+      if allow_empty:\n+        condition = math_ops.reduce_all(test_op)\n+      else:\n+        empty_check = _binary_all_empty_or_all_non_empty(x, y)\n+        condition = math_ops.logical_and(\n+            empty_check, math_ops.reduce_all(test_op))\n       if condition:\n         return\n \n"
                },
                {
                    "old_start": 362,
                    "old_length": 11,
                    "new_start": 385,
                    "new_length": 22,
                    "hunk_buggy": "['         ]\\n', '       if message is not None:\\n', '         data = [message] + list(data)\\n', '-      condition = math_ops.reduce_all(op_func(x, y))\\n', '       x_static = tensor_util.constant_value(x)\\n', '       y_static = tensor_util.constant_value(y)\\n', '       if x_static is not None and y_static is not None:\\n', '-        condition_static = np.all(static_func(x_static, y_static))\\n', '         _assert_static(condition_static, data)\\n', '       return control_flow_ops.Assert(condition, data, summarize=summarize)\\n', ' ']",
                    "hunk_fix": "@@ -362,11 +385,22 @@ def _binary_assert(sym, opname, op_func, static_func, x, y, data, summarize,\n         ]\n       if message is not None:\n         data = [message] + list(data)\n-      condition = math_ops.reduce_all(op_func(x, y))\n+      if allow_empty:\n+        condition = math_ops.reduce_all(op_func(x, y))\n+      else:\n+        empty_check = _binary_all_empty_or_all_non_empty(x, y)\n+        condition = math_ops.logical_and(\n+            empty_check, math_ops.reduce_all(op_func(x, y)))\n       x_static = tensor_util.constant_value(x)\n       y_static = tensor_util.constant_value(y)\n       if x_static is not None and y_static is not None:\n-        condition_static = np.all(static_func(x_static, y_static))\n+        if allow_empty:\n+          condition_static = np.all(static_func(x_static, y_static))\n+        else:\n+          empty_check_static = ((x_static.size == 0 and y_static.size == 0) or\n+                                (x_static.size != 0 and y_static.size != 0))\n+          condition_static = empty_check_static and np.all(\n+              static_func(x_static, y_static))\n         _assert_static(condition_static, data)\n       return control_flow_ops.Assert(condition, data, summarize=summarize)\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 221,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/ed06859189722af4dc8e4abd655926df066e587a",
    "date": "2019-09-19T17:01:25+08:00",
    "message": "Add format check.",
    "changes": [
        {
            "name": "mkl_layout_pass.cc",
            "path": "tensorflow/core/graph/mkl_layout_pass.cc",
            "patches": [
                {
                    "old_start": 2774,
                    "old_length": 6,
                    "new_start": 2774,
                    "new_length": 7,
                    "hunk_buggy": "['     std::vector<int32> new_strides;\\n', '     std::vector<int32> new_ksize;\\n', '     if (strides.size() == 5) {\\n', '       // `strides` and `ksize` also need to be changed according to\\n', '       // `data_format`. In this case, from `NDHWC` to `NCDHW`.\\n', '       new_strides = {strides[NDHWC::dim::N], strides[NDHWC::dim::C],\\n']",
                    "hunk_fix": "@@ -2774,6 +2774,7 @@ void MklLayoutRewritePass::CopyAttrsPooling(const Node* orig_node,\n     std::vector<int32> new_strides;\n     std::vector<int32> new_ksize;\n     if (strides.size() == 5) {\n+      DCHECK(data_format == \"NCDHW\");\n       // `strides` and `ksize` also need to be changed according to\n       // `data_format`. In this case, from `NDHWC` to `NCDHW`.\n       new_strides = {strides[NDHWC::dim::N], strides[NDHWC::dim::C],\n"
                },
                {
                    "old_start": 2787,
                    "old_length": 7,
                    "new_start": 2788,
                    "new_length": 7,
                    "hunk_buggy": "['     } else {\\n', '       // `strides` and `ksize` also need to be changed according to\\n', '       // `data_format`. In this case, from `NHWC` to `NCHW`.\\n', '-\\n', '       new_strides = {strides[NHWC::dim::N], strides[NHWC::dim::C],\\n', '                      strides[NHWC::dim::H], strides[NHWC::dim::W]};\\n', ' ']",
                    "hunk_fix": "@@ -2787,7 +2788,7 @@ void MklLayoutRewritePass::CopyAttrsPooling(const Node* orig_node,\n     } else {\n       // `strides` and `ksize` also need to be changed according to\n       // `data_format`. In this case, from `NHWC` to `NCHW`.\n-\n+      DCHECK(data_format == \"NCHW\");\n       new_strides = {strides[NHWC::dim::N], strides[NHWC::dim::C],\n                      strides[NHWC::dim::H], strides[NHWC::dim::W]};\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 222,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/c040db5e9003cc20016586df9f2964db83b98c4f",
    "date": "2019-09-12T17:48:49-07:00",
    "message": "[XLA] Add a defensive check in dynamic dimension inference to prevent scalar reshape with dynamic dimension.\n\nIn theory we can just ignore a [1] -> [] reshape, but adding a check here for now.\n\nPiperOrigin-RevId: 268804389",
    "changes": [
        {
            "name": "dynamic_dimension_inference.cc",
            "path": "tensorflow/compiler/xla/service/dynamic_dimension_inference.cc",
            "patches": [
                {
                    "old_start": 449,
                    "old_length": 10,
                    "new_start": 449,
                    "new_length": 15,
                    "hunk_buggy": "[' \\n', ' Status DynamicDimensionInferenceVisitor::HandleReshape(HloInstruction* hlo) {\\n', '   return ForEachOperandDynamicDimension(\\n', '-      hlo, [&](HloInstruction* operand, ShapeIndex index, int64 dimension,\\n', '-               int64 operand_index, HloInstruction* dynamic_size,\\n', '-               DimensionConstraint constraint) {\\n', '         HloInstruction* reshape = hlo;\\n', '         // Reshape is supported as long as it is the most\\n', '         // major one and it is combining with other non-dynamic dimensions.\\n', '         const int64 output_most_major = reshape->shape().dimensions(0);']",
                    "hunk_fix": "@@ -449,10 +449,15 @@ Status DynamicDimensionInferenceVisitor::HandleElementwiseBinary(\n \n Status DynamicDimensionInferenceVisitor::HandleReshape(HloInstruction* hlo) {\n   return ForEachOperandDynamicDimension(\n-      hlo, [&](HloInstruction* operand, ShapeIndex index, int64 dimension,\n-               int64 operand_index, HloInstruction* dynamic_size,\n-               DimensionConstraint constraint) {\n+      hlo,\n+      [&](HloInstruction* operand, ShapeIndex index, int64 dimension,\n+          int64 operand_index, HloInstruction* dynamic_size,\n+          DimensionConstraint constraint) -> Status {\n         HloInstruction* reshape = hlo;\n+        TF_RET_CHECK(reshape->shape().rank() > 0)\n+            << \"Reshaping a dynamic dimension into a scalar, which has \"\n+               \"undefined behavior. The offending instruction is: \"\n+            << reshape->ToString();\n         // Reshape is supported as long as it is the most\n         // major one and it is combining with other non-dynamic dimensions.\n         const int64 output_most_major = reshape->shape().dimensions(0);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 223,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/0197a2d8a3070af763cb67227835ee63df095e6d",
    "date": "2019-09-02T16:57:38-07:00",
    "message": "Add a check to catch out-of-bound access on invalid Graphs\n\nThe existing Check trying to catch malformed graph is not robust when\nan op is registered with an expected number of inputs but has data edges\nbeyond this.\n\nPiperOrigin-RevId: 266826557",
    "changes": [
        {
            "name": "graph.cc",
            "path": "tensorflow/core/graph/graph.cc",
            "patches": [
                {
                    "old_start": 678,
                    "old_length": 6,
                    "new_start": 678,
                    "new_length": 10,
                    "hunk_buggy": "['       if (edge->IsControlEdge()) {\\n', '         inputs.push_back(edge);\\n', '       } else {\\n', '         CHECK(inputs[edge->dst_input()] == nullptr)\\n', '             << \"Edge \" << edge->src()->DebugString() << \":\"\\n', '             << edge->dst()->DebugString() << \" with dst_input \"']",
                    "hunk_fix": "@@ -678,6 +678,10 @@ void Graph::ToGraphDefSubRange(GraphDef* graph_def, int from_node_id) const {\n       if (edge->IsControlEdge()) {\n         inputs.push_back(edge);\n       } else {\n+        DCHECK(edge->dst_input() < inputs.size())\n+            << \"Edge \" << edge->DebugString()\n+            << \" is overflowing the expected number of inputs (\"\n+            << node->num_inputs() << \") for node \" << node->DebugString();\n         CHECK(inputs[edge->dst_input()] == nullptr)\n             << \"Edge \" << edge->src()->DebugString() << \":\"\n             << edge->dst()->DebugString() << \" with dst_input \""
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 224,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/e009644f034fa0ca4df910a812432cab3458d440",
    "date": "2019-08-16T00:20:41-04:00",
    "message": "Add one error check in cuda_dnn for int8 to float convolution.",
    "changes": [
        {
            "name": "cuda_dnn.cc",
            "path": "tensorflow/stream_executor/cuda/cuda_dnn.cc",
            "patches": [
                {
                    "old_start": 2934,
                    "old_length": 6,
                    "new_start": 2934,
                    "new_length": 16,
                    "hunk_buggy": "['           \"This configuration has potential integer overflow in \"\\n', '           \"cuDNNv5 and cuDNNv6. See b/68264959.\");\\n', '     }\\n', '     return port::Status::OK();\\n', '   };\\n', ' ']",
                    "hunk_fix": "@@ -2934,6 +2934,16 @@ port::Status CudnnSupport::DoConvolve(\n           \"This configuration has potential integer overflow in \"\n           \"cuDNNv5 and cuDNNv6. See b/68264959.\");\n     }\n+    if (CUDNN_VERSION < 8000) {\n+      if (algorithm_desc.algo_id() ==\n+              CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM &&\n+          ToCudnnDataType(element_type) == CUDNN_DATA_INT8 &&\n+          ToCudnnDataType(output_type) == CUDNN_DATA_FLOAT) {\n+        return port::Status(\n+            port::error::FAILED_PRECONDITION,\n+            \"This configuration potentially produces incorrect results.\");\n+      }\n+    }\n     return port::Status::OK();\n   };\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 225,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/b86513673b98ac6c4458033fcda718365539afae",
    "date": "2019-08-13T20:32:38+02:00",
    "message": "added check for zero stride values to strided slice",
    "changes": [
        {
            "name": "model_builder.cc",
            "path": "tensorflow/lite/delegates/gpu/common/model_builder.cc",
            "patches": [
                {
                    "old_start": 1807,
                    "old_length": 6,
                    "new_start": 1807,
                    "new_length": 9,
                    "hunk_buggy": "['       RETURN_IF_ERROR(\\n', '           ReadAttribsWithBatch(reader, tf_options, input->tensor.shape, &attr));\\n', '     }\\n', '     if (attr.strides.h < 0 || attr.strides.w < 0 || attr.strides.c < 0) {\\n', '       return UnimplementedError(\"Reverse slices are not supported.\");\\n', '     }']",
                    "hunk_fix": "@@ -1807,6 +1807,9 @@ class StridedSliceOperationParser : public TFLiteOperationParser {\n       RETURN_IF_ERROR(\n           ReadAttribsWithBatch(reader, tf_options, input->tensor.shape, &attr));\n     }\n+    if (attr.strides.h == 0 || attr.strides.w == 0 || attr.strides.c == 0) {\n+      return InvalidArgumentError(\"stride values must be non-zero\");\n+    }\n     if (attr.strides.h < 0 || attr.strides.w < 0 || attr.strides.c < 0) {\n       return UnimplementedError(\"Reverse slices are not supported.\");\n     }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 226,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/a607eb012b1bc4f6dbe263ad99caa76d84ae3ab2",
    "date": "2019-08-12T19:38:29+02:00",
    "message": "fix output shape check for strided slice always failing when stride != 1",
    "changes": [
        {
            "name": "model_builder.cc",
            "path": "tensorflow/lite/delegates/gpu/common/model_builder.cc",
            "patches": [
                {
                    "old_start": 1810,
                    "old_length": 13,
                    "new_start": 1810,
                    "new_length": 16,
                    "hunk_buggy": "['     if (attr.strides.h < 0 || attr.strides.w < 0 || attr.strides.c < 0) {\\n', '       return UnimplementedError(\"Reverse slices are not supported.\");\\n', '     }\\n', '-    if (attr.ends.h - attr.starts.h != out_shape.h) {\\n', '       return UnimplementedError(\"Output height doesn\\'t match\");\\n', '     }\\n', '-    if (attr.ends.w - attr.starts.w != out_shape.w) {\\n', '       return UnimplementedError(\"Output width doesn\\'t match\");\\n', '     }\\n', '-    if (attr.ends.c - attr.starts.c != out_shape.c) {\\n', '       return UnimplementedError(\"Output channels don\\'t match\");\\n', '     }\\n', '     node->operation.attributes = attr;']",
                    "hunk_fix": "@@ -1810,13 +1810,16 @@ class StridedSliceOperationParser : public TFLiteOperationParser {\n     if (attr.strides.h < 0 || attr.strides.w < 0 || attr.strides.c < 0) {\n       return UnimplementedError(\"Reverse slices are not supported.\");\n     }\n-    if (attr.ends.h - attr.starts.h != out_shape.h) {\n+    if ((attr.ends.h - attr.starts.h + attr.strides.h - 1) / attr.strides.h !=\n+        out_shape.h) {\n       return UnimplementedError(\"Output height doesn't match\");\n     }\n-    if (attr.ends.w - attr.starts.w != out_shape.w) {\n+    if ((attr.ends.w - attr.starts.w + attr.strides.w - 1) / attr.strides.w !=\n+        out_shape.w) {\n       return UnimplementedError(\"Output width doesn't match\");\n     }\n-    if (attr.ends.c - attr.starts.c != out_shape.c) {\n+    if ((attr.ends.c - attr.starts.c + attr.strides.c - 1) / attr.strides.c !=\n+        out_shape.c) {\n       return UnimplementedError(\"Output channels don't match\");\n     }\n     node->operation.attributes = attr;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 227,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/b234ff0ee4ce87d21a3e5306b678e1fb4b1fedfc",
    "date": "2019-08-06T09:59:40-07:00",
    "message": "Fixed division by zero, by checking the number of GPUs in GenericLayoutOptimizer.\n\nPiperOrigin-RevId: 261934091",
    "changes": [
        {
            "name": "generic_layout_optimizer.cc",
            "path": "tensorflow/core/grappler/optimizers/generic_layout_optimizer.cc",
            "patches": [
                {
                    "old_start": 99,
                    "old_length": 6,
                    "new_start": 99,
                    "new_length": 8,
                    "hunk_buggy": "['     }\\n', '   }\\n', ' \\n', '   return (static_cast<float>(num_conv2d_gpu_fp16) /\\n', '           static_cast<float>(num_conv2d_gpu)) >= kConv2DGPUFP16Threshold;\\n', ' }']",
                    "hunk_fix": "@@ -99,6 +99,8 @@ inline bool NumConv2DOnDeviceWithDataTypeOverThreshold(\n     }\n   }\n \n+  if (num_conv2d_gpu == 0) return false;\n+\n   return (static_cast<float>(num_conv2d_gpu_fp16) /\n           static_cast<float>(num_conv2d_gpu)) >= kConv2DGPUFP16Threshold;\n }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 228,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/9baa064387b0a114c3fcec88abaa0568834e8e34",
    "date": "2019-08-03T07:37:29-07:00",
    "message": "Only apply check for non-tensor case\n\nPiperOrigin-RevId: 261481490",
    "changes": [
        {
            "name": "array_ops.py",
            "path": "tensorflow/python/ops/array_ops.py",
            "patches": [
                {
                    "old_start": 2807,
                    "old_length": 11,
                    "new_start": 2807,
                    "new_length": 11,
                    "hunk_buggy": "['   if mode == \"CONSTANT\":\\n', '     # TODO(rjryan): Once the forward compatibility period (3 weeks) have passed\\n', '     # remove the \"Pad\" fallback here.\\n', '-    if constant_values != 0:\\n', '       result = gen_array_ops.pad_v2(\\n', '           tensor, paddings, constant_values, name=name)\\n', '-    else:\\n', '-      result = gen_array_ops.pad(tensor, paddings, name=name)\\n', '   elif mode == \"REFLECT\":\\n', '     result = gen_array_ops.mirror_pad(\\n', '         tensor, paddings, mode=\"REFLECT\", name=name)']",
                    "hunk_fix": "@@ -2807,11 +2807,11 @@ def pad(tensor, paddings, mode=\"CONSTANT\", name=None, constant_values=0):  # pyl\n   if mode == \"CONSTANT\":\n     # TODO(rjryan): Once the forward compatibility period (3 weeks) have passed\n     # remove the \"Pad\" fallback here.\n-    if constant_values != 0:\n+    if not tensor_util.is_tensor(constant_values) and constant_values == 0:\n+      result = gen_array_ops.pad(tensor, paddings, name=name)\n+    else:\n       result = gen_array_ops.pad_v2(\n           tensor, paddings, constant_values, name=name)\n-    else:\n-      result = gen_array_ops.pad(tensor, paddings, name=name)\n   elif mode == \"REFLECT\":\n     result = gen_array_ops.mirror_pad(\n         tensor, paddings, mode=\"REFLECT\", name=name)"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 229,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/9c14f6ba30d96241978188998de47a388822365f",
    "date": "2019-08-02T17:44:03-07:00",
    "message": "Only apply check for non-tensor case\n\nPiperOrigin-RevId: 261408908",
    "changes": [
        {
            "name": "backend.py",
            "path": "tensorflow/python/keras/backend.py",
            "patches": [
                {
                    "old_start": 4031,
                    "old_length": 17,
                    "new_start": 4031,
                    "new_length": 19,
                    "hunk_buggy": "['   if training is None:\\n', '     training = learning_phase()\\n', ' \\n', '-  if training == 1 or training is True:\\n', '-    if callable(x):\\n', '-      return x()\\n', '-    else:\\n', '-      return x\\n', ' \\n', '-  elif training == 0 or training is False:\\n', '-    if callable(alt):\\n', '-      return alt()\\n', '-    else:\\n', '-      return alt\\n', ' \\n', '   # else: assume learning phase is a placeholder tensor.\\n', '   x = switch(training, x, alt)']",
                    "hunk_fix": "@@ -4031,17 +4031,19 @@ def in_train_phase(x, alt, training=None):\n   if training is None:\n     training = learning_phase()\n \n-  if training == 1 or training is True:\n-    if callable(x):\n-      return x()\n-    else:\n-      return x\n+  # TODO(b/138862903): Handle the case when training is tensor.\n+  if not tensor_util.is_tensor(training):\n+    if training == 1 or training is True:\n+      if callable(x):\n+        return x()\n+      else:\n+        return x\n \n-  elif training == 0 or training is False:\n-    if callable(alt):\n-      return alt()\n-    else:\n-      return alt\n+    elif training == 0 or training is False:\n+      if callable(alt):\n+        return alt()\n+      else:\n+        return alt\n \n   # else: assume learning phase is a placeholder tensor.\n   x = switch(training, x, alt)"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 230,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/30bd9d5bcc64097d21872486a5726d756ed7067b",
    "date": "2019-07-31T15:32:21-07:00",
    "message": "Explicitly handle Tensors in start & stop\n\nThe current check was doing a identity check in order to handle both\ntensors and integers. This becomes problematic when enabling tensor\nequality. Instead we explicitly check for Tensor type and only compare\nwith sys.maxsize for non-Tensors.\n\nPiperOrigin-RevId: 261004200",
    "changes": [
        {
            "name": "array_ops.py",
            "path": "tensorflow/python/ops/array_ops.py",
            "patches": [
                {
                    "old_start": 704,
                    "old_length": 13,
                    "new_start": 704,
                    "new_length": 15,
                    "hunk_buggy": "[\"       # python doesn't always use None when constructing ranges\\n\", '       # for example a[:] gives slice(None,sys.maxsize,None)\\n', '       # whereas a[::1] gives slice(None,None,None)\\n', '-      if s.start is not None and s.start is not sys.maxsize:\\n', '         _check_index(s.start)\\n', '         begin.append(s.start)\\n', '       else:\\n', '         begin.append(0)\\n', '         begin_mask |= (1 << index)\\n', '-      if s.stop is not None and s.stop != sys.maxsize:\\n', '         _check_index(s.stop)\\n', '         end.append(s.stop)\\n', '       else:']",
                    "hunk_fix": "@@ -704,13 +704,15 @@ def _slice_helper(tensor, slice_spec, var=None):\n       # python doesn't always use None when constructing ranges\n       # for example a[:] gives slice(None,sys.maxsize,None)\n       # whereas a[::1] gives slice(None,None,None)\n-      if s.start is not None and s.start is not sys.maxsize:\n+      if s.start is not None and (isinstance(s.start, ops.Tensor) or\n+                                  s.start != sys.maxsize):\n         _check_index(s.start)\n         begin.append(s.start)\n       else:\n         begin.append(0)\n         begin_mask |= (1 << index)\n-      if s.stop is not None and s.stop != sys.maxsize:\n+      if s.stop is not None and (isinstance(s.stop, ops.Tensor) or\n+                                 s.stop != sys.maxsize):\n         _check_index(s.stop)\n         end.append(s.stop)\n       else:"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 231,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/89fa1ae2cb34eab0e6137e72e6fab01f6c5bc164",
    "date": "2019-07-26T15:53:20+03:00",
    "message": "Fix check for cloning FunctionLibraryRuntime",
    "changes": [
        {
            "name": "function.cc",
            "path": "tensorflow/core/common_runtime/function.cc",
            "patches": [
                {
                    "old_start": 1246,
                    "old_length": 7,
                    "new_start": 1246,
                    "new_length": 7,
                    "hunk_buggy": "['       env_, graph_def_version_, optimizer_.options(), custom_kernel_creator_,\\n', '       out_lib_def, out_pflr, skip_flib_def));\\n', '   *out_flr = (*out_pflr)->GetFLR(device_->name());\\n', '-  if (out_flr != nullptr) {\\n', '     return Status::OK();\\n', '   } else {\\n', '     return errors::Internal(\"Cloning FunctionLibraryRuntime failed.\");']",
                    "hunk_fix": "@@ -1246,7 +1246,7 @@ Status FunctionLibraryRuntimeImpl::Clone(\n       env_, graph_def_version_, optimizer_.options(), custom_kernel_creator_,\n       out_lib_def, out_pflr, skip_flib_def));\n   *out_flr = (*out_pflr)->GetFLR(device_->name());\n-  if (out_flr != nullptr) {\n+  if (*out_flr != nullptr) {\n     return Status::OK();\n   } else {\n     return errors::Internal(\"Cloning FunctionLibraryRuntime failed.\");"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 232,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/150a6c06b281246cb5a075a704fceeb257bb63af",
    "date": "2019-07-23T19:53:07-07:00",
    "message": "Add a check on the 0th dimension of filter for DepthwiseConv.\n\nPiperOrigin-RevId: 259662414",
    "changes": [
        {
            "name": "depthwise_conv.cc",
            "path": "tensorflow/lite/kernels/depthwise_conv.cc",
            "patches": [
                {
                    "old_start": 113,
                    "old_length": 6,
                    "new_start": 113,
                    "new_length": 8,
                    "hunk_buggy": "['                               data_type == kTfLiteInt8);\\n', '   TF_LITE_ENSURE_EQ(context, output->type, data_type);\\n', '   TF_LITE_ENSURE_EQ(context, filter->type, data_type);\\n', ' \\n', '   if (hasBias) {\\n', '     bias = GetInput(context, node, kBiasTensor);']",
                    "hunk_fix": "@@ -113,6 +113,8 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n                               data_type == kTfLiteInt8);\n   TF_LITE_ENSURE_EQ(context, output->type, data_type);\n   TF_LITE_ENSURE_EQ(context, filter->type, data_type);\n+  // Filter in DepthwiseConv is expected to be [1, H, W, O].\n+  TF_LITE_ENSURE_EQ(context, SizeOfDimension(filter, 0), 1);\n \n   if (hasBias) {\n     bias = GetInput(context, node, kBiasTensor);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 233,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/3a7b36bca7f43ce4f0d0791ce0e0d84ece8683d9",
    "date": "2019-07-23T08:12:53-07:00",
    "message": "[Grappler] Remove DCHECK from a MutableGraphView CanDedupControlWithRegularInput check.\n\nPiperOrigin-RevId: 259537618",
    "changes": [
        {
            "name": "mutable_graph_view.cc",
            "path": "tensorflow/core/grappler/mutable_graph_view.cc",
            "patches": [
                {
                    "old_start": 89,
                    "old_length": 8,
                    "new_start": 89,
                    "new_length": 9,
                    "hunk_buggy": "[' bool CanDedupControlWithRegularInput(const MutableGraphView& graph,\\n', '                                      absl::string_view control_node_name) {\\n', '   NodeDef* control_node = graph.GetNode(control_node_name);\\n', '-  DCHECK(control_node != nullptr)\\n', '-      << \"Didn\\'t find a node for control dependency: \" << control_node_name;\\n', '   return CanDedupControlWithRegularInput(graph, *control_node);\\n', ' }\\n', ' ']",
                    "hunk_fix": "@@ -89,8 +89,9 @@ bool CanDedupControlWithRegularInput(const MutableGraphView& graph,\n bool CanDedupControlWithRegularInput(const MutableGraphView& graph,\n                                      absl::string_view control_node_name) {\n   NodeDef* control_node = graph.GetNode(control_node_name);\n-  DCHECK(control_node != nullptr)\n-      << \"Didn't find a node for control dependency: \" << control_node_name;\n+  if (control_node == nullptr) {\n+    return false;\n+  }\n   return CanDedupControlWithRegularInput(graph, *control_node);\n }\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 234,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/467730fe90282a75f15f67d701b278e86cfad65e",
    "date": "2019-06-23T01:53:59+00:00",
    "message": "Fix dimension check for tf.keras.losses.BinaryCrossentropy\n\nThis fix tries to address the issue raised in 30040 where\ntf.keras.losses.BinaryCrossentropy does not check the dimension match:\n```\nimport numpy as np\nimport tensorflow as tf\n\ny_true = np.array([[1.], [1.], [1.], [0.], [1.], [0.], [0.], [1.], [1.], [0.]]).astype(np.float32)\ny_pred = np.array([[0.], [0.], [0.], [1.], [1.], [0.], [0.], [1.], [0.], [1.]]).astype(np.float32)\nbce = tf.keras.losses.BinaryCrossentropy()\nprint(bce(np.squeeze(y_true), y_pred).numpy()) # should fail\n```\nThe reason was that broadcasting was applied directly.\nThis fix adds dimension check to throw an error if there is a mismatch.\n\nThis fix fixes 30040.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "changes": [
        {
            "name": "backend.py",
            "path": "tensorflow/python/keras/backend.py",
            "patches": [
                {
                    "old_start": 4313,
                    "old_length": 6,
                    "new_start": 4313,
                    "new_length": 12,
                    "hunk_buggy": "['   if not from_logits:\\n', '     if (isinstance(output, (ops.EagerTensor, variables_module.Variable)) or\\n', \"         output.op.type != 'Sigmoid'):\\n\", '       epsilon_ = _constant_to_tensor(epsilon(), output.dtype.base_dtype)\\n', '       output = clip_ops.clip_by_value(output, epsilon_, 1. - epsilon_)\\n', ' ']",
                    "hunk_fix": "@@ -4313,6 +4313,12 @@ def binary_crossentropy(target, output, from_logits=False):\n   if not from_logits:\n     if (isinstance(output, (ops.EagerTensor, variables_module.Variable)) or\n         output.op.type != 'Sigmoid'):\n+      try:\n+        target.get_shape().merge_with(output.get_shape())\n+      except ValueError:\n+        raise ValueError(\n+            \"target and output must have the same shape (%s vs %s)\" %\n+            (target.get_shape(), output.get_shape()))\n       epsilon_ = _constant_to_tensor(epsilon(), output.dtype.base_dtype)\n       output = clip_ops.clip_by_value(output, epsilon_, 1. - epsilon_)\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 235,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/1c49c13ba59961cf7581e3e29b951db8faca94f5",
    "date": "2019-06-17T02:56:00-07:00",
    "message": "Add type check for reduction axis in reducer operation.\n\nPiperOrigin-RevId: 253543782",
    "changes": [
        {
            "name": "reduce.cc",
            "path": "tensorflow/lite/kernels/reduce.cc",
            "patches": [
                {
                    "old_start": 211,
                    "old_length": 6,
                    "new_start": 211,
                    "new_length": 7,
                    "hunk_buggy": "['   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\\n', ' \\n', '   OpContext op_context(context, node);\\n', '   TF_LITE_ENSURE_OK(context, InitializeTemporaries(context, node, &op_context));\\n', ' \\n', '   TfLiteTensor* resolved_axis = GetTemporary(context, node, /*index=*/1);']",
                    "hunk_fix": "@@ -211,6 +211,7 @@ TfLiteStatus PrepareSimple(TfLiteContext* context, TfLiteNode* node) {\n   TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);\n \n   OpContext op_context(context, node);\n+  TF_LITE_ENSURE_TYPES_EQ(context, op_context.axis->type, kTfLiteInt32);\n   TF_LITE_ENSURE_OK(context, InitializeTemporaries(context, node, &op_context));\n \n   TfLiteTensor* resolved_axis = GetTemporary(context, node, /*index=*/1);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 236,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/43a8963c73718f97a4425722a65b611d2ef0b69f",
    "date": "2019-05-23T16:04:02-07:00",
    "message": "Added non-negative check for n.",
    "changes": [
        {
            "name": "dct_ops.py",
            "path": "tensorflow/python/ops/signal/dct_ops.py",
            "patches": [
                {
                    "old_start": 32,
                    "old_length": 6,
                    "new_start": 32,
                    "new_length": 8,
                    "hunk_buggy": "['   \"\"\"Checks that DCT/IDCT arguments are compatible and well formed.\"\"\"\\n', '   if axis != -1:\\n', '     raise NotImplementedError(\"axis must be -1. Got: %s\" % axis)\\n', '   if dct_type not in (1, 2, 3):\\n', '     raise ValueError(\"Only Types I, II and III (I)DCT are supported.\")\\n', '   if dct_type == 1:\\n']",
                    "hunk_fix": "@@ -32,6 +32,8 @@ def _validate_dct_arguments(input_tensor, dct_type, n, axis, norm):\n   \"\"\"Checks that DCT/IDCT arguments are compatible and well formed.\"\"\"\n   if axis != -1:\n     raise NotImplementedError(\"axis must be -1. Got: %s\" % axis)\n+  if n is not None and n < 1:\n+    raise ValueError(\"n should be an integer greater than 1 or None\")\n   if dct_type not in (1, 2, 3):\n     raise ValueError(\"Only Types I, II and III (I)DCT are supported.\")\n   if dct_type == 1:\n"
                },
                {
                    "old_start": 81,
                    "old_length": 7,
                    "new_start": 83,
                    "new_length": 8,
                    "hunk_buggy": "[' \\n', '   Raises:\\n', '     ValueError: If `type` is not `1`, `2` or `3`, `axis` is\\n', \"-      not `-1`, or `norm` is not `None` or `'ortho'`.\\n\", '     ValueError: If `type` is `1` and `norm` is `ortho`.\\n', ' \\n', '   [dct]: https://en.wikipedia.org/wiki/Discrete_cosine_transform']",
                    "hunk_fix": "@@ -81,7 +83,8 @@ def dct(input, type=2, n=None, axis=-1, norm=None, name=None):  # pylint: disabl\n \n   Raises:\n     ValueError: If `type` is not `1`, `2` or `3`, `axis` is\n-      not `-1`, or `norm` is not `None` or `'ortho'`.\n+      not `-1`, `n` is not `None` or greater than 0, \n+      or `norm` is not `None` or `'ortho'`.\n     ValueError: If `type` is `1` and `norm` is `ortho`.\n \n   [dct]: https://en.wikipedia.org/wiki/Discrete_cosine_transform"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 237,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/1ff493ed1a2059f82f7607a7f0a0aa2ce8d5a542",
    "date": "2019-05-03T12:05:12-07:00",
    "message": "Replace a defensive check with TF_RET_CHECK\n\nPiperOrigin-RevId: 246544023",
    "changes": [
        {
            "name": "device_util.cc",
            "path": "tensorflow/compiler/jit/device_util.cc",
            "patches": [
                {
                    "old_start": 94,
                    "old_length": 10,
                    "new_start": 94,
                    "new_length": 8,
                    "hunk_buggy": "[' \\n', '   absl::flat_hash_set<absl::string_view> device_names_set;\\n', '   for (absl::string_view device_name : device_names) {\\n', '-    if (!device_name.empty()) {\\n', '-      // TODO(sanjoy): Figure out if this is necessary.\\n', '-      device_names_set.insert(device_name);\\n', '-    }\\n', '   }\\n', ' \\n', '   absl::optional<absl::string_view> maybe_gpu_device;']",
                    "hunk_fix": "@@ -94,10 +94,8 @@ Status PickDeviceForXlaImpl(absl::Span<const string> device_names,\n \n   absl::flat_hash_set<absl::string_view> device_names_set;\n   for (absl::string_view device_name : device_names) {\n-    if (!device_name.empty()) {\n-      // TODO(sanjoy): Figure out if this is necessary.\n-      device_names_set.insert(device_name);\n-    }\n+    TF_RET_CHECK(!device_name.empty());\n+    device_names_set.insert(device_name);\n   }\n \n   absl::optional<absl::string_view> maybe_gpu_device;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 238,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/867a918bd3d40afeca6b96430671a098134e7905",
    "date": "2019-05-01T19:18:07-07:00",
    "message": "CUDA Driver: do better error reporting if checking the pointer properties failed.\n\nThere are many reasons why an operation can fail, propagate the error instead\nof assuming the cause.\n\nPiperOrigin-RevId: 246226334",
    "changes": [
        {
            "name": "cuda_driver.cc",
            "path": "tensorflow/stream_executor/cuda/cuda_driver.cc",
            "patches": [
                {
                    "old_start": 172,
                    "old_length": 6,
                    "new_start": 172,
                    "new_length": 9,
                    "hunk_buggy": "['   cudaPointerAttributes attributes;\\n', '   cudaError_t err =\\n', '       cudaPointerGetAttributes(&attributes, reinterpret_cast<const void*>(ptr));\\n', '   // If we failed, reset cuda error status to avoid poisoning cuda streams.\\n', '   if (err != cudaSuccess) cudaGetLastError();\\n', '   bool points_to_host_memory = (err == cudaErrorInvalidValue ||']",
                    "hunk_fix": "@@ -172,6 +172,9 @@ void CheckPointerIsValid(const PtrT ptr, absl::string_view name) {\n   cudaPointerAttributes attributes;\n   cudaError_t err =\n       cudaPointerGetAttributes(&attributes, reinterpret_cast<const void*>(ptr));\n+  CHECK(err == cudaSuccess || err == cudaErrorInvalidValue)\n+      << \"Unexpected CUDA error: \" << cudaGetErrorString(err);\n+\n   // If we failed, reset cuda error status to avoid poisoning cuda streams.\n   if (err != cudaSuccess) cudaGetLastError();\n   bool points_to_host_memory = (err == cudaErrorInvalidValue ||"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 239,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/3acc8eaf602b3e9a009f54e1e0164644dd793831",
    "date": "2019-04-28T01:06:23-07:00",
    "message": "Add sanity check for resize-bilinear input shape.\n\nPiperOrigin-RevId: 245618186",
    "changes": [
        {
            "name": "resize_bilinear.cc",
            "path": "tensorflow/lite/kernels/resize_bilinear.cc",
            "patches": [
                {
                    "old_start": 40,
                    "old_length": 9,
                    "new_start": 40,
                    "new_length": 12,
                    "hunk_buggy": "['                                 const TfLiteTensor* input,\\n', '                                 const TfLiteTensor* size,\\n', '                                 TfLiteTensor* output) {\\n', '   TfLiteIntArray* output_size = TfLiteIntArrayCreate(4);\\n', '   output_size->data[0] = input->dims->data[0];\\n', '-  const int32* size_data = GetTensorData<int32>(size);\\n', '   output_size->data[1] = size_data[0];\\n', '   output_size->data[2] = size_data[1];\\n', '   output_size->data[3] = input->dims->data[3];']",
                    "hunk_fix": "@@ -40,9 +40,12 @@ TfLiteStatus ResizeOutputTensor(TfLiteContext* context,\n                                 const TfLiteTensor* input,\n                                 const TfLiteTensor* size,\n                                 TfLiteTensor* output) {\n+  const int32* size_data = GetTensorData<int32>(size);\n+  // Sanity check, the up/down sampling size should always be positive.\n+  TF_LITE_ENSURE(context, size_data[0] > 0);\n+  TF_LITE_ENSURE(context, size_data[1] > 0);\n   TfLiteIntArray* output_size = TfLiteIntArrayCreate(4);\n   output_size->data[0] = input->dims->data[0];\n-  const int32* size_data = GetTensorData<int32>(size);\n   output_size->data[1] = size_data[0];\n   output_size->data[2] = size_data[1];\n   output_size->data[3] = input->dims->data[3];"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 240,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/f54bc43f1117004208df6da34e422bf628fc3c23",
    "date": "2019-04-25T14:51:27-07:00",
    "message": "Update error message when tf.functions cross merge_call boundary when using MirroredStrategy\n\nPiperOrigin-RevId: 245302699",
    "changes": [
        {
            "name": "mirrored_strategy.py",
            "path": "tensorflow/python/distribute/mirrored_strategy.py",
            "patches": [
                {
                    "old_start": 957,
                    "old_length": 20,
                    "new_start": 957,
                    "new_length": 20,
                    "hunk_buggy": "['     # `tf.function` and there is a merge_call in `fn`. This breaks because each\\n', '     # thread tries to create a distinct tf.function. Each tf.function creation\\n', '     # takes a lock, and so if there is a merge call in the middle, the lock is\\n', '-    # never releases and subsequent replica threads cannot proceed to define\\n', '     # their own functions. Checking for the graph being the same is one way for\\n', \"     # us to check this didn't happen.\\n\", '     if ops.get_default_graph() != t.graph:\\n', '       raise RuntimeError(\\n', '-          \"`merge_call` called while defining a new graph. \"\\n', '-          \"This can happen if the function `fn` passed to \"\\n', '-          \"`strategy.experimental_run()` or \"\\n', '-          \"`strategy.extended.call_for_each_replica()` is decorated with \"\\n', '-          \"`@tf.function`. In this case, wrap the call to \"\\n', '-          \"`strategy.experimental_run()` or \"\\n', '-          \"`strategy.extended.call_for_each_replica()` with `@tf.function` \"\\n', '-          \"instead of `fn`. This will avoid mismatching graphs and also \"\\n', '-          \"improve performance.\")\\n', ' \\n', '     t.has_paused.set()\\n', '     t.should_run.wait()']",
                    "hunk_fix": "@@ -957,20 +957,20 @@ class MirroredReplicaContext(distribute_lib.ReplicaContext):\n     # `tf.function` and there is a merge_call in `fn`. This breaks because each\n     # thread tries to create a distinct tf.function. Each tf.function creation\n     # takes a lock, and so if there is a merge call in the middle, the lock is\n-    # never releases and subsequent replica threads cannot proceed to define\n+    # never released and subsequent replica threads cannot proceed to define\n     # their own functions. Checking for the graph being the same is one way for\n     # us to check this didn't happen.\n     if ops.get_default_graph() != t.graph:\n       raise RuntimeError(\n-          \"`merge_call` called while defining a new graph. \"\n-          \"This can happen if the function `fn` passed to \"\n-          \"`strategy.experimental_run()` or \"\n-          \"`strategy.extended.call_for_each_replica()` is decorated with \"\n-          \"`@tf.function`. In this case, wrap the call to \"\n-          \"`strategy.experimental_run()` or \"\n-          \"`strategy.extended.call_for_each_replica()` with `@tf.function` \"\n-          \"instead of `fn`. This will avoid mismatching graphs and also \"\n-          \"improve performance.\")\n+          \"`merge_call` called while defining a new graph or a tf.function. \"\n+          \"This can often happen if the function `fn` passed to \"\n+          \"`strategy.experimental_run()` is decorated with \"\n+          \"`@tf.function` (or contains a nested `@tf.function`), and `fn` \"\n+          \"contains a synchronization point, such as aggregating gradients. \"\n+          \"This behavior is not yet supported. Instead, please wrap the entire \"\n+          \"call `strategy.experimental_run(fn)` in a `@tf.function`, and avoid \"\n+          \"nested `tf.function`s that may potentially cross a synchronization \"\n+          \"boundary.\")\n \n     t.has_paused.set()\n     t.should_run.wait()"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 241,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/7008e41f183ae9de3f4656067932b36afa822ef2",
    "date": "2019-04-22T19:11:37-07:00",
    "message": "Fix the check for empty reduction indices\n\n- In the general case indices can be any rank.",
    "changes": [
        {
            "name": "constant_folding.cc",
            "path": "tensorflow/core/grappler/optimizers/constant_folding.cc",
            "patches": [
                {
                    "old_start": 2396,
                    "old_length": 8,
                    "new_start": 2396,
                    "new_length": 13,
                    "hunk_buggy": "['   }\\n', '   const TensorProto& reduction_indices_tensor =\\n', '       reductions_indices->attr().at(\"value\").tensor();\\n', '-  *indices_is_empty =\\n', '-      reduction_indices_tensor.tensor_shape().dim(0).size() == 0;\\n', '   return true;\\n', ' }\\n', ' ']",
                    "hunk_fix": "@@ -2396,8 +2396,13 @@ bool ConstantFolding::IsReductionWithConstantIndices(\n   }\n   const TensorProto& reduction_indices_tensor =\n       reductions_indices->attr().at(\"value\").tensor();\n-  *indices_is_empty =\n-      reduction_indices_tensor.tensor_shape().dim(0).size() == 0;\n+  *indices_is_empty = false;\n+  for (const auto& dim : reduction_indices_tensor.tensor_shape().dim()) {\n+    if (dim.size() == 0) {\n+      *indices_is_empty = true;\n+      break;\n+    }\n+  }\n   return true;\n }\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 242,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/236660d0fccff6f59f29a1936dc731d783722e28",
    "date": "2019-03-29T15:32:56-07:00",
    "message": "[XLA:GPU] Fix host conv checker canonicalization for f16 and nans.\n\nThe GPU-side checker is correct, but the host-side checker was canonicalizing\nnan to F16_MAX.  The effect of this is that you'd get a \"conv mismatch!\" error\nbut no description of exactly what mismatched.\n\nPiperOrigin-RevId: 241062942",
    "changes": [
        {
            "name": "buffer_comparator.cc",
            "path": "tensorflow/compiler/xla/service/gpu/buffer_comparator.cc",
            "patches": [
                {
                    "old_start": 251,
                    "old_length": 6,
                    "new_start": 251,
                    "new_length": 9,
                    "hunk_buggy": "['   const auto canonicalize = [](ComparisonType a) -> ComparisonType {\\n', '     if (std::is_same<ElementType, Eigen::half>::value && a) {\\n', '       constexpr float kMaxFp16Value = 65504.;\\n', '       if (a < 0) {\\n', '         return -(kMaxFp16Value + 1);\\n', '       }']",
                    "hunk_fix": "@@ -251,6 +251,9 @@ Status HostCompare(se::Stream* stream, se::DeviceMemoryBase lhs,\n   const auto canonicalize = [](ComparisonType a) -> ComparisonType {\n     if (std::is_same<ElementType, Eigen::half>::value && a) {\n       constexpr float kMaxFp16Value = 65504.;\n+      if (std::isnan(a)) {\n+        return a;\n+      }\n       if (a < 0) {\n         return -(kMaxFp16Value + 1);\n       }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 243,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/04b97cde86550995da57d16d81084006456ccce5",
    "date": "2019-03-26T02:42:20+00:00",
    "message": "Fix segmentation fault with tf.stack an keras's Input in TF2.0\n\nThis fix tries to address the issue raised in 26879 where\ntf.stack encounters segmantation fault with keras's Input in TF2.0:\n```\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input\n\nprint(tf.__version__)\n\ninput_ = Input((128, 128, 1), dtype='float32')\nprint(input_)\noutput = tf.stack(input_, axis=1)\n```\n\nThe issue is that in eager wrap, `PySequence_Fast_GET_ITEM`\ntries to access an object not through `PySequence_Fast`.\n\nThis fix adds the `PySequence_Fast` and checks the return value\nto make sure it is not nullptr.\n\nThis fix fixes 26879.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "changes": [
        {
            "name": "pywrap_tfe_src.cc",
            "path": "tensorflow/python/eager/pywrap_tfe_src.cc",
            "patches": [
                {
                    "old_start": 1838,
                    "old_length": 8,
                    "new_start": 1838,
                    "new_length": 12,
                    "hunk_buggy": "['                 << item->ob_type->tp_name;\\n', '         return false;\\n', '       }\\n', '-      for (Py_ssize_t j = 0; j < PySequence_Fast_GET_SIZE(item); j++) {\\n', '-        PyObject* inner_item = PySequence_Fast_GET_ITEM(item, j);\\n', '         if (!CheckOneInput(inner_item)) {\\n', '           VLOG(1) << \"Falling back to slow path for Op \\\\\"\" << op_def.name()\\n', '                   << \"\\\\\", Input \\\\\"\" << op_def.input_arg(i).name()']",
                    "hunk_fix": "@@ -1838,8 +1838,12 @@ bool CheckInputsOk(PyObject* seq, int start_index,\n                 << item->ob_type->tp_name;\n         return false;\n       }\n-      for (Py_ssize_t j = 0; j < PySequence_Fast_GET_SIZE(item); j++) {\n-        PyObject* inner_item = PySequence_Fast_GET_ITEM(item, j);\n+      tensorflow::Safe_PyObjectPtr fast_item(PySequence_Fast(item, \"Could not parse sequence.\"));\n+      if (fast_item.get() == nullptr) {\n+        return false;\n+      }\n+      for (Py_ssize_t j = 0; j < PySequence_Fast_GET_SIZE(fast_item.get()); j++) {\n+        PyObject* inner_item = PySequence_Fast_GET_ITEM(fast_item.get(), j);\n         if (!CheckOneInput(inner_item)) {\n           VLOG(1) << \"Falling back to slow path for Op \\\"\" << op_def.name()\n                   << \"\\\", Input \\\"\" << op_def.input_arg(i).name()"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 244,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/31849c61e0432009baabdfafc2ec1a1aed1a40e8",
    "date": "2019-03-22T12:14:52-07:00",
    "message": "Small change in tf.nn.sufficient_statistics to guard against unknown shapes.\n\nUse is_fully_defined instead of checking shape.dims[d] as the dims variable may be None, if the rank is unknown.\n\nPiperOrigin-RevId: 239829841",
    "changes": [
        {
            "name": "nn_impl.py",
            "path": "tensorflow/python/ops/nn_impl.py",
            "patches": [
                {
                    "old_start": 842,
                    "old_length": 7,
                    "new_start": 842,
                    "new_length": 8,
                    "hunk_buggy": "['   with ops.name_scope(name, \"sufficient_statistics\", [x, shift]):\\n', '     x = ops.convert_to_tensor(x, name=\"x\")\\n', '     x_shape = x.get_shape()\\n', '-    if all(x_shape.dims[d].value is not None for d in axes):\\n', '       counts = 1\\n', '       for d in axes:\\n', '         counts *= x_shape.dims[d].value']",
                    "hunk_fix": "@@ -842,7 +842,8 @@ def sufficient_statistics(x, axes, shift=None, keep_dims=None, name=None,\n   with ops.name_scope(name, \"sufficient_statistics\", [x, shift]):\n     x = ops.convert_to_tensor(x, name=\"x\")\n     x_shape = x.get_shape()\n-    if all(x_shape.dims[d].value is not None for d in axes):\n+    if x_shape.rank is not None and all(\n+        x_shape.dims[d].value is not None for d in axes):\n       counts = 1\n       for d in axes:\n         counts *= x_shape.dims[d].value"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 245,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/bf686faeddcca97be6ad7b6421cb26ab1c3cea2c",
    "date": "2019-03-20T11:50:59-07:00",
    "message": "TFLite: Enhance input check for ResizeNearestNeghbor\nPiperOrigin-RevId: 239440642",
    "changes": [
        {
            "name": "resize_nearest_neighbor.cc",
            "path": "tensorflow/lite/kernels/resize_nearest_neighbor.cc",
            "patches": [
                {
                    "old_start": 57,
                    "old_length": 11,
                    "new_start": 57,
                    "new_length": 13,
                    "hunk_buggy": "['   const TfLiteTensor* size = GetInput(context, node, kSizeTensor);\\n', '   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\\n', ' \\n', '-  // TODO(ahentz): Our current implementations rely on the inputs being 4D.\\n', '   TF_LITE_ENSURE_EQ(context, NumDimensions(input), 4);\\n', '   TF_LITE_ENSURE_EQ(context, NumDimensions(size), 1);\\n', '-\\n', '   TF_LITE_ENSURE_EQ(context, size->type, kTfLiteInt32);\\n', '   output->type = input->type;\\n', ' \\n', '   if (!IsConstantTensor(size)) {']",
                    "hunk_fix": "@@ -57,11 +57,13 @@ TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {\n   const TfLiteTensor* size = GetInput(context, node, kSizeTensor);\n   TfLiteTensor* output = GetOutput(context, node, kOutputTensor);\n \n-  // TODO(ahentz): Our current implementations rely on the inputs being 4D.\n+  // TODO(ahentz): Our current implementations rely on the input being 4D,\n+  // and the size being 1D tensor with exactly 2 elements.\n   TF_LITE_ENSURE_EQ(context, NumDimensions(input), 4);\n   TF_LITE_ENSURE_EQ(context, NumDimensions(size), 1);\n-\n   TF_LITE_ENSURE_EQ(context, size->type, kTfLiteInt32);\n+  TF_LITE_ENSURE_EQ(context, size->dims->data[0], 2);\n+\n   output->type = input->type;\n \n   if (!IsConstantTensor(size)) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 246,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/ed043aec4962dfdc3c58e2ad90dacb557dafcf4e",
    "date": "2019-03-19T19:59:10+05:30",
    "message": "Lite: ResizeTensor Dim size check added to avoid reallocation if no change",
    "changes": [
        {
            "name": "subgraph.cc",
            "path": "tensorflow/lite/core/subgraph.cc",
            "patches": [
                {
                    "old_start": 740,
                    "old_length": 6,
                    "new_start": 740,
                    "new_length": 18,
                    "hunk_buggy": "[' TfLiteStatus Subgraph::ResizeTensor(TfLiteContext* context,\\n', '                                     TfLiteTensor* tensor,\\n', '                                     TfLiteIntArray* new_size) {\\n', '   // Note here that context->impl_ is recovering the this pointer for an\\n', '   // instance of Interpreter to call into the member function ResizeTensorImpl\\n', '   // (this function is static).']",
                    "hunk_fix": "@@ -740,6 +740,18 @@ TfLiteStatus Subgraph::Invoke() {\n TfLiteStatus Subgraph::ResizeTensor(TfLiteContext* context,\n                                     TfLiteTensor* tensor,\n                                     TfLiteIntArray* new_size) {\n+  // If the dimensions don't change, avoiding\n+  // unnecessary (re)allocations.\n+  //\n+  // Note that it's required to check `tensor->data.raw != nullptr`. Otherwise\n+  // the subgraph won't allocate memory for a dynamic tensor when its size\n+  // is equal to the original tensor size.\n+  if (tensor->data.raw != nullptr &&\n+      EqualArrayAndTfLiteIntArray(tensor->dims, new_size->size,\n+                                  new_size->data)) {\n+    return kTfLiteOk;\n+  }\n+\n   // Note here that context->impl_ is recovering the this pointer for an\n   // instance of Interpreter to call into the member function ResizeTensorImpl\n   // (this function is static)."
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 247,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/c2cec131f107fde9c54f48a9b74248617d850549",
    "date": "2019-03-14T10:58:26-07:00",
    "message": "Add the shape and dtype validation for TensorDatasetOp",
    "changes": [
        {
            "name": "tensor_dataset_op.cc",
            "path": "tensorflow/core/kernels/data/tensor_dataset_op.cc",
            "patches": [
                {
                    "old_start": 26,
                    "old_length": 14,
                    "new_start": 26,
                    "new_length": 35,
                    "hunk_buggy": "[' \\n', ' class TensorDatasetOp : public DatasetOpKernel {\\n', '  public:\\n', '-  explicit TensorDatasetOp(OpKernelConstruction* ctx) : DatasetOpKernel(ctx) {}\\n', ' \\n', '   void MakeDataset(OpKernelContext* ctx, DatasetBase** output) override {\\n', '     OpInputList inputs;\\n', '     OP_REQUIRES_OK(ctx, ctx->input_list(\"components\", &inputs));\\n', '-    // TODO(mrry): Validate that the shapes of the \"components\" tensors match\\n', '-    // the \"shapes\" attr.;\\n', '     std::vector<Tensor> components(inputs.begin(), inputs.end());\\n', '     *output = new Dataset(ctx, std::move(components));\\n', '   }\\n', ' \\n']",
                    "hunk_fix": "@@ -26,14 +26,35 @@ namespace {\n \n class TensorDatasetOp : public DatasetOpKernel {\n  public:\n-  explicit TensorDatasetOp(OpKernelConstruction* ctx) : DatasetOpKernel(ctx) {}\n+  explicit TensorDatasetOp(OpKernelConstruction* ctx) : DatasetOpKernel(ctx) {\n+    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"Toutput_types\", &output_types_));\n+    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"output_shapes\", &output_shapes_));\n+  }\n \n   void MakeDataset(OpKernelContext* ctx, DatasetBase** output) override {\n     OpInputList inputs;\n     OP_REQUIRES_OK(ctx, ctx->input_list(\"components\", &inputs));\n-    // TODO(mrry): Validate that the shapes of the \"components\" tensors match\n-    // the \"shapes\" attr.;\n     std::vector<Tensor> components(inputs.begin(), inputs.end());\n+    OP_REQUIRES(ctx, components.size() == output_shapes_.size(),\n+                errors::InvalidArgument(\n+                    \"The size of components should be same with that of \"\n+                    \"output_shapes, but got \",\n+                    components.size(), \" vs. \", output_shapes_.size()));\n+    OP_REQUIRES(ctx, components.size() == output_types_.size(),\n+                errors::InvalidArgument(\n+                    \"The size of components should be same with that of \"\n+                    \"Toutput_types, but got \",\n+                    components.size(), \" vs. \", output_types_.size()));\n+\n+    for (int i = 0; i < components.size(); ++i) {\n+      OP_REQUIRES(ctx, components[i].dtype() == output_types_[i],\n+                  errors::InvalidArgument(\"The dtypes of components should be \"\n+                                          \"same with Toutput_types\"));\n+      OP_REQUIRES(ctx, output_shapes_[i].IsIdenticalTo(components[i].shape()),\n+                  errors::InvalidArgument(\"The shapes of components should be \"\n+                                          \"same with output_shapes\"));\n+    }\n+\n     *output = new Dataset(ctx, std::move(components));\n   }\n \n"
                },
                {
                    "old_start": 137,
                    "old_length": 6,
                    "new_start": 158,
                    "new_length": 9,
                    "hunk_buggy": "['     DataTypeVector dtypes_;\\n', '     std::vector<PartialTensorShape> shapes_;\\n', '   };\\n', ' };\\n', ' \\n', ' REGISTER_KERNEL_BUILDER(Name(\"TensorDataset\").Device(DEVICE_CPU),']",
                    "hunk_fix": "@@ -137,6 +158,9 @@ class TensorDatasetOp : public DatasetOpKernel {\n     DataTypeVector dtypes_;\n     std::vector<PartialTensorShape> shapes_;\n   };\n+\n+  DataTypeVector output_types_;\n+  std::vector<PartialTensorShape> output_shapes_;\n };\n \n REGISTER_KERNEL_BUILDER(Name(\"TensorDataset\").Device(DEVICE_CPU),"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 248,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/b8c517ab4ef0bd851ef2f8187935fd3a90261af5",
    "date": "2019-03-06T09:26:33-03:00",
    "message": "Reinstate eager check inside _GradientsHelper",
    "changes": [
        {
            "name": "gradients_util.py",
            "path": "tensorflow/python/ops/gradients_util.py",
            "patches": [
                {
                    "old_start": 542,
                    "old_length": 6,
                    "new_start": 542,
                    "new_length": 9,
                    "hunk_buggy": "['                      unconnected_gradients=UnconnectedGradients.NONE,\\n', '                      src_graph=None):\\n', '   \"\"\"Implementation of gradients().\"\"\"\\n', '   if src_graph is None:\\n', '     src_graph = ops.get_default_graph()\\n', '   try:']",
                    "hunk_fix": "@@ -542,6 +542,9 @@ def _GradientsHelper(ys,\n                      unconnected_gradients=UnconnectedGradients.NONE,\n                      src_graph=None):\n   \"\"\"Implementation of gradients().\"\"\"\n+  if context.executing_eagerly():\n+    raise RuntimeError(\"tf.gradients is not supported when eager execution \"\n+                       \"is enabled. Use tf.GradientTape instead.\")\n   if src_graph is None:\n     src_graph = ops.get_default_graph()\n   try:"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 249,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/083fd8c4b23104f6b27a871c6469629ace4ee9c3",
    "date": "2019-03-04T07:10:05-08:00",
    "message": "Don't check soname on Windows\n\nThis allow users to specify a certain CUDA version on Windows again.\n\nWithout this, we get the following error:\n\n```\nCuda Configuration Error: None of the libraries match their SONAME: C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.1/lib/x64/cudart.lib\n```\n\nPiperOrigin-RevId: 236646104",
    "changes": [
        {
            "name": "cuda_configure.bzl",
            "path": "third_party/gpus/cuda_configure.bzl",
            "patches": [
                {
                    "old_start": 735,
                    "old_length": 7,
                    "new_start": 735,
                    "new_length": 7,
                    "hunk_buggy": "['     for path in [repository_ctx.path(path) for path in paths]:\\n', '         if not path.exists:\\n', '             continue\\n', '-        if check_soname and objdump != None:\\n', '             output = repository_ctx.execute([objdump, \"-p\", str(path)]).stdout\\n', '             output = [line for line in output.splitlines() if \"SONAME\" in line]\\n', '             sonames = [line.strip().split(\" \")[-1] for line in output]']",
                    "hunk_fix": "@@ -735,7 +735,7 @@ def find_lib(repository_ctx, paths, check_soname = True):\n     for path in [repository_ctx.path(path) for path in paths]:\n         if not path.exists:\n             continue\n-        if check_soname and objdump != None:\n+        if check_soname and objdump != None and not _is_windows(repository_ctx):\n             output = repository_ctx.execute([objdump, \"-p\", str(path)]).stdout\n             output = [line for line in output.splitlines() if \"SONAME\" in line]\n             sonames = [line.strip().split(\" \")[-1] for line in output]"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 250,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/ebeb598c2d1f341d6d641bf58c370cf7b43f6e37",
    "date": "2019-03-01T17:31:59-08:00",
    "message": "Correctly check shape not None in Keras `add_weight`.\nWhen calling Keras add_weight with a np list, as written the `shape or\n()` \"trick\" results in the following exception:\n\"\"\"ValueError: The truth value of an array with more than one element is\nambiguous. Use a.any() or a.all()\"\"\"\nThis change fixes the problem by using an explicit `if`.\n\nPiperOrigin-RevId: 236407103",
    "changes": [
        {
            "name": "base_layer.py",
            "path": "tensorflow/python/keras/engine/base_layer.py",
            "patches": [
                {
                    "old_start": 310,
                    "old_length": 7,
                    "new_start": 310,
                    "new_length": 8,
                    "hunk_buggy": "['       ValueError: When giving unsupported dtype and no initializer or when\\n', '         trainable has been set to True with synchronization set as `ON_READ`.\\n', '     \"\"\"\\n', '-    shape = shape or ()\\n', '     # Validate optional keyword arguments.\\n', '     for kwarg in kwargs:\\n', \"       if kwarg not in ['getter', 'collections', 'experimental_autocast']:\"]",
                    "hunk_fix": "@@ -310,7 +310,8 @@ class Layer(trackable.Trackable):\n       ValueError: When giving unsupported dtype and no initializer or when\n         trainable has been set to True with synchronization set as `ON_READ`.\n     \"\"\"\n-    shape = shape or ()\n+    if shape is None:\n+      shape = ()\n     # Validate optional keyword arguments.\n     for kwarg in kwargs:\n       if kwarg not in ['getter', 'collections', 'experimental_autocast']:"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 251,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/1e38a0025c9a983bf3229299109b5b1781215c7e",
    "date": "2019-01-30T14:18:25-08:00",
    "message": "[XLA] CHECK that sparse indices are in range in MutableLiteralBase::AppendSparseElement.\n\nPreviously there was no range-checking on sparse elements' indices.\n\nPiperOrigin-RevId: 231662246",
    "changes": [
        {
            "name": "literal.h",
            "path": "tensorflow/compiler/xla/literal.h",
            "patches": [
                {
                    "old_start": 958,
                    "old_length": 12,
                    "new_start": 958,
                    "new_length": 15,
                    "hunk_buggy": "[' void MutableLiteralBase::AppendSparseElement(\\n', '     absl::Span<const int64> multi_index, NativeT value,\\n', '     const ShapeIndex& shape_index) {\\n', '-  // TODO(jlebar): CHECK that multi_index is in range?\\n', '   Piece& p = piece(shape_index);\\n', '   const Shape& subshape = p.subshape();\\n', '   CHECK(LayoutUtil::IsSparseArray(subshape));\\n', '   int64 rank = subshape.rank();\\n', '   CHECK_EQ(multi_index.size(), rank);\\n', '   int64 last_element = p.sparse_indices()->index_count();\\n', '   CHECK_LT(last_element, LayoutUtil::MaxSparseElements(subshape.layout()));\\n', '   p.sparse_indices()->Append(multi_index);']",
                    "hunk_fix": "@@ -958,12 +958,15 @@ template <typename NativeT>\n void MutableLiteralBase::AppendSparseElement(\n     absl::Span<const int64> multi_index, NativeT value,\n     const ShapeIndex& shape_index) {\n-  // TODO(jlebar): CHECK that multi_index is in range?\n   Piece& p = piece(shape_index);\n   const Shape& subshape = p.subshape();\n   CHECK(LayoutUtil::IsSparseArray(subshape));\n   int64 rank = subshape.rank();\n   CHECK_EQ(multi_index.size(), rank);\n+  for (int64 i = 0; i < rank; ++i) {\n+    CHECK_GE(multi_index[i], 0);\n+    CHECK_LT(multi_index[i], subshape.dimensions(i));\n+  }\n   int64 last_element = p.sparse_indices()->index_count();\n   CHECK_LT(last_element, LayoutUtil::MaxSparseElements(subshape.layout()));\n   p.sparse_indices()->Append(multi_index);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 252,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/823b694639a3f49b6adbf9e73a08c529d583878e",
    "date": "2019-01-29T01:48:40-08:00",
    "message": "Add bounds checking when looking at the stack in TF Registry.\n\nThis was breaking integration with R, fixes #25262.\n\nPiperOrigin-RevId: 231367381",
    "changes": [
        {
            "name": "registry.py",
            "path": "tensorflow/python/framework/registry.py",
            "patches": [
                {
                    "old_start": 64,
                    "old_length": 8,
                    "new_start": 64,
                    "new_length": 12,
                    "hunk_buggy": "['     # stack trace is [this_function, Register(), user_function,...]\\n', '     # so the user function is #2.\\n', '     stack = tf_stack.extract_stack()\\n', '-    user_function = stack[2]\\n', '-    location_tag = tf_stack.convert_stack([user_function])[0]\\n', '     self._registry[name] = {_TYPE_TAG: candidate, _LOCATION_TAG: location_tag}\\n', ' \\n', '   def list(self):']",
                    "hunk_fix": "@@ -64,8 +64,12 @@ class Registry(object):\n     # stack trace is [this_function, Register(), user_function,...]\n     # so the user function is #2.\n     stack = tf_stack.extract_stack()\n-    user_function = stack[2]\n-    location_tag = tf_stack.convert_stack([user_function])[0]\n+    stack_index = min(2, len(stack)-1)\n+    if stack_index >= 0:\n+      user_function = stack[stack_index]\n+      location_tag = tf_stack.convert_stack([user_function])[0]\n+    else:\n+      location_tag = \"UNKNOWN\"\n     self._registry[name] = {_TYPE_TAG: candidate, _LOCATION_TAG: location_tag}\n \n   def list(self):"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 253,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/1a73fdfa83bd50695a7d374d14a5cb3835d94d9e",
    "date": "2019-01-09T14:41:51-08:00",
    "message": "Add extra check incase segmenter does not exclude CPU in order to prevent segfault",
    "changes": [
        {
            "name": "convert_graph.cc",
            "path": "tensorflow/contrib/tensorrt/convert/convert_graph.cc",
            "patches": [
                {
                    "old_start": 355,
                    "old_length": 12,
                    "new_start": 355,
                    "new_length": 18,
                    "hunk_buggy": "['     const auto& node_name = (*it)->name();\\n', '     if (segment_nodes.count(node_name) == 0) continue;\\n', '     auto node = *it;\\n', '-    // TODO: check for CPU device here\\n', \"-    // If device is CPU, we should've caught that in the segmenter. Fall back here.\\n\", '-\\n', '     auto node_device = node->requested_device();\\n', '     if (!node_device.empty()) {\\n', '-      segment_devices.insert(node_device);\\n', '     } else {\\n', '       if (node->has_assigned_device_name()) {\\n', '         segment_devices.insert(node->assigned_device_name());']",
                    "hunk_fix": "@@ -355,12 +355,18 @@ tensorflow::Status GetEngineInfo(\n     const auto& node_name = (*it)->name();\n     if (segment_nodes.count(node_name) == 0) continue;\n     auto node = *it;\n-    // TODO: check for CPU device here\n-    // If device is CPU, we should've caught that in the segmenter. Fall back here.\n-\n     auto node_device = node->requested_device();\n     if (!node_device.empty()) {\n-      segment_devices.insert(node_device);\n+      // If device is CPU, we should've caught that in the segmenter.\n+      DeviceNameUtils::ParsedName parsed_name;\n+      DeviceNameUtils::ParseFullName(node_device, &parsed_name);\n+      if (parsed_name.type == \"CPU\") {\n+        LOG(WARNING) << \"Node \" << node->name() << \" was assigned to the CPU \"\n+                     << \"but did not get excluded by the segmenter. \"\n+                     << \"Attempting to place on GPU.\";\n+      } else {\n+        segment_devices.insert(node_device);\n+      }\n     } else {\n       if (node->has_assigned_device_name()) {\n         segment_devices.insert(node->assigned_device_name());"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 254,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/c7c4a42c4372ca560ea415fe3a798e18286cedec",
    "date": "2019-01-07T13:38:08-08:00",
    "message": "Fix an error in keras input_layer.Input() dtype type checking.\n\nPiperOrigin-RevId: 228215444",
    "changes": [
        {
            "name": "input_layer.py",
            "path": "tensorflow/python/keras/engine/input_layer.py",
            "patches": [
                {
                    "old_start": 77,
                    "old_length": 8,
                    "new_start": 77,
                    "new_length": 9,
                    "hunk_buggy": "['         dtype = backend.floatx()\\n', '       else:\\n', '         dtype = backend.dtype(input_tensor)\\n', '-    elif input_tensor and input_tensor.dtype != dtype:\\n', \"-      raise ValueError('`input_tensor.dtype` differs from `dtype`.')\\n\", '     super(InputLayer, self).__init__(dtype=dtype, name=name)\\n', '     self.built = True\\n', '     self.sparse = sparse']",
                    "hunk_fix": "@@ -77,8 +77,9 @@ class InputLayer(base_layer.Layer):\n         dtype = backend.floatx()\n       else:\n         dtype = backend.dtype(input_tensor)\n-    elif input_tensor and input_tensor.dtype != dtype:\n-      raise ValueError('`input_tensor.dtype` differs from `dtype`.')\n+    elif input_tensor is not None and input_tensor.dtype != dtype:\n+      raise ValueError('`input_tensor.dtype` differs from `dtype`: %s vs. %s' %\n+                       (input_tensor.dtype, dtype))\n     super(InputLayer, self).__init__(dtype=dtype, name=name)\n     self.built = True\n     self.sparse = sparse"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 255,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/7e2d53c1c371f38c7f0ef13c1c06336b22a195c0",
    "date": "2019-01-03T12:51:36-08:00",
    "message": "[tf.data] Adds the expected check for better debugging.\n\nPiperOrigin-RevId: 227733165",
    "changes": [
        {
            "name": "scan_dataset_op.cc",
            "path": "tensorflow/core/kernels/data/experimental/scan_dataset_op.cc",
            "patches": [
                {
                    "old_start": 186,
                    "old_length": 6,
                    "new_start": 186,
                    "new_length": 8,
                    "hunk_buggy": "[' \\n', '         Status s = instantiated_captured_func_->Run(ctx, std::move(args),\\n', '                                                     &state_and_output);\\n', '         if (s.ok()) {\\n', '           state_.clear();\\n', '           size_t i = 0;']",
                    "hunk_fix": "@@ -186,6 +186,8 @@ class ScanDatasetOp : public UnaryDatasetOpKernel {\n \n         Status s = instantiated_captured_func_->Run(ctx, std::move(args),\n                                                     &state_and_output);\n+        DCHECK(state_and_output.size() <=\n+               dataset()->state_types_.size() + output_dtypes().size());\n         if (s.ok()) {\n           state_.clear();\n           size_t i = 0;"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 256,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/7578e120de2a3a5282ced8d41881f19363f83466",
    "date": "2018-12-09T11:22:52-05:00",
    "message": "Fix crash on closing the app when classifier failed to initialize\n\nWhen testing on an API 21 emulator, the classifier fails to initialize.\n`E/TfLiteCameraDemo: Failed to initialize an image classifier.`\n\nIn this situation, the app crashes when pressing Back to exit.  Here's the cause:\n```\njava.lang.NullPointerException: Attempt to invoke virtual method 'void com.example.android.tflitecamerademo.ImageClassifier.close()' on a null object reference\n                                                                                        at com.example.android.tflitecamerademo.Camera2BasicFragment.onDestroy(Camera2BasicFragment.java:331)\n                                                                                        at android.app.Fragment.performDestroy(Fragment.java:2266)\n```\nThe fix is to check for null before calling `.close()`.\n\nI'll investigate why the classifier is failing to initialize separately. :-)",
    "changes": [
        {
            "name": "Camera2BasicFragment.java",
            "path": "tensorflow/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/Camera2BasicFragment.java",
            "patches": [
                {
                    "old_start": 476,
                    "old_length": 7,
                    "new_start": 476,
                    "new_length": 9,
                    "hunk_buggy": "[' \\n', '   @Override\\n', '   public void onDestroy() {\\n', '-    classifier.close();\\n', '     super.onDestroy();\\n', '   }\\n', ' ']",
                    "hunk_fix": "@@ -476,7 +476,9 @@ public class Camera2BasicFragment extends Fragment\n \n   @Override\n   public void onDestroy() {\n-    classifier.close();\n+    if (classifier != null) {\n+      classifier.close();\n+    }\n     super.onDestroy();\n   }\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 257,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/25bae42b3022b00788a29ae6c400922c31f88231",
    "date": "2018-11-28T02:46:06+00:00",
    "message": "Add additional length check for inputs\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "changes": [
        {
            "name": "sparse_ops.py",
            "path": "tensorflow/python/ops/sparse_ops.py",
            "patches": [
                {
                    "old_start": 323,
                    "old_length": 7,
                    "new_start": 323,
                    "new_length": 7,
                    "hunk_buggy": "['       gen_sparse_ops.sparse_concat(inds, vals, shapes, axis, name=name))\\n', ' \\n', '   shapes_value = [tensor_util.constant_value(shape) for shape in shapes]\\n', '-  if all(shape is not None for shape in shapes_value):\\n', '     dim = sum(shape[axis] for shape in shapes_value)\\n', '     output_shape = shapes_value[0]\\n', '     output_shape[axis] = dim']",
                    "hunk_fix": "@@ -323,7 +323,7 @@ def sparse_concat(axis,\n       gen_sparse_ops.sparse_concat(inds, vals, shapes, axis, name=name))\n \n   shapes_value = [tensor_util.constant_value(shape) for shape in shapes]\n-  if all(shape is not None for shape in shapes_value):\n+  if len(shapes_value) != 0 and all(shape is not None for shape in shapes_value):\n     dim = sum(shape[axis] for shape in shapes_value)\n     output_shape = shapes_value[0]\n     output_shape[axis] = dim"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 258,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/4c75fb1cb917320acb386cf26adeb8e5151ca4f6",
    "date": "2018-11-20T10:01:44-08:00",
    "message": "Improve error message reporting for check_numerics gradient.\n\nAt present the op message is only printed if the numeric check fails during\nthe op's 'forward' computation. If the check fails during the gradient, there is no\nidentifier on *which* op's gradient failed.\n\nThis is the Python equivalent of\nhttps://github.com/tensorflow/tensorflow/commit/7e48bada\n\nPiperOrigin-RevId: 222262823",
    "changes": [
        {
            "name": "array_grad.py",
            "path": "tensorflow/python/ops/array_grad.py",
            "patches": [
                {
                    "old_start": 489,
                    "old_length": 10,
                    "new_start": 489,
                    "new_length": 12,
                    "hunk_buggy": "[' \\n', ' \\n', ' @ops.RegisterGradient(\"CheckNumerics\")\\n', '-def _CheckNumericsGrad(_, grad):\\n', '   \"\"\"Gradient for check_numerics op.\"\"\"\\n', '   return array_ops.check_numerics(\\n', '-      grad, \"Not a number (NaN) or infinity (Inf) values detected in gradient.\")\\n', ' \\n', ' \\n', ' @ops.RegisterGradient(\"PlaceholderWithDefault\")']",
                    "hunk_fix": "@@ -489,10 +489,12 @@ def _GatherNdGrad(op, grad):\n \n \n @ops.RegisterGradient(\"CheckNumerics\")\n-def _CheckNumericsGrad(_, grad):\n+def _CheckNumericsGrad(op, grad):\n   \"\"\"Gradient for check_numerics op.\"\"\"\n   return array_ops.check_numerics(\n-      grad, \"Not a number (NaN) or infinity (Inf) values detected in gradient.\")\n+      grad,\n+      \"Not a number (NaN) or infinity (Inf) values detected in gradient. %s\" %\n+      op.get_attr(\"message\"))\n \n \n @ops.RegisterGradient(\"PlaceholderWithDefault\")"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 259,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/c9b4689bc4d4024aa16b7d6cfc1c65fa1ed8486e",
    "date": "2018-11-06T12:32:01-05:00",
    "message": "Removed no longer supported call to in_eager_execution\n\nchanges to model_analyser.analyse(...):\r\n- Swapped context.in_eager_execution() to the currently supported context.executing_eagerly().\r\n- Added negation to eager check. In all likelihood, the negation was always supposed to be there since getting default graph in eager mode does not make sense. The current `if` condition is likely a bug. The proposed fix is consistent with other functions in this module, e.g., `profile(...)`, line 339.",
    "changes": [
        {
            "name": "model_analyzer.py",
            "path": "tensorflow/python/profiler/model_analyzer.py",
            "patches": [
                {
                    "old_start": 398,
                    "old_length": 7,
                    "new_start": 398,
                    "new_length": 7,
                    "hunk_buggy": "['   Returns:\\n', '     Returns AdviceProto proto\\n', '   \"\"\"\\n', '-  if not graph and context.in_eager_execution():\\n', '     graph = ops.get_default_graph()\\n', ' \\n', '   if options == _DEFAULT_ADVISE_OPTIONS:']",
                    "hunk_fix": "@@ -398,7 +398,7 @@ def advise(graph=None, run_meta=None, options=_DEFAULT_ADVISE_OPTIONS):\n   Returns:\n     Returns AdviceProto proto\n   \"\"\"\n-  if not graph and context.in_eager_execution():\n+  if not graph and not context.executing_eagerly():\n     graph = ops.get_default_graph()\n \n   if options == _DEFAULT_ADVISE_OPTIONS:"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 260,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/2f3b69e4976d3b14eaa6ae070eb68f37d1556d98",
    "date": "2018-10-23T22:30:07+02:00",
    "message": "Changed empty check",
    "changes": [
        {
            "name": "util.py",
            "path": "tensorflow/python/training/checkpointable/util.py",
            "patches": [
                {
                    "old_start": 971,
                    "old_length": 9,
                    "new_start": 971,
                    "new_length": 7,
                    "hunk_buggy": "['     for checkpointable_object in list_objects(self._root_checkpointable):\\n', '       # Remove data structures that do not contain any variables from\\n', '       # restoration checks.\\n', '-      if (isinstance(checkpointable_object,\\n', '-                     data_structures.CheckpointableDataStructure) and\\n', '-              len(checkpointable_object.variables) == 0):\\n', '         continue\\n', '       self._checkpoint.all_python_objects.add(checkpointable_object)\\n', '     unused_python_objects = (']",
                    "hunk_fix": "@@ -971,9 +971,7 @@ class CheckpointLoadStatus(_LoadStatus):\n     for checkpointable_object in list_objects(self._root_checkpointable):\n       # Remove data structures that do not contain any variables from\n       # restoration checks.\n-      if (isinstance(checkpointable_object,\n-                     data_structures.CheckpointableDataStructure) and\n-              len(checkpointable_object.variables) == 0):\n+      if not checkpointable_object._checkpoint_dependencies:\n         continue\n       self._checkpoint.all_python_objects.add(checkpointable_object)\n     unused_python_objects = ("
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 261,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/40918f36823973e816bd50766b1f447225b1bb9b",
    "date": "2018-10-22T15:30:18-07:00",
    "message": "Make the type check error message more informative for contrib.layers.fully_connected.\n\nPiperOrigin-RevId: 218244077",
    "changes": [
        {
            "name": "layers.py",
            "path": "tensorflow/contrib/layers/python/layers/layers.py",
            "patches": [
                {
                    "old_start": 1823,
                    "old_length": 8,
                    "new_start": 1823,
                    "new_length": 8,
                    "hunk_buggy": "['     ValueError: If x has rank less than 2 or if its last dimension is not set.\\n', '   \"\"\"\\n', '   if not isinstance(num_outputs, six.integer_types):\\n', \"-    raise ValueError('num_outputs should be int or long, got %s.' %\\n\", '-                     (num_outputs,))\\n', ' \\n', '   layer_variable_getter = _build_variable_getter({\\n', \"       'bias': 'biases',\"]",
                    "hunk_fix": "@@ -1823,8 +1823,8 @@ def fully_connected(inputs,\n     ValueError: If x has rank less than 2 or if its last dimension is not set.\n   \"\"\"\n   if not isinstance(num_outputs, six.integer_types):\n-    raise ValueError('num_outputs should be int or long, got %s.' %\n-                     (num_outputs,))\n+    raise ValueError('num_outputs type should be one of %s, got %s.' % (\n+        list(six.integer_types), type(num_outputs)))\n \n   layer_variable_getter = _build_variable_getter({\n       'bias': 'biases',"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 262,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/a12b8c4afdca3ac2945d62b3b83ca2599ab360f9",
    "date": "2018-10-02T17:00:29-04:00",
    "message": "[xla] Improve validation of Broadcast shape\n\nIf one misreads the semantics of this instruction, it's easy to cause\nan out of bounds access into the dimensions here. Add an extra check\nto return a proper error to the user rather than crashing in that\ncase.\n\nRef #22130",
    "changes": [
        {
            "name": "hlo_verifier.cc",
            "path": "tensorflow/compiler/xla/service/hlo_verifier.cc",
            "patches": [
                {
                    "old_start": 313,
                    "old_length": 8,
                    "new_start": 313,
                    "new_length": 9,
                    "hunk_buggy": "['        operand_dimension < ShapeUtil::Rank(operand_shape);\\n', '        ++operand_dimension) {\\n', '     int64 output_dimension = broadcast->dimensions()[operand_dimension];\\n', '-    TF_RET_CHECK(broadcast->shape().dimensions(output_dimension) ==\\n', '-                 operand_shape.dimensions(operand_dimension))\\n', '         << broadcast->ToString() << \" operand shape \" << operand_shape;\\n', '   }\\n', '   return Status::OK();']",
                    "hunk_fix": "@@ -313,8 +313,9 @@ Status ShapeVerifier::HandleBroadcast(HloInstruction* broadcast) {\n        operand_dimension < ShapeUtil::Rank(operand_shape);\n        ++operand_dimension) {\n     int64 output_dimension = broadcast->dimensions()[operand_dimension];\n-    TF_RET_CHECK(broadcast->shape().dimensions(output_dimension) ==\n-                 operand_shape.dimensions(operand_dimension))\n+    TF_RET_CHECK((output_dimension < ShapeUtil::Rank(broadcast->shape())) &&\n+                 (broadcast->shape().dimensions(output_dimension) ==\n+                 operand_shape.dimensions(operand_dimension)))\n         << broadcast->ToString() << \" operand shape \" << operand_shape;\n   }\n   return Status::OK();"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 263,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/5cedb0427bd4db4117182da8bc0680dd555b4f49",
    "date": "2018-09-26T10:37:15-07:00",
    "message": "Add checks for dilation_rate.\n\nPiperOrigin-RevId: 214627202",
    "changes": [
        {
            "name": "depthwiseconv_uint8.h",
            "path": "tensorflow/contrib/lite/kernels/internal/optimized/depthwiseconv_uint8.h",
            "patches": [
                {
                    "old_start": 1701,
                    "old_length": 6,
                    "new_start": 1701,
                    "new_length": 8,
                    "hunk_buggy": "['   const int output_shift = params.output_shift;\\n', '   const int dilation_width_factor = params.dilation_width_factor;\\n', '   const int dilation_height_factor = params.dilation_height_factor;\\n', '   TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\\n', '   TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);\\n', '   TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);']",
                    "hunk_fix": "@@ -1701,6 +1701,8 @@ inline void DepthwiseConv(\n   const int output_shift = params.output_shift;\n   const int dilation_width_factor = params.dilation_width_factor;\n   const int dilation_height_factor = params.dilation_height_factor;\n+  TFLITE_DCHECK_GE(dilation_width_factor, 1);\n+  TFLITE_DCHECK_GE(dilation_height_factor, 1);\n   TFLITE_DCHECK_EQ(input_shape.DimensionsCount(), 4);\n   TFLITE_DCHECK_EQ(filter_shape.DimensionsCount(), 4);\n   TFLITE_DCHECK_EQ(output_shape.DimensionsCount(), 4);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 264,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/05ec322172958f6e67e4bcaef4681e6aa54fabeb",
    "date": "2018-09-19T10:55:51-07:00",
    "message": "Return error message with illegal input rather than check-failing in op_kernel.\n\nPiperOrigin-RevId: 213653853",
    "changes": [
        {
            "name": "xla_launch_util.cc",
            "path": "tensorflow/compiler/jit/xla_launch_util.cc",
            "patches": [
                {
                    "old_start": 275,
                    "old_length": 6,
                    "new_start": 275,
                    "new_length": 8,
                    "hunk_buggy": "['       VLOG(2) << \"Retval \" << i << \" shape \" << shape.DebugString() << \" type \"\\n', '               << DataTypeString(type);\\n', '       if (type == DT_RESOURCE) {\\n', '         ctx->set_output(i, ctx->input(kernel->outputs[i].input_index));\\n', '       } else {\\n', '         se::DeviceMemoryBase buffer = output.buffer({output_num});']",
                    "hunk_fix": "@@ -275,6 +275,8 @@ Status XlaComputationLaunchContext::PopulateOutputs(\n       VLOG(2) << \"Retval \" << i << \" shape \" << shape.DebugString() << \" type \"\n               << DataTypeString(type);\n       if (type == DT_RESOURCE) {\n+        TF_RET_CHECK(kernel->outputs[i].input_index >= 0)\n+            << \"Invalid input for outputs \" << i;\n         ctx->set_output(i, ctx->input(kernel->outputs[i].input_index));\n       } else {\n         se::DeviceMemoryBase buffer = output.buffer({output_num});"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 265,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/a76646d4b4ad5d56b5e63c139985bbd1eb98dd90",
    "date": "2018-09-17T17:58:44-07:00",
    "message": "Add type checking at the beginning of tpu.shard().\n\nOtherwise a message like \"TypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn.\" will be thrown, which is confusing.\n\nPiperOrigin-RevId: 213371676",
    "changes": [
        {
            "name": "tpu.py",
            "path": "tensorflow/contrib/tpu/python/tpu/tpu.py",
            "patches": [
                {
                    "old_start": 847,
                    "old_length": 8,
                    "new_start": 847,
                    "new_length": 12,
                    "hunk_buggy": "['   if num_shards <= 0:\\n', '     raise ValueError(\"num_shards must be a positive integer.\")\\n', ' \\n', '   # Converts inputs to Tensors.\\n', '-  inputs = [] if inputs is None else [ops.convert_to_tensor(x) for x in inputs]\\n', ' \\n', '   if input_shard_axes is None:\\n', '     input_shard_axes = [0] * len(inputs)']",
                    "hunk_fix": "@@ -847,8 +847,12 @@ def shard(computation,\n   if num_shards <= 0:\n     raise ValueError(\"num_shards must be a positive integer.\")\n \n+  inputs = [] if inputs is None else inputs\n+  if not isinstance(inputs, list):\n+    raise TypeError(\"tpu.shard()'s inputs must be a list of Tensors or None.\")\n+\n   # Converts inputs to Tensors.\n-  inputs = [] if inputs is None else [ops.convert_to_tensor(x) for x in inputs]\n+  inputs = [ops.convert_to_tensor(x) for x in inputs]\n \n   if input_shard_axes is None:\n     input_shard_axes = [0] * len(inputs)"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 266,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/f8ec0f101bac066faa2e917ac714ca9eea310eac",
    "date": "2018-09-13T22:40:49-07:00",
    "message": "adding checks that pad fusion works only Conv2D",
    "changes": [
        {
            "name": "mkl_conv_ops.cc",
            "path": "tensorflow/core/kernels/mkl_conv_ops.cc",
            "patches": [
                {
                    "old_start": 900,
                    "old_length": 7,
                    "new_start": 900,
                    "new_length": 10,
                    "hunk_buggy": "[' \\n', '       bool isConv2D = (strides_.size() == 4);\\n', '       // TODO(Intel-tf) Add check to make sure padEnabled is true only for 2D\\n', '-\\n', '       // Create memory for user data.\\n', '       // Describe how the inputs and outputs of Convolution look like. Also\\n', '       // specify buffers containing actual input and output data.']",
                    "hunk_fix": "@@ -900,7 +900,10 @@ class MklConvOp : public OpKernel {\n \n       bool isConv2D = (strides_.size() == 4);\n       // TODO(Intel-tf) Add check to make sure padEnabled is true only for 2D\n-\n+      if(!isConv2D){\n+        OP_REQUIRES(context, padEnabled,\n+                errors::InvalidArgument(\"Pad+Conv fusion only works for 2D\"));\n+      }\n       // Create memory for user data.\n       // Describe how the inputs and outputs of Convolution look like. Also\n       // specify buffers containing actual input and output data."
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 267,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/51d72a7d7f74784b68916819edd04e890b36f957",
    "date": "2018-09-14T05:59:05+08:00",
    "message": "Modified \"_check_is_tensor_or_operation\" to check if \"x\" is \"tensor_like\"",
    "changes": [
        {
            "name": "model_fn.py",
            "path": "tensorflow/python/estimator/model_fn.py",
            "patches": [
                {
                    "old_start": 26,
                    "old_length": 6,
                    "new_start": 26,
                    "new_length": 7,
                    "hunk_buggy": "[' from tensorflow.python.estimator.export import export_output as export_output_lib\\n', ' from tensorflow.python.framework import ops\\n', ' from tensorflow.python.framework import tensor_shape\\n', ' from tensorflow.python.keras.metrics import Metric\\n', ' from tensorflow.python.ops import array_ops\\n', ' from tensorflow.python.saved_model import signature_constants\\n']",
                    "hunk_fix": "@@ -26,6 +26,7 @@ import six\n from tensorflow.python.estimator.export import export_output as export_output_lib\n from tensorflow.python.framework import ops\n from tensorflow.python.framework import tensor_shape\n+from tensorflow.python.framework import tensor_util\n from tensorflow.python.keras.metrics import Metric\n from tensorflow.python.ops import array_ops\n from tensorflow.python.saved_model import signature_constants\n"
                },
                {
                    "old_start": 466,
                    "old_length": 7,
                    "new_start": 467,
                    "new_length": 7,
                    "hunk_buggy": "[' \\n', ' \\n', ' def _check_is_tensor_or_operation(x, name):\\n', '-  if not (isinstance(x, ops.Operation) or isinstance(x, ops.Tensor)):\\n', \"     raise TypeError('{} must be Operation or Tensor, given: {}'.format(name, x))\\n\", ' \\n', ' ']",
                    "hunk_fix": "@@ -466,7 +467,7 @@ class _TPUEstimatorSpec(\n \n \n def _check_is_tensor_or_operation(x, name):\n-  if not (isinstance(x, ops.Operation) or isinstance(x, ops.Tensor)):\n+  if not (isinstance(x, ops.Operation) or tensor_util.is_tensor(x)):\n     raise TypeError('{} must be Operation or Tensor, given: {}'.format(name, x))\n \n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 268,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/9c1f14322484e44a93b77619ffd2e24b9b7a9b1d",
    "date": "2018-08-28T13:50:03-07:00",
    "message": "Fix error message in TF-keras dataset shape check\n\n(Dimension and tensor # were transposed in the error message)\n\nPiperOrigin-RevId: 210598445",
    "changes": [
        {
            "name": "keras_support.py",
            "path": "tensorflow/contrib/tpu/python/tpu/keras_support.py",
            "patches": [
                {
                    "old_start": 619,
                    "old_length": 7,
                    "new_start": 619,
                    "new_length": 7,
                    "hunk_buggy": "[\"               'currently requires static shapes. The provided '\\n\", \"               'dataset only has a partially defined shape. '\\n\", \"               '(Dimension %d of output tensor %d is not statically known '\\n\", \"-              'for output shapes: %s.%s)' % (i, j, dataset.output_shapes, hint))\\n\", ' \\n', '   @property\\n', '   def dummy_x(self):']",
                    "hunk_fix": "@@ -619,7 +619,7 @@ class TPUDatasetInfeedManager(TPUInfeedManager):\n               'currently requires static shapes. The provided '\n               'dataset only has a partially defined shape. '\n               '(Dimension %d of output tensor %d is not statically known '\n-              'for output shapes: %s.%s)' % (i, j, dataset.output_shapes, hint))\n+              'for output shapes: %s.%s)' % (j, i, dataset.output_shapes, hint))\n \n   @property\n   def dummy_x(self):"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 269,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/abd645085b1dd1496df847b05a1934d471a2f2c0",
    "date": "2018-08-01T01:06:22-07:00",
    "message": "Use the correct device ordinal to check whether the device the executable was\nbuilt for is equivalent to the device the it will run on.\n\nBefore this patch, if the device to run on was provided via a stream without\nsetting the device ordinal in the ExecutableRunOptions, we would check the\ndefault device against the device the executable was built for.\n\nPiperOrigin-RevId: 206892902",
    "changes": [
        {
            "name": "local_client.cc",
            "path": "tensorflow/compiler/xla/client/local_client.cc",
            "patches": [
                {
                    "old_start": 101,
                    "old_length": 11,
                    "new_start": 101,
                    "new_length": 14,
                    "hunk_buggy": "['     }\\n', '   }\\n', ' \\n', '-  // Verify that the device the executable was built for is equivalent to the\\n', '-  // device it will run on.\\n', '-  int run_device_ordinal = run_options.device_ordinal() == -1\\n', '-                               ? backend_->default_device_ordinal()\\n', '-                               : run_options.device_ordinal();\\n', '   TF_ASSIGN_OR_RETURN(bool devices_equivalent,\\n', '                       backend_->devices_equivalent(\\n', '                           run_device_ordinal, build_options_.device_ordinal()));']",
                    "hunk_fix": "@@ -101,11 +101,14 @@ Status LocalExecutable::ValidateExecutionOptions(\n     }\n   }\n \n-  // Verify that the device the executable was built for is equivalent to the\n-  // device it will run on.\n-  int run_device_ordinal = run_options.device_ordinal() == -1\n-                               ? backend_->default_device_ordinal()\n-                               : run_options.device_ordinal();\n+  // Verify that the device the executable was built for is equivalent\n+  // to the device it will run on.\n+  int run_device_ordinal = run_options.device_ordinal();\n+  if (run_device_ordinal == -1) {\n+    run_device_ordinal = run_options.stream() != nullptr\n+                             ? run_options.stream()->parent()->device_ordinal()\n+                             : backend_->default_device_ordinal();\n+  }\n   TF_ASSIGN_OR_RETURN(bool devices_equivalent,\n                       backend_->devices_equivalent(\n                           run_device_ordinal, build_options_.device_ordinal()));"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 270,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/27de8e717c1bec91398f5a6be6c7287b657fc960",
    "date": "2018-07-27T07:20:25+00:00",
    "message": "Improve shape function for CudnnRNNParamsSize\n\nIn cudnn_rnn_ops.cc, the CudnnRNNParamsSize does not\nhave restrictions on num_layers, num_units, and input_size,\nthough they all should be scalars.\n\nThis fix adds the shape check of num_layers, num_units, and input_size\nfor CudnnRNNParamsSize.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "changes": [
        {
            "name": "cudnn_rnn_ops.cc",
            "path": "tensorflow/core/ops/cudnn_rnn_ops.cc",
            "patches": [
                {
                    "old_start": 52,
                    "old_length": 6,
                    "new_start": 52,
                    "new_length": 12,
                    "hunk_buggy": "['     .Attr(\"seed2: int = 0\")\\n', '     .Output(\"params_size: S\")\\n', '     .SetShapeFn([](InferenceContext* c) {\\n', '       c->set_output(0, c->Vector(1));\\n', '       return Status::OK();\\n', '     });']",
                    "hunk_fix": "@@ -52,6 +52,12 @@ REGISTER_OP(\"CudnnRNNParamsSize\")\n     .Attr(\"seed2: int = 0\")\n     .Output(\"params_size: S\")\n     .SetShapeFn([](InferenceContext* c) {\n+      ShapeHandle unused;\n+      // num_layers, num_units, and input_size should be scalars.\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n+\n       c->set_output(0, c->Vector(1));\n       return Status::OK();\n     });"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 271,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/f0bf6c5191d224f229808f4b321158d890a481e0",
    "date": "2018-07-25T14:47:41-07:00",
    "message": "Minor change for better error msg in eager input type checking\n\nPiperOrigin-RevId: 206058281",
    "changes": [
        {
            "name": "execute.cc",
            "path": "tensorflow/core/common_runtime/eager/execute.cc",
            "patches": [
                {
                    "old_start": 175,
                    "old_length": 7,
                    "new_start": 175,
                    "new_length": 7,
                    "hunk_buggy": "['     tensorflow::TensorHandle* handle = op->Inputs()[i];\\n', '     if (handle->dtype != kernel->input_type(i)) {\\n', '       return errors::InvalidArgument(\\n', '-          \"cannot compute \", op->Name(), \" as input #\", i,\\n', '           \" was expected to be a \", DataTypeString(kernel->input_type(i)),\\n', '           \" tensor but is a \", DataTypeString(handle->dtype), \" tensor\");\\n', '     }']",
                    "hunk_fix": "@@ -175,7 +175,7 @@ Status ValidateInputTypeAndPlacement(EagerContext* ctx, Device* op_device,\n     tensorflow::TensorHandle* handle = op->Inputs()[i];\n     if (handle->dtype != kernel->input_type(i)) {\n       return errors::InvalidArgument(\n-          \"cannot compute \", op->Name(), \" as input #\", i,\n+          \"cannot compute \", op->Name(), \" as input #\", i, \"(zero-based)\",\n           \" was expected to be a \", DataTypeString(kernel->input_type(i)),\n           \" tensor but is a \", DataTypeString(handle->dtype), \" tensor\");\n     }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 272,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/8eb773d6c23de29dccfc3cf3da441a8552ed13ed",
    "date": "2018-07-19T10:34:48-07:00",
    "message": "[XLA] Better shape size validation for sparse arrays.\n\nPiperOrigin-RevId: 205262376",
    "changes": [
        {
            "name": "shape_util.cc",
            "path": "tensorflow/compiler/xla/shape_util.cc",
            "patches": [
                {
                    "old_start": 883,
                    "old_length": 40,
                    "new_start": 883,
                    "new_length": 51,
                    "hunk_buggy": "['   }\\n', ' \\n', '   int64 shape_size = [&shape]() {\\n', '-    int64 shape_size;\\n', '     if (LayoutUtil::IsSparseArray(shape)) {\\n', '-      shape_size = LayoutUtil::MaxSparseElements(shape.layout());\\n', '-      if (shape_size < 0) {\\n', '-        return shape_size;\\n', '       }\\n', '-      shape_size = MultiplyWithoutOverflow(shape_size, ShapeUtil::Rank(shape));\\n', '-      if (shape_size < 0) {\\n', '-        return shape_size;\\n', '       }\\n', '-      shape_size = MultiplyWithoutOverflow(shape_size, sizeof(int64));\\n', '-      if (shape_size < 0) {\\n', '-        return shape_size;\\n', '       }\\n', '     }\\n', ' \\n', '-    shape_size = 1;\\n', '-\\n', '     // This is intentionally unconditional: even if the shape is sparse, we want\\n', '     // to verify the densified version has a reasonable size.\\n', '     if (shape.dimensions().empty()) {\\n', '-      return shape_size;\\n', '     }\\n', ' \\n', '     for (int64 dim : shape.dimensions()) {\\n', '-      shape_size = MultiplyWithoutOverflow(shape_size, dim);\\n', '-      if (shape_size < 0) {\\n', '-        return shape_size;\\n', '       }\\n', '     }\\n', '-    shape_size = MultiplyWithoutOverflow(\\n', '-        shape_size, ByteSizeOfPrimitiveType(shape.element_type()));\\n', '-\\n', '-    return shape_size;\\n', '   }();\\n', ' \\n', '   if (shape_size < 0) {']",
                    "hunk_fix": "@@ -883,40 +883,51 @@ StatusOr<Shape> ParseShapeStringInternal(tensorflow::StringPiece* s) {\n   }\n \n   int64 shape_size = [&shape]() {\n-    int64 shape_size;\n     if (LayoutUtil::IsSparseArray(shape)) {\n-      shape_size = LayoutUtil::MaxSparseElements(shape.layout());\n-      if (shape_size < 0) {\n-        return shape_size;\n+      int64 max_sparse_elements = LayoutUtil::MaxSparseElements(shape.layout());\n+      if (max_sparse_elements < 0) {\n+        return max_sparse_elements;\n       }\n-      shape_size = MultiplyWithoutOverflow(shape_size, ShapeUtil::Rank(shape));\n-      if (shape_size < 0) {\n-        return shape_size;\n+      int64 sparse_elements_size = MultiplyWithoutOverflow(\n+          max_sparse_elements, ByteSizeOfPrimitiveType(shape.element_type()));\n+      if (sparse_elements_size < 0) {\n+        return sparse_elements_size;\n       }\n-      shape_size = MultiplyWithoutOverflow(shape_size, sizeof(int64));\n-      if (shape_size < 0) {\n-        return shape_size;\n+      int64 sparse_indices_size =\n+          MultiplyWithoutOverflow(max_sparse_elements, ShapeUtil::Rank(shape));\n+      if (sparse_indices_size < 0) {\n+        return sparse_indices_size;\n+      }\n+      sparse_indices_size =\n+          MultiplyWithoutOverflow(sparse_indices_size, sizeof(int64));\n+      if (sparse_indices_size < 0) {\n+        return sparse_indices_size;\n+      }\n+      // At this point, both sparse_indices_size and sparse_elements_size are\n+      // non-negative, so we can easily check if adding them wraps.\n+      if (static_cast<uint64>(sparse_elements_size) +\n+              static_cast<uint64>(sparse_indices_size) >\n+          INT64_MAX) {\n+        return static_cast<int64>(-1);\n       }\n     }\n \n-    shape_size = 1;\n-\n     // This is intentionally unconditional: even if the shape is sparse, we want\n     // to verify the densified version has a reasonable size.\n+    int64 dense_shape_size = 1;\n     if (shape.dimensions().empty()) {\n-      return shape_size;\n+      return dense_shape_size;\n     }\n \n     for (int64 dim : shape.dimensions()) {\n-      shape_size = MultiplyWithoutOverflow(shape_size, dim);\n-      if (shape_size < 0) {\n-        return shape_size;\n+      dense_shape_size = MultiplyWithoutOverflow(dense_shape_size, dim);\n+      if (dense_shape_size < 0) {\n+        return dense_shape_size;\n       }\n     }\n-    shape_size = MultiplyWithoutOverflow(\n-        shape_size, ByteSizeOfPrimitiveType(shape.element_type()));\n-\n-    return shape_size;\n+    dense_shape_size = MultiplyWithoutOverflow(\n+        dense_shape_size, ByteSizeOfPrimitiveType(shape.element_type()));\n+    return dense_shape_size;\n   }();\n \n   if (shape_size < 0) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 273,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/9596f534200201bc8206b297f17ec3c5cc9fcff8",
    "date": "2018-06-21T12:35:45-07:00",
    "message": "Merge pull request #20153 from permutohedra/master\n\nFix OOB check for result_index in header generation",
    "changes": [
        {
            "name": "codegen.cc",
            "path": "tensorflow/compiler/aot/codegen.cc",
            "patches": [
                {
                    "old_start": 287,
                    "old_length": 7,
                    "new_start": 287,
                    "new_length": 7,
                    "hunk_buggy": "['   TF_RETURN_IF_ERROR(ValidateFeedFetchCppNames(config));\\n', '   const int64 result_index = compile_result.aot->result_buffer_index();\\n', '   const xla::BufferSizes& temp_sizes = compile_result.aot->buffer_sizes();\\n', '-  if (result_index < 0 || result_index > temp_sizes.size()) {\\n', '     return errors::InvalidArgument(\"result index: \", result_index,\\n', '                                    \" is outside the range of temp sizes: [0,\",\\n', '                                    temp_sizes.size(), \")\");']",
                    "hunk_fix": "@@ -287,7 +287,7 @@ Status GenerateHeader(const CodegenOpts& opts, const tf2xla::Config& config,\n   TF_RETURN_IF_ERROR(ValidateFeedFetchCppNames(config));\n   const int64 result_index = compile_result.aot->result_buffer_index();\n   const xla::BufferSizes& temp_sizes = compile_result.aot->buffer_sizes();\n-  if (result_index < 0 || result_index > temp_sizes.size()) {\n+  if (result_index < 0 || result_index >= temp_sizes.size()) {\n     return errors::InvalidArgument(\"result index: \", result_index,\n                                    \" is outside the range of temp sizes: [0,\",\n                                    temp_sizes.size(), \")\");"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 274,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/58759659ee547a957c5d36e72f2274ab34fdb6cb",
    "date": "2018-06-20T11:02:39-07:00",
    "message": "Fix OOB check for result_index in header generation",
    "changes": [
        {
            "name": "codegen.cc",
            "path": "tensorflow/compiler/aot/codegen.cc",
            "patches": [
                {
                    "old_start": 287,
                    "old_length": 7,
                    "new_start": 287,
                    "new_length": 7,
                    "hunk_buggy": "['   TF_RETURN_IF_ERROR(ValidateFeedFetchCppNames(config));\\n', '   const int64 result_index = compile_result.aot->result_buffer_index();\\n', '   const xla::BufferSizes& temp_sizes = compile_result.aot->buffer_sizes();\\n', '-  if (result_index < 0 || result_index > temp_sizes.size()) {\\n', '     return errors::InvalidArgument(\"result index: \", result_index,\\n', '                                    \" is outside the range of temp sizes: [0,\",\\n', '                                    temp_sizes.size(), \")\");']",
                    "hunk_fix": "@@ -287,7 +287,7 @@ Status GenerateHeader(const CodegenOpts& opts, const tf2xla::Config& config,\n   TF_RETURN_IF_ERROR(ValidateFeedFetchCppNames(config));\n   const int64 result_index = compile_result.aot->result_buffer_index();\n   const xla::BufferSizes& temp_sizes = compile_result.aot->buffer_sizes();\n-  if (result_index < 0 || result_index > temp_sizes.size()) {\n+  if (result_index < 0 || result_index >= temp_sizes.size()) {\n     return errors::InvalidArgument(\"result index: \", result_index,\n                                    \" is outside the range of temp sizes: [0,\",\n                                    temp_sizes.size(), \")\");"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 275,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/3550ef89bc66d03b6e2db8e47bf7b038d9f4ceff",
    "date": "2018-06-18T14:17:23-07:00",
    "message": "Convert CheckInputsSize to return a Status instead of CHECK-failing, and convert existing callsites to TF_QCHECK_OK the call.\n\nThis moves us towards the goal of returning Statuses instead of check-failing in ImportTensorFlowNode().\n\nPiperOrigin-RevId: 201056489",
    "changes": [
        {
            "name": "import_tensorflow.cc",
            "path": "tensorflow/contrib/lite/toco/import_tensorflow.cc",
            "patches": [
                {
                    "old_start": 426,
                    "old_length": 18,
                    "new_start": 426,
                    "new_length": 19,
                    "hunk_buggy": "['         return i;\\n', '       }\\n', '     }\\n', '-    return node.input_size();\\n', '-  } else {\\n', '-    return node.input_size();\\n', '   }\\n', ' }\\n', ' \\n', '-void CheckInputsCount(const NodeDef& node,\\n', '-                      const TensorFlowImportFlags& tf_import_flags,\\n', '-                      int expected_input_count) {\\n', '-  QCHECK_EQ(GetInputsCount(node, tf_import_flags), expected_input_count)\\n', '-      << node.op() << \" node expects \" << expected_input_count\\n', '-      << \" input(s) other than control dependencies: \" << node.DebugString();\\n', ' }\\n', ' \\n', ' template <ArrayDataType T>\\n']",
                    "hunk_fix": "@@ -426,18 +426,19 @@ int GetInputsCount(const NodeDef& node,\n         return i;\n       }\n     }\n-    return node.input_size();\n-  } else {\n-    return node.input_size();\n   }\n+  return node.input_size();\n }\n \n-void CheckInputsCount(const NodeDef& node,\n-                      const TensorFlowImportFlags& tf_import_flags,\n-                      int expected_input_count) {\n-  QCHECK_EQ(GetInputsCount(node, tf_import_flags), expected_input_count)\n-      << node.op() << \" node expects \" << expected_input_count\n-      << \" input(s) other than control dependencies: \" << node.DebugString();\n+tensorflow::Status CheckInputsCount(\n+    const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\n+    int expected_input_count) {\n+  if (GetInputsCount(node, tf_import_flags) != expected_input_count) {\n+    return tensorflow::errors::FailedPrecondition(\n+        node.op(), \" node expects \", expected_input_count,\n+        \" input(s) other than control dependencies: \", node.DebugString());\n+  }\n+  return tensorflow::Status::OK();\n }\n \n template <ArrayDataType T>\n"
                },
                {
                    "old_start": 504,
                    "old_length": 7,
                    "new_start": 505,
                    "new_length": 7,
                    "hunk_buggy": "['     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\\n', '     Model* model) {\\n', '   CHECK_EQ(node.op(), \"Conv2D\");\\n', '-  CheckInputsCount(node, tf_import_flags, 2);\\n', ' \\n', '   // We only support NHWC, which is the default data_format.\\n', \"   // So if data_format is not defined, we're all good.\\n\"]",
                    "hunk_fix": "@@ -504,7 +505,7 @@ tensorflow::Status ConvertConvOperator(\n     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\n     Model* model) {\n   CHECK_EQ(node.op(), \"Conv2D\");\n-  CheckInputsCount(node, tf_import_flags, 2);\n+  TF_RETURN_IF_ERROR(CheckInputsCount(node, tf_import_flags, 2));\n \n   // We only support NHWC, which is the default data_format.\n   // So if data_format is not defined, we're all good.\n"
                },
                {
                    "old_start": 578,
                    "old_length": 7,
                    "new_start": 579,
                    "new_length": 7,
                    "hunk_buggy": "['     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\\n', '     Model* model) {\\n', '   CHECK_EQ(node.op(), \"DepthwiseConv2dNative\");\\n', '-  CheckInputsCount(node, tf_import_flags, 2);\\n', ' \\n', '   // We only support NHWC, which is the default data_format.\\n', \"   // So if data_format is not defined, we're all good.\\n\"]",
                    "hunk_fix": "@@ -578,7 +579,7 @@ tensorflow::Status ConvertDepthwiseConvOperator(\n     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\n     Model* model) {\n   CHECK_EQ(node.op(), \"DepthwiseConv2dNative\");\n-  CheckInputsCount(node, tf_import_flags, 2);\n+  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 2));\n \n   // We only support NHWC, which is the default data_format.\n   // So if data_format is not defined, we're all good.\n"
                },
                {
                    "old_start": 632,
                    "old_length": 7,
                    "new_start": 633,
                    "new_length": 7,
                    "hunk_buggy": "['     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\\n', '     Model* model) {\\n', '   CHECK_EQ(node.op(), \"DepthToSpace\");\\n', '-  CheckInputsCount(node, tf_import_flags, 1);\\n', ' \\n', '   CHECK_EQ(GetDataTypeAttr(node, \"T\"), DT_FLOAT);\\n', '   auto* op = new DepthToSpaceOperator;\\n']",
                    "hunk_fix": "@@ -632,7 +633,7 @@ tensorflow::Status ConvertDepthToSpaceOperator(\n     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\n     Model* model) {\n   CHECK_EQ(node.op(), \"DepthToSpace\");\n-  CheckInputsCount(node, tf_import_flags, 1);\n+  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 1));\n \n   CHECK_EQ(GetDataTypeAttr(node, \"T\"), DT_FLOAT);\n   auto* op = new DepthToSpaceOperator;\n"
                },
                {
                    "old_start": 648,
                    "old_length": 7,
                    "new_start": 649,
                    "new_length": 7,
                    "hunk_buggy": "['     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\\n', '     Model* model) {\\n', '   CHECK_EQ(node.op(), \"SpaceToDepth\");\\n', '-  CheckInputsCount(node, tf_import_flags, 1);\\n', ' \\n', '   tensorflow::DataType dtype = GetDataTypeAttr(node, \"T\");\\n', '   if (dtype != DT_FLOAT && dtype != DT_UINT8 && dtype != DT_INT32 &&\\n']",
                    "hunk_fix": "@@ -648,7 +649,7 @@ tensorflow::Status ConvertSpaceToDepthOperator(\n     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\n     Model* model) {\n   CHECK_EQ(node.op(), \"SpaceToDepth\");\n-  CheckInputsCount(node, tf_import_flags, 1);\n+  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 1));\n \n   tensorflow::DataType dtype = GetDataTypeAttr(node, \"T\");\n   if (dtype != DT_FLOAT && dtype != DT_UINT8 && dtype != DT_INT32 &&\n"
                },
                {
                    "old_start": 671,
                    "old_length": 7,
                    "new_start": 672,
                    "new_length": 7,
                    "hunk_buggy": "['     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\\n', '     Model* model) {\\n', '   CHECK_EQ(node.op(), \"BiasAdd\");\\n', '-  CheckInputsCount(node, tf_import_flags, 2);\\n', ' \\n', '   const auto& input_name = node.input(0);\\n', '   const auto& bias_name = node.input(1);\\n']",
                    "hunk_fix": "@@ -671,7 +672,7 @@ tensorflow::Status ConvertBiasAddOperator(\n     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\n     Model* model) {\n   CHECK_EQ(node.op(), \"BiasAdd\");\n-  CheckInputsCount(node, tf_import_flags, 2);\n+  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 2));\n \n   const auto& input_name = node.input(0);\n   const auto& bias_name = node.input(1);\n"
                },
                {
                    "old_start": 688,
                    "old_length": 7,
                    "new_start": 689,
                    "new_length": 7,
                    "hunk_buggy": "['     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\\n', '     Model* model) {\\n', '   CHECK_EQ(node.op(), \"RandomUniform\");\\n', '-  CheckInputsCount(node, tf_import_flags, 1);\\n', ' \\n', '   CHECK_EQ(GetDataTypeAttr(node, \"T\"), DT_INT32);\\n', '   auto op = absl::make_unique<RandomUniformOperator>();\\n']",
                    "hunk_fix": "@@ -688,7 +689,7 @@ tensorflow::Status ConvertRandomUniform(\n     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\n     Model* model) {\n   CHECK_EQ(node.op(), \"RandomUniform\");\n-  CheckInputsCount(node, tf_import_flags, 1);\n+  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 1));\n \n   CHECK_EQ(GetDataTypeAttr(node, \"T\"), DT_INT32);\n   auto op = absl::make_unique<RandomUniformOperator>();\n"
                },
                {
                    "old_start": 728,
                    "old_length": 7,
                    "new_start": 729,
                    "new_length": 7,
                    "hunk_buggy": "['     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\\n', '     Model* model) {\\n', '   CHECK_EQ(node.op(), \"FakeQuantWithMinMaxArgs\");\\n', '-  CheckInputsCount(node, tf_import_flags, 1);\\n', '   auto* op = new FakeQuantOperator;\\n', '   op->inputs.push_back(node.input(0));\\n', '   op->minmax.reset(new MinMax);\\n']",
                    "hunk_fix": "@@ -728,7 +729,7 @@ tensorflow::Status ConvertFakeQuantWithMinMaxArgs(\n     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\n     Model* model) {\n   CHECK_EQ(node.op(), \"FakeQuantWithMinMaxArgs\");\n-  CheckInputsCount(node, tf_import_flags, 1);\n+  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 1));\n   auto* op = new FakeQuantOperator;\n   op->inputs.push_back(node.input(0));\n   op->minmax.reset(new MinMax);\n"
                },
                {
                    "old_start": 765,
                    "old_length": 7,
                    "new_start": 766,
                    "new_length": 7,
                    "hunk_buggy": "['     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\\n', '     Model* model) {\\n', '   CHECK_EQ(node.op(), \"Squeeze\");\\n', '-  CheckInputsCount(node, tf_import_flags, 1);\\n', '   auto* op = new SqueezeOperator;\\n', '   op->inputs.push_back(node.input(0));\\n', '   op->outputs.push_back(node.name());\\n']",
                    "hunk_fix": "@@ -765,7 +766,7 @@ tensorflow::Status ConvertSqueezeOperator(\n     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\n     Model* model) {\n   CHECK_EQ(node.op(), \"Squeeze\");\n-  CheckInputsCount(node, tf_import_flags, 1);\n+  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 1));\n   auto* op = new SqueezeOperator;\n   op->inputs.push_back(node.input(0));\n   op->outputs.push_back(node.name());\n"
                },
                {
                    "old_start": 786,
                    "old_length": 7,
                    "new_start": 787,
                    "new_length": 7,
                    "hunk_buggy": "['     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\\n', '     Model* model) {\\n', '   CHECK_EQ(node.op(), \"Sum\");\\n', '-  CheckInputsCount(node, tf_import_flags, 2);\\n', '   auto* op = new TensorFlowSumOperator;\\n', '   op->inputs.push_back(node.input(0));\\n', '   op->inputs.push_back(node.input(1));\\n']",
                    "hunk_fix": "@@ -786,7 +787,7 @@ tensorflow::Status ConvertSumOperator(\n     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\n     Model* model) {\n   CHECK_EQ(node.op(), \"Sum\");\n-  CheckInputsCount(node, tf_import_flags, 2);\n+  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 2));\n   auto* op = new TensorFlowSumOperator;\n   op->inputs.push_back(node.input(0));\n   op->inputs.push_back(node.input(1));\n"
                },
                {
                    "old_start": 802,
                    "old_length": 7,
                    "new_start": 803,
                    "new_length": 7,
                    "hunk_buggy": "['     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\\n', '     Model* model) {\\n', '   CHECK_EQ(node.op(), \"Split\");\\n', '-  CheckInputsCount(node, tf_import_flags, 2);\\n', '   auto* op = new TensorFlowSplitOperator;\\n', '   op->inputs.push_back(node.input(0));\\n', '   op->inputs.push_back(node.input(1));\\n']",
                    "hunk_fix": "@@ -802,7 +803,7 @@ tensorflow::Status ConvertSplitOperator(\n     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\n     Model* model) {\n   CHECK_EQ(node.op(), \"Split\");\n-  CheckInputsCount(node, tf_import_flags, 2);\n+  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 2));\n   auto* op = new TensorFlowSplitOperator;\n   op->inputs.push_back(node.input(0));\n   op->inputs.push_back(node.input(1));\n"
                },
                {
                    "old_start": 820,
                    "old_length": 7,
                    "new_start": 821,
                    "new_length": 7,
                    "hunk_buggy": "['     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\\n', '     Model* model) {\\n', '   CHECK_EQ(node.op(), \"Switch\");\\n', '-  CheckInputsCount(node, tf_import_flags, 2);\\n', '   auto* op = new TensorFlowSwitchOperator;\\n', '   op->inputs.push_back(node.input(0));\\n', '   op->inputs.push_back(node.input(1));\\n']",
                    "hunk_fix": "@@ -820,7 +821,7 @@ tensorflow::Status ConvertSwitchOperator(\n     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\n     Model* model) {\n   CHECK_EQ(node.op(), \"Switch\");\n-  CheckInputsCount(node, tf_import_flags, 2);\n+  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 2));\n   auto* op = new TensorFlowSwitchOperator;\n   op->inputs.push_back(node.input(0));\n   op->inputs.push_back(node.input(1));\n"
                },
                {
                    "old_start": 835,
                    "old_length": 7,
                    "new_start": 836,
                    "new_length": 7,
                    "hunk_buggy": "['     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\\n', '     Model* model) {\\n', '   CHECK_EQ(node.op(), \"Softmax\");\\n', '-  CheckInputsCount(node, tf_import_flags, 1);\\n', '   const auto& input_name = node.input(0);\\n', '   auto* softmax = new SoftmaxOperator;\\n', '   softmax->inputs.push_back(input_name);\\n']",
                    "hunk_fix": "@@ -835,7 +836,7 @@ tensorflow::Status ConvertSoftmaxOperator(\n     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\n     Model* model) {\n   CHECK_EQ(node.op(), \"Softmax\");\n-  CheckInputsCount(node, tf_import_flags, 1);\n+  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 1));\n   const auto& input_name = node.input(0);\n   auto* softmax = new SoftmaxOperator;\n   softmax->inputs.push_back(input_name);\n"
                },
                {
                    "old_start": 851,
                    "old_length": 7,
                    "new_start": 852,
                    "new_length": 7,
                    "hunk_buggy": "['     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\\n', '     Model* model) {\\n', '   CHECK_EQ(node.op(), \"LRN\");\\n', '-  CheckInputsCount(node, tf_import_flags, 1);\\n', '   const auto& input_name = node.input(0);\\n', '   auto* lrn = new LocalResponseNormalizationOperator;\\n', '   lrn->inputs.push_back(input_name);\\n']",
                    "hunk_fix": "@@ -851,7 +852,7 @@ tensorflow::Status ConvertLRNOperator(\n     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\n     Model* model) {\n   CHECK_EQ(node.op(), \"LRN\");\n-  CheckInputsCount(node, tf_import_flags, 1);\n+  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 1));\n   const auto& input_name = node.input(0);\n   auto* lrn = new LocalResponseNormalizationOperator;\n   lrn->inputs.push_back(input_name);\n"
                },
                {
                    "old_start": 868,
                    "old_length": 7,
                    "new_start": 869,
                    "new_length": 7,
                    "hunk_buggy": "['     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\\n', '     Model* model) {\\n', '   CHECK_EQ(node.op(), \"MaxPool\");\\n', '-  CheckInputsCount(node, tf_import_flags, 1);\\n', '   const auto& input_name = node.input(0);\\n', '   // We only support NHWC, which is the default data_format.\\n', \"   // So if data_format is not defined, we're all good.\\n\"]",
                    "hunk_fix": "@@ -868,7 +869,7 @@ tensorflow::Status ConvertMaxPoolOperator(\n     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\n     Model* model) {\n   CHECK_EQ(node.op(), \"MaxPool\");\n-  CheckInputsCount(node, tf_import_flags, 1);\n+  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 1));\n   const auto& input_name = node.input(0);\n   // We only support NHWC, which is the default data_format.\n   // So if data_format is not defined, we're all good.\n"
                },
                {
                    "old_start": 911,
                    "old_length": 7,
                    "new_start": 912,
                    "new_length": 7,
                    "hunk_buggy": "['     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\\n', '     Model* model) {\\n', '   CHECK_EQ(node.op(), \"AvgPool\");\\n', '-  CheckInputsCount(node, tf_import_flags, 1);\\n', '   const auto& input_name = node.input(0);\\n', '   // We only support NHWC, which is the default data_format.\\n', \"   // So if data_format is not defined, we're all good.\\n\"]",
                    "hunk_fix": "@@ -911,7 +912,7 @@ tensorflow::Status ConvertAvgPoolOperator(\n     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\n     Model* model) {\n   CHECK_EQ(node.op(), \"AvgPool\");\n-  CheckInputsCount(node, tf_import_flags, 1);\n+  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 1));\n   const auto& input_name = node.input(0);\n   // We only support NHWC, which is the default data_format.\n   // So if data_format is not defined, we're all good.\n"
                },
                {
                    "old_start": 949,
                    "old_length": 7,
                    "new_start": 950,
                    "new_length": 7,
                    "hunk_buggy": "[' tensorflow::Status ConvertBatchMatMulOperator(\\n', '     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\\n', '     Model* model) {\\n', '-  CheckInputsCount(node, tf_import_flags, 2);\\n', ' \\n', '   // https://www.tensorflow.org/versions/r0.12/api_docs/python/math_ops/matrix_math_functions\\n', '   CHECK(!HasAttr(node, \"adj_a\") || (GetBoolAttr(node, \"adj_a\") == false));\\n']",
                    "hunk_fix": "@@ -949,7 +950,7 @@ tensorflow::Status ConvertAvgPoolOperator(\n tensorflow::Status ConvertBatchMatMulOperator(\n     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\n     Model* model) {\n-  CheckInputsCount(node, tf_import_flags, 2);\n+  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 2));\n \n   // https://www.tensorflow.org/versions/r0.12/api_docs/python/math_ops/matrix_math_functions\n   CHECK(!HasAttr(node, \"adj_a\") || (GetBoolAttr(node, \"adj_a\") == false));\n"
                },
                {
                    "old_start": 965,
                    "old_length": 7,
                    "new_start": 966,
                    "new_length": 7,
                    "hunk_buggy": "[' tensorflow::Status ConvertMatMulOperator(\\n', '     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\\n', '     Model* model) {\\n', '-  CheckInputsCount(node, tf_import_flags, 2);\\n', ' \\n', \"   // Transpose flags should be easy to support, but we don't have a\\n\", '   // GraphDef with them to test on at the moment.\\n']",
                    "hunk_fix": "@@ -965,7 +966,7 @@ tensorflow::Status ConvertBatchMatMulOperator(\n tensorflow::Status ConvertMatMulOperator(\n     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\n     Model* model) {\n-  CheckInputsCount(node, tf_import_flags, 2);\n+  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 2));\n \n   // Transpose flags should be easy to support, but we don't have a\n   // GraphDef with them to test on at the moment.\n"
                },
                {
                    "old_start": 1030,
                    "old_length": 7,
                    "new_start": 1031,
                    "new_length": 7,
                    "hunk_buggy": "[' tensorflow::Status ConvertSimpleOperator(\\n', '     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\\n', '     Model* model) {\\n', '-  CheckInputsCount(node, tf_import_flags, NumInputs);\\n', '   return ConvertSimpleOperator<Op>(node, tf_import_flags, model);\\n', ' }\\n', ' \\n']",
                    "hunk_fix": "@@ -1030,7 +1031,7 @@ template <typename Op, unsigned int NumInputs>\n tensorflow::Status ConvertSimpleOperator(\n     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\n     Model* model) {\n-  CheckInputsCount(node, tf_import_flags, NumInputs);\n+  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, NumInputs));\n   return ConvertSimpleOperator<Op>(node, tf_import_flags, model);\n }\n \n"
                },
                {
                    "old_start": 1038,
                    "old_length": 7,
                    "new_start": 1039,
                    "new_length": 7,
                    "hunk_buggy": "['     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\\n', '     Model* model) {\\n', '   CHECK_EQ(node.op(), \"Max\");\\n', '-  CheckInputsCount(node, tf_import_flags, 2);\\n', '   auto* op = new TensorFlowMaxOperator;\\n', '   op->inputs.push_back(node.input(0));\\n', '   op->inputs.push_back(node.input(1));\\n']",
                    "hunk_fix": "@@ -1038,7 +1039,7 @@ tensorflow::Status ConvertMaxOperator(\n     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\n     Model* model) {\n   CHECK_EQ(node.op(), \"Max\");\n-  CheckInputsCount(node, tf_import_flags, 2);\n+  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 2));\n   auto* op = new TensorFlowMaxOperator;\n   op->inputs.push_back(node.input(0));\n   op->inputs.push_back(node.input(1));\n"
                },
                {
                    "old_start": 1054,
                    "old_length": 7,
                    "new_start": 1055,
                    "new_length": 7,
                    "hunk_buggy": "['     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\\n', '     Model* model) {\\n', '   CHECK_EQ(node.op(), \"Min\");\\n', '-  CheckInputsCount(node, tf_import_flags, 2);\\n', '   auto* op = new TensorFlowMinOperator;\\n', '   op->inputs.push_back(node.input(0));\\n', '   op->inputs.push_back(node.input(1));\\n']",
                    "hunk_fix": "@@ -1054,7 +1055,7 @@ tensorflow::Status ConvertMinOperator(\n     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\n     Model* model) {\n   CHECK_EQ(node.op(), \"Min\");\n-  CheckInputsCount(node, tf_import_flags, 2);\n+  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 2));\n   auto* op = new TensorFlowMinOperator;\n   op->inputs.push_back(node.input(0));\n   op->inputs.push_back(node.input(1));\n"
                },
                {
                    "old_start": 1100,
                    "old_length": 7,
                    "new_start": 1101,
                    "new_length": 7,
                    "hunk_buggy": "['   CHECK_EQ(node.op(), \"StridedSlice\");\\n', '   // TODO(soroosh): The 4th input (strides) should be e optional, to be\\n', '   // consistent with TF.\\n', '-  CheckInputsCount(node, tf_import_flags, 4);\\n', ' \\n', '   auto* op = new StridedSliceOperator;\\n', '   for (const auto& input : node.input()) {\\n']",
                    "hunk_fix": "@@ -1100,7 +1101,7 @@ tensorflow::Status ConvertStridedSliceOperator(\n   CHECK_EQ(node.op(), \"StridedSlice\");\n   // TODO(soroosh): The 4th input (strides) should be e optional, to be\n   // consistent with TF.\n-  CheckInputsCount(node, tf_import_flags, 4);\n+  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 4));\n \n   auto* op = new StridedSliceOperator;\n   for (const auto& input : node.input()) {\n"
                },
                {
                    "old_start": 1128,
                    "old_length": 7,
                    "new_start": 1129,
                    "new_length": 7,
                    "hunk_buggy": "['     Model* model) {\\n', '   CHECK(node.op() == \"Placeholder\" || node.op() == \"LegacyFedInput\");\\n', '   if (node.op() == \"Placeholder\") {\\n', '-    CheckInputsCount(node, tf_import_flags, 0);\\n', '   }\\n', '   auto& array = model->GetOrCreateArray(node.name());\\n', '   if (node.attr().count(\"dtype\")) {\\n']",
                    "hunk_fix": "@@ -1128,7 +1129,7 @@ tensorflow::Status ConvertPlaceholderOperator(\n     Model* model) {\n   CHECK(node.op() == \"Placeholder\" || node.op() == \"LegacyFedInput\");\n   if (node.op() == \"Placeholder\") {\n-    CheckInputsCount(node, tf_import_flags, 0);\n+    TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 0));\n   }\n   auto& array = model->GetOrCreateArray(node.name());\n   if (node.attr().count(\"dtype\")) {\n"
                },
                {
                    "old_start": 1166,
                    "old_length": 7,
                    "new_start": 1167,
                    "new_length": 7,
                    "hunk_buggy": "['     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\\n', '     Model* model) {\\n', '   CHECK_EQ(node.op(), \"Cast\");\\n', '-  CheckInputsCount(node, tf_import_flags, 1);\\n', '   const auto tf_src_dtype = GetDataTypeAttr(node, \"SrcT\");\\n', '   const auto tf_dst_dtype = GetDataTypeAttr(node, \"DstT\");\\n', '   auto* op = new CastOperator;\\n']",
                    "hunk_fix": "@@ -1166,7 +1167,7 @@ tensorflow::Status ConvertCastOperator(\n     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\n     Model* model) {\n   CHECK_EQ(node.op(), \"Cast\");\n-  CheckInputsCount(node, tf_import_flags, 1);\n+  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 1));\n   const auto tf_src_dtype = GetDataTypeAttr(node, \"SrcT\");\n   const auto tf_dst_dtype = GetDataTypeAttr(node, \"DstT\");\n   auto* op = new CastOperator;\n"
                },
                {
                    "old_start": 1182,
                    "old_length": 7,
                    "new_start": 1183,
                    "new_length": 7,
                    "hunk_buggy": "['     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\\n', '     Model* model) {\\n', '   CHECK_EQ(node.op(), \"Floor\");\\n', '-  CheckInputsCount(node, tf_import_flags, 1);\\n', '   const auto data_type = GetDataTypeAttr(node, \"T\");\\n', '   CHECK(data_type == DT_FLOAT);\\n', '   auto* op = new FloorOperator;\\n']",
                    "hunk_fix": "@@ -1182,7 +1183,7 @@ tensorflow::Status ConvertFloorOperator(\n     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\n     Model* model) {\n   CHECK_EQ(node.op(), \"Floor\");\n-  CheckInputsCount(node, tf_import_flags, 1);\n+  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 1));\n   const auto data_type = GetDataTypeAttr(node, \"T\");\n   CHECK(data_type == DT_FLOAT);\n   auto* op = new FloorOperator;\n"
                },
                {
                    "old_start": 1196,
                    "old_length": 8,
                    "new_start": 1197,
                    "new_length": 10,
                    "hunk_buggy": "['     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\\n', '     Model* model) {\\n', '   CHECK(node.op() == \"Gather\" || node.op() == \"GatherV2\");\\n', '-  if (node.op() == \"Gather\") CheckInputsCount(node, tf_import_flags, 2);\\n', '-  if (node.op() == \"GatherV2\") CheckInputsCount(node, tf_import_flags, 3);\\n', '   const auto indices_data_type = GetDataTypeAttr(node, \"Tindices\");\\n', '   CHECK(indices_data_type == DT_INT32 || indices_data_type == DT_INT64);\\n', '   auto* op = new GatherOperator;\\n']",
                    "hunk_fix": "@@ -1196,8 +1197,10 @@ tensorflow::Status ConvertGatherOperator(\n     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\n     Model* model) {\n   CHECK(node.op() == \"Gather\" || node.op() == \"GatherV2\");\n-  if (node.op() == \"Gather\") CheckInputsCount(node, tf_import_flags, 2);\n-  if (node.op() == \"GatherV2\") CheckInputsCount(node, tf_import_flags, 3);\n+  if (node.op() == \"Gather\")\n+    TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 2));\n+  if (node.op() == \"GatherV2\")\n+    TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 3));\n   const auto indices_data_type = GetDataTypeAttr(node, \"Tindices\");\n   CHECK(indices_data_type == DT_INT32 || indices_data_type == DT_INT64);\n   auto* op = new GatherOperator;\n"
                },
                {
                    "old_start": 1214,
                    "old_length": 7,
                    "new_start": 1217,
                    "new_length": 7,
                    "hunk_buggy": "['     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\\n', '     Model* model) {\\n', '   CHECK_EQ(node.op(), \"ArgMax\");\\n', '-  CheckInputsCount(node, tf_import_flags, 2);\\n', '   const auto axis_data_type =\\n', '       HasAttr(node, \"Tidx\") ? GetDataTypeAttr(node, \"Tidx\") : DT_INT32;\\n', '   const auto output_type = HasAttr(node, \"output_type\")\\n']",
                    "hunk_fix": "@@ -1214,7 +1217,7 @@ tensorflow::Status ConvertArgMaxOperator(\n     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\n     Model* model) {\n   CHECK_EQ(node.op(), \"ArgMax\");\n-  CheckInputsCount(node, tf_import_flags, 2);\n+  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 2));\n   const auto axis_data_type =\n       HasAttr(node, \"Tidx\") ? GetDataTypeAttr(node, \"Tidx\") : DT_INT32;\n   const auto output_type = HasAttr(node, \"output_type\")\n"
                },
                {
                    "old_start": 1235,
                    "old_length": 7,
                    "new_start": 1238,
                    "new_length": 7,
                    "hunk_buggy": "['     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\\n', '     Model* model) {\\n', '   CHECK_EQ(node.op(), \"ResizeBilinear\");\\n', '-  CheckInputsCount(node, tf_import_flags, 2);\\n', '   auto* op = new ResizeBilinearOperator;\\n', ' \\n', '   op->align_corners = false;\\n']",
                    "hunk_fix": "@@ -1235,7 +1238,7 @@ tensorflow::Status ConvertResizeBilinearOperator(\n     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\n     Model* model) {\n   CHECK_EQ(node.op(), \"ResizeBilinear\");\n-  CheckInputsCount(node, tf_import_flags, 2);\n+  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 2));\n   auto* op = new ResizeBilinearOperator;\n \n   op->align_corners = false;\n"
                },
                {
                    "old_start": 1254,
                    "old_length": 7,
                    "new_start": 1257,
                    "new_length": 7,
                    "hunk_buggy": "['     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\\n', '     Model* model) {\\n', '   CHECK_EQ(node.op(), \"BatchNormWithGlobalNormalization\");\\n', '-  CheckInputsCount(node, tf_import_flags, 5);\\n', ' \\n', '   // TODO(ahentz): to really match tensorflow we need to add variance_epsilon\\n', '   // to the input, before feeding it into TensorFlowRsqrtOperator.\\n']",
                    "hunk_fix": "@@ -1254,7 +1257,7 @@ tensorflow::Status ConvertBatchNormWithGlobalNormalizationOperator(\n     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\n     Model* model) {\n   CHECK_EQ(node.op(), \"BatchNormWithGlobalNormalization\");\n-  CheckInputsCount(node, tf_import_flags, 5);\n+  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 5));\n \n   // TODO(ahentz): to really match tensorflow we need to add variance_epsilon\n   // to the input, before feeding it into TensorFlowRsqrtOperator.\n"
                },
                {
                    "old_start": 1304,
                    "old_length": 7,
                    "new_start": 1307,
                    "new_length": 7,
                    "hunk_buggy": "['     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\\n', '     Model* model) {\\n', '   CHECK_EQ(node.op(), \"FusedBatchNorm\");\\n', '-  CheckInputsCount(node, tf_import_flags, 5);\\n', ' \\n', '   // Declare shortcuts for the inputs.\\n', '   const string& gamma_input = node.input(1);\\n']",
                    "hunk_fix": "@@ -1304,7 +1307,7 @@ tensorflow::Status ConvertFusedBatchNormOperator(\n     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\n     Model* model) {\n   CHECK_EQ(node.op(), \"FusedBatchNorm\");\n-  CheckInputsCount(node, tf_import_flags, 5);\n+  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 5));\n \n   // Declare shortcuts for the inputs.\n   const string& gamma_input = node.input(1);\n"
                },
                {
                    "old_start": 1357,
                    "old_length": 7,
                    "new_start": 1360,
                    "new_length": 7,
                    "hunk_buggy": "['     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\\n', '     Model* model) {\\n', '   CHECK_EQ(node.op(), \"SpaceToBatchND\");\\n', '-  CheckInputsCount(node, tf_import_flags, 3);\\n', '   CHECK_EQ(GetDataTypeAttr(node, \"Tblock_shape\"), DT_INT32);\\n', '   CHECK_EQ(GetDataTypeAttr(node, \"Tpaddings\"), DT_INT32);\\n', '   auto* op = new SpaceToBatchNDOperator;\\n']",
                    "hunk_fix": "@@ -1357,7 +1360,7 @@ tensorflow::Status ConvertSpaceToBatchNDOperator(\n     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\n     Model* model) {\n   CHECK_EQ(node.op(), \"SpaceToBatchND\");\n-  CheckInputsCount(node, tf_import_flags, 3);\n+  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 3));\n   CHECK_EQ(GetDataTypeAttr(node, \"Tblock_shape\"), DT_INT32);\n   CHECK_EQ(GetDataTypeAttr(node, \"Tpaddings\"), DT_INT32);\n   auto* op = new SpaceToBatchNDOperator;\n"
                },
                {
                    "old_start": 1373,
                    "old_length": 7,
                    "new_start": 1376,
                    "new_length": 7,
                    "hunk_buggy": "['     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\\n', '     Model* model) {\\n', '   CHECK_EQ(node.op(), \"BatchToSpaceND\");\\n', '-  CheckInputsCount(node, tf_import_flags, 3);\\n', '   CHECK_EQ(GetDataTypeAttr(node, \"Tblock_shape\"), DT_INT32);\\n', '   CHECK_EQ(GetDataTypeAttr(node, \"Tcrops\"), DT_INT32);\\n', '   auto* op = new BatchToSpaceNDOperator;\\n']",
                    "hunk_fix": "@@ -1373,7 +1376,7 @@ tensorflow::Status ConvertBatchToSpaceNDOperator(\n     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\n     Model* model) {\n   CHECK_EQ(node.op(), \"BatchToSpaceND\");\n-  CheckInputsCount(node, tf_import_flags, 3);\n+  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 3));\n   CHECK_EQ(GetDataTypeAttr(node, \"Tblock_shape\"), DT_INT32);\n   CHECK_EQ(GetDataTypeAttr(node, \"Tcrops\"), DT_INT32);\n   auto* op = new BatchToSpaceNDOperator;\n"
                },
                {
                    "old_start": 1389,
                    "old_length": 7,
                    "new_start": 1392,
                    "new_length": 7,
                    "hunk_buggy": "['     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\\n', '     Model* model) {\\n', '   CHECK_EQ(node.op(), \"Mean\");\\n', '-  CheckInputsCount(node, tf_import_flags, 2);\\n', '   auto* op = new MeanOperator;\\n', '   op->inputs.push_back(node.input(0));\\n', '   op->inputs.push_back(node.input(1));\\n']",
                    "hunk_fix": "@@ -1389,7 +1392,7 @@ tensorflow::Status ConvertMeanOperator(\n     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\n     Model* model) {\n   CHECK_EQ(node.op(), \"Mean\");\n-  CheckInputsCount(node, tf_import_flags, 2);\n+  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 2));\n   auto* op = new MeanOperator;\n   op->inputs.push_back(node.input(0));\n   op->inputs.push_back(node.input(1));\n"
                },
                {
                    "old_start": 1436,
                    "old_length": 7,
                    "new_start": 1439,
                    "new_length": 7,
                    "hunk_buggy": "['     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\\n', '     Model* model) {\\n', '   CHECK_EQ(node.op(), \"Conv2DBackpropInput\");\\n', '-  CheckInputsCount(node, tf_import_flags, 3);\\n', '   auto* op = new TransposeConvOperator;\\n', '   op->inputs.push_back(node.input(0));\\n', '   op->inputs.push_back(node.input(1));\\n']",
                    "hunk_fix": "@@ -1436,7 +1439,7 @@ tensorflow::Status ConvertTransposeConvOperator(\n     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\n     Model* model) {\n   CHECK_EQ(node.op(), \"Conv2DBackpropInput\");\n-  CheckInputsCount(node, tf_import_flags, 3);\n+  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 3));\n   auto* op = new TransposeConvOperator;\n   op->inputs.push_back(node.input(0));\n   op->inputs.push_back(node.input(1));\n"
                },
                {
                    "old_start": 1507,
                    "old_length": 7,
                    "new_start": 1510,
                    "new_length": 7,
                    "hunk_buggy": "['     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\\n', '     Model* model) {\\n', '   CHECK_EQ(node.op(), \"Range\");\\n', '-  CheckInputsCount(node, tf_import_flags, 3);\\n', '   auto* op = new RangeOperator;\\n', '   if (HasAttr(node, \"Tidx\")) {\\n', '     const auto dtype = toco::GetDataTypeAttr(node, \"Tidx\");\\n']",
                    "hunk_fix": "@@ -1507,7 +1510,7 @@ tensorflow::Status ConvertRangeOperator(\n     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\n     Model* model) {\n   CHECK_EQ(node.op(), \"Range\");\n-  CheckInputsCount(node, tf_import_flags, 3);\n+  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 3));\n   auto* op = new RangeOperator;\n   if (HasAttr(node, \"Tidx\")) {\n     const auto dtype = toco::GetDataTypeAttr(node, \"Tidx\");\n"
                },
                {
                    "old_start": 1722,
                    "old_length": 7,
                    "new_start": 1725,
                    "new_length": 7,
                    "hunk_buggy": "['         model, node.name() + \"k\", {static_cast<int32>(GetIntAttr(node, \"k\"))});\\n', '     op->inputs.push_back(k_array);\\n', '   } else {\\n', '-    CheckInputsCount(node, tf_import_flags, 2);\\n', '     op->inputs.push_back(node.input(1));\\n', '   }\\n', '   // The op has two outputs.\\n']",
                    "hunk_fix": "@@ -1722,7 +1725,7 @@ tensorflow::Status ConvertTopKV2Operator(\n         model, node.name() + \"k\", {static_cast<int32>(GetIntAttr(node, \"k\"))});\n     op->inputs.push_back(k_array);\n   } else {\n-    CheckInputsCount(node, tf_import_flags, 2);\n+    TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 2));\n     op->inputs.push_back(node.input(1));\n   }\n   // The op has two outputs.\n"
                },
                {
                    "old_start": 1738,
                    "old_length": 7,
                    "new_start": 1741,
                    "new_length": 7,
                    "hunk_buggy": "['   auto op = absl::make_unique<DynamicPartitionOperator>();\\n', '   CHECK(HasAttr(node, \"num_partitions\"));\\n', '   op->num_partitions = GetIntAttr(node, \"num_partitions\");\\n', '-  CheckInputsCount(node, tf_import_flags, 2);\\n', '   op->inputs.push_back(node.input(0));\\n', '   op->inputs.push_back(node.input(1));\\n', '   CHECK_GT(op->num_partitions, 1);\\n']",
                    "hunk_fix": "@@ -1738,7 +1741,7 @@ tensorflow::Status ConvertDynamicPartitionOperator(\n   auto op = absl::make_unique<DynamicPartitionOperator>();\n   CHECK(HasAttr(node, \"num_partitions\"));\n   op->num_partitions = GetIntAttr(node, \"num_partitions\");\n-  CheckInputsCount(node, tf_import_flags, 2);\n+  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 2));\n   op->inputs.push_back(node.input(0));\n   op->inputs.push_back(node.input(1));\n   CHECK_GT(op->num_partitions, 1);\n"
                },
                {
                    "old_start": 1760,
                    "old_length": 7,
                    "new_start": 1763,
                    "new_length": 7,
                    "hunk_buggy": "['   CHECK(HasAttr(node, \"N\"));\\n', '   op->num_partitions = GetIntAttr(node, \"N\");\\n', '   // Expect all ID partitions + all value partitions.\\n', '-  CheckInputsCount(node, tf_import_flags, op->num_partitions * 2);\\n', '   for (int i = 0; i < op->num_partitions * 2; ++i) {\\n', '     op->inputs.push_back(node.input(i));\\n', '   }\\n']",
                    "hunk_fix": "@@ -1760,7 +1763,7 @@ tensorflow::Status ConvertDynamicStitchOperator(\n   CHECK(HasAttr(node, \"N\"));\n   op->num_partitions = GetIntAttr(node, \"N\");\n   // Expect all ID partitions + all value partitions.\n-  CheckInputsCount(node, tf_import_flags, op->num_partitions * 2);\n+  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, op->num_partitions * 2));\n   for (int i = 0; i < op->num_partitions * 2; ++i) {\n     op->inputs.push_back(node.input(i));\n   }\n"
                },
                {
                    "old_start": 1773,
                    "old_length": 7,
                    "new_start": 1776,
                    "new_length": 7,
                    "hunk_buggy": "['     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\\n', '     Model* model) {\\n', '   CHECK_EQ(node.op(), \"SparseToDense\");\\n', '-  CheckInputsCount(node, tf_import_flags, 4);\\n', ' \\n', '   auto* op = new SparseToDenseOperator;\\n', '   for (const string& input : node.input()) {']",
                    "hunk_fix": "@@ -1773,7 +1776,7 @@ tensorflow::Status ConvertSparseToDenseOperator(\n     const NodeDef& node, const TensorFlowImportFlags& tf_import_flags,\n     Model* model) {\n   CHECK_EQ(node.op(), \"SparseToDense\");\n-  CheckInputsCount(node, tf_import_flags, 4);\n+  TF_QCHECK_OK(CheckInputsCount(node, tf_import_flags, 4));\n \n   auto* op = new SparseToDenseOperator;\n   for (const string& input : node.input()) {"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 276,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/c1b9ac9f215a3a83f7f0b6233bf4cef0b3e74598",
    "date": "2018-06-06T07:53:20-07:00",
    "message": "Error checking in c/python code.\n\nPiperOrigin-RevId: 199465056",
    "changes": [
        {
            "name": "util.cc",
            "path": "tensorflow/python/util/util.cc",
            "patches": [
                {
                    "old_start": 243,
                    "old_length": 6,
                    "new_start": 243,
                    "new_length": 9,
                    "hunk_buggy": "['                               std::vector<Safe_PyObjectPtr>* next_values) {\\n', '   PyObject* item;\\n', '   PyObject* iterator = PyObject_GetIter(nested);\\n', '   while ((item = PyIter_Next(iterator)) != nullptr) {\\n', '     next_values->emplace_back(item);\\n', '   }']",
                    "hunk_fix": "@@ -243,6 +243,9 @@ bool GetNextValuesForIterable(PyObject* nested,\n                               std::vector<Safe_PyObjectPtr>* next_values) {\n   PyObject* item;\n   PyObject* iterator = PyObject_GetIter(nested);\n+  if (iterator == nullptr || PyErr_Occurred()) {\n+    return false;\n+  }\n   while ((item = PyIter_Next(iterator)) != nullptr) {\n     next_values->emplace_back(item);\n   }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 277,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/4a1d1c8413a3752af7dc91a7128e202660b0f05c",
    "date": "2018-05-29T21:15:16+00:00",
    "message": "Fix mismatch of shape restriction in DrawBoundingBoxes\n\nIn the kernel of DrawBoundingBoxes, the shape of the input\nimages should be 4-D. Though in the shape function,\nat the end `UnchangedShapeWithRankAtLeast(c, 3)` was used instead\n(at the beginning of the shape function the validation is\n`WithRank(c->input(0), 4, &images)` which is correct).\n\nThis fix address the discrepancy by changing to `UnchangedShape`.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "changes": [
        {
            "name": "image_ops.cc",
            "path": "tensorflow/core/ops/image_ops.cc",
            "patches": [
                {
                    "old_start": 454,
                    "old_length": 7,
                    "new_start": 454,
                    "new_length": 9,
                    "hunk_buggy": "['       DimensionHandle unused;\\n', '       TF_RETURN_IF_ERROR(c->WithValue(c->Dim(boxes, 2), 4, &unused));\\n', ' \\n', '-      return shape_inference::UnchangedShapeWithRankAtLeast(c, 3);\\n', '     });\\n', ' \\n', ' // --------------------------------------------------------------------------']",
                    "hunk_fix": "@@ -454,7 +454,9 @@ REGISTER_OP(\"DrawBoundingBoxes\")\n       DimensionHandle unused;\n       TF_RETURN_IF_ERROR(c->WithValue(c->Dim(boxes, 2), 4, &unused));\n \n-      return shape_inference::UnchangedShapeWithRankAtLeast(c, 3);\n+      // The rank of the input image (rank = 4) has already been restricted\n+      // above, and the output is of the same shape as the input.\n+      return shape_inference::UnchangedShape(c);\n     });\n \n // --------------------------------------------------------------------------"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 278,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/e4471c403a9e9430839900bd92c067d04580a51b",
    "date": "2018-05-21T15:48:11-07:00",
    "message": "Merge pull request #19254 from yongtang/05122018-compute_accidental_hits\n\nAdd additional shape validation to `compute_accidental_hits`",
    "changes": [
        {
            "name": "candidate_sampling_ops.cc",
            "path": "tensorflow/core/ops/candidate_sampling_ops.cc",
            "patches": [
                {
                    "old_start": 145,
                    "old_length": 12,
                    "new_start": 145,
                    "new_length": 15,
                    "hunk_buggy": "['       int64 num_true;\\n', '       TF_RETURN_IF_ERROR(c->GetAttr(\"num_true\", &num_true));\\n', ' \\n', '-      // Validate true_classes.\\n', '       ShapeHandle true_classes;\\n', '       TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 2, &true_classes));\\n', '       DimensionHandle unused;\\n', '       TF_RETURN_IF_ERROR(\\n', '           c->WithValue(c->Dim(true_classes, 1), num_true, &unused));\\n', ' \\n', '       // All three outputs are the same shape.\\n', '       ShapeHandle v = c->Vector(InferenceContext::kUnknownDim);']",
                    "hunk_fix": "@@ -145,12 +145,15 @@ REGISTER_OP(\"ComputeAccidentalHits\")\n       int64 num_true;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"num_true\", &num_true));\n \n-      // Validate true_classes.\n+      // Validate true_classes, must be a matrix.\n       ShapeHandle true_classes;\n       TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 2, &true_classes));\n       DimensionHandle unused;\n       TF_RETURN_IF_ERROR(\n           c->WithValue(c->Dim(true_classes, 1), num_true, &unused));\n+      // Validate sampled_candidates, must be a vector.\n+      ShapeHandle sampled_candidates;\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &sampled_candidates));\n \n       // All three outputs are the same shape.\n       ShapeHandle v = c->Vector(InferenceContext::kUnknownDim);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 279,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/694b75a24bce416555425dedc58b0cdcd0d52c1e",
    "date": "2018-05-21T14:38:18-07:00",
    "message": "Merge pull request #19257 from yongtang/05122018-QuantizedReluX\n\nShape validation of `max_features` in `QuantizedReluX`",
    "changes": [
        {
            "name": "nn_ops.cc",
            "path": "tensorflow/core/ops/nn_ops.cc",
            "patches": [
                {
                    "old_start": 1453,
                    "old_length": 6,
                    "new_start": 1453,
                    "new_length": 7,
                    "hunk_buggy": "['       ShapeHandle unused;\\n', '       TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\\n', '       TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\\n', '       c->set_output(1, c->Scalar());\\n', '       c->set_output(2, c->Scalar());\\n', '       return Status::OK();']",
                    "hunk_fix": "@@ -1453,6 +1453,7 @@ REGISTER_OP(\"QuantizedReluX\")\n       ShapeHandle unused;\n       TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n       TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n       c->set_output(1, c->Scalar());\n       c->set_output(2, c->Scalar());\n       return Status::OK();"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 280,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/5daf3b9131254baa1182fc29d63bafd4b055e0ea",
    "date": "2018-05-17T15:12:24-07:00",
    "message": "Merge pull request #19294 from yongtang/03122018-MapAndBatchDataset\n\nAdd shape validation in shape function of MapAndBatchDataset",
    "changes": [
        {
            "name": "dataset_ops.cc",
            "path": "tensorflow/core/ops/dataset_ops.cc",
            "patches": [
                {
                    "old_start": 206,
                    "old_length": 7,
                    "new_start": 206,
                    "new_length": 17,
                    "hunk_buggy": "['     .Attr(\"Targuments: list(type) >= 0\")\\n', '     .Attr(\"output_types: list(type) >= 1\")\\n', '     .Attr(\"output_shapes: list(shape) >= 1\")\\n', '-    .SetShapeFn(shape_inference::ScalarShape);\\n', ' \\n', ' REGISTER_OP(\"MapAndBatchDatasetV2\")\\n', '     .Input(\"input_dataset: variant\")\\n']",
                    "hunk_fix": "@@ -206,7 +206,17 @@ REGISTER_OP(\"MapAndBatchDataset\")\n     .Attr(\"Targuments: list(type) >= 0\")\n     .Attr(\"output_types: list(type) >= 1\")\n     .Attr(\"output_shapes: list(shape) >= 1\")\n-    .SetShapeFn(shape_inference::ScalarShape);\n+    .SetShapeFn([](shape_inference::InferenceContext* c) {\n+      // Use index from the end to retrieve the Input shapes,\n+      // so that to avoid guessing the length of \"other_arguments\".\n+      // batch_size, num_parallel_batches, and drop_remainder are 0-D scalars.\n+      shape_inference::ShapeHandle unused;\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(c->num_inputs() - 3), 0, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(c->num_inputs() - 2), 0, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(c->num_inputs() - 1), 0, &unused));\n+\n+      return shape_inference::ScalarShape(c);\n+    });\n \n REGISTER_OP(\"MapAndBatchDatasetV2\")\n     .Input(\"input_dataset: variant\")\n"
                },
                {
                    "old_start": 219,
                    "old_length": 7,
                    "new_start": 229,
                    "new_length": 17,
                    "hunk_buggy": "['     .Attr(\"Targuments: list(type) >= 0\")\\n', '     .Attr(\"output_types: list(type) >= 1\")\\n', '     .Attr(\"output_shapes: list(shape) >= 1\")\\n', '-    .SetShapeFn(shape_inference::ScalarShape);\\n', ' \\n', ' REGISTER_OP(\"PrefetchDataset\")\\n', '     .Input(\"input_dataset: variant\")']",
                    "hunk_fix": "@@ -219,7 +229,17 @@ REGISTER_OP(\"MapAndBatchDatasetV2\")\n     .Attr(\"Targuments: list(type) >= 0\")\n     .Attr(\"output_types: list(type) >= 1\")\n     .Attr(\"output_shapes: list(shape) >= 1\")\n-    .SetShapeFn(shape_inference::ScalarShape);\n+    .SetShapeFn([](shape_inference::InferenceContext* c) {\n+      // Use index from the end to retrieve the Input shapes,\n+      // so that to avoid guessing the length of \"other_arguments\".\n+      // batch_size, num_parallel_calls, and drop_remainder are 0-D scalars.\n+      shape_inference::ShapeHandle unused;\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(c->num_inputs() - 3), 0, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(c->num_inputs() - 2), 0, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(c->num_inputs() - 1), 0, &unused));\n+\n+      return shape_inference::ScalarShape(c);\n+    });\n \n REGISTER_OP(\"PrefetchDataset\")\n     .Input(\"input_dataset: variant\")"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 281,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/181ca305a7954ce86a453a39db0b4f6d10b82720",
    "date": "2018-05-15T14:51:40+00:00",
    "message": "Add shape validation in shape function of MapAndBatchDataset\n\nIn MapAndBatchDataset, batch_size, num_parallel_batches,\nand drop_remainder are 0-D scalars. This fix adds\nthe shape check to those Inputs.\n\nNote since the Input of `other_arguments` is a list and is\nbefore `batch_size`, the shape of the `batch_size` and others\ncould not be obtained through index like `c->input(2)` etc directly.\nIt is still possible to obtain the ShapeHandle with names `c->input(\"batch_size\", &batch_size)`,\nthough.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "changes": [
        {
            "name": "dataset_ops.cc",
            "path": "tensorflow/core/ops/dataset_ops.cc",
            "patches": [
                {
                    "old_start": 206,
                    "old_length": 7,
                    "new_start": 206,
                    "new_length": 36,
                    "hunk_buggy": "['     .Attr(\"Targuments: list(type) >= 0\")\\n', '     .Attr(\"output_types: list(type) >= 1\")\\n', '     .Attr(\"output_shapes: list(shape) >= 1\")\\n', '-    .SetShapeFn(shape_inference::ScalarShape);\\n', ' \\n', ' REGISTER_OP(\"MapAndBatchDatasetV2\")\\n', '     .Input(\"input_dataset: variant\")']",
                    "hunk_fix": "@@ -206,7 +206,36 @@ REGISTER_OP(\"MapAndBatchDataset\")\n     .Attr(\"Targuments: list(type) >= 0\")\n     .Attr(\"output_types: list(type) >= 1\")\n     .Attr(\"output_shapes: list(shape) >= 1\")\n-    .SetShapeFn(shape_inference::ScalarShape);\n+    .SetShapeFn([](shape_inference::InferenceContext* c) {\n+      shape_inference::ShapeHandle unused;\n+\n+      // Use name (vs. index like c->input(1)) to retrieve the Input shapes,\n+      // so that to avoid guessing the length of \"other_arguments\".\n+\n+      // batch_size, num_parallel_batches, and drop_remainder are 0-D scalars.\n+      std::vector<shape_inference::ShapeHandle> batch_size;\n+      TF_RETURN_IF_ERROR(c->input(\"batch_size\", &batch_size));\n+      if (batch_size.size() != 1) {\n+        return errors::InvalidArgument(\"Requires list(batch_size) == 1\");\n+      }\n+      TF_RETURN_IF_ERROR(c->WithRank(batch_size[0], 0, &unused));\n+\n+      std::vector<shape_inference::ShapeHandle> num_parallel_batches;\n+      TF_RETURN_IF_ERROR(c->input(\"num_parallel_batches\", &num_parallel_batches));\n+      if (num_parallel_batches.size() != 1) {\n+        return errors::InvalidArgument(\"Requires list(num_parallel_batches) == 1\");\n+      }\n+      TF_RETURN_IF_ERROR(c->WithRank(num_parallel_batches[0], 0, &unused));\n+\n+      std::vector<shape_inference::ShapeHandle> drop_remainder;\n+      TF_RETURN_IF_ERROR(c->input(\"drop_remainder\", &drop_remainder));\n+      if (drop_remainder.size() != 1) {\n+        return errors::InvalidArgument(\"Requires list(drop_remainder) == 1\");\n+      }\n+      TF_RETURN_IF_ERROR(c->WithRank(drop_remainder[0], 0, &unused));\n+\n+      return shape_inference::ScalarShape(c);\n+    });\n \n REGISTER_OP(\"MapAndBatchDatasetV2\")\n     .Input(\"input_dataset: variant\")"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 282,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/356f360e8772a2697ec0d30036237342549803f5",
    "date": "2018-05-14T00:32:50+00:00",
    "message": "Add additional shape validation to `compute_accidental_hits`\n\nIn `compute_accidental_hits`, the `sampled_candidates` must\nbe a vector, as is shown in the kernel implementation in\n`tensorflow/core/kernels/candidate_sampler_ops.cc`.\n\nThis fix adds shape validation of `sampled_candidates`\nin the shape function whenever possible.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "changes": [
        {
            "name": "candidate_sampling_ops.cc",
            "path": "tensorflow/core/ops/candidate_sampling_ops.cc",
            "patches": [
                {
                    "old_start": 145,
                    "old_length": 12,
                    "new_start": 145,
                    "new_length": 15,
                    "hunk_buggy": "['       int64 num_true;\\n', '       TF_RETURN_IF_ERROR(c->GetAttr(\"num_true\", &num_true));\\n', ' \\n', '-      // Validate true_classes.\\n', '       ShapeHandle true_classes;\\n', '       TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 2, &true_classes));\\n', '       DimensionHandle unused;\\n', '       TF_RETURN_IF_ERROR(\\n', '           c->WithValue(c->Dim(true_classes, 1), num_true, &unused));\\n', ' \\n', '       // All three outputs are the same shape.\\n', '       ShapeHandle v = c->Vector(InferenceContext::kUnknownDim);']",
                    "hunk_fix": "@@ -145,12 +145,15 @@ REGISTER_OP(\"ComputeAccidentalHits\")\n       int64 num_true;\n       TF_RETURN_IF_ERROR(c->GetAttr(\"num_true\", &num_true));\n \n-      // Validate true_classes.\n+      // Validate true_classes, must be a matrix.\n       ShapeHandle true_classes;\n       TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 2, &true_classes));\n       DimensionHandle unused;\n       TF_RETURN_IF_ERROR(\n           c->WithValue(c->Dim(true_classes, 1), num_true, &unused));\n+      // Validate sampled_candidates, must be a vector.\n+      ShapeHandle sampled_candidates;\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 1, &sampled_candidates));\n \n       // All three outputs are the same shape.\n       ShapeHandle v = c->Vector(InferenceContext::kUnknownDim);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 283,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/7c88788e63f3a747d2794175076db551d768734e",
    "date": "2018-05-14T00:32:31+00:00",
    "message": "Shape validation of `max_features` in `QuantizedReluX`\n\nIn shape function of QuantizedReluX, `max_value` and\n`min_features` have shape validation but not `max_features`.\nThis fix add restriction to `max_features` as well.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "changes": [
        {
            "name": "nn_ops.cc",
            "path": "tensorflow/core/ops/nn_ops.cc",
            "patches": [
                {
                    "old_start": 1452,
                    "old_length": 6,
                    "new_start": 1452,
                    "new_length": 7,
                    "hunk_buggy": "['       ShapeHandle unused;\\n', '       TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\\n', '       TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\\n', '       c->set_output(1, c->Scalar());\\n', '       c->set_output(2, c->Scalar());\\n', '       return Status::OK();']",
                    "hunk_fix": "@@ -1452,6 +1452,7 @@ REGISTER_OP(\"QuantizedReluX\")\n       ShapeHandle unused;\n       TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n       TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n       c->set_output(1, c->Scalar());\n       c->set_output(2, c->Scalar());\n       return Status::OK();"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 284,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/ff6be80a1ec3c353ebd0d17e2f0b46d9097310db",
    "date": "2018-05-11T09:48:32-07:00",
    "message": "Improve the shape function for ParameterizedTruncatedNormal (#19215)\n\nThe parameters of ParameterizedTruncatedNormal should\r\nbe 0-D or 1-D, which is checked in ther kernel functions.\r\nThere is no check in the shape function of the ops.\r\n\r\nThis fix improves the shape function and checks the\r\nparameters of ParameterizedTruncatedNormal whever possible.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "changes": [
        {
            "name": "random_ops.cc",
            "path": "tensorflow/core/ops/random_ops.cc",
            "patches": [
                {
                    "old_start": 72,
                    "old_length": 7,
                    "new_start": 72,
                    "new_length": 15,
                    "hunk_buggy": "['     .Attr(\"seed2: int = 0\")\\n', '     .Attr(\"dtype: {half,bfloat16,float,double}\")\\n', '     .Attr(\"T: {int32, int64}\")\\n', '-    .SetShapeFn(shape_inference::RandomShape);\\n', ' \\n', ' REGISTER_OP(\"TruncatedNormal\")\\n', '     .Input(\"shape: T\")']",
                    "hunk_fix": "@@ -72,7 +72,15 @@ REGISTER_OP(\"ParameterizedTruncatedNormal\")\n     .Attr(\"seed2: int = 0\")\n     .Attr(\"dtype: {half,bfloat16,float,double}\")\n     .Attr(\"T: {int32, int64}\")\n-    .SetShapeFn(shape_inference::RandomShape);\n+    .SetShapeFn([](InferenceContext* c) {\n+      ShapeHandle unused;\n+      // Parameters must be 0-d or 1-d.\n+      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(1), 1, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(2), 1, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(3), 1, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(4), 1, &unused));\n+      return shape_inference::RandomShape(c);\n+    });\n \n REGISTER_OP(\"TruncatedNormal\")\n     .Input(\"shape: T\")"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 285,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/e5cfbd0eceb4dca98b388b13acff499a5420f863",
    "date": "2018-04-22T20:00:54-07:00",
    "message": "Fix more for cuda version check.",
    "changes": [
        {
            "name": "conv_ops_gpu.h",
            "path": "tensorflow/core/kernels/conv_ops_gpu.h",
            "patches": [
                {
                    "old_start": 144,
                    "old_length": 7,
                    "new_start": 144,
                    "new_length": 7,
                    "hunk_buggy": "['       perftools::gputools::StreamExecutor* stream_exec) const {\\n', '     // Skip this check for cuDNN 7 and newer.\\n', '     auto version = stream_exec->AsDnn()->GetVersion();\\n', '-    if (version.ok() && std::get<0>(version.ValueOrDie()) >= 7) {\\n', '       return true;\\n', '     }\\n', '     return ShouldIncludeWinogradNonfusedAlgoPreCudnn7<T>();']",
                    "hunk_fix": "@@ -144,7 +144,7 @@ class ConvParameters {\n       perftools::gputools::StreamExecutor* stream_exec) const {\n     // Skip this check for cuDNN 7 and newer.\n     auto version = stream_exec->AsDnn()->GetVersion();\n-    if (version.ok() && std::get<0>(version.ValueOrDie()) >= 7) {\n+    if (version.ok() && version.ValueOrDie().major_version() >= 7) {\n       return true;\n     }\n     return ShouldIncludeWinogradNonfusedAlgoPreCudnn7<T>();"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 286,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/76619c8dea0e480fd48e3b4dcfe0249eb24216b8",
    "date": "2018-04-19T09:13:53-07:00",
    "message": "Validation in shape functions of Dataset ops (#18680)\n\n* Add shape check for PrependFromQueueAndPaddedBatchDataset\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\n* Add comment for shape check\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\n* Add shape check for FixedLengthRecordDataset\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\n* Add check for filenames as well\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\n* Clang-format -i --style=google for file format\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\n* Add shape check for SqlDataset\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "changes": [
        {
            "name": "dataset_ops.cc",
            "path": "tensorflow/core/ops/dataset_ops.cc",
            "patches": [
                {
                    "old_start": 459,
                    "old_length": 7,
                    "new_start": 459,
                    "new_length": 14,
                    "hunk_buggy": "['     .Attr(\"output_shapes: list(shape) >= 1\")\\n', '     .SetIsStateful()  // TODO(b/65524810): Source dataset ops must be marked\\n', '                       // stateful to inhibit constant folding.\\n', '-    .SetShapeFn(shape_inference::ScalarShape);\\n', ' \\n', ' REGISTER_OP(\"FixedLengthRecordDataset\")\\n', '     .Input(\"filenames: string\")\\n']",
                    "hunk_fix": "@@ -459,7 +459,14 @@ REGISTER_OP(\"SqlDataset\")\n     .Attr(\"output_shapes: list(shape) >= 1\")\n     .SetIsStateful()  // TODO(b/65524810): Source dataset ops must be marked\n                       // stateful to inhibit constant folding.\n-    .SetShapeFn(shape_inference::ScalarShape);\n+    .SetShapeFn([](shape_inference::InferenceContext* c) {\n+      shape_inference::ShapeHandle unused;\n+      // driver_name, data_source_name, and query should be scalars.\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n+      return shape_inference::ScalarShape(c);\n+    });\n \n REGISTER_OP(\"FixedLengthRecordDataset\")\n     .Input(\"filenames: string\")\n"
                },
                {
                    "old_start": 470,
                    "old_length": 7,
                    "new_start": 477,
                    "new_length": 18,
                    "hunk_buggy": "['     .Output(\"handle: variant\")\\n', '     .SetIsStateful()  // TODO(b/65524810): Source dataset ops must be marked\\n', '                       // stateful to inhibit constant folding.\\n', '-    .SetShapeFn(shape_inference::ScalarShape);\\n', ' \\n', ' REGISTER_OP(\"TFRecordDataset\")\\n', '     .Input(\"filenames: string\")\\n']",
                    "hunk_fix": "@@ -470,7 +477,18 @@ REGISTER_OP(\"FixedLengthRecordDataset\")\n     .Output(\"handle: variant\")\n     .SetIsStateful()  // TODO(b/65524810): Source dataset ops must be marked\n                       // stateful to inhibit constant folding.\n-    .SetShapeFn(shape_inference::ScalarShape);\n+    .SetShapeFn([](shape_inference::InferenceContext* c) {\n+      shape_inference::ShapeHandle unused;\n+      // `filenames` must be a scalar or a vector.\n+      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(0), 1, &unused));\n+      // header_bytes, record_bytes, footer_bytes, buffer_size should be\n+      // scalars.\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n+      return shape_inference::ScalarShape(c);\n+    });\n \n REGISTER_OP(\"TFRecordDataset\")\n     .Input(\"filenames: string\")\n"
                },
                {
                    "old_start": 609,
                    "old_length": 7,
                    "new_start": 627,
                    "new_length": 12,
                    "hunk_buggy": "['     // length of `output_types` is `N`, the `output_shapes` are\\n', '     // (as far as possible to tell statically) compatible with `padded_shapes`,\\n', '     // and that `padding_values` are all scalars.\\n', '-    .SetShapeFn(shape_inference::ScalarShape);\\n', ' \\n', ' REGISTER_OP(\"EnqueueInQueueDataset\")\\n', '     .Input(\"queue: variant\")']",
                    "hunk_fix": "@@ -609,7 +627,12 @@ REGISTER_OP(\"PrependFromQueueAndPaddedBatchDataset\")\n     // length of `output_types` is `N`, the `output_shapes` are\n     // (as far as possible to tell statically) compatible with `padded_shapes`,\n     // and that `padding_values` are all scalars.\n-    .SetShapeFn(shape_inference::ScalarShape);\n+    .SetShapeFn([](shape_inference::InferenceContext* c) {\n+      shape_inference::ShapeHandle unused;\n+      // batch_size should be a scalar.\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n+      return shape_inference::ScalarShape(c);\n+    });\n \n REGISTER_OP(\"EnqueueInQueueDataset\")\n     .Input(\"queue: variant\")"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 287,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/b71b6b8ca9ade8b39d77f0373210fe58dfccf4f4",
    "date": "2018-04-19T09:13:35-07:00",
    "message": "Shape validation with random/shuffle related Dataset ops (#18682)\n\n* Add shape check for CacheDataset\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\n* Add shape check for ShuffleAndRepeatDataset\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\n* Add check for ShuffleDataset\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\n* Add shape check for RandomDataset\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\n* Add RangeDataset shape check\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\n* Sanitize with clang-format -i --style=Google\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "changes": [
        {
            "name": "dataset_ops.cc",
            "path": "tensorflow/core/ops/dataset_ops.cc",
            "patches": [
                {
                    "old_start": 357,
                    "old_length": 7,
                    "new_start": 357,
                    "new_length": 14,
                    "hunk_buggy": "['     .Attr(\"output_shapes: list(shape) >= 1\")\\n', '     .SetIsStateful()  // TODO(b/65524810): Source dataset ops must be marked\\n', '                       // stateful to inhibit constant folding.\\n', '-    .SetShapeFn(shape_inference::ScalarShape);\\n', ' \\n', ' REGISTER_OP(\"RandomDataset\")\\n', '     .Input(\"seed: int64\")\\n']",
                    "hunk_fix": "@@ -357,7 +357,14 @@ REGISTER_OP(\"RangeDataset\")\n     .Attr(\"output_shapes: list(shape) >= 1\")\n     .SetIsStateful()  // TODO(b/65524810): Source dataset ops must be marked\n                       // stateful to inhibit constant folding.\n-    .SetShapeFn(shape_inference::ScalarShape);\n+    .SetShapeFn([](shape_inference::InferenceContext* c) {\n+      shape_inference::ShapeHandle unused;\n+      // start, stop, and step should be scalars.\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n+      return shape_inference::ScalarShape(c);\n+    });\n \n REGISTER_OP(\"RandomDataset\")\n     .Input(\"seed: int64\")\n"
                },
                {
                    "old_start": 367,
                    "old_length": 7,
                    "new_start": 374,
                    "new_length": 13,
                    "hunk_buggy": "['     .Attr(\"output_shapes: list(shape) >= 1\")\\n', '     .SetIsStateful()  // TODO(b/65524810): Source dataset ops must be marked\\n', '                       // stateful to inhibit constant folding.\\n', '-    .SetShapeFn(shape_inference::ScalarShape);\\n', ' \\n', ' REGISTER_OP(\"ShuffleDataset\")\\n', '     .Input(\"input_dataset: variant\")\\n']",
                    "hunk_fix": "@@ -367,7 +374,13 @@ REGISTER_OP(\"RandomDataset\")\n     .Attr(\"output_shapes: list(shape) >= 1\")\n     .SetIsStateful()  // TODO(b/65524810): Source dataset ops must be marked\n                       // stateful to inhibit constant folding.\n-    .SetShapeFn(shape_inference::ScalarShape);\n+    .SetShapeFn([](shape_inference::InferenceContext* c) {\n+      shape_inference::ShapeHandle unused;\n+      // buffer_size, seed, and seed2 should be scalars.\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(0), 0, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n+      return shape_inference::ScalarShape(c);\n+    });\n \n REGISTER_OP(\"ShuffleDataset\")\n     .Input(\"input_dataset: variant\")\n"
                },
                {
                    "old_start": 378,
                    "old_length": 7,
                    "new_start": 391,
                    "new_length": 14,
                    "hunk_buggy": "['     .Attr(\"reshuffle_each_iteration: bool = true\")\\n', '     .Attr(\"output_types: list(type) >= 1\")\\n', '     .Attr(\"output_shapes: list(shape) >= 1\")\\n', '-    .SetShapeFn(shape_inference::ScalarShape);\\n', ' \\n', ' REGISTER_OP(\"ShuffleAndRepeatDataset\")\\n', '     .Input(\"input_dataset: variant\")\\n']",
                    "hunk_fix": "@@ -378,7 +391,14 @@ REGISTER_OP(\"ShuffleDataset\")\n     .Attr(\"reshuffle_each_iteration: bool = true\")\n     .Attr(\"output_types: list(type) >= 1\")\n     .Attr(\"output_shapes: list(shape) >= 1\")\n-    .SetShapeFn(shape_inference::ScalarShape);\n+    .SetShapeFn([](shape_inference::InferenceContext* c) {\n+      shape_inference::ShapeHandle unused;\n+      // buffer_size, seed, and seed2 should be scalars.\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n+      return shape_inference::ScalarShape(c);\n+    });\n \n REGISTER_OP(\"ShuffleAndRepeatDataset\")\n     .Input(\"input_dataset: variant\")\n"
                },
                {
                    "old_start": 389,
                    "old_length": 7,
                    "new_start": 409,
                    "new_length": 15,
                    "hunk_buggy": "['     .Output(\"handle: variant\")\\n', '     .Attr(\"output_types: list(type) >= 1\")\\n', '     .Attr(\"output_shapes: list(shape) >= 1\")\\n', '-    .SetShapeFn(shape_inference::ScalarShape);\\n', ' \\n', ' REGISTER_OP(\"CacheDataset\")\\n', '     .Input(\"input_dataset: variant\")\\n']",
                    "hunk_fix": "@@ -389,7 +409,15 @@ REGISTER_OP(\"ShuffleAndRepeatDataset\")\n     .Output(\"handle: variant\")\n     .Attr(\"output_types: list(type) >= 1\")\n     .Attr(\"output_shapes: list(shape) >= 1\")\n-    .SetShapeFn(shape_inference::ScalarShape);\n+    .SetShapeFn([](shape_inference::InferenceContext* c) {\n+      shape_inference::ShapeHandle unused;\n+      // buffer_size, seed, seed2, and count should be scalars.\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(3), 0, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(4), 0, &unused));\n+      return shape_inference::ScalarShape(c);\n+    });\n \n REGISTER_OP(\"CacheDataset\")\n     .Input(\"input_dataset: variant\")\n"
                },
                {
                    "old_start": 397,
                    "old_length": 7,
                    "new_start": 425,
                    "new_length": 12,
                    "hunk_buggy": "['     .Output(\"handle: variant\")\\n', '     .Attr(\"output_types: list(type) >= 1\")\\n', '     .Attr(\"output_shapes: list(shape) >= 1\")\\n', '-    .SetShapeFn(shape_inference::ScalarShape);\\n', ' \\n', ' REGISTER_OP(\"TextLineDataset\")\\n', '     .Input(\"filenames: string\")']",
                    "hunk_fix": "@@ -397,7 +425,12 @@ REGISTER_OP(\"CacheDataset\")\n     .Output(\"handle: variant\")\n     .Attr(\"output_types: list(type) >= 1\")\n     .Attr(\"output_shapes: list(shape) >= 1\")\n-    .SetShapeFn(shape_inference::ScalarShape);\n+    .SetShapeFn([](shape_inference::InferenceContext* c) {\n+      shape_inference::ShapeHandle unused;\n+      // filename should be a scalar.\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n+      return shape_inference::ScalarShape(c);\n+    });\n \n REGISTER_OP(\"TextLineDataset\")\n     .Input(\"filenames: string\")"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 288,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/50f6683ca50e6d4e7008d6d1b437b407d6a62e92",
    "date": "2018-04-19T09:13:21-07:00",
    "message": "Add shape check for batch related Dataset ops (#18683)\n\n* Add shape check for PrefetchDataset\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\n* Add BatchDataset shape check\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\n* Add shape check for SlideDataset\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\n* Add shape check for DenseToSparseBatchDataset\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n\r\n* Sanitize with clang-format -i --style=Google\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "changes": [
        {
            "name": "dataset_ops.cc",
            "path": "tensorflow/core/ops/dataset_ops.cc",
            "patches": [
                {
                    "old_start": 199,
                    "old_length": 7,
                    "new_start": 199,
                    "new_length": 12,
                    "hunk_buggy": "['     .Output(\"handle: variant\")\\n', '     .Attr(\"output_types: list(type) >= 1\")\\n', '     .Attr(\"output_shapes: list(shape) >= 1\")\\n', '-    .SetShapeFn(shape_inference::ScalarShape);\\n', ' \\n', ' REGISTER_OP(\"ScanDataset\")\\n', '     .Input(\"input_dataset: variant\")\\n']",
                    "hunk_fix": "@@ -199,7 +199,12 @@ REGISTER_OP(\"PrefetchDataset\")\n     .Output(\"handle: variant\")\n     .Attr(\"output_types: list(type) >= 1\")\n     .Attr(\"output_shapes: list(shape) >= 1\")\n-    .SetShapeFn(shape_inference::ScalarShape);\n+    .SetShapeFn([](shape_inference::InferenceContext* c) {\n+      shape_inference::ShapeHandle unused;\n+      // buffer_size should be a scalar.\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n+      return shape_inference::ScalarShape(c);\n+    });\n \n REGISTER_OP(\"ScanDataset\")\n     .Input(\"input_dataset: variant\")\n"
                },
                {
                    "old_start": 283,
                    "old_length": 7,
                    "new_start": 288,
                    "new_length": 12,
                    "hunk_buggy": "['     .Output(\"handle: variant\")\\n', '     .Attr(\"output_types: list(type) >= 1\")\\n', '     .Attr(\"output_shapes: list(shape) >= 1\")\\n', '-    .SetShapeFn(shape_inference::ScalarShape);\\n', ' \\n', ' // TODO(mrry): move SlideDataset to contrib in the future.\\n', ' REGISTER_OP(\"SlideDataset\")\\n']",
                    "hunk_fix": "@@ -283,7 +288,12 @@ REGISTER_OP(\"BatchDataset\")\n     .Output(\"handle: variant\")\n     .Attr(\"output_types: list(type) >= 1\")\n     .Attr(\"output_shapes: list(shape) >= 1\")\n-    .SetShapeFn(shape_inference::ScalarShape);\n+    .SetShapeFn([](shape_inference::InferenceContext* c) {\n+      shape_inference::ShapeHandle unused;\n+      // batch_size should be a scalar.\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n+      return shape_inference::ScalarShape(c);\n+    });\n \n // TODO(mrry): move SlideDataset to contrib in the future.\n REGISTER_OP(\"SlideDataset\")\n"
                },
                {
                    "old_start": 293,
                    "old_length": 7,
                    "new_start": 303,
                    "new_length": 13,
                    "hunk_buggy": "['     .Output(\"handle: variant\")\\n', '     .Attr(\"output_types: list(type) >= 1\")\\n', '     .Attr(\"output_shapes: list(shape) >= 1\")\\n', '-    .SetShapeFn(shape_inference::ScalarShape);\\n', ' \\n', ' REGISTER_OP(\"PaddedBatchDataset\")\\n', '     .Input(\"input_dataset: variant\")\\n']",
                    "hunk_fix": "@@ -293,7 +303,13 @@ REGISTER_OP(\"SlideDataset\")\n     .Output(\"handle: variant\")\n     .Attr(\"output_types: list(type) >= 1\")\n     .Attr(\"output_shapes: list(shape) >= 1\")\n-    .SetShapeFn(shape_inference::ScalarShape);\n+    .SetShapeFn([](shape_inference::InferenceContext* c) {\n+      shape_inference::ShapeHandle unused;\n+      // window_size and stride should be scalars.\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n+      return shape_inference::ScalarShape(c);\n+    });\n \n REGISTER_OP(\"PaddedBatchDataset\")\n     .Input(\"input_dataset: variant\")\n"
                },
                {
                    "old_start": 323,
                    "old_length": 7,
                    "new_start": 339,
                    "new_length": 14,
                    "hunk_buggy": "['     .Output(\"handle: variant\")\\n', '     .Attr(\"output_types: list(type) >= 1\")\\n', '     .Attr(\"output_shapes: list(shape) >= 1\")\\n', '-    .SetShapeFn(shape_inference::ScalarShape);\\n', ' \\n', ' REGISTER_OP(\"RangeDataset\")\\n', '     .Input(\"start: int64\")']",
                    "hunk_fix": "@@ -323,7 +339,14 @@ REGISTER_OP(\"DenseToSparseBatchDataset\")\n     .Output(\"handle: variant\")\n     .Attr(\"output_types: list(type) >= 1\")\n     .Attr(\"output_shapes: list(shape) >= 1\")\n-    .SetShapeFn(shape_inference::ScalarShape);\n+    .SetShapeFn([](shape_inference::InferenceContext* c) {\n+      shape_inference::ShapeHandle unused;\n+      // batch_size should be a scalar.\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n+      // row_shape should be a 1-D vector.\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 1, &unused));\n+      return shape_inference::ScalarShape(c);\n+    });\n \n REGISTER_OP(\"RangeDataset\")\n     .Input(\"start: int64\")"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 289,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/edb29d02765b45e712042725dc06b65f5e610327",
    "date": "2018-04-19T09:10:53-07:00",
    "message": "Merge pull request #18650 from imsheridan/add_shape_check_TextLineDataset\n\nAdd shape check to TextLineDataset op",
    "changes": [
        {
            "name": "dataset_ops.cc",
            "path": "tensorflow/core/ops/dataset_ops.cc",
            "patches": [
                {
                    "old_start": 383,
                    "old_length": 10,
                    "new_start": 383,
                    "new_length": 16,
                    "hunk_buggy": "['     .Output(\"handle: variant\")\\n', '     .SetIsStateful()  // TODO(b/65524810): Source dataset ops must be marked\\n', '                       // stateful to inhibit constant folding.\\n', '-    .SetShapeFn(shape_inference::ScalarShape);  // TODO(mrry): validate\\n', '-                                                // that `filenames` is\\n', '-                                                // a scalar or a\\n', '-                                                // vector.\\n', ' \\n', ' REGISTER_OP(\"SqlDataset\")\\n', '     .Input(\"driver_name: string\")']",
                    "hunk_fix": "@@ -383,10 +383,16 @@ REGISTER_OP(\"TextLineDataset\")\n     .Output(\"handle: variant\")\n     .SetIsStateful()  // TODO(b/65524810): Source dataset ops must be marked\n                       // stateful to inhibit constant folding.\n-    .SetShapeFn(shape_inference::ScalarShape);  // TODO(mrry): validate\n-                                                // that `filenames` is\n-                                                // a scalar or a\n-                                                // vector.\n+    .SetShapeFn([](shape_inference::InferenceContext* c) {\n+      shape_inference::ShapeHandle unused;\n+      // `filenames` must be a scalar or a vector.\n+      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(0), 1, &unused));\n+      return shape_inference::ScalarShape(c);\n+      // `compression_type` could only be a scalar.\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n+      // `buffer_size` could only be a scalar.\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n+    });\n \n REGISTER_OP(\"SqlDataset\")\n     .Input(\"driver_name: string\")"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 290,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/9187be7adff07be82856add498aa3ff4b5f95998",
    "date": "2018-04-19T00:05:05+08:00",
    "message": "add checks for compression_type and buffer_size also",
    "changes": [
        {
            "name": "dataset_ops.cc",
            "path": "tensorflow/core/ops/dataset_ops.cc",
            "patches": [
                {
                    "old_start": 388,
                    "old_length": 6,
                    "new_start": 388,
                    "new_length": 10,
                    "hunk_buggy": "['       // `filenames` must be a scalar or a vector.\\n', '       TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(0), 1, &unused));\\n', '       return shape_inference::ScalarShape(c);\\n', '     });\\n', ' \\n', ' REGISTER_OP(\"SqlDataset\")']",
                    "hunk_fix": "@@ -388,6 +388,10 @@ REGISTER_OP(\"TextLineDataset\")\n       // `filenames` must be a scalar or a vector.\n       TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(0), 1, &unused));\n       return shape_inference::ScalarShape(c);\n+      // `compression_type` could only be a scalar.\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n+      // `buffer_size` could only be a scalar.\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n     });\n \n REGISTER_OP(\"SqlDataset\")"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 291,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/779664494d43b18a812361197dcbea2f25912c02",
    "date": "2018-04-18T20:12:14+08:00",
    "message": "Add shape check to TextLineDataset op",
    "changes": [
        {
            "name": "dataset_ops.cc",
            "path": "tensorflow/core/ops/dataset_ops.cc",
            "patches": [
                {
                    "old_start": 383,
                    "old_length": 10,
                    "new_start": 383,
                    "new_length": 12,
                    "hunk_buggy": "['     .Output(\"handle: variant\")\\n', '     .SetIsStateful()  // TODO(b/65524810): Source dataset ops must be marked\\n', '                       // stateful to inhibit constant folding.\\n', '-    .SetShapeFn(shape_inference::ScalarShape);  // TODO(mrry): validate\\n', '-                                                // that `filenames` is\\n', '-                                                // a scalar or a\\n', '-                                                // vector.\\n', ' \\n', ' REGISTER_OP(\"SqlDataset\")\\n', '     .Input(\"driver_name: string\")']",
                    "hunk_fix": "@@ -383,10 +383,12 @@ REGISTER_OP(\"TextLineDataset\")\n     .Output(\"handle: variant\")\n     .SetIsStateful()  // TODO(b/65524810): Source dataset ops must be marked\n                       // stateful to inhibit constant folding.\n-    .SetShapeFn(shape_inference::ScalarShape);  // TODO(mrry): validate\n-                                                // that `filenames` is\n-                                                // a scalar or a\n-                                                // vector.\n+    .SetShapeFn([](shape_inference::InferenceContext* c) {\n+      shape_inference::ShapeHandle unused;\n+      // `filenames` must be a scalar or a vector.\n+      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(0), 1, &unused));\n+      return shape_inference::ScalarShape(c);\n+    });\n \n REGISTER_OP(\"SqlDataset\")\n     .Input(\"driver_name: string\")"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 292,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/48f7e377963a951b77cbf111675931fd4248b090",
    "date": "2018-04-17T14:59:50-07:00",
    "message": "Merge pull request #18615 from yongtang/04162018-TFRecordDataset-shape\n\nAdd shape check to TFRecordDataset",
    "changes": [
        {
            "name": "dataset_ops.cc",
            "path": "tensorflow/core/ops/dataset_ops.cc",
            "patches": [
                {
                    "old_start": 417,
                    "old_length": 7,
                    "new_start": 417,
                    "new_length": 16,
                    "hunk_buggy": "['     .Output(\"handle: variant\")\\n', '     .SetIsStateful()  // TODO(b/65524810): Source dataset ops must be marked\\n', '                       // stateful to inhibit constant folding.\\n', '-    .SetShapeFn(shape_inference::ScalarShape);\\n', ' \\n', ' REGISTER_OP(\"Iterator\")\\n', '     .Output(\"handle: resource\")']",
                    "hunk_fix": "@@ -417,7 +417,16 @@ REGISTER_OP(\"TFRecordDataset\")\n     .Output(\"handle: variant\")\n     .SetIsStateful()  // TODO(b/65524810): Source dataset ops must be marked\n                       // stateful to inhibit constant folding.\n-    .SetShapeFn(shape_inference::ScalarShape);\n+    .SetShapeFn([](shape_inference::InferenceContext* c) {\n+      shape_inference::ShapeHandle unused;\n+      // `filenames` must be a scalar or a vector.\n+      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(0), 1, &unused));\n+      // `compression_type` could only be a scalar.\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused));\n+      // `buffer_size` could only be a scalar.\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused));\n+      return shape_inference::ScalarShape(c);\n+    });\n \n REGISTER_OP(\"Iterator\")\n     .Output(\"handle: resource\")"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 293,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/c4dea2255c71037c9cade9cbd1d7820b3429b3fa",
    "date": "2018-04-17T01:31:54+00:00",
    "message": "Add shape check for buffer_size with TFRecordDataset\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "changes": [
        {
            "name": "dataset_ops.cc",
            "path": "tensorflow/core/ops/dataset_ops.cc",
            "patches": [
                {
                    "old_start": 423,
                    "old_length": 6,
                    "new_start": 423,
                    "new_length": 8,
                    "hunk_buggy": "['       TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(0), 1, &unused));\\n', '       // `compression_type` could only be a scalar.\\n', '       TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused) );\\n', '       return shape_inference::ScalarShape(c);\\n', '     });\\n', ' ']",
                    "hunk_fix": "@@ -423,6 +423,8 @@ REGISTER_OP(\"TFRecordDataset\")\n       TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(0), 1, &unused));\n       // `compression_type` could only be a scalar.\n       TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused) );\n+      // `buffer_size` could only be a scalar.\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(2), 0, &unused) );\n       return shape_inference::ScalarShape(c);\n     });\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 294,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/d97ffbdf362fa7d06ef8d946c8620ff7a3a50a08",
    "date": "2018-04-17T01:30:42+00:00",
    "message": "Add shape check for compression_type in TFrecordDataset\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "changes": [
        {
            "name": "dataset_ops.cc",
            "path": "tensorflow/core/ops/dataset_ops.cc",
            "patches": [
                {
                    "old_start": 421,
                    "old_length": 6,
                    "new_start": 421,
                    "new_length": 8,
                    "hunk_buggy": "['       shape_inference::ShapeHandle unused;\\n', '       // `filenames` must be a scalar or a vector.\\n', '       TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(0), 1, &unused));\\n', '       return shape_inference::ScalarShape(c);\\n', '     });\\n', ' ']",
                    "hunk_fix": "@@ -421,6 +421,8 @@ REGISTER_OP(\"TFRecordDataset\")\n       shape_inference::ShapeHandle unused;\n       // `filenames` must be a scalar or a vector.\n       TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(0), 1, &unused));\n+      // `compression_type` could only be a scalar.\n+      TF_RETURN_IF_ERROR(c->WithRank(c->input(1), 0, &unused) );\n       return shape_inference::ScalarShape(c);\n     });\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 295,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/7586dee9aa8b4b63143ab658ca59658aaed0df97",
    "date": "2018-04-17T01:28:30+00:00",
    "message": "Add shape check to TFRecordDataset\n\nThe inputs of TFRecordDataset have the requirements for shapes.\nHowever, the check was not done in the shape function. This fix\nadds shape checks whenever possible.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "changes": [
        {
            "name": "dataset_ops.cc",
            "path": "tensorflow/core/ops/dataset_ops.cc",
            "patches": [
                {
                    "old_start": 417,
                    "old_length": 7,
                    "new_start": 417,
                    "new_length": 12,
                    "hunk_buggy": "['     .Output(\"handle: variant\")\\n', '     .SetIsStateful()  // TODO(b/65524810): Source dataset ops must be marked\\n', '                       // stateful to inhibit constant folding.\\n', '-    .SetShapeFn(shape_inference::ScalarShape);\\n', ' \\n', ' REGISTER_OP(\"Iterator\")\\n', '     .Output(\"handle: resource\")']",
                    "hunk_fix": "@@ -417,7 +417,12 @@ REGISTER_OP(\"TFRecordDataset\")\n     .Output(\"handle: variant\")\n     .SetIsStateful()  // TODO(b/65524810): Source dataset ops must be marked\n                       // stateful to inhibit constant folding.\n-    .SetShapeFn(shape_inference::ScalarShape);\n+    .SetShapeFn([](shape_inference::InferenceContext* c) {\n+      shape_inference::ShapeHandle unused;\n+      // `filenames` must be a scalar or a vector.\n+      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(0), 1, &unused));\n+      return shape_inference::ScalarShape(c);\n+    });\n \n REGISTER_OP(\"Iterator\")\n     .Output(\"handle: resource\")"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 296,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/851177fee860211e2fabcb019d644e75b7f701b0",
    "date": "2018-04-17T01:17:51+00:00",
    "message": "Add shape check for shift of tf.roll\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "changes": [
        {
            "name": "manip_ops.cc",
            "path": "tensorflow/core/ops/manip_ops.cc",
            "patches": [
                {
                    "old_start": 32,
                    "old_length": 6,
                    "new_start": 32,
                    "new_length": 8,
                    "hunk_buggy": "['       shape_inference::ShapeHandle unused;\\n', '       // The `input` must be 1-D or higher\\n', '       TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &unused));\\n', '       // The `axis` must be scalar or 1-D.\\n', '       TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(2), 1, &unused));\\n', ' ']",
                    "hunk_fix": "@@ -32,6 +32,8 @@ REGISTER_OP(\"Roll\")\n       shape_inference::ShapeHandle unused;\n       // The `input` must be 1-D or higher\n       TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &unused));\n+      // The `shift` must be scalar or 1-D.\n+      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(1), 1, &unused));\n       // The `axis` must be scalar or 1-D.\n       TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(2), 1, &unused));\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 297,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/3f796ff8c9e6d7ff88f99c056b78e88fb0b31114",
    "date": "2018-04-17T01:17:51+00:00",
    "message": "Add axis shape check for tf.roll\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "changes": [
        {
            "name": "manip_ops.cc",
            "path": "tensorflow/core/ops/manip_ops.cc",
            "patches": [
                {
                    "old_start": 32,
                    "old_length": 6,
                    "new_start": 32,
                    "new_length": 8,
                    "hunk_buggy": "['       shape_inference::ShapeHandle unused;\\n', '       // The `input` must be 1-D or higher\\n', '       TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &unused));\\n', ' \\n', '       return shape_inference::UnchangedShape(c);\\n', '     });']",
                    "hunk_fix": "@@ -32,6 +32,8 @@ REGISTER_OP(\"Roll\")\n       shape_inference::ShapeHandle unused;\n       // The `input` must be 1-D or higher\n       TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &unused));\n+      // The `axis` must be scalar or 1-D.\n+      TF_RETURN_IF_ERROR(c->WithRankAtMost(c->input(2), 1, &unused));\n \n       return shape_inference::UnchangedShape(c);\n     });"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 298,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/10467d29e05d9957a6e3cb2335f8eeba1fd8896e",
    "date": "2018-04-17T01:17:51+00:00",
    "message": "Improve shape function check for `tf.roll`\n\nThe `tf.roll` op has requirements for the shape of inputs. However,\nthe shape of the inputs are only done at the runtime inside the kernel.\nThis fix improve the shape function so that the check could be\ndone early if shape is already known in the shape function.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>",
    "changes": [
        {
            "name": "manip_ops.cc",
            "path": "tensorflow/core/ops/manip_ops.cc",
            "patches": [
                {
                    "old_start": 28,
                    "old_length": 6,
                    "new_start": 28,
                    "new_length": 12,
                    "hunk_buggy": "['     .Attr(\"T: type\")\\n', '     .Attr(\"Tshift: {int32,int64}\")\\n', '     .Attr(\"Taxis: {int32,int64}\")\\n', '-    .SetShapeFn(shape_inference::UnchangedShape);\\n', ' \\n', ' }  // namespace tensorflow']",
                    "hunk_fix": "@@ -28,6 +28,12 @@ REGISTER_OP(\"Roll\")\n     .Attr(\"T: type\")\n     .Attr(\"Tshift: {int32,int64}\")\n     .Attr(\"Taxis: {int32,int64}\")\n-    .SetShapeFn(shape_inference::UnchangedShape);\n+    .SetShapeFn([](shape_inference::InferenceContext* c) {\n+      shape_inference::ShapeHandle unused;\n+      // The `input` must be 1-D or higher\n+      TF_RETURN_IF_ERROR(c->WithRankAtLeast(c->input(0), 1, &unused));\n+\n+      return shape_inference::UnchangedShape(c);\n+    });\n \n }  // namespace tensorflow"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 299,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/fffbe5a26da2d6fab5a3eb648cefef49db4d38de",
    "date": "2018-04-12T15:22:37-07:00",
    "message": "Check if the session has been deleted before releasing a callable.\n\nIn some versions of Python, the Session._session field may be cleared\n(in `Session.__del__()`) before a callable that has a reference to\nthat Session is deleted. Add a defensive check in the\n`Session._Callable.__del__()` method.\n\nPiperOrigin-RevId: 192679796",
    "changes": [
        {
            "name": "session.py",
            "path": "tensorflow/python/client/session.py",
            "patches": [
                {
                    "old_start": 1454,
                    "old_length": 7,
                    "new_start": 1454,
                    "new_length": 10,
                    "hunk_buggy": "['               self._session._session, self._handle, args, status, None)\\n', ' \\n', '     def __del__(self):\\n', '-      if self._handle is not None:\\n', '         with errors.raise_exception_on_not_ok_status() as status:\\n', '           if self._session._created_with_new_api:\\n', '             tf_session.TF_SessionReleaseCallable(']",
                    "hunk_fix": "@@ -1454,7 +1454,10 @@ class BaseSession(SessionInterface):\n               self._session._session, self._handle, args, status, None)\n \n     def __del__(self):\n-      if self._handle is not None:\n+      # NOTE(mrry): It is possible that `self._session.__del__()` could be\n+      # called before this destructor, in which case `self._session._session`\n+      # will be `None`.\n+      if self._handle is not None and self._session._session is not None:\n         with errors.raise_exception_on_not_ok_status() as status:\n           if self._session._created_with_new_api:\n             tf_session.TF_SessionReleaseCallable("
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 300,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/7535f6beb7ba95bf54e1513b0c2c51b844a7a49f",
    "date": "2018-04-06T09:56:07-07:00",
    "message": "Bounds-check node ID before getting it's name (#18090)\n\nWhen the edge is either a frame enter or exit edge then\r\nDescribeCycle() would segfault.",
    "changes": [
        {
            "name": "mark_for_compilation_pass.cc",
            "path": "tensorflow/compiler/jit/mark_for_compilation_pass.cc",
            "patches": [
                {
                    "old_start": 35,
                    "old_length": 6,
                    "new_start": 35,
                    "new_length": 7,
                    "hunk_buggy": "[' #include \"tensorflow/core/framework/types.h\"\\n', ' #include \"tensorflow/core/graph/algorithm.h\"\\n', ' #include \"tensorflow/core/graph/control_flow.h\"\\n', ' #include \"tensorflow/core/lib/strings/strcat.h\"\\n', ' #include \"tensorflow/core/public/version.h\"\\n', ' \\n']",
                    "hunk_fix": "@@ -35,6 +35,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/types.h\"\n #include \"tensorflow/core/graph/algorithm.h\"\n #include \"tensorflow/core/graph/control_flow.h\"\n+#include \"tensorflow/core/kernels/bounds_check.h\"\n #include \"tensorflow/core/lib/strings/strcat.h\"\n #include \"tensorflow/core/public/version.h\"\n \n"
                },
                {
                    "old_start": 432,
                    "old_length": 6,
                    "new_start": 433,
                    "new_length": 9,
                    "hunk_buggy": "['   }\\n', ' \\n', '   auto node_name = [&cycles, &graph](int node_id) {\\n', '     auto* node = graph.FindNodeId(node_id);\\n', '     if (node == nullptr) {\\n', '       return string(\"(null)\");']",
                    "hunk_fix": "@@ -432,6 +433,9 @@ string DescribeCycle(const GraphCycles& cycles, const Graph& graph, int src,\n   }\n \n   auto node_name = [&cycles, &graph](int node_id) {\n+    if (!FastBoundsCheck(node_id, graph.num_node_ids())) {\n+      return string(\"(null)\");\n+    }\n     auto* node = graph.FindNodeId(node_id);\n     if (node == nullptr) {\n       return string(\"(null)\");"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 301,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/e5496b556734bb1d8de85311092804e0150b3009",
    "date": "2018-02-21T08:27:17-05:00",
    "message": "Remove extraneous check for Eager mode (#17125)\n\nThe check is already made once at the start of the method",
    "changes": [
        {
            "name": "control_flow_ops.py",
            "path": "tensorflow/python/ops/control_flow_ops.py",
            "patches": [
                {
                    "old_start": 178,
                    "old_length": 8,
                    "new_start": 178,
                    "new_length": 6,
                    "hunk_buggy": "['             condition, data, summarize, name=\"Assert\")\\n', ' \\n', '       guarded_assert = cond(condition, no_op, true_assert, name=\"AssertGuard\")\\n', '-      if context.in_eager_mode():\\n', '-        return\\n', '       return guarded_assert.op\\n', ' \\n', ' ']",
                    "hunk_fix": "@@ -178,8 +178,6 @@ def Assert(condition, data, summarize=None, name=None):\n             condition, data, summarize, name=\"Assert\")\n \n       guarded_assert = cond(condition, no_op, true_assert, name=\"AssertGuard\")\n-      if context.in_eager_mode():\n-        return\n       return guarded_assert.op\n \n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 302,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/cb2828a844ccaf0394e602d15fd95e45073729a2",
    "date": "2018-02-02T17:13:17-08:00",
    "message": "Check that the type of an implicitly dereferenced tensor matches the expected input type.\n\nThe dtype of a tensor reference can change between the point when it is \"produced\" by an\noperation and consumed by the next operation. This evades checks in the executor that the\ntype of tensor on each edge matches the type signatures of the producing and consuming operation, which could lead to undefined behavior. Although there is no existing operation that changes the type of a tensor reference, it is possible to use the OpKernelContext API to do so, so we add a further check in the runtime to defend against operations that might be added in the future.\n\nPiperOrigin-RevId: 184356242",
    "changes": [
        {
            "name": "executor.cc",
            "path": "tensorflow/core/common_runtime/executor.cc",
            "patches": [
                {
                    "old_start": 1776,
                    "old_length": 6,
                    "new_start": 1776,
                    "new_length": 19,
                    "hunk_buggy": "['         entry->ref_mu = nullptr;\\n', ' \\n', '         inp->tensor = entry->val.get();\\n', '       }\\n', '     }\\n', '   }']",
                    "hunk_fix": "@@ -1776,6 +1776,19 @@ Status ExecutorState::PrepareInputs(const NodeItem& item, Entry* first_input,\n         entry->ref_mu = nullptr;\n \n         inp->tensor = entry->val.get();\n+        // The dtype of entry->ref could have been changed by another operation\n+        // that ran after the operation that \"produced\" it executed, so\n+        // re-validate that the type of the dereferenced tensor matches the\n+        // expected input type.\n+        if (item.input_type(i) != inp->tensor->dtype()) {\n+          return AttachDef(\n+              errors::InvalidArgument(\n+                  i, \"-th input expects type \",\n+                  DataTypeString(item.input_type(i)),\n+                  \" but automatically dereferenced input tensor has type \",\n+                  DataTypeString(inp->tensor->dtype())),\n+              item.kernel->def());\n+        }\n       }\n     }\n   }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 303,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/201982013046116767545cda18137b38abb39468",
    "date": "2018-01-08T13:33:45-08:00",
    "message": "toco: Fix missing check for buffer in ResizeBilinear.\n\nPiperOrigin-RevId: 181209975",
    "changes": [
        {
            "name": "propagate_fixed_sizes.cc",
            "path": "tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc",
            "patches": [
                {
                    "old_start": 631,
                    "old_length": 6,
                    "new_start": 631,
                    "new_length": 9,
                    "hunk_buggy": "['   const auto& output_size_shape = output_size_array.shape();\\n', '   CHECK_EQ(output_size_shape.dimensions_count(), 1);\\n', '   CHECK_EQ(output_size_shape.dims(0), 2);\\n', '   std::vector<int32> output_shape =\\n', '       output_size_array.GetBuffer<ArrayDataType::kInt32>().data;\\n', '   model->arrays[op->outputs[0]]->copy_shape(']",
                    "hunk_fix": "@@ -631,6 +631,9 @@ void ProcessResizeBilinearOperator(Model* model, ResizeBilinearOperator* op) {\n   const auto& output_size_shape = output_size_array.shape();\n   CHECK_EQ(output_size_shape.dimensions_count(), 1);\n   CHECK_EQ(output_size_shape.dims(0), 2);\n+  if (!output_size_array.buffer) {\n+    return;\n+  }\n   std::vector<int32> output_shape =\n       output_size_array.GetBuffer<ArrayDataType::kInt32>().data;\n   model->arrays[op->outputs[0]]->copy_shape("
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 304,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/db10718b38b2884cb5ed46d33c135c079f649d16",
    "date": "2017-11-03T20:09:38-07:00",
    "message": "Handle nil return from TF_TensorData (#14082)\n\nWith some memory allocators, attempting to allocate 0 bytes will return\r\na null pointer. This specifically happens when building tensorflow with\r\nmkl support. If TF_TensorData returns null, the go code to create a\r\nslice from the data leads to a null pointer exception. This fixes the\r\nissue by checking for the nil return and returning a slice zero value to\r\n(nil) to the caller. Fixes #13764.",
    "changes": [
        {
            "name": "tensor.go",
            "path": "tensorflow/go/tensor.go",
            "patches": [
                {
                    "old_start": 207,
                    "old_length": 6,
                    "new_start": 207,
                    "new_length": 9,
                    "hunk_buggy": "[' func tensorData(c *C.TF_Tensor) []byte {\\n', ' \\t// See: https://github.com/golang/go/wiki/cgo#turning-c-arrays-into-go-slices\\n', ' \\tcbytes := C.TF_TensorData(c)\\n', ' \\tlength := int(C.TF_TensorByteSize(c))\\n', ' \\tslice := (*[1 << 30]byte)(unsafe.Pointer(cbytes))[:length:length]\\n', ' \\treturn slice']",
                    "hunk_fix": "@@ -207,6 +207,9 @@ func (t *Tensor) WriteContentsTo(w io.Writer) (int64, error) {\n func tensorData(c *C.TF_Tensor) []byte {\n \t// See: https://github.com/golang/go/wiki/cgo#turning-c-arrays-into-go-slices\n \tcbytes := C.TF_TensorData(c)\n+\tif cbytes == nil {\n+\t\treturn nil\n+\t}\n \tlength := int(C.TF_TensorByteSize(c))\n \tslice := (*[1 << 30]byte)(unsafe.Pointer(cbytes))[:length:length]\n \treturn slice"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 305,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/1d6dae88efef68dd7fbeeb5c39ea0f69c1c721c1",
    "date": "2017-10-30T14:19:44-07:00",
    "message": "Add check to tf.device when called with a function in eager mode.\n\nPiperOrigin-RevId: 173947845",
    "changes": [
        {
            "name": "ops.py",
            "path": "tensorflow/python/framework/ops.py",
            "patches": [
                {
                    "old_start": 4339,
                    "old_length": 11,
                    "new_start": 4339,
                    "new_length": 18,
                    "hunk_buggy": "['   Returns:\\n', '     A context manager that specifies the default device to use for newly\\n', '     created ops.\\n', '   \"\"\"\\n', '   if context.in_graph_mode():\\n', '     return get_default_graph().device(device_name_or_function)\\n', '   else:\\n', '     # TODO(agarwal): support device functions in EAGER mode.\\n', '     return context.device(device_name_or_function)\\n', ' \\n', ' ']",
                    "hunk_fix": "@@ -4339,11 +4339,18 @@ def device(device_name_or_function):\n   Returns:\n     A context manager that specifies the default device to use for newly\n     created ops.\n+\n+  Raises:\n+    RuntimeError: If eager execution is enabled and a function is passed in.\n   \"\"\"\n   if context.in_graph_mode():\n     return get_default_graph().device(device_name_or_function)\n   else:\n     # TODO(agarwal): support device functions in EAGER mode.\n+    if callable(device_name_or_function):\n+      raise RuntimeError(\n+          \"tf.device does not support functions when eager execution \"\n+          \"is enabled.\")\n     return context.device(device_name_or_function)\n \n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 306,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/cfb13fa789bcf1cdbbf0fd38cf7568b7098ab99b",
    "date": "2017-10-17T14:01:03-07:00",
    "message": "Added an additional check on the length of the values and boundaries lists.\n\nPiperOrigin-RevId: 172510229",
    "changes": [
        {
            "name": "learning_rate_decay.py",
            "path": "tensorflow/python/training/learning_rate_decay.py",
            "patches": [
                {
                    "old_start": 130,
                    "old_length": 8,
                    "new_start": 130,
                    "new_length": 12,
                    "hunk_buggy": "[' \\n', '   Raises:\\n', '     ValueError: if types of `x` and `boundaries` do not match, or types of all\\n', '-        `values` do not match.\\n', '   \"\"\"\\n', '   with ops.name_scope(name, \"PiecewiseConstant\",\\n', '                       [x, boundaries, values, name]) as name:\\n', '     x = ops.convert_to_tensor(x)']",
                    "hunk_fix": "@@ -130,8 +130,12 @@ def piecewise_constant(x, boundaries, values, name=None):\n \n   Raises:\n     ValueError: if types of `x` and `boundaries` do not match, or types of all\n-        `values` do not match.\n+        `values` do not match or\n+        the number of elements in the lists does not match.\n   \"\"\"\n+  if len(boundaries) != len(values) - 1:\n+    raise ValueError(\n+        \"The length of boundaries should be 1 less than the length of values\")\n   with ops.name_scope(name, \"PiecewiseConstant\",\n                       [x, boundaries, values, name]) as name:\n     x = ops.convert_to_tensor(x)"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 307,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/7db8e4fbc0be952daea74a2c3f501183d6006e61",
    "date": "2017-09-23T14:51:33+08:00",
    "message": "ENH: check x and y is empty dict",
    "changes": [
        {
            "name": "numpy_io.py",
            "path": "tensorflow/python/estimator/inputs/numpy_io.py",
            "patches": [
                {
                    "old_start": 89,
                    "old_length": 6,
                    "new_start": 89,
                    "new_length": 7,
                    "hunk_buggy": "['     ValueError: if the shape of `y` mismatches the shape of values in `x` (i.e.,\\n', '       values in `x` have same shape).\\n', '     ValueError: if duplicate keys are in both `x` and `y` when `y` is a dict.\\n', '     TypeError: `x` is not a dict or `shuffle` is not bool.\\n', '   \"\"\"\\n', ' \\n']",
                    "hunk_fix": "@@ -89,6 +89,7 @@ def numpy_input_fn(x,\n     ValueError: if the shape of `y` mismatches the shape of values in `x` (i.e.,\n       values in `x` have same shape).\n     ValueError: if duplicate keys are in both `x` and `y` when `y` is a dict.\n+    ValueError: if x or y is a empty dict.\n     TypeError: `x` is not a dict or `shuffle` is not bool.\n   \"\"\"\n \n"
                },
                {
                    "old_start": 100,
                    "old_length": 6,
                    "new_start": 101,
                    "new_length": 8,
                    "hunk_buggy": "['     \"\"\"Numpy input function.\"\"\"\\n', '     if not isinstance(x, dict):\\n', \"       raise TypeError('x must be dict; got {}'.format(type(x).__name__))\\n\", ' \\n', '     # Make a shadow copy and also ensure the order of iteration is consistent.\\n', '     ordered_dict_data = collections.OrderedDict(\\n']",
                    "hunk_fix": "@@ -100,6 +101,8 @@ def numpy_input_fn(x,\n     \"\"\"Numpy input function.\"\"\"\n     if not isinstance(x, dict):\n       raise TypeError('x must be dict; got {}'.format(type(x).__name__))\n+    if not x:\n+      raise ValueError('x cannot be empty')\n \n     # Make a shadow copy and also ensure the order of iteration is consistent.\n     ordered_dict_data = collections.OrderedDict(\n"
                },
                {
                    "old_start": 107,
                    "old_length": 9,
                    "new_start": 110,
                    "new_length": 12,
                    "hunk_buggy": "['     # Deep copy keys which is a view in python 3\\n', '     feature_keys = list(ordered_dict_data.keys())\\n', ' \\n', '-    if y:\\n', '       target_keys = None\\n', '     elif isinstance(y, dict):\\n', '       ordered_dict_y = collections.OrderedDict(\\n', '         sorted(y.items(), key=lambda t: t[0]))\\n', '       target_keys = list(ordered_dict_y.keys())\\n']",
                    "hunk_fix": "@@ -107,9 +110,12 @@ def numpy_input_fn(x,\n     # Deep copy keys which is a view in python 3\n     feature_keys = list(ordered_dict_data.keys())\n \n-    if y:\n+    if y is None:\n       target_keys = None\n     elif isinstance(y, dict):\n+      if not y:\n+        raise ValueError('y cannot be empty dict, use None instead.')\n+\n       ordered_dict_y = collections.OrderedDict(\n         sorted(y.items(), key=lambda t: t[0]))\n       target_keys = list(ordered_dict_y.keys())\n"
                },
                {
                    "old_start": 128,
                    "old_length": 7,
                    "new_start": 134,
                    "new_length": 7,
                    "hunk_buggy": "['       shape_dict_of_x = {k: ordered_dict_data[k].shape\\n', '                          for k in feature_keys}\\n', ' \\n', '-      if target_keys:\\n', '         shape_of_y = None\\n', '       elif isinstance(target_keys, string_types):\\n', '         shape_of_y = y.shape\\n']",
                    "hunk_fix": "@@ -128,7 +134,7 @@ def numpy_input_fn(x,\n       shape_dict_of_x = {k: ordered_dict_data[k].shape\n                          for k in feature_keys}\n \n-      if target_keys:\n+      if target_keys is None:\n         shape_of_y = None\n       elif isinstance(target_keys, string_types):\n         shape_of_y = y.shape\n"
                },
                {
                    "old_start": 157,
                    "old_length": 7,
                    "new_start": 163,
                    "new_length": 7,
                    "hunk_buggy": "['       batch.pop(0)\\n', ' \\n', '     features = dict(zip(feature_keys, batch[:len(feature_keys)]))\\n', '-    if target_keys:\\n', '       return features\\n', '     elif isinstance(target_keys, string_types):\\n', '       target = batch[-1]']",
                    "hunk_fix": "@@ -157,7 +163,7 @@ def numpy_input_fn(x,\n       batch.pop(0)\n \n     features = dict(zip(feature_keys, batch[:len(feature_keys)]))\n-    if target_keys:\n+    if target_keys is None:\n       return features\n     elif isinstance(target_keys, string_types):\n       target = batch[-1]"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 308,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/645f2c0cef75e80cdcaaaceca94a785191b9f423",
    "date": "2017-09-07T17:04:36-07:00",
    "message": "Merge pull request #12620 from DavidNorman/add-braodcast-verifier\n\n[XLA] Add a check to the HLO verifier for badly formatted Broadcasts.",
    "changes": [
        {
            "name": "hlo_verifier.cc",
            "path": "tensorflow/compiler/xla/service/hlo_verifier.cc",
            "patches": [
                {
                    "old_start": 437,
                    "old_length": 6,
                    "new_start": 437,
                    "new_length": 15,
                    "hunk_buggy": "['               << \" computation: \" << computation.get();\\n', '         }\\n', '       }\\n', ' \\n', '       auto previous = instructions.find(instruction->name());\\n', '       TF_RET_CHECK(previous == instructions.end())']",
                    "hunk_fix": "@@ -437,6 +437,15 @@ StatusOr<bool> HloVerifier::Run(HloModule* module) {\n               << \" computation: \" << computation.get();\n         }\n       }\n+      if (instruction->opcode() == HloOpcode::kBroadcast) {\n+        // If you see this failure then someone has confused the difference\n+        // between the HLO broadcast op, and the UserComputation broadcast\n+        // op.  See https://groups.google.com/forum/#!topic/xla-dev/9LqijHmTt_I\n+        // or ComputationLowerer::Visit()\n+        TF_RET_CHECK(instruction->dimensions().size() ==\n+                     ShapeUtil::Rank(instruction->operand(0)->shape()))\n+                << \"Broadcast HLO has invalid number of dimensions.\";\n+      }\n \n       auto previous = instructions.find(instruction->name());\n       TF_RET_CHECK(previous == instructions.end())"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 309,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/95166f5960322cc784a7e8f339a701da80a41a1e",
    "date": "2017-08-03T19:57:12-07:00",
    "message": "Add a null check on enter_ctx and update the null check on merge_ctx\n\nPiperOrigin-RevId: 164058574",
    "changes": [
        {
            "name": "graph_properties.cc",
            "path": "tensorflow/core/grappler/costs/graph_properties.cc",
            "patches": [
                {
                    "old_start": 70,
                    "old_length": 6,
                    "new_start": 70,
                    "new_length": 7,
                    "hunk_buggy": "[' Status UpdateEnter(ShapeRefiner* shape_refiner, const Node* node, bool relax,\\n', '                    std::queue<const Node*>* new_shapes) {\\n', '   auto enter_ctx = shape_refiner->GetContext(node);\\n', '   for (int i = 0; i < enter_ctx->num_outputs(); i++) {\\n', '     TF_RETURN_IF_ERROR(shape_refiner->SetShape(node, i, enter_ctx->input(0)));\\n', '   }\\n']",
                    "hunk_fix": "@@ -70,6 +70,7 @@ Status ShapeOfMergeNode(const Node* node, InferenceContext* c) {\n Status UpdateEnter(ShapeRefiner* shape_refiner, const Node* node, bool relax,\n                    std::queue<const Node*>* new_shapes) {\n   auto enter_ctx = shape_refiner->GetContext(node);\n+  CHECK_NE(enter_ctx, nullptr);\n   for (int i = 0; i < enter_ctx->num_outputs(); i++) {\n     TF_RETURN_IF_ERROR(shape_refiner->SetShape(node, i, enter_ctx->input(0)));\n   }\n"
                },
                {
                    "old_start": 82,
                    "old_length": 7,
                    "new_start": 83,
                    "new_length": 7,
                    "hunk_buggy": "['         continue;\\n', '       }\\n', '       InferenceContext* merge_ctx = shape_refiner->GetContext(dst);\\n', '-      DCHECK_NE(merge_ctx, nullptr);\\n', '       TF_RETURN_IF_ERROR(ShapeOfMergeNode(dst, merge_ctx));\\n', '       new_shapes->push(dst);\\n', '     }']",
                    "hunk_fix": "@@ -82,7 +83,7 @@ Status UpdateEnter(ShapeRefiner* shape_refiner, const Node* node, bool relax,\n         continue;\n       }\n       InferenceContext* merge_ctx = shape_refiner->GetContext(dst);\n-      DCHECK_NE(merge_ctx, nullptr);\n+      CHECK_NE(merge_ctx, nullptr);\n       TF_RETURN_IF_ERROR(ShapeOfMergeNode(dst, merge_ctx));\n       new_shapes->push(dst);\n     }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 310,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/b65d9ec2b78c7c23e368ed4eec7b4deb89dcd712",
    "date": "2017-07-21T13:29:29-07:00",
    "message": "Fix value error generated on is_scalar check (#10391)\n\n* Fix value error generated on is_scalar check\r\n\r\n`is_scalar = shape is not None and not shape` raises a value error when shape is a scalar, \"ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\"\r\n\r\n* Update variable_scope.py\r\n\r\n* Fix",
    "changes": [
        {
            "name": "variable_scope.py",
            "path": "tensorflow/python/ops/variable_scope.py",
            "patches": [
                {
                    "old_start": 300,
                    "old_length": 7,
                    "new_start": 300,
                    "new_length": 8,
                    "hunk_buggy": "['                      initializer=None, regularizer=None, reuse=None,\\n', '                      trainable=True, collections=None, caching_device=None,\\n', '                      partitioner=None, validate_shape=True, use_resource=None):\\n', '-      is_scalar = shape is not None and not shape\\n', '       # Partitioned variable case\\n', '       if partitioner is not None and not is_scalar:\\n', '         if not callable(partitioner):']",
                    "hunk_fix": "@@ -300,7 +300,8 @@ class _VariableStore(object):\n                      initializer=None, regularizer=None, reuse=None,\n                      trainable=True, collections=None, caching_device=None,\n                      partitioner=None, validate_shape=True, use_resource=None):\n-      is_scalar = shape is not None and not shape\n+      is_scalar = (shape is not None and isinstance(shape, collections_lib.Sequence)\n+                   and len(shape) == 0)\n       # Partitioned variable case\n       if partitioner is not None and not is_scalar:\n         if not callable(partitioner):"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 311,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/e05f78a9b688a8ae37b1a03bfc4459e18e3b88e4",
    "date": "2017-06-13T22:19:53-07:00",
    "message": "After synchronizing CUDA device, check for errors.\n\nPiperOrigin-RevId: 158939543",
    "changes": [
        {
            "name": "cuda_driver.cc",
            "path": "tensorflow/stream_executor/cuda/cuda_driver.cc",
            "patches": [
                {
                    "old_start": 15,
                    "old_length": 6,
                    "new_start": 15,
                    "new_length": 7,
                    "hunk_buggy": "[' \\n', ' #include \"tensorflow/stream_executor/cuda/cuda_driver.h\"\\n', ' \\n', ' #include <stdint.h>\\n', ' #include <stdlib.h>\\n', ' #include <map>\\n']",
                    "hunk_fix": "@@ -15,6 +15,7 @@ limitations under the License.\n \n #include \"tensorflow/stream_executor/cuda/cuda_driver.h\"\n \n+#include <cuda_runtime.h>\n #include <stdint.h>\n #include <stdlib.h>\n #include <map>\n"
                },
                {
                    "old_start": 1093,
                    "old_length": 12,
                    "new_start": 1094,
                    "new_length": 19,
                    "hunk_buggy": "[' \\n', ' /* static */ bool CUDADriver::SynchronizeContext(CudaContext* context) {\\n', '   ScopedActivateContext activation{context};\\n', '-  CUresult res = cuCtxSynchronize();\\n', '   if (res != CUDA_SUCCESS) {\\n', '     LOG(ERROR) << \"could not synchronize on CUDA context: \" << ToString(res)\\n', '                << \" :: \" << port::CurrentStackTrace();\\n', '     return false;\\n', '   }\\n', ' \\n', '   return true;\\n', ' }']",
                    "hunk_fix": "@@ -1093,12 +1094,19 @@ CUDADriver::ContextGetSharedMemConfig(CudaContext* context) {\n \n /* static */ bool CUDADriver::SynchronizeContext(CudaContext* context) {\n   ScopedActivateContext activation{context};\n-  CUresult res = cuCtxSynchronize();\n+  const CUresult res = cuCtxSynchronize();\n   if (res != CUDA_SUCCESS) {\n     LOG(ERROR) << \"could not synchronize on CUDA context: \" << ToString(res)\n                << \" :: \" << port::CurrentStackTrace();\n     return false;\n   }\n+  const auto cudart_error = cudaPeekAtLastError();\n+  if (cudart_error != cudaSuccess) {\n+    LOG(ERROR) << \"could not synchronize on CUDA context: \"\n+               << cudaGetErrorString(cudart_error)\n+               << \" :: \" << port::CurrentStackTrace();\n+    return false;\n+  }\n \n   return true;\n }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 312,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/70ade1b64f65d0a2275672d27129627ff116a997",
    "date": "2017-06-13T16:58:21-07:00",
    "message": "Fix defect: shuffle_batch gives ZeroDivisionError when computing capacity stat (#10477)\n\n* Fix defect: shuffle_batch gives ZeroDivisionError when computing capacity stat\r\n\r\n* Cover < case in error checking",
    "changes": [
        {
            "name": "input.py",
            "path": "tensorflow/python/training/input.py",
            "patches": [
                {
                    "old_start": 762,
                    "old_length": 6,
                    "new_start": 762,
                    "new_length": 9,
                    "hunk_buggy": "['   tensor_list = _as_tensor_list(tensors)\\n', '   with ops.name_scope(name, \"shuffle_batch\",\\n', '                       list(tensor_list) + [keep_input]) as name:\\n', '     tensor_list = _validate(tensor_list)\\n', '     keep_input = _validate_keep_input(keep_input, enqueue_many)\\n', '     tensor_list, sparse_info = _store_sparse_tensors(']",
                    "hunk_fix": "@@ -762,6 +762,9 @@ def _shuffle_batch(tensors, batch_size, capacity, min_after_dequeue,\n   tensor_list = _as_tensor_list(tensors)\n   with ops.name_scope(name, \"shuffle_batch\",\n                       list(tensor_list) + [keep_input]) as name:\n+    if capacity <= min_after_dequeue:\n+      raise ValueError(\"capacity %d must be bigger than min_after_dequeue %d.\"\n+                       % (capacity, min_after_dequeue))\n     tensor_list = _validate(tensor_list)\n     keep_input = _validate_keep_input(keep_input, enqueue_many)\n     tensor_list, sparse_info = _store_sparse_tensors("
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 313,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/7254b098e04c5deba029b06967803422cdf329e6",
    "date": "2017-05-22T15:25:48-04:00",
    "message": "Merge pull request #10068 from zasdfgbnm/unknown_shape_of_rank\n\nInferenceContext::UnknownShapeOfRank support unknown rank, check rank>=0",
    "changes": [
        {
            "name": "shape_inference.cc",
            "path": "tensorflow/core/framework/shape_inference.cc",
            "patches": [
                {
                    "old_start": 521,
                    "old_length": 6,
                    "new_start": 521,
                    "new_length": 10,
                    "hunk_buggy": "[' \\n', ' ShapeHandle InferenceContext::UnknownShapeOfRank(int64 rank) {\\n', '   CHECK_LE(rank, kint32max) << \"rank must be less than kint32max\";\\n', '   std::vector<DimensionHandle> dims(rank);\\n', '   for (int32 i = 0; i < rank; ++i) {\\n', '     dims[i] = UnknownDim();']",
                    "hunk_fix": "@@ -521,6 +521,10 @@ ShapeHandle InferenceContext::UnknownShape() {\n \n ShapeHandle InferenceContext::UnknownShapeOfRank(int64 rank) {\n   CHECK_LE(rank, kint32max) << \"rank must be less than kint32max\";\n+  if(rank == kUnknownRank) {\n+    return UnknownShape();\n+  }\n+  CHECK_GE(rank, 0) << \"rank must not be negative\";\n   std::vector<DimensionHandle> dims(rank);\n   for (int32 i = 0; i < rank; ++i) {\n     dims[i] = UnknownDim();"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 314,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/41deb95a7bde735d3c8b9adedd8b1fe8c1ef2732",
    "date": "2017-05-20T22:19:01-04:00",
    "message": "support unknown rank, check rank>=0",
    "changes": [
        {
            "name": "shape_inference.cc",
            "path": "tensorflow/core/framework/shape_inference.cc",
            "patches": [
                {
                    "old_start": 521,
                    "old_length": 6,
                    "new_start": 521,
                    "new_length": 10,
                    "hunk_buggy": "[' \\n', ' ShapeHandle InferenceContext::UnknownShapeOfRank(int64 rank) {\\n', '   CHECK_LE(rank, kint32max) << \"rank must be less than kint32max\";\\n', '   std::vector<DimensionHandle> dims(rank);\\n', '   for (int32 i = 0; i < rank; ++i) {\\n', '     dims[i] = UnknownDim();']",
                    "hunk_fix": "@@ -521,6 +521,10 @@ ShapeHandle InferenceContext::UnknownShape() {\n \n ShapeHandle InferenceContext::UnknownShapeOfRank(int64 rank) {\n   CHECK_LE(rank, kint32max) << \"rank must be less than kint32max\";\n+  if(rank == kUnknownRank) {\n+    return UnknownShape();\n+  }\n+  CHECK_GE(rank,0) << \"rank must not be negative\";\n   std::vector<DimensionHandle> dims(rank);\n   for (int32 i = 0; i < rank; ++i) {\n     dims[i] = UnknownDim();"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 315,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/c7f79bb75b5b83c3011e164ccd617a6ada910ea4",
    "date": "2017-05-10T15:29:07-07:00",
    "message": "Merged commit includes the following changes:\n155427981  by andrewharp <andrewharp@google.com>:\n\n    StatSummarizer: Put size check error message outside of if block where it belongs.\n\n--\n155427811  by A. Unique TensorFlower <gardener@tensorflow.org>:\n\n    Internal change\n\nPiperOrigin-RevId: 155427981",
    "changes": [
        {
            "name": "stat_summarizer.cc",
            "path": "tensorflow/core/util/stat_summarizer.cc",
            "patches": [
                {
                    "old_start": 50,
                    "old_length": 20,
                    "new_start": 50,
                    "new_length": 23,
                    "hunk_buggy": "['       }\\n', '       const auto& stored = detail->outputs[slot];\\n', '       const auto& current = output.tensor_description();\\n', '-      bool do_shapes_match = true;\\n', '-      if (stored.shape().dim_size() != current.shape().dim_size()) {\\n', '-        do_shapes_match = false;\\n', '-      } else {\\n', '         for (int i = 0; i < stored.shape().dim_size(); ++i) {\\n', '           if (stored.shape().dim(i).size() != current.shape().dim(i).size()) {\\n', '-            do_shapes_match = false;\\n', '           }\\n', '         }\\n', ' \\n', '-        if ((stored.dtype() != current.dtype()) || !do_shapes_match) {\\n', '-          LOG(WARNING) << \"Output tensor changed between runs for \\'\"\\n', '-                       << ns.node_name();\\n', '-        }\\n', '       }\\n', '     }\\n', '   }']",
                    "hunk_fix": "@@ -50,20 +50,23 @@ void StatSummarizer::Validate(const Detail* detail,\n       }\n       const auto& stored = detail->outputs[slot];\n       const auto& current = output.tensor_description();\n-      bool do_shapes_match = true;\n-      if (stored.shape().dim_size() != current.shape().dim_size()) {\n-        do_shapes_match = false;\n-      } else {\n+\n+      bool do_tensors_match =\n+          (stored.dtype() == current.dtype()) &&\n+          (stored.shape().dim_size() == current.shape().dim_size());\n+\n+      if (do_tensors_match) {\n         for (int i = 0; i < stored.shape().dim_size(); ++i) {\n           if (stored.shape().dim(i).size() != current.shape().dim(i).size()) {\n-            do_shapes_match = false;\n+            do_tensors_match = false;\n+            break;\n           }\n         }\n+      }\n \n-        if ((stored.dtype() != current.dtype()) || !do_shapes_match) {\n-          LOG(WARNING) << \"Output tensor changed between runs for '\"\n-                       << ns.node_name();\n-        }\n+      if (!do_tensors_match) {\n+        LOG(WARNING) << \"Output tensor changed between runs for '\"\n+                     << ns.node_name();\n       }\n     }\n   }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 316,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/0a9b39caefd437fec742ae48b25061abd6e2699b",
    "date": "2017-04-28T18:04:24-07:00",
    "message": "When allocating GPU constants, check to see if the destination\ntensor is intialized early (because we ran out of memory) and report\nit as such.\n\nFixes #7025.\nChange: 154603030",
    "changes": [
        {
            "name": "gpu_device.cc",
            "path": "tensorflow/core/common_runtime/gpu/gpu_device.cc",
            "patches": [
                {
                    "old_start": 465,
                    "old_length": 6,
                    "new_start": 465,
                    "new_length": 14,
                    "hunk_buggy": "['                               DataTypeString(parsed.dtype()), \" tensor\");\\n', '     }\\n', '     Tensor copy(GetAllocator(alloc_attrs), parsed.dtype(), parsed.shape());\\n', '     port::Tracing::ScopedAnnotation annotation(\"MakeTensorFromProto\");\\n', '     Notification n;\\n', '     device_contexts_[0]->CopyCPUTensorToDevice(&parsed, this, &copy,']",
                    "hunk_fix": "@@ -465,6 +465,14 @@ Status BaseGPUDevice::MakeTensorFromProto(const TensorProto& tensor_proto,\n                               DataTypeString(parsed.dtype()), \" tensor\");\n     }\n     Tensor copy(GetAllocator(alloc_attrs), parsed.dtype(), parsed.shape());\n+\n+    // If the tensor is not initialized, we likely ran out of memory.\n+    if (!copy.IsInitialized()) {\n+      return errors::ResourceExhausted(\n+          \"OOM when allocating tensor of shape \", parsed.shape().DebugString(),\n+          \" and type \", DataTypeString(parsed.dtype()));\n+    }\n+\n     port::Tracing::ScopedAnnotation annotation(\"MakeTensorFromProto\");\n     Notification n;\n     device_contexts_[0]->CopyCPUTensorToDevice(&parsed, this, &copy,"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 317,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/fa8381593d0cbe354cb54d691e0a8c42bf4b69d0",
    "date": "2017-04-27T14:47:43-07:00",
    "message": "Move Batch op input validation before enqueueing to the batch scheduler, because earlier error detection is better (and also so batch->size() doesn't crash if #dims==0 :).\nChange: 154470659",
    "changes": [
        {
            "name": "batch_kernels.cc",
            "path": "tensorflow/contrib/batching/kernels/batch_kernels.cc",
            "patches": [
                {
                    "old_start": 239,
                    "old_length": 7,
                    "new_start": 239,
                    "new_length": 18,
                    "hunk_buggy": "['     OpInputList tensors;\\n', '     TF_RETURN_IF_ERROR(context->input_list(\"in_tensors\", &tensors));\\n', '     for (int i = 0; i < tensors.size(); ++i) {\\n', '-      batch_components->inputs.push_back(tensors[i]);\\n', '     }\\n', '     batch_components->context = context;\\n', '     batch_components->done_callback = std::move(done_callback);\\n']",
                    "hunk_fix": "@@ -239,7 +239,18 @@ class BatchResource : public ResourceBase {\n     OpInputList tensors;\n     TF_RETURN_IF_ERROR(context->input_list(\"in_tensors\", &tensors));\n     for (int i = 0; i < tensors.size(); ++i) {\n-      batch_components->inputs.push_back(tensors[i]);\n+      const Tensor& tensor = tensors[i];\n+      if (tensor.shape().dims() == 0) {\n+        return errors::InvalidArgument(\n+            \"Batching input tensors must have at least one dimension\");\n+      }\n+      if (tensors.size() >= 2 &&\n+          tensor.shape().dim_size(0) != tensors[0].shape().dim_size(0)) {\n+        return errors::InvalidArgument(\n+            \"Batching input tensors supplied in a given op invocation must \"\n+            \"have equal 0th-dimension size\");\n+      }\n+      batch_components->inputs.push_back(tensor);\n     }\n     batch_components->context = context;\n     batch_components->done_callback = std::move(done_callback);\n"
                },
                {
                    "old_start": 279,
                    "old_length": 21,
                    "new_start": 290,
                    "new_length": 6,
                    "hunk_buggy": "['         return errors::InvalidArgument(\\n', '             \"Batching inputs must have equal number of edges\");\\n', '       }\\n', '-\\n', '-      for (int edge = 0; edge < task.inputs.size(); ++edge) {\\n', '-        const Tensor& tensor = task.inputs[edge];\\n', '-\\n', '-        if (tensor.shape().dims() == 0) {\\n', '-          return errors::InvalidArgument(\\n', '-              \"Batching input tensors must have at least one dimension\");\\n', '-        }\\n', '-\\n', '-        if (tensor.shape().dim_size(0) != task.inputs[0].shape().dim_size(0)) {\\n', '-          return errors::InvalidArgument(\\n', '-              \"Batching input tensors supplied in a given op invocation must \"\\n', '-              \"have equal 0th-dimension size\");\\n', '-        }\\n', '-      }\\n', '     }\\n', ' \\n', '     return Status::OK();']",
                    "hunk_fix": "@@ -279,21 +290,6 @@ class BatchResource : public ResourceBase {\n         return errors::InvalidArgument(\n             \"Batching inputs must have equal number of edges\");\n       }\n-\n-      for (int edge = 0; edge < task.inputs.size(); ++edge) {\n-        const Tensor& tensor = task.inputs[edge];\n-\n-        if (tensor.shape().dims() == 0) {\n-          return errors::InvalidArgument(\n-              \"Batching input tensors must have at least one dimension\");\n-        }\n-\n-        if (tensor.shape().dim_size(0) != task.inputs[0].shape().dim_size(0)) {\n-          return errors::InvalidArgument(\n-              \"Batching input tensors supplied in a given op invocation must \"\n-              \"have equal 0th-dimension size\");\n-        }\n-      }\n     }\n \n     return Status::OK();"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 318,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/bc7b64fe998cb0f118eace5bc29b52554eeda3f1",
    "date": "2017-03-17T19:49:05-07:00",
    "message": "Added back the channel dimension check as a known channel dimension is required by creating beta.\nChange: 150511737",
    "changes": [
        {
            "name": "layers.py",
            "path": "tensorflow/contrib/layers/python/layers/layers.py",
            "patches": [
                {
                    "old_start": 230,
                    "old_length": 7,
                    "new_start": 230,
                    "new_length": 9,
                    "hunk_buggy": "[\"                        ' Expected 2 or 4 but got %d' % (\\n\", '                            inputs.name, original_rank))\\n', '     if original_rank == 2:\\n', '-      channels = array_ops.shape(inputs)[-1]\\n', '       new_shape = [-1, 1, 1, channels]\\n', '       if data_format == DATA_FORMAT_NCHW:\\n', '         new_shape = [-1, channels, 1, 1]\\n']",
                    "hunk_fix": "@@ -230,7 +230,9 @@ def _fused_batch_norm(\n                        ' Expected 2 or 4 but got %d' % (\n                            inputs.name, original_rank))\n     if original_rank == 2:\n-      channels = array_ops.shape(inputs)[-1]\n+      channels = inputs.get_shape()[-1].value\n+      if channels is None:\n+        raise ValueError('`C` dimension must be known but is None')\n       new_shape = [-1, 1, 1, channels]\n       if data_format == DATA_FORMAT_NCHW:\n         new_shape = [-1, channels, 1, 1]\n"
                },
                {
                    "old_start": 350,
                    "old_length": 7,
                    "new_start": 352,
                    "new_length": 7,
                    "hunk_buggy": "[' \\n', '     outputs.set_shape(inputs_shape)\\n', '     if original_shape.ndims == 2:\\n', '-      outputs = array_ops.reshape(outputs, array_ops.shape[original_inputs])\\n', '     if activation_fn is not None:\\n', '       outputs = activation_fn(outputs)\\n', '     return utils.collect_named_outputs(outputs_collections,\\n']",
                    "hunk_fix": "@@ -350,7 +352,7 @@ def _fused_batch_norm(\n \n     outputs.set_shape(inputs_shape)\n     if original_shape.ndims == 2:\n-      outputs = array_ops.reshape(outputs, array_ops.shape[original_inputs])\n+      outputs = array_ops.reshape(outputs, array_ops.shape(original_inputs))\n     if activation_fn is not None:\n       outputs = activation_fn(outputs)\n     return utils.collect_named_outputs(outputs_collections,\n"
                },
                {
                    "old_start": 1230,
                    "old_length": 7,
                    "new_start": 1232,
                    "new_length": 7,
                    "hunk_buggy": "['     batch_dim, spatial_dims = input_shape[0], input_shape[1:]\\n', '     if all(spatial_dims):\\n', '       outputs.set_shape([batch_dim,\\n', '-                        functools.reduce(lambda x, y: x * y, spatial_dims)])\\n', '     else:\\n', '       outputs.set_shape([batch_dim, None])\\n', ' ']",
                    "hunk_fix": "@@ -1230,7 +1232,7 @@ def flatten(inputs,\n     batch_dim, spatial_dims = input_shape[0], input_shape[1:]\n     if all(spatial_dims):\n       outputs.set_shape([batch_dim,\n-                        functools.reduce(lambda x, y: x * y, spatial_dims)])\n+                         functools.reduce(lambda x, y: x * y, spatial_dims)])\n     else:\n       outputs.set_shape([batch_dim, None])\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 319,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/fb1c4cd8283f262bca95ccd04df6f9eb4ae1da0c",
    "date": "2017-03-17T13:44:35-07:00",
    "message": "Add None check for seq_len_mask before reshape.\nChange: 150477638",
    "changes": [
        {
            "name": "dynamic_attention_wrapper.py",
            "path": "tensorflow/contrib/seq2seq/python/ops/dynamic_attention_wrapper.py",
            "patches": [
                {
                    "old_start": 88,
                    "old_length": 10,
                    "new_start": 88,
                    "new_length": 13,
                    "hunk_buggy": "['     rank = m.get_shape().ndims\\n', '     rank = rank if rank is not None else array_ops.rank(m)\\n', '     extra_ones = array_ops.ones(rank - 2, dtype=dtypes.int32)\\n', '-    seq_len_mask = array_ops.reshape(\\n', '-        seq_len_mask,\\n', '-        array_ops.concat((array_ops.shape(seq_len_mask), extra_ones), 0))\\n', '-    return m * seq_len_mask if memory_sequence_length is not None else m\\n', '   return nest.map_structure(lambda m: _maybe_mask(m, seq_len_mask), memory)\\n', ' \\n', ' ']",
                    "hunk_fix": "@@ -88,10 +88,13 @@ def _prepare_memory(memory, memory_sequence_length, check_inner_dims_defined):\n     rank = m.get_shape().ndims\n     rank = rank if rank is not None else array_ops.rank(m)\n     extra_ones = array_ops.ones(rank - 2, dtype=dtypes.int32)\n-    seq_len_mask = array_ops.reshape(\n-        seq_len_mask,\n-        array_ops.concat((array_ops.shape(seq_len_mask), extra_ones), 0))\n-    return m * seq_len_mask if memory_sequence_length is not None else m\n+    if memory_sequence_length is not None:\n+      seq_len_mask = array_ops.reshape(\n+          seq_len_mask,\n+          array_ops.concat((array_ops.shape(seq_len_mask), extra_ones), 0))\n+      return m * seq_len_mask\n+    else:\n+      return m\n   return nest.map_structure(lambda m: _maybe_mask(m, seq_len_mask), memory)\n \n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 320,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/ca170f34d9174d6981850855190a398393aa921e",
    "date": "2017-03-17T13:27:50-07:00",
    "message": "[Tensorflow] Add check fail when user passes a tensor with nullptr to lookup.\nChange: 150474503",
    "changes": [
        {
            "name": "tensor_bundle.cc",
            "path": "tensorflow/core/util/tensor_bundle/tensor_bundle.cc",
            "patches": [
                {
                    "old_start": 700,
                    "old_length": 6,
                    "new_start": 700,
                    "new_length": 7,
                    "hunk_buggy": "[' }\\n', ' \\n', ' Status BundleReader::Lookup(StringPiece key, Tensor* val) {\\n', '   BundleEntryProto entry;\\n', '   TF_RETURN_IF_ERROR(GetBundleEntryProto(key, &entry));\\n', ' \\n']",
                    "hunk_fix": "@@ -700,6 +700,7 @@ Status BundleReader::GetValue(const BundleEntryProto& entry, Tensor* val) {\n }\n \n Status BundleReader::Lookup(StringPiece key, Tensor* val) {\n+  CHECK(val != nullptr);\n   BundleEntryProto entry;\n   TF_RETURN_IF_ERROR(GetBundleEntryProto(key, &entry));\n \n"
                },
                {
                    "old_start": 726,
                    "old_length": 6,
                    "new_start": 727,
                    "new_length": 7,
                    "hunk_buggy": "[' \\n', ' Status BundleReader::LookupSlice(StringPiece full_tensor_key,\\n', '                                  const TensorSlice& slice_spec, Tensor* val) {\\n', '   BundleEntryProto entry;\\n', '   TF_RETURN_IF_ERROR(GetBundleEntryProto(full_tensor_key, &entry));\\n', '   return GetSliceValue(full_tensor_key, entry, slice_spec, val);']",
                    "hunk_fix": "@@ -726,6 +727,7 @@ Status BundleReader::LookupTensorSlices(StringPiece key,\n \n Status BundleReader::LookupSlice(StringPiece full_tensor_key,\n                                  const TensorSlice& slice_spec, Tensor* val) {\n+  CHECK(val != nullptr);\n   BundleEntryProto entry;\n   TF_RETURN_IF_ERROR(GetBundleEntryProto(full_tensor_key, &entry));\n   return GetSliceValue(full_tensor_key, entry, slice_spec, val);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 321,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/a0ca4bcb81dfd07fdb1c7872b5852f84cfc1a081",
    "date": "2017-03-16T16:48:57-07:00",
    "message": "Fix separable convolution bias check\nChange: 150385615",
    "changes": [
        {
            "name": "convolutional.py",
            "path": "tensorflow/python/layers/convolutional.py",
            "patches": [
                {
                    "old_start": 844,
                    "old_length": 7,
                    "new_start": 844,
                    "new_length": 7,
                    "hunk_buggy": "['       # Reshape to channels first\\n', '       outputs = array_ops.transpose(outputs, (0, 3, 1, 2))\\n', ' \\n', '-    if self.bias:\\n', '       outputs = nn.bias_add(\\n', '           outputs,\\n', '           self.bias,']",
                    "hunk_fix": "@@ -844,7 +844,7 @@ class SeparableConv2D(Conv2D):\n       # Reshape to channels first\n       outputs = array_ops.transpose(outputs, (0, 3, 1, 2))\n \n-    if self.bias:\n+    if self.bias is not None:\n       outputs = nn.bias_add(\n           outputs,\n           self.bias,"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 322,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/cd8d0bf58ad554588012898161c91fa453bbf7f0",
    "date": "2017-02-21T14:07:40-08:00",
    "message": "contrib/android: Fix bug.\n\nAddress edge case where runStats is null and the interface is closed.\nChange: 148139848",
    "changes": [
        {
            "name": "TensorFlowInferenceInterface.java",
            "path": "tensorflow/contrib/android/java/org/tensorflow/contrib/android/TensorFlowInferenceInterface.java",
            "patches": [
                {
                    "old_start": 169,
                    "old_length": 7,
                    "new_start": 169,
                    "new_length": 9,
                    "hunk_buggy": "['     closeFetches();\\n', '     sess.close();\\n', '     g.close();\\n', '-    runStats.close();\\n', '     runStats = null;\\n', '     enableStats = false;\\n', '   }']",
                    "hunk_fix": "@@ -169,7 +169,9 @@ public class TensorFlowInferenceInterface {\n     closeFetches();\n     sess.close();\n     g.close();\n-    runStats.close();\n+    if (runStats != null) {\n+      runStats.close();\n+    }\n     runStats = null;\n     enableStats = false;\n   }"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 323,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/02703f9525696f4788496745f6756585c1c546a3",
    "date": "2017-02-14T22:50:18-08:00",
    "message": "Fix crash in range sampler by adding a range check in the sampler op.\nChange: 147562709",
    "changes": [
        {
            "name": "candidate_sampler_ops.cc",
            "path": "tensorflow/core/kernels/candidate_sampler_ops.cc",
            "patches": [
                {
                    "old_start": 47,
                    "old_length": 6,
                    "new_start": 47,
                    "new_length": 12,
                    "hunk_buggy": "['     OP_REQUIRES(context, true_classes.dim_size(1) == num_true_,\\n', '                 errors::InvalidArgument(\"true_classes must have \"\\n', '                                         \"num_true columns\"));\\n', ' \\n', '     // Output candidates and expected_count.\\n', '     Tensor* out_sampled_candidates = nullptr;\\n']",
                    "hunk_fix": "@@ -47,6 +47,12 @@ class BaseCandidateSamplerOp : public OpKernel {\n     OP_REQUIRES(context, true_classes.dim_size(1) == num_true_,\n                 errors::InvalidArgument(\"true_classes must have \"\n                                         \"num_true columns\"));\n+    CHECK(sampler_) << \"CandidateSamplerOp did not set sampler_\";\n+\n+    if (unique_) {\n+      OP_REQUIRES(context, num_sampled_ <= sampler_->range(),\n+                  errors::InvalidArgument(\"Sampler's range is too small.\"));\n+    }\n \n     // Output candidates and expected_count.\n     Tensor* out_sampled_candidates = nullptr;\n"
                },
                {
                    "old_start": 73,
                    "old_length": 8,
                    "new_start": 79,
                    "new_length": 6,
                    "hunk_buggy": "['     gtl::MutableArraySlice<float> sampled_expected_count(\\n', '         out_sampled_expected_count->vec<float>().data(), num_sampled_);\\n', ' \\n', '-    CHECK(sampler_) << \"CandidateSamplerOp did not set sampler_\";\\n', '-\\n', '     // Approximately conservatively estimate the number of samples required.\\n', '     // In cases where rejection sampling is used we may occasionally use more\\n', '     // samples than expected, which will result in reused random bits.']",
                    "hunk_fix": "@@ -73,8 +79,6 @@ class BaseCandidateSamplerOp : public OpKernel {\n     gtl::MutableArraySlice<float> sampled_expected_count(\n         out_sampled_expected_count->vec<float>().data(), num_sampled_);\n \n-    CHECK(sampler_) << \"CandidateSamplerOp did not set sampler_\";\n-\n     // Approximately conservatively estimate the number of samples required.\n     // In cases where rejection sampling is used we may occasionally use more\n     // samples than expected, which will result in reused random bits."
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 324,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/a5b8d6c4694e4cd3e3cc4a162053ab0dfa6e174f",
    "date": "2016-12-02T10:20:52-08:00",
    "message": "Relax the check for whether the relevant aggregation dimensions are known ahead of time.",
    "changes": [
        {
            "name": "nn_impl.py",
            "path": "tensorflow/python/ops/nn_impl.py",
            "patches": [
                {
                    "old_start": 463,
                    "old_length": 7,
                    "new_start": 463,
                    "new_length": 7,
                    "hunk_buggy": "['   with ops.name_scope(name, \"sufficient_statistics\", [x, shift]):\\n', '     x = ops.convert_to_tensor(x, name=\"x\")\\n', '     x_shape = x.get_shape()\\n', '-    if x_shape.is_fully_defined():\\n', '       counts = 1\\n', '       for d in axes:\\n', '         counts *= x_shape[d].value']",
                    "hunk_fix": "@@ -463,7 +463,7 @@ def sufficient_statistics(x, axes, shift=None, keep_dims=False, name=None):\n   with ops.name_scope(name, \"sufficient_statistics\", [x, shift]):\n     x = ops.convert_to_tensor(x, name=\"x\")\n     x_shape = x.get_shape()\n-    if x_shape.is_fully_defined():\n+    if all(x_shape[d].value is not None for d in axes):\n       counts = 1\n       for d in axes:\n         counts *= x_shape[d].value"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 325,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/5e0c9fff657498f9a74da38b2ce1b4721698a388",
    "date": "2016-12-01T09:46:02-08:00",
    "message": "Add bounds checks to jpeg parsing code.\nChange: 140740851",
    "changes": [
        {
            "name": "jpeg_handle.cc",
            "path": "tensorflow/core/lib/jpeg/jpeg_handle.cc",
            "patches": [
                {
                    "old_start": 147,
                    "old_length": 8,
                    "new_start": 147,
                    "new_length": 16,
                    "hunk_buggy": "[' // -----------------------------------------------------------------------------\\n', ' void MemSkipInputData(j_decompress_ptr cinfo, long jump) {\\n', '   MemSourceMgr *src = reinterpret_cast<MemSourceMgr *>(cinfo->src);\\n', '-  src->pub.bytes_in_buffer -= jump;\\n', '-  src->pub.next_input_byte += jump;\\n', ' }\\n', ' \\n', ' // -----------------------------------------------------------------------------']",
                    "hunk_fix": "@@ -147,8 +147,16 @@ void MemTermSource(j_decompress_ptr cinfo) {}\n // -----------------------------------------------------------------------------\n void MemSkipInputData(j_decompress_ptr cinfo, long jump) {\n   MemSourceMgr *src = reinterpret_cast<MemSourceMgr *>(cinfo->src);\n-  src->pub.bytes_in_buffer -= jump;\n-  src->pub.next_input_byte += jump;\n+  if (jump < 0) {\n+    return;\n+  }\n+  if (jump > src->pub.bytes_in_buffer) {\n+    src->pub.bytes_in_buffer = 0;\n+    (void)MemFillInputBuffer(cinfo);  // warn with a fake EOI or error\n+  } else {\n+    src->pub.bytes_in_buffer -= jump;\n+    src->pub.next_input_byte += jump;\n+  }\n }\n \n // -----------------------------------------------------------------------------"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 326,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/c6899c721f3a4b4f2e71ae4e6d1767341112ff93",
    "date": "2016-10-06T16:38:54-07:00",
    "message": "bug fix when iterators stops at multiple of batch_size (#4777)\n\n* bug fix when iterators stops at multiple of batch_size\r\n\r\nFor more information on the issue see\r\nhttps://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/ZEzEa1TyYuE\r\n\r\n* reverted changes back as were unnecessary\r\n\r\n* _feed_fn() edge case fix in StreamingDataFeeder\r\n\r\n* Update graph_actions.py\r\n\r\n* Update graph_actions.py\r\n\r\n* Added space around `==`\r\n\r\n* Removed `else` for legiblity\r\n\r\n* reusing previous StopIteration to preserve stack trace",
    "changes": [
        {
            "name": "data_feeder.py",
            "path": "tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py",
            "patches": [
                {
                    "old_start": 507,
                    "old_length": 6,
                    "new_start": 507,
                    "new_length": 8,
                    "hunk_buggy": "['           inp[i, :] = six.next(self._x)\\n', '         except StopIteration:\\n', '           self.stopped = True\\n', '           inp = inp[:i, :]\\n', '           if self._y is not None:\\n', '             out = out[:i]']",
                    "hunk_fix": "@@ -507,6 +507,8 @@ class StreamingDataFeeder(DataFeeder):\n           inp[i, :] = six.next(self._x)\n         except StopIteration:\n           self.stopped = True\n+          if i == 0:\n+            raise\n           inp = inp[:i, :]\n           if self._y is not None:\n             out = out[:i]"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 327,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/8876a1796aeced8f89c279cbc98db9c7957ddbd1",
    "date": "2016-09-22T06:03:33-07:00",
    "message": "Updated check for existence of TensorFlow objects to 'is not None' rather than 'if [object]'.\nChange: 133944683",
    "changes": [
        {
            "name": "learning.py",
            "path": "tensorflow/contrib/slim/python/slim/learning.py",
            "patches": [
                {
                    "old_start": 663,
                    "old_length": 7,
                    "new_start": 663,
                    "new_length": 7,
                    "hunk_buggy": "[\"       raise ValueError('Cannot provide trace_every_n_steps because '\\n\", \"                        'logdir=None')\\n\", ' \\n', '-  if sync_optimizer and startup_delay_steps > 0:\\n', '     raise ValueError(\\n', \"         'startup_delay_steps must be zero when sync_optimizer is supplied.')\\n\", ' \\n']",
                    "hunk_fix": "@@ -663,7 +663,7 @@ def train(train_op,\n       raise ValueError('Cannot provide trace_every_n_steps because '\n                        'logdir=None')\n \n-  if sync_optimizer and startup_delay_steps > 0:\n+  if sync_optimizer is not None and startup_delay_steps > 0:\n     raise ValueError(\n         'startup_delay_steps must be zero when sync_optimizer is supplied.')\n \n"
                },
                {
                    "old_start": 697,
                    "old_length": 7,
                    "new_start": 697,
                    "new_length": 7,
                    "hunk_buggy": "[' \\n', '     cleanup_op = None\\n', ' \\n', '-    if is_chief and sync_optimizer:\\n', '       if not isinstance(sync_optimizer,\\n', '                         sync_replicas_optimizer.SyncReplicasOptimizer):\\n', '         raise ValueError(\\n']",
                    "hunk_fix": "@@ -697,7 +697,7 @@ def train(train_op,\n \n     cleanup_op = None\n \n-    if is_chief and sync_optimizer:\n+    if is_chief and sync_optimizer is not None:\n       if not isinstance(sync_optimizer,\n                         sync_replicas_optimizer.SyncReplicasOptimizer):\n         raise ValueError(\n"
                },
                {
                    "old_start": 761,
                    "old_length": 7,
                    "new_start": 761,
                    "new_length": 7,
                    "hunk_buggy": "['                              number_of_steps or sys.maxint))\\n', '         sv.start_queue_runners(sess)\\n', \"         logging.info('Starting Queues.')\\n\", '-        if is_chief and sync_optimizer:\\n', '           sv.start_queue_runners(sess, [chief_queue_runner])\\n', '         try:\\n', '           while not sv.should_stop():']",
                    "hunk_fix": "@@ -761,7 +761,7 @@ def train(train_op,\n                              number_of_steps or sys.maxint))\n         sv.start_queue_runners(sess)\n         logging.info('Starting Queues.')\n-        if is_chief and sync_optimizer:\n+        if is_chief and sync_optimizer is not None:\n           sv.start_queue_runners(sess, [chief_queue_runner])\n         try:\n           while not sv.should_stop():"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 328,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/aa54f547f04c3007b26df2379c6cf5f081948d0b",
    "date": "2016-08-18T09:16:11-07:00",
    "message": "Updated the check_numerics function to also validate the gradient corresponding\nto the tensor it's validating\nChange: 130645982",
    "changes": [
        {
            "name": "array_grad.py",
            "path": "tensorflow/python/ops/array_grad.py",
            "patches": [
                {
                    "old_start": 298,
                    "old_length": 7,
                    "new_start": 298,
                    "new_length": 8,
                    "hunk_buggy": "[' @ops.RegisterGradient(\"CheckNumerics\")\\n', ' def _CheckNumericsGrad(_, grad):\\n', '   \"\"\"Gradient for check_numerics op.\"\"\"\\n', '-  return grad\\n', ' \\n', ' \\n', ' @ops.RegisterGradient(\"Identity\")']",
                    "hunk_fix": "@@ -298,7 +298,8 @@ def _GatherNdGrad(unused_op, unused_grad):\n @ops.RegisterGradient(\"CheckNumerics\")\n def _CheckNumericsGrad(_, grad):\n   \"\"\"Gradient for check_numerics op.\"\"\"\n-  return grad\n+  return array_ops.check_numerics(\n+      grad, \"Not a number (NaN) or infinity (Inf) values detected in gradient.\")\n \n \n @ops.RegisterGradient(\"Identity\")"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 329,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/314d9cd9b607460f8bfea80fc828b1521ca18443",
    "date": "2016-07-21T14:25:35-07:00",
    "message": "Fix segfault in MacOS when GPU is not available (#3448)\n\n* Fix sigsegv in MacOS when GPU is not available\r\n\r\n* Change info link to point to Apple docs\r\n\r\n* Invert NULL checking logic\r\n\r\n* fix indentation\r\n\r\n* Re-align to get change to fit in 80 chars\r\n\r\n* Run clang-format \u2014style=google on changes",
    "changes": [
        {
            "name": "cuda_diagnostics.cc",
            "path": "tensorflow/stream_executor/cuda/cuda_diagnostics.cc",
            "patches": [
                {
                    "old_start": 314,
                    "old_length": 8,
                    "new_start": 314,
                    "new_length": 17,
                    "hunk_buggy": "['   if (CFDictionaryGetValueIfPresent(kext_infos, kDriverKextIdentifier, (const void**)&cuda_driver_info)) {\\n', '     // NOTE: OSX CUDA driver does not currently store the same driver version\\n', '     // in kCFBundleVersionKey as is returned by cuDriverGetVersion\\n', '-    const char * version = CFStringGetCStringPtr((CFStringRef)CFDictionaryGetValue(cuda_driver_info, kCFBundleVersionKey), kCFStringEncodingUTF8);\\n', '     CFRelease(kext_infos);\\n', '     return StringToDriverVersion(version);\\n', '   }\\n', '   CFRelease(kext_infos);']",
                    "hunk_fix": "@@ -314,8 +314,17 @@ port::StatusOr<DriverVersion> Diagnostician::FindKernelDriverVersion() {\n   if (CFDictionaryGetValueIfPresent(kext_infos, kDriverKextIdentifier, (const void**)&cuda_driver_info)) {\n     // NOTE: OSX CUDA driver does not currently store the same driver version\n     // in kCFBundleVersionKey as is returned by cuDriverGetVersion\n-    const char * version = CFStringGetCStringPtr((CFStringRef)CFDictionaryGetValue(cuda_driver_info, kCFBundleVersionKey), kCFStringEncodingUTF8);\n     CFRelease(kext_infos);\n+    const CFStringRef str = (CFStringRef)CFDictionaryGetValue(\n+        cuda_driver_info, kCFBundleVersionKey);\n+    const char *version = CFStringGetCStringPtr(str, kCFStringEncodingUTF8);\n+\n+    // version can be NULL in which case treat it as empty string\n+    // see\n+    // https://developer.apple.com/library/mac/documentation/CoreFoundation/Conceptual/CFStrings/Articles/AccessingContents.html#//apple_ref/doc/uid/20001184-100980-TPXREF112\n+    if (version == NULL) {\n+      return StringToDriverVersion(\"\");\n+    }\n     return StringToDriverVersion(version);\n   }\n   CFRelease(kext_infos);"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 330,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/a93ac5f7d147ae8fe946de33ad654161ae851352",
    "date": "2016-07-06T17:04:56-07:00",
    "message": "For quantization values where range_min == range_max, use the lowest_quantized.\n Add needed checks for divide-by-zero.\nChange: 126753893",
    "changes": [
        {
            "name": "quantization_utils.h",
            "path": "tensorflow/contrib/quantization/kernels/quantization_utils.h",
            "patches": [
                {
                    "old_start": 33,
                    "old_length": 8,
                    "new_start": 33,
                    "new_length": 10,
                    "hunk_buggy": "[\" // uses doubles and int64's to make sure we have enough room.\\n\", ' template <class T>\\n', ' int64 FloatToQuantizedUnclamped(float input, float range_min, float range_max) {\\n', '   if (range_min == range_max) {\\n', '-    return 0;\\n', '   }\\n', '   const int number_of_bits = sizeof(T) * 8;\\n', '   const int64 number_of_steps = static_cast<int64>(1) << number_of_bits;\\n']",
                    "hunk_fix": "@@ -33,8 +33,10 @@ namespace tensorflow {\n // uses doubles and int64's to make sure we have enough room.\n template <class T>\n int64 FloatToQuantizedUnclamped(float input, float range_min, float range_max) {\n+  const int64 lowest_quantized =\n+      static_cast<double>(Eigen::NumTraits<T>::lowest());\n   if (range_min == range_max) {\n-    return 0;\n+    return lowest_quantized;\n   }\n   const int number_of_bits = sizeof(T) * 8;\n   const int64 number_of_steps = static_cast<int64>(1) << number_of_bits;\n"
                },
                {
                    "old_start": 43,
                    "old_length": 8,
                    "new_start": 45,
                    "new_length": 6,
                    "hunk_buggy": "['   const double range_scale = (number_of_steps / range);\\n', '   int64 quantized =\\n', '       (round(input * range_scale) - round(range_min * range_scale));\\n', '-  const int64 lowest_quantized =\\n', '-      static_cast<double>(Eigen::NumTraits<T>::lowest());\\n', '   quantized += lowest_quantized;\\n', '   return quantized;\\n', ' }\\n']",
                    "hunk_fix": "@@ -43,8 +45,6 @@ int64 FloatToQuantizedUnclamped(float input, float range_min, float range_max) {\n   const double range_scale = (number_of_steps / range);\n   int64 quantized =\n       (round(input * range_scale) - round(range_min * range_scale));\n-  const int64 lowest_quantized =\n-      static_cast<double>(Eigen::NumTraits<T>::lowest());\n   quantized += lowest_quantized;\n   return quantized;\n }\n"
                },
                {
                    "old_start": 167,
                    "old_length": 7,
                    "new_start": 167,
                    "new_length": 9,
                    "hunk_buggy": "[' \\n', '   FloatToQuantizedStruct(float range_min, float range_max)\\n', '       : range_min(range_min),\\n', '-        range_scale((number_of_steps - 1.0) / (range_max - range_min)),\\n', '         range_min_scaled(round(range_min * range_scale)) {}\\n', ' \\n', '   const float range_min;\\n']",
                    "hunk_fix": "@@ -167,7 +167,9 @@ struct FloatToQuantizedStruct {\n \n   FloatToQuantizedStruct(float range_min, float range_max)\n       : range_min(range_min),\n-        range_scale((number_of_steps - 1.0) / (range_max - range_min)),\n+        range_scale(range_max == range_min\n+                        ? 0.0\n+                        : (number_of_steps - 1.0) / (range_max - range_min)),\n         range_min_scaled(round(range_min * range_scale)) {}\n \n   const float range_min;\n"
                },
                {
                    "old_start": 210,
                    "old_length": 7,
                    "new_start": 212,
                    "new_length": 9,
                    "hunk_buggy": "['   const int64 recip_output_range_fp =\\n', '       static_cast<int64>(recip_output_range * (1 << fp_shift));\\n', '   const int64 range_scale_fp =\\n', '-      static_cast<int64>(255.0 * (1 << fp_shift) * input_range / output_range);\\n', '   const int64 input_offset_fp =\\n', '       (min_input * recip_output_range_fp) + (range_scale_fp >> 1);\\n', '   const int64 output_offset_fp =']",
                    "hunk_fix": "@@ -210,7 +212,9 @@ inline void RequantizeManyInNewRange<qint32, quint8>(\n   const int64 recip_output_range_fp =\n       static_cast<int64>(recip_output_range * (1 << fp_shift));\n   const int64 range_scale_fp =\n-      static_cast<int64>(255.0 * (1 << fp_shift) * input_range / output_range);\n+      output_range == 0.0 ? 0.0\n+                          : static_cast<int64>(255.0 * (1 << fp_shift) *\n+                                               input_range / output_range);\n   const int64 input_offset_fp =\n       (min_input * recip_output_range_fp) + (range_scale_fp >> 1);\n   const int64 output_offset_fp ="
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 331,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/1220ba3ab332d6233a84d660cafb3d4e29958224",
    "date": "2016-03-18T08:47:58-07:00",
    "message": "Fix two potential asynchrony bounds-check bugs in transpose op.\nChange: 117518926",
    "changes": [
        {
            "name": "transpose_op.cc",
            "path": "tensorflow/core/kernels/transpose_op.cc",
            "patches": [
                {
                    "old_start": 23,
                    "old_length": 6,
                    "new_start": 23,
                    "new_length": 7,
                    "hunk_buggy": "[' #include \"tensorflow/core/framework/register_types.h\"\\n', ' #include \"tensorflow/core/framework/tensor.h\"\\n', ' #include \"tensorflow/core/framework/tensor_shape.h\"\\n', ' #include \"tensorflow/core/kernels/transpose_functor.h\"\\n', ' #include \"tensorflow/core/lib/core/status.h\"\\n', ' #include \"tensorflow/core/lib/strings/str_util.h\"\\n']",
                    "hunk_fix": "@@ -23,6 +23,7 @@ limitations under the License.\n #include \"tensorflow/core/framework/register_types.h\"\n #include \"tensorflow/core/framework/tensor.h\"\n #include \"tensorflow/core/framework/tensor_shape.h\"\n+#include \"tensorflow/core/kernels/bounds_check.h\"\n #include \"tensorflow/core/kernels/transpose_functor.h\"\n #include \"tensorflow/core/lib/core/status.h\"\n #include \"tensorflow/core/lib/strings/str_util.h\"\n"
                },
                {
                    "old_start": 55,
                    "old_length": 8,
                    "new_start": 56,
                    "new_length": 8,
                    "hunk_buggy": "['     auto Tout = output->vec<int32>();\\n', '     std::fill_n(Tout.data(), N, -1);\\n', '     for (int i = 0; i < N; ++i) {\\n', '-      const int32 d = Tin(i);\\n', '-      OP_REQUIRES(context, 0 <= d && d < N,\\n', '                   errors::InvalidArgument(d, \" is not between 0 and \", N));\\n', '       OP_REQUIRES(context, Tout(d) == -1,\\n', '                   errors::InvalidArgument(d, \" is duplicated in the input.\"));\\n']",
                    "hunk_fix": "@@ -55,8 +56,8 @@ class InvertPermutationOp : public OpKernel {\n     auto Tout = output->vec<int32>();\n     std::fill_n(Tout.data(), N, -1);\n     for (int i = 0; i < N; ++i) {\n-      const int32 d = Tin(i);\n-      OP_REQUIRES(context, 0 <= d && d < N,\n+      const int32 d = internal::SubtleMustCopy(Tin(i));\n+      OP_REQUIRES(context, FastBoundsCheck(d, N),\n                   errors::InvalidArgument(d, \" is not between 0 and \", N));\n       OP_REQUIRES(context, Tout(d) == -1,\n                   errors::InvalidArgument(d, \" is duplicated in the input.\"));\n"
                },
                {
                    "old_start": 107,
                    "old_length": 7,
                    "new_start": 108,
                    "new_length": 10,
                    "hunk_buggy": "['               errors::InvalidArgument(\\n', '                   \"transpose expects a vector of size \", input.dims(),\\n', '                   \". But input(1) is a vector of size \", Vperm.size()));\\n', '-  const int32* perm_begin = reinterpret_cast<const int32*>(Vperm.data());\\n', '   const std::vector<int32> permutation(perm_begin, perm_begin + dims);\\n', '   TensorShape shape;\\n', ' ']",
                    "hunk_fix": "@@ -107,7 +108,10 @@ void TransposeOp::Compute(OpKernelContext* ctx) {\n               errors::InvalidArgument(\n                   \"transpose expects a vector of size \", input.dims(),\n                   \". But input(1) is a vector of size \", Vperm.size()));\n-  const int32* perm_begin = reinterpret_cast<const int32*>(Vperm.data());\n+  // using volatile instead of SubtleMustCopy here so that the\n+  // asynchrony boundary is permutation.\n+  const volatile int32* perm_begin =\n+      reinterpret_cast<const volatile int32*>(Vperm.data());\n   const std::vector<int32> permutation(perm_begin, perm_begin + dims);\n   TensorShape shape;\n "
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
},
{
    "Id": 332,
    "LibraryName": "tensorflow",
    "commit_link": "https://github.com/tensorflow/tensorflow/commit/55aec0a33011773240f6696393952c984ca8de16",
    "date": "2016-03-08T17:20:28-08:00",
    "message": "Add explicit not-`None` checks for the height and width in `resize_images()`.\n\nThis was previously raising a `FutureWarning` when the height and/or width were dynamic.\nChange: 116699992",
    "changes": [
        {
            "name": "image_ops.py",
            "path": "tensorflow/python/ops/image_ops.py",
            "patches": [
                {
                    "old_start": 644,
                    "old_length": 7,
                    "new_start": 644,
                    "new_length": 8,
                    "hunk_buggy": "['   new_width_const = tensor_util.constant_value(new_width)\\n', '   new_height_const = tensor_util.constant_value(new_height)\\n', ' \\n', '-  if width == new_width_const and height == new_height_const:\\n', '     if not is_batch:\\n', '       images = array_ops.squeeze(images, squeeze_dims=[0])\\n', '     return images']",
                    "hunk_fix": "@@ -644,7 +644,8 @@ def resize_images(images,\n   new_width_const = tensor_util.constant_value(new_width)\n   new_height_const = tensor_util.constant_value(new_height)\n \n-  if width == new_width_const and height == new_height_const:\n+  if new_width_const is not None and new_height_const is not None and (\n+      width == new_width_const and height == new_height_const):\n     if not is_batch:\n       images = array_ops.squeeze(images, squeeze_dims=[0])\n     return images"
                }
            ],
            "whole_deleted": "",
            "whole_added": "",
            "whole_hunk": ""
        }
    ]
}]